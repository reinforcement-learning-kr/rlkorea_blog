{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/img/1.jpg","path":"img/1.jpg","modified":1,"renderable":0},{"_id":"source/img/Equation_Q_c51.png","path":"img/Equation_Q_c51.png","modified":1,"renderable":0},{"_id":"source/img/Equation_loss_c51.png","path":"img/Equation_loss_c51.png","modified":1,"renderable":0},{"_id":"source/img/Exp_ContinuousBandit.png","path":"img/Exp_ContinuousBandit.png","modified":1,"renderable":0},{"_id":"source/img/Equation_target1_c51.png","path":"img/Equation_target1_c51.png","modified":1,"renderable":0},{"_id":"source/img/Exp_OctopusArm.png","path":"img/Exp_OctopusArm.png","modified":1,"renderable":0},{"_id":"source/img/Fig2.png","path":"img/Fig2.png","modified":1,"renderable":0},{"_id":"source/img/HuberLoss.png","path":"img/HuberLoss.png","modified":1,"renderable":0},{"_id":"source/img/QR_cdf_abnormal.png","path":"img/QR_cdf_abnormal.png","modified":1,"renderable":0},{"_id":"source/img/QR_cdf_normal.png","path":"img/QR_cdf_normal.png","modified":1,"renderable":0},{"_id":"source/img/Quantile_Huber_Loss.png","path":"img/Quantile_Huber_Loss.png","modified":1,"renderable":0},{"_id":"source/img/Ref_Degris2012b_Theorem1.png","path":"img/Ref_Degris2012b_Theorem1.png","modified":1,"renderable":0},{"_id":"source/img/Quantile_Huber_Loss_final.png","path":"img/Quantile_Huber_Loss_final.png","modified":1,"renderable":0},{"_id":"source/img/bellman_equation.png","path":"img/bellman_equation.png","modified":1,"renderable":0},{"_id":"source/img/bellman_operation.png","path":"img/bellman_operation.png","modified":1,"renderable":0},{"_id":"source/img/bimodal_distribution.png","path":"img/bimodal_distribution.png","modified":1,"renderable":0},{"_id":"source/img/cdf.png","path":"img/cdf.png","modified":1,"renderable":0},{"_id":"source/img/cdf_function.png","path":"img/cdf_function.png","modified":1,"renderable":0},{"_id":"source/img/midpoint.png","path":"img/midpoint.png","modified":1,"renderable":0},{"_id":"source/img/mixure_policy.png","path":"img/mixure_policy.png","modified":1,"renderable":0},{"_id":"source/img/paper_c51.png","path":"img/paper_c51.png","modified":1,"renderable":0},{"_id":"source/img/projection2.png","path":"img/projection2.png","modified":1,"renderable":0},{"_id":"source/img/projection1.png","path":"img/projection1.png","modified":1,"renderable":0},{"_id":"source/img/qr_dqn_loss2.png","path":"img/qr_dqn_loss2.png","modified":1,"renderable":0},{"_id":"source/img/qr_dqn_loss3_1.png","path":"img/qr_dqn_loss3_1.png","modified":1,"renderable":0},{"_id":"source/img/qr_dqn_loss3_2.png","path":"img/qr_dqn_loss3_2.png","modified":1,"renderable":0},{"_id":"source/img/quantile.png","path":"img/quantile.png","modified":1,"renderable":0},{"_id":"source/img/quantile_regression.png","path":"img/quantile_regression.png","modified":1,"renderable":0},{"_id":"source/img/quantile_regression_loss.png","path":"img/quantile_regression_loss.png","modified":1,"renderable":0},{"_id":"source/img/tvd.png","path":"img/tvd.png","modified":1,"renderable":0},{"_id":"source/img/wasserstein.png","path":"img/wasserstein.png","modified":1,"renderable":0},{"_id":"source/img/Exp_ContinuousRL.png","path":"img/Exp_ContinuousRL.png","modified":1,"renderable":0},{"_id":"source/img/Exp_OctopusArm_Ref.png","path":"img/Exp_OctopusArm_Ref.png","modified":1,"renderable":0},{"_id":"source/img/Fig0.png","path":"img/Fig0.png","modified":1,"renderable":0},{"_id":"source/img/Ref_Degris2012b_offpolicygradient.png","path":"img/Ref_Degris2012b_offpolicygradient.png","modified":1,"renderable":0},{"_id":"source/img/cross_entropy_loss.png","path":"img/cross_entropy_loss.png","modified":1,"renderable":0},{"_id":"source/img/distributionalRL.png","path":"img/distributionalRL.png","modified":1,"renderable":0},{"_id":"source/img/gamma_contraction.png","path":"img/gamma_contraction.png","modified":1,"renderable":0},{"_id":"source/img/importance_sampling.png","path":"img/importance_sampling.png","modified":1,"renderable":0},{"_id":"source/img/env1.png","path":"img/env1.png","modified":1,"renderable":0},{"_id":"source/img/heuristic_approx.png","path":"img/heuristic_approx.png","modified":1,"renderable":0},{"_id":"source/img/kld.png","path":"img/kld.png","modified":1,"renderable":0},{"_id":"source/img/paper_qrdqn.png","path":"img/paper_qrdqn.png","modified":1,"renderable":0},{"_id":"source/img/projection3.png","path":"img/projection3.png","modified":1,"renderable":0},{"_id":"source/img/qr_dqn_loss2_1.png","path":"img/qr_dqn_loss2_1.png","modified":1,"renderable":0},{"_id":"source/img/qr_dqn_loss1.png","path":"img/qr_dqn_loss1.png","modified":1,"renderable":0},{"_id":"source/img/qr_dqn_loss2_2.png","path":"img/qr_dqn_loss2_2.png","modified":1,"renderable":0},{"_id":"source/img/qr_dqn_loss3.png","path":"img/qr_dqn_loss3.png","modified":1,"renderable":0},{"_id":"source/img/quantile_huber.png","path":"img/quantile_huber.png","modified":1,"renderable":0},{"_id":"source/img/qr_dqn_loss4.png","path":"img/qr_dqn_loss4.png","modified":1,"renderable":0},{"_id":"source/img/result_c51_2.png","path":"img/result_c51_2.png","modified":1,"renderable":0},{"_id":"source/img/sample-based.png","path":"img/sample-based.png","modified":1,"renderable":0},{"_id":"source/img/single.png","path":"img/single.png","modified":1,"renderable":0},{"_id":"source/img/sparse_good_result_c51.png","path":"img/sparse_good_result_c51.png","modified":1,"renderable":0},{"_id":"source/img/support_and_prob.png","path":"img/support_and_prob.png","modified":1,"renderable":0},{"_id":"source/img/surrogate.png","path":"img/surrogate.png","modified":1,"renderable":0},{"_id":"source/img/vine1.png","path":"img/vine1.png","modified":1,"renderable":0},{"_id":"source/img/wasserstein_graph.png","path":"img/wasserstein_graph.png","modified":1,"renderable":0},{"_id":"source/img/QR_DQN_Atari1.png","path":"img/QR_DQN_Atari1.png","modified":1,"renderable":0},{"_id":"source/img/c51_qrdqn.png","path":"img/c51_qrdqn.png","modified":1,"renderable":0},{"_id":"source/img/contraction.png","path":"img/contraction.png","modified":1,"renderable":0},{"_id":"source/img/irl/lets-do-irl-guide_1.png","path":"img/irl/lets-do-irl-guide_1.png","modified":1,"renderable":0},{"_id":"source/img/irl/linear_irl_4.png","path":"img/irl/linear_irl_4.png","modified":1,"renderable":0},{"_id":"source/img/irl/linear_irl_8.png","path":"img/irl/linear_irl_8.png","modified":1,"renderable":0},{"_id":"source/img/irl/mmp_2.png","path":"img/irl/mmp_2.png","modified":1,"renderable":0},{"_id":"source/img/irl/mmp_3.png","path":"img/irl/mmp_3.png","modified":1,"renderable":0},{"_id":"source/img/irl/mmp_4.png","path":"img/irl/mmp_4.png","modified":1,"renderable":0},{"_id":"source/img/irl/mmp_6.png","path":"img/irl/mmp_6.png","modified":1,"renderable":0},{"_id":"source/img/midpoint_graph.png","path":"img/midpoint_graph.png","modified":1,"renderable":0},{"_id":"themes/clean-blog/source/css/article.styl","path":"css/article.styl","modified":1,"renderable":1},{"_id":"themes/clean-blog/source/css/base.styl","path":"css/base.styl","modified":1,"renderable":1},{"_id":"themes/clean-blog/source/css/mixins.styl","path":"css/mixins.styl","modified":1,"renderable":1},{"_id":"themes/clean-blog/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"themes/clean-blog/source/css/variables.styl","path":"css/variables.styl","modified":1,"renderable":1},{"_id":"themes/clean-blog/source/img/about-bg.jpg","path":"img/about-bg.jpg","modified":1,"renderable":1},{"_id":"themes/clean-blog/source/img/favicon.ico","path":"img/favicon.ico","modified":1,"renderable":1},{"_id":"source/img/QR_DQN_Atari2.png","path":"img/QR_DQN_Atari2.png","modified":1,"renderable":0},{"_id":"source/img/QR_result_value.png","path":"img/QR_result_value.png","modified":1,"renderable":0},{"_id":"source/img/irl/gail_1.png","path":"img/irl/gail_1.png","modified":1,"renderable":0},{"_id":"source/img/irl/linear_irl_2.png","path":"img/irl/linear_irl_2.png","modified":1,"renderable":0},{"_id":"source/img/irl/linear_irl_3.png","path":"img/irl/linear_irl_3.png","modified":1,"renderable":0},{"_id":"source/img/irl/linear_irl_5.png","path":"img/irl/linear_irl_5.png","modified":1,"renderable":0},{"_id":"source/img/irl/linear_irl_6.png","path":"img/irl/linear_irl_6.png","modified":1,"renderable":0},{"_id":"source/img/irl/mmp_7.png","path":"img/irl/mmp_7.png","modified":1,"renderable":0},{"_id":"source/img/minimizer_lemma.png","path":"img/minimizer_lemma.png","modified":1,"renderable":0},{"_id":"source/img/qr_dqn_compare2.png","path":"img/qr_dqn_compare2.png","modified":1,"renderable":0},{"_id":"source/img/result_c51_3.png","path":"img/result_c51_3.png","modified":1,"renderable":0},{"_id":"source/img/state_visitation_change.png","path":"img/state_visitation_change.png","modified":1,"renderable":0},{"_id":"source/img/vine2.png","path":"img/vine2.png","modified":1,"renderable":0},{"_id":"source/img/QR-DQN_algorithm.png","path":"img/QR-DQN_algorithm.png","modified":1,"renderable":0},{"_id":"source/img/irl/linear_irl_1.png","path":"img/irl/linear_irl_1.png","modified":1,"renderable":0},{"_id":"source/img/irl/maxent_1.png","path":"img/irl/maxent_1.png","modified":1,"renderable":0},{"_id":"source/img/irl/mmp_1.png","path":"img/irl/mmp_1.png","modified":1,"renderable":0},{"_id":"source/img/irl/mmp_9.png","path":"img/irl/mmp_9.png","modified":1,"renderable":0},{"_id":"source/img/Fig1.png","path":"img/Fig1.png","modified":1,"renderable":0},{"_id":"source/img/irl/app_1.png","path":"img/irl/app_1.png","modified":1,"renderable":0},{"_id":"source/img/irl/linear_irl_10.png","path":"img/irl/linear_irl_10.png","modified":1,"renderable":0},{"_id":"source/img/irl/linear_irl_7.png","path":"img/irl/linear_irl_7.png","modified":1,"renderable":0},{"_id":"source/img/irl/linear_irl_9.png","path":"img/irl/linear_irl_9.png","modified":1,"renderable":0},{"_id":"source/img/irl/mmp_5.png","path":"img/irl/mmp_5.png","modified":1,"renderable":0},{"_id":"source/img/result_value_wasserstein.png","path":"img/result_value_wasserstein.png","modified":1,"renderable":0},{"_id":"source/img/QR_DQN_Atari3.png","path":"img/QR_DQN_Atari3.png","modified":1,"renderable":0},{"_id":"source/img/qr_dqn_compare.png","path":"img/qr_dqn_compare.png","modified":1,"renderable":0},{"_id":"themes/clean-blog/source/img/home-bg.jpg","path":"img/home-bg.jpg","modified":1,"renderable":1},{"_id":"source/img/irl/vail_1.png","path":"img/irl/vail_1.png","modified":1,"renderable":0},{"_id":"source/img/result_c51.png","path":"img/result_c51.png","modified":1,"renderable":0},{"_id":"themes/clean-blog/source/img/contact-bg.jpg","path":"img/contact-bg.jpg","modified":1,"renderable":1},{"_id":"source/img/irl/lets-do-irl-guide_2.png","path":"img/irl/lets-do-irl-guide_2.png","modified":1,"renderable":0},{"_id":"source/img/irl/mmp_12.png","path":"img/irl/mmp_12.png","modified":1,"renderable":0},{"_id":"source/img/policy_change.png","path":"img/policy_change.png","modified":1,"renderable":0},{"_id":"source/img/network_output.png","path":"img/network_output.png","modified":1,"renderable":0},{"_id":"source/img/algorithm_c51.png","path":"img/algorithm_c51.png","modified":1,"renderable":0},{"_id":"source/img/irl/mmp_10.png","path":"img/irl/mmp_10.png","modified":1,"renderable":0},{"_id":"source/img/irl/mmp_11.png","path":"img/irl/mmp_11.png","modified":1,"renderable":0},{"_id":"source/img/irl/mmp_8.png","path":"img/irl/mmp_8.png","modified":1,"renderable":0}],"Cache":[{"_id":"themes/clean-blog/LICENSE","hash":"34ac5e147727699d1bbc346b014f2e0bdfbb0947","modified":1549538492436},{"_id":"themes/clean-blog/_config.yml","hash":"b1ca780b469c8333a295a15d390e6ad6c1179550","modified":1549538492443},{"_id":"themes/clean-blog/README.md","hash":"779e999931169acdbabf43ab0e70471e8631583a","modified":1549538492439},{"_id":"source/_posts/0_lets-do-irl-guide.md","hash":"59517a6a49d1d618c31e1d82c7fc957d9a8039d8","modified":1549538491904},{"_id":"source/_posts/0_pg-travel-guide.md","hash":"b9a9e108a8a32ac374233fc0cc4eaed1bea78d8f","modified":1549538491912},{"_id":"source/_posts/1_linear-irl.md","hash":"4d43e813071ce2a491a394799e565a524c62d87e","modified":1549538491916},{"_id":"source/_posts/2_app.md","hash":"7d8d7ebc99706c62cb31b3e76890f71e4f2fe351","modified":1549538815963},{"_id":"source/_posts/1_sutton-pg.md","hash":"85375a216e9809f3c9e13ada96c571fb974b4830","modified":1549538491920},{"_id":"source/_posts/2_dpg.md","hash":"68f871f041ba8a2731df6171f62fc31dbd64fc2e","modified":1549538491930},{"_id":"source/_posts/3_ddpg.md","hash":"2063415ffeb79d441c812a24cd68ba0e8ebf9bf4","modified":1549538491934},{"_id":"source/_posts/3_mmp.md","hash":"ad29ef06e4a097023d87bb3b94a05578db533b6b","modified":1549538491938},{"_id":"source/_posts/4_npg.md","hash":"a6f81c58ea59c7677f4ba6b49d7e4b185e05a39f","modified":1549538491947},{"_id":"source/_posts/4_maxent.md","hash":"cb446d7cdbee03f58140c874f6471946a6280223","modified":1549538491943},{"_id":"source/_posts/5_gail.md","hash":"64e348d16e7af752881037ef2895aa9840dcf731","modified":1549538491950},{"_id":"source/_posts/5_trpo.md","hash":"037229f004ec751dd78ca9188522c030c37c03fa","modified":1549538491958},{"_id":"source/_posts/6_vail.md","hash":"056618edb744131ef3f62a98142f1e44d21c87a3","modified":1549538491967},{"_id":"source/_posts/6_gae.md","hash":"4aa719c604c6e6855b78093571a4067b7e0dbe93","modified":1549538491963},{"_id":"source/_posts/7_ppo.md","hash":"3de4dfc6a7d982b36bdb20d9cb6ad4cd37f57e66","modified":1549538491972},{"_id":"source/_posts/8_implement.md","hash":"add93d94fb742cf8820b2deda4a571c268ccb13e","modified":1549538491978},{"_id":"source/_posts/C51.md","hash":"9f6ed2dfd43b4d1f3963b8f381a9cffae60e9ec5","modified":1549538491983},{"_id":"source/_posts/Distributional_intro.md","hash":"a960b95af178b2f7c207204a808a46b1302e9481","modified":1549538491987},{"_id":"source/_posts/QR-DQN.md","hash":"a5547b978cda262c53b15e9c39220b18880710ab","modified":1549538491998},{"_id":"source/_posts/IQN.md","hash":"eaad8fd729fbef1722bd29bebccf2ecdf5768a83","modified":1549538491993},{"_id":"source/_posts/robot_arm_intro.md","hash":"da05043437746bcb931796abc7a1cce296199bd8","modified":1549538492003},{"_id":"source/img/1.jpg","hash":"179e50805da05b9a2417192bff85de448f5de998","modified":1549538492012},{"_id":"source/img/Equation_Q_c51.png","hash":"95ca3928b11f044a8002d39f1f565d0fba59f004","modified":1549538492018},{"_id":"source/img/Equation_loss_c51.png","hash":"858002ed96c174ae4db2fd0362fd3eaec4010e57","modified":1549538492021},{"_id":"source/img/Exp_ContinuousBandit.png","hash":"72eddde3b296070075706688038f1267bfd04d31","modified":1549538492030},{"_id":"source/img/Equation_target1_c51.png","hash":"1a2feed93141d5df901c066169ee6f4ec0350e65","modified":1549538492025},{"_id":"source/img/Exp_OctopusArm.png","hash":"f0ae59d0c2a6600ef961fd0b2ea8903dcfdbd4d9","modified":1549538492038},{"_id":"source/img/Fig2.png","hash":"78c0a43eca5bc2f98e2d1301b0e14f8b84636bcc","modified":1549538492055},{"_id":"source/img/HuberLoss.png","hash":"19149c4b97a245fab1b81c49d6551f1b091d4742","modified":1549538492060},{"_id":"source/img/QR_cdf_abnormal.png","hash":"3c33016fa37441df4d2e2eb3c323c8ca340fe9d0","modified":1549538492086},{"_id":"source/img/QR_cdf_normal.png","hash":"bde40482716ea08c0034330c9ad28bdcd2a778cf","modified":1549538492089},{"_id":"source/img/Quantile_Huber_Loss.png","hash":"a05f9ca34f625372a36f8698ffb73dcf9bd1577c","modified":1549538492097},{"_id":"source/img/Ref_Degris2012b_Theorem1.png","hash":"154f42239538c3dd0508e815014d84685e18f89b","modified":1549538492103},{"_id":"source/img/Quantile_Huber_Loss_final.png","hash":"624e33cc2e28dfdd6c82af6ede3987472af5522b","modified":1549538492100},{"_id":"source/img/bellman_equation.png","hash":"ccf5ba8e8a0e2d9c1a21b555262eeb3de5ba8cfc","modified":1549538492115},{"_id":"source/img/bellman_operation.png","hash":"866e1e7498fddf80c55f0ce92cb0ed63f4649945","modified":1549538492118},{"_id":"source/img/bimodal_distribution.png","hash":"3ebe49f9b7b2107bc875434f8f0e98c58035b937","modified":1549538492121},{"_id":"source/img/cdf.png","hash":"3794f2b52f3640cddf43833f208ba39713cace4f","modified":1549538492129},{"_id":"source/img/cdf_function.png","hash":"b3e6692c22e12aa74213a279f25e6d17d94b3c9e","modified":1549538492132},{"_id":"source/img/midpoint.png","hash":"3ac89bb7c3b44392873f8c9e78435da9ad51954f","modified":1549538492272},{"_id":"source/img/mixure_policy.png","hash":"a29de731fa31bb01bfe2d32714d2c330b62ebf1c","modified":1549538492283},{"_id":"source/img/paper_c51.png","hash":"c8e5ef7598aff278595969c1113cca1782af195b","modified":1549538492293},{"_id":"source/img/projection2.png","hash":"4f1148ff5733c1ea02fbeb8e24001219c828b3f6","modified":1549538492309},{"_id":"source/img/projection1.png","hash":"ce32db1ae264da86e9605d904939607ca1ec5a7d","modified":1549538492304},{"_id":"source/img/qr_dqn_loss2.png","hash":"1fb0982cf379e670310445d631f0457c0176bb1f","modified":1549538492333},{"_id":"source/img/qr_dqn_loss3_1.png","hash":"2f1507d4a2fc6a5c1b53eb8df0f5361957252e42","modified":1549538492348},{"_id":"source/img/qr_dqn_loss3_2.png","hash":"8fabf760bd70d0dfa8aea0857a6b8ce2e9153e5c","modified":1549538492351},{"_id":"source/img/quantile.png","hash":"c82f40ad86c03675fcef38a8bf5451eba57e3310","modified":1549538492357},{"_id":"source/img/quantile_regression.png","hash":"85602ed7106c5357ab147de92a78353ddd998c30","modified":1549538492364},{"_id":"source/img/quantile_regression_loss.png","hash":"a3341301f906bac52099abc21d5a46be0079b1c3","modified":1549538492368},{"_id":"source/img/tvd.png","hash":"48832e9dfdc030c0eec69f4eb3a6df6f79e443b2","modified":1549538492415},{"_id":"source/img/wasserstein.png","hash":"e6536f48f45d0ef30f940163199930f58a27bc6e","modified":1549538492427},{"_id":"themes/clean-blog/languages/de.yml","hash":"02a98ba2b93e30a00ae7979fbe90b767a27290f0","modified":1549538492446},{"_id":"themes/clean-blog/languages/default.yml","hash":"3cd0873b310cbf2fe022ee18d55a6113b347ea09","modified":1549538492449},{"_id":"themes/clean-blog/languages/en.yml","hash":"3cd0873b310cbf2fe022ee18d55a6113b347ea09","modified":1549538492452},{"_id":"themes/clean-blog/languages/es.yml","hash":"fb089145368422ac47da9eb00fed05b15c904aa2","modified":1549538492454},{"_id":"themes/clean-blog/languages/no.yml","hash":"5ce3a1043ff85cecf83f3b5b0cdad2df44fa0192","modified":1549538492460},{"_id":"themes/clean-blog/languages/fr.yml","hash":"10e3529b8492d7a7601d5b35b44d8fc9e8ea8d1b","modified":1549538492457},{"_id":"themes/clean-blog/languages/pl.yml","hash":"6dc5d1b2aa75ae4c527089a770f43bafb91d80f4","modified":1549538492463},{"_id":"themes/clean-blog/languages/ru.yml","hash":"2cfaf93704ea4ac3f374c69bab89ca31916faa33","modified":1549538492468},{"_id":"themes/clean-blog/languages/pt.yml","hash":"6a31d548092af8af9f25d859063b0589c23ce13a","modified":1549538492466},{"_id":"themes/clean-blog/languages/zh-CN.yml","hash":"6d712d9eb6ba12213dcd76b532cd86e9da83cfa3","modified":1549538492470},{"_id":"themes/clean-blog/languages/zh-TW.yml","hash":"45c84384a05fdb7e32a3e2d498ea180be7dccfa9","modified":1549538492472},{"_id":"themes/clean-blog/layout/archive.ejs","hash":"c3aa4a76ee8b59b0e12ddbe951a9852176058eac","modified":1549538492548},{"_id":"themes/clean-blog/layout/index.ejs","hash":"50701592f70be267e4a6adf44b9cba78610682de","modified":1549538492552},{"_id":"themes/clean-blog/layout/layout.ejs","hash":"acc791dc4346c135ce4cac3cdeba6f96708115e5","modified":1549538492554},{"_id":"themes/clean-blog/layout/page.ejs","hash":"b585761947e289f0e37380780cfdebee0674c378","modified":1549538492560},{"_id":"themes/clean-blog/layout/post.ejs","hash":"38382e9bbeb6b8d2eafbd53fff2984111f524c1a","modified":1549538492562},{"_id":"source/img/Exp_ContinuousRL.png","hash":"85b9ebfb164631804d2fb6be0164e49cd1db746b","modified":1549538492035},{"_id":"source/img/Exp_OctopusArm_Ref.png","hash":"ebf69a16fc09e447442808f8cc46d770bed0cf10","modified":1549538492043},{"_id":"source/img/Fig0.png","hash":"5ee6f9f865ef39b89400ce8f265d158bacb6074a","modified":1549538492047},{"_id":"source/img/Ref_Degris2012b_offpolicygradient.png","hash":"46f0f17392ff40927579899fd10bdaf6bf562ebd","modified":1549538492106},{"_id":"source/img/cross_entropy_loss.png","hash":"d322d19c60a7feae9b5ad9ab30e8c0a44a938976","modified":1549538492139},{"_id":"source/img/distributionalRL.png","hash":"5ee6f9f865ef39b89400ce8f265d158bacb6074a","modified":1549538492143},{"_id":"source/img/gamma_contraction.png","hash":"b261cd2365e7c82a4875069af64c8b847fe8ef4c","modified":1549538492150},{"_id":"source/img/importance_sampling.png","hash":"557cb910e6cbaa52570749c971f30a9ac99f5b90","modified":1549538492157},{"_id":"source/img/env1.png","hash":"f1a5ffede27ccebb15134ea53e0089558e2c2bdf","modified":1549538492147},{"_id":"source/img/heuristic_approx.png","hash":"5f2474685413fe5220cabe92f8efd55541c19654","modified":1549538492154},{"_id":"source/img/kld.png","hash":"ec5b1eebc409fbe8b5a2a1d6ab8cdbb921b39d42","modified":1549538492269},{"_id":"source/img/paper_qrdqn.png","hash":"2dbc23cec5507125d3b9549423dd8122f63fbf4d","modified":1549538492296},{"_id":"source/img/projection3.png","hash":"a0e4be50cc4aab5d5c1cd5fe4f7c9621e31f8644","modified":1549538492313},{"_id":"source/img/qr_dqn_loss2_1.png","hash":"208242a7562c1501b0a7fc580986cef205760755","modified":1549538492336},{"_id":"source/img/qr_dqn_loss1.png","hash":"4213a73d4b179485f7584f6d4589d373686f2a50","modified":1549538492329},{"_id":"source/img/qr_dqn_loss2_2.png","hash":"af2e8f27782f8cfc2679ccc1e889aeab2e83b119","modified":1549538492339},{"_id":"source/img/qr_dqn_loss3.png","hash":"e5818f437d85a97e9c1d8ab1c2fac56890914ac3","modified":1549538492344},{"_id":"source/img/quantile_huber.png","hash":"9e6c2dcda4877919570a20db7ebd6bc282a11d32","modified":1549538492362},{"_id":"source/img/qr_dqn_loss4.png","hash":"551f0577f526eab7dbb8e911930400027e22f95f","modified":1549538492354},{"_id":"source/img/result_c51_2.png","hash":"5b021b3579b5d56eb94453b4cee467df379600ac","modified":1549538492378},{"_id":"source/img/sample-based.png","hash":"c4b79bd8d64eaa2ce36fb0a3ee0981b5455cce30","modified":1549538492389},{"_id":"source/img/single.png","hash":"52a63dd842c7448ebf20fa53d5485bd0364250b9","modified":1549538492393},{"_id":"source/img/sparse_good_result_c51.png","hash":"3d3502a4d8a9901b66873ec80929ad902d9f4072","modified":1549538492396},{"_id":"source/img/support_and_prob.png","hash":"96cd7222c77900e786988d7f68d1bb599522e1aa","modified":1549538492405},{"_id":"source/img/surrogate.png","hash":"a0f501e64f45b6edde6282688416c513a27b6dff","modified":1549538492409},{"_id":"source/img/vine1.png","hash":"acea47e2739665b82e467660d2f1e382ef0307b6","modified":1549538492419},{"_id":"source/img/wasserstein_graph.png","hash":"bf7880290a2ca1deef68809322b0c8a92513561a","modified":1549538492431},{"_id":"source/img/QR_DQN_Atari1.png","hash":"a90527d9c6aa056845689812491d9c64d0d35e93","modified":1549538492070},{"_id":"source/img/c51_qrdqn.png","hash":"3b9b688501ce16e8c390e7ef3935c1bd956ccaff","modified":1549538492124},{"_id":"source/img/contraction.png","hash":"a14befe6131db21051ca4a4967172c50b6aef2c2","modified":1549538492136},{"_id":"source/img/irl/lets-do-irl-guide_1.png","hash":"81729d384ad101727c76fd9e6e8e777fa9b0410d","modified":1549538492169},{"_id":"source/img/irl/linear_irl_4.png","hash":"b10422a88093a2f47dba1e0a423f5380c25652e5","modified":1549538492192},{"_id":"source/img/irl/linear_irl_8.png","hash":"0bc6c1fb2687199be1baec45d4e76a8e978cfdb7","modified":1549538492208},{"_id":"source/img/irl/mmp_2.png","hash":"624787539274ff84b81b31860d0dc50187d7c0e0","modified":1549538492239},{"_id":"source/img/irl/mmp_3.png","hash":"af0dfb8aaf2b2ee44f0e4c46a3dd6c76d6df235b","modified":1549538492242},{"_id":"source/img/irl/mmp_4.png","hash":"a20c01826d914e7d75543164dc9dd38e0af14e92","modified":1549538492245},{"_id":"source/img/irl/mmp_6.png","hash":"70c0fc92d18ae18609f77a9270b7446fb4b79ed0","modified":1549538492251},{"_id":"source/img/midpoint_graph.png","hash":"e3d94e6caa0f3ac1eeb95374b4756c3a3d0b570d","modified":1549538492276},{"_id":"themes/clean-blog/layout/_partial/after-footer.ejs","hash":"036660a8a8079ae9fb956de9836595801817ca08","modified":1549538492480},{"_id":"themes/clean-blog/layout/_partial/article-archive.ejs","hash":"75b9b24c226eda4884752a868177b590d3e8b06f","modified":1549538492484},{"_id":"themes/clean-blog/layout/_partial/article-categories.ejs","hash":"b494e4a50f63d66e545da449af18a7198a057bd6","modified":1549538492488},{"_id":"themes/clean-blog/layout/_partial/article-full.ejs","hash":"9ad684a9a129504ba39b8435309d780873cfe4d6","modified":1549538492494},{"_id":"themes/clean-blog/layout/_partial/article-index.ejs","hash":"ad112adf312337c60d436fdd127e948fe81b3dd1","modified":1549538492498},{"_id":"themes/clean-blog/layout/_partial/article-tags.ejs","hash":"12524df0c1ce5136a8f88cea2fe550f9e1b47b19","modified":1549538492502},{"_id":"themes/clean-blog/layout/_partial/comments.ejs","hash":"5cafed11b6cfbf22e83674676c0edc6c4420cc0d","modified":1549538492507},{"_id":"themes/clean-blog/layout/_partial/footer.ejs","hash":"9276f30f22d1743639fbbc8618395403b45bb01d","modified":1549538492513},{"_id":"themes/clean-blog/layout/_partial/gallery.ejs","hash":"15e9562d0f6146e25e22856693d1312cd3ade4af","modified":1549538492516},{"_id":"themes/clean-blog/layout/_partial/google-analytics.ejs","hash":"b287b7f66a53b51c7cd872ad1b15dfd20fb3e35d","modified":1549538492522},{"_id":"themes/clean-blog/layout/_partial/head.ejs","hash":"892cb26678be948aa4ab505b9c876bf6efc1bb51","modified":1549538492529},{"_id":"themes/clean-blog/layout/_partial/menu.ejs","hash":"f71c49d7d35cc8b6f3ca1177f3213de5353c43c7","modified":1549538492533},{"_id":"themes/clean-blog/layout/_partial/menu_origin.ejs","hash":"8da0a2915736daedb48887a221da9ce7ccabc4fc","modified":1549538492537},{"_id":"themes/clean-blog/layout/_partial/pagination.ejs","hash":"cd61e4dbbf6020ad094c8e66ec06e8c38ebcd122","modified":1549538492540},{"_id":"themes/clean-blog/layout/_partial/tag-category-index.ejs","hash":"008b4ed0b6fd6dc81bc0655ccc46e43eb310706b","modified":1549538492543},{"_id":"themes/clean-blog/source/css/article.styl","hash":"768418ecaa2ff17f6fa81d72096627e05579fe93","modified":1549538492567},{"_id":"themes/clean-blog/source/css/base.styl","hash":"eaca9824b69e3a924596082d0658b789d22a16ea","modified":1549538492570},{"_id":"themes/clean-blog/source/css/mixins.styl","hash":"a88190320a376bf1d00ca2e5f2043fec6803b27c","modified":1549538492573},{"_id":"themes/clean-blog/source/css/style.styl","hash":"7200d572751c1f5888b3d0df25bf503a3f31ca2d","modified":1549538492577},{"_id":"themes/clean-blog/source/css/variables.styl","hash":"06e16f64020cbddf2c2c49ef45f940141a1b2ada","modified":1549538492580},{"_id":"themes/clean-blog/source/img/about-bg.jpg","hash":"d39126a6456f2bac0169d1779304725f179c9900","modified":1549538492584},{"_id":"themes/clean-blog/source/img/favicon.ico","hash":"3412e0d657aa5a6cfbbfcf4ef398572c24035565","modified":1549538492590},{"_id":"source/img/QR_DQN_Atari2.png","hash":"c9e7daa0c6cdcb957555c7bf81b4c38c40aae1bc","modified":1549538492075},{"_id":"source/img/QR_result_value.png","hash":"981c958a49d9cb1b381ab1d4cac341a80de4339b","modified":1549538492094},{"_id":"source/img/irl/gail_1.png","hash":"f5c1538cfbcd4a75183a850e903b8f06cb421d59","modified":1549538492166},{"_id":"source/img/irl/linear_irl_2.png","hash":"c4e474976fb7e17feeab8ee85744c95281161ae6","modified":1549538492185},{"_id":"source/img/irl/linear_irl_3.png","hash":"33b26e16a2a8f41d4c8e0d84d19cfb36ba487dbe","modified":1549538492188},{"_id":"source/img/irl/linear_irl_5.png","hash":"5d55ef6a3d969afad1491559f63b3caea5599a8d","modified":1549538492197},{"_id":"source/img/irl/linear_irl_6.png","hash":"4cc4a3823e931b32202eb92036c4118e4a80c5e2","modified":1549538492200},{"_id":"source/img/irl/mmp_7.png","hash":"30c107270f057314a34f17c61221a9842aaea746","modified":1549538492253},{"_id":"source/img/minimizer_lemma.png","hash":"53253d113dfb80ce1ef3064aeb259cbca762aae7","modified":1549538492280},{"_id":"source/img/qr_dqn_compare2.png","hash":"2c362493d52d37df34abd5952f9a6b3c2f8cf9b7","modified":1549538492324},{"_id":"source/img/result_c51_3.png","hash":"2ed64c8c79edd126679a90d7a06b02bc3a7da3a2","modified":1549538492382},{"_id":"source/img/state_visitation_change.png","hash":"81c9b7879e5cb69e65519415b5ba72dbf09c19ee","modified":1549538492401},{"_id":"source/img/vine2.png","hash":"80103e41a869da947827726b5719bb30a33de414","modified":1549538492423},{"_id":"source/img/QR-DQN_algorithm.png","hash":"597c49c068b852fab28f5f046fa2db380a209d05","modified":1549538492066},{"_id":"source/img/irl/linear_irl_1.png","hash":"560fcc9b5c2198a07811af710e13b45d0f23dcf7","modified":1549538492178},{"_id":"source/img/irl/maxent_1.png","hash":"c30cf96b389b4723b7cedf46286b61d5bb59ce5a","modified":1549538492218},{"_id":"source/img/irl/mmp_1.png","hash":"d96a3e5b859905edd988c67ffa5f2fd12204718a","modified":1549538492221},{"_id":"source/img/irl/mmp_9.png","hash":"b1946dbed17848fe6ca40ab95c49c2f2cce9b919","modified":1549538492263},{"_id":"source/img/Fig1.png","hash":"b8ca2161021f8be14db26a2d0fa261ab8239306a","modified":1549538492052},{"_id":"source/img/irl/app_1.png","hash":"13ce8071bd278c9b776da75f2df7f39ca271ef89","modified":1549538492162},{"_id":"source/img/irl/linear_irl_10.png","hash":"df91955c7f5e4cf8ea9eeab496348f31b6818ae6","modified":1549538492182},{"_id":"source/img/irl/linear_irl_7.png","hash":"609584ba619fcc087e18b21fd417c8efbb7d25e7","modified":1549538492204},{"_id":"source/img/irl/linear_irl_9.png","hash":"77f642f64ba418f829ccde9f2db1c3408cc474b1","modified":1549538492215},{"_id":"source/img/irl/mmp_5.png","hash":"295dd9a1836394aa89c7fbda042b676c3b2cb005","modified":1549538492249},{"_id":"source/img/result_value_wasserstein.png","hash":"efe2efd6103a4a7dfc305fa5df4fbe600a372395","modified":1549538492386},{"_id":"source/img/QR_DQN_Atari3.png","hash":"f5f4d9ab9a58aaea0c5b7597f40e61bcd8899264","modified":1549538492082},{"_id":"source/img/qr_dqn_compare.png","hash":"2cff28c2f2471e8ecda714ea13f69263ec2bae77","modified":1549538492319},{"_id":"themes/clean-blog/source/img/home-bg.jpg","hash":"990f6f9dd0ecb5348bfcc47305553d58c0d8f326","modified":1549538492596},{"_id":"source/img/irl/vail_1.png","hash":"489fd057c80e76c85283b364800d2d3d7267b472","modified":1549538492266},{"_id":"source/img/result_c51.png","hash":"6bd0147ed4471c0280cc975177adac17717069c0","modified":1549538492374},{"_id":"themes/clean-blog/source/img/contact-bg.jpg","hash":"6af63305c923899017e727b5ca968a2703bc08cf","modified":1549538492588},{"_id":"source/img/irl/lets-do-irl-guide_2.png","hash":"673a4940a9d033a622bd3e1dcf626c105d402a57","modified":1549538492173},{"_id":"source/img/irl/mmp_12.png","hash":"e3d4e60c2b34fbe8106651559d05f46396e72cc4","modified":1549538492237},{"_id":"source/img/policy_change.png","hash":"e1fd4af3c5e8236d14eed042b9eec7358479626d","modified":1549538492301},{"_id":"source/img/network_output.png","hash":"ce9b87195afad9c4d9d9dbcc5652e0e4e41e6203","modified":1549538492289},{"_id":"source/img/algorithm_c51.png","hash":"f5a68401b52ec133970e5cb08e8100467b28b0c5","modified":1549538492112},{"_id":"source/img/irl/mmp_10.png","hash":"70c0f9fb4359aeff6daa01b2b65fd3eea6237291","modified":1549538492228},{"_id":"source/img/irl/mmp_11.png","hash":"890216b48aa95d6446b7deef17a6eee88ff4acd3","modified":1549538492232},{"_id":"source/img/irl/mmp_8.png","hash":"230fc7ba8ef62a198fb9f40e89e892499f0c5b0e","modified":1549538492260}],"Category":[{"name":"프로젝트","_id":"cjrujltym00025wfeykwmiki4"},{"name":"논문 정리","_id":"cjrujltzs000o5wfe2dkxtr3c"}],"Data":[],"Page":[],"Post":[{"title":"Let's do Inverse RL Guide","date":"2019-01-21T15:00:00.000Z","author":"이동민, 이승현","subtitle":"Let's do Inverse RL Guide","_content":"\n---\n\n# 0. Inverse RL의 세계로\n\n반갑습니다! 저희는 Inverse RL을 흐름을 살펴보기 위해 모인 IRL 프로젝트 팀입니다.\n\n강화학습에서 reward라는 요소는 굉장히 중요합니다. 왜냐하면 agent라는 아이가 유일하게 학습할 수 있는 요소이기 때문입니다. 일반적으로 강화학습에서는 사람이 reward를 일일히 정해주지만, 실제로 그 reward에 따라 \"desirable\"  action이 나오지 않을 수도 있습니다. 여기서 생각해볼 수 있는 것이 바로 \"expert\"의 행동을 통해 reward를 찾는 것입니다.\n\n저희는 Andrew Ng의 논문인 Linear IRL과 Pieter Abbeel의 논문인 APP를 필두로 하여 MMP, MaxEnt, 그리고 보통 IRL을 통해 얻어진 reward로 다시 RL을 풀어서 policy를 얻어야하지만, 이 과정을 한번에 풀어버리는 GAIL, 최근 들어 GAIL을 뛰어넘는 VAIL까지 살펴보고자 합니다.\n\n<center> <img src=\"../../../../img/irl/lets-do-irl-guide_2.png\" width=\"900\"> </center>\n\n논문의 순서는 다음과 같습니다.\n\n1. [Linear_IRL](http://ai.stanford.edu/~ang/papers/icml00-irl.pdf)\n2. [APP](http://people.eecs.berkeley.edu/~russell/classes/cs294/s11/readings/Abbeel+Ng:2004.pdf)\n3. [MMP](https://www.ri.cmu.edu/pub_files/pub4/ratliff_nathan_2006_1/ratliff_nathan_2006_1.pdf)\n4. [MaxEnt](http://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf)\n5. [GAIL](https://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf)\n6. [VAIL](https://arxiv.org/pdf/1810.00821.pdf)\n\n위와 같이 총 6가지 논문들을 리뷰하여 블로그로 정리하였습니다. 각 순서에 맞춰 보시는 것을 권장해드립니다.\n\n<br><br>\n\n# 1. \\[Linear_IRL\\] Algorithms for Inverse Reinforcement Learning\n\n[Linear_IRL 여행하기](https://reinforcement-learning-kr.github.io/2019/01/28/1_linear-irl/)\n\n짧은 글 소개, 요약\n\n[Linear_IRL 여행하기](https://reinforcement-learning-kr.github.io/2019/01/28/1_linear-irl/)\n\n<br><br>\n\n# 2. \\[APP\\] Apprenticeship Learning via Inverse Reinforcement Learning\n\n[APP 여행하기](https://reinforcement-learning-kr.github.io/2019/02/01/2_app/)\n[APP Code]()\n\n짧은 글 소개, 요약\n\n[APP 여행하기](https://reinforcement-learning-kr.github.io/2019/02/01/2_app/)\n[APP Code]()\n\n<br><br>\n\n# 3. \\[MMP\\] Maximum Margin Planning\n\n[MMP 여행하기]()\n\n짧은 글 소개, 요약\n\n[MMP 여행하기]()\n\n<br><br>\n\n# 4. \\[MaxEnt\\] Maximum Entropy Inverse Reinforcement Learning\n\n[MaxEnt 여행하기]()\n[MaxEnt Code]()\n\n짧은 글 소개, 요약\n\n[MaxEnt 여행하기]()\n[MaxEnt Code]()\n\n<br><br>\n\n# 5. \\[GAIL\\] Generative Adversarial Imitation Learning\n\n[GAIL 여행하기]()\n[GAIL Code]()\n\n짧은 글 소개, 요약\n\n[GAIL 여행하기]()\n[GAIL Code]()\n\n<br><br>\n\n# 6. \\[VAIL\\] Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow\n\n[VAIL 여행하기]()\n[VAIL Code]()\n\n짧은 글 소개, 요약\n\n[VAIL 여행하기]()\n[VAIL Code]()\n\n<br><br>\n\n# Team\n\n이동민 : [Github](https://github.com/dongminleeai), [Facebook](https://www.facebook.com/dongminleeai)\n\n윤승제 : [Github](https://github.com/sjYoondeltar), [Facebook](https://www.facebook.com/seungje.yoon)\n\n이승현 : [Github](https://github.com/Clyde21c), [Facebook](https://www.facebook.com/Clyde21c)\n\n이건희 : [Github](https://github.com/Geonhee-LEE), [Facebook](https://www.facebook.com/Geonheeee)\n\n김준태 : [Github](https://github.com/OPAYA), [Facebook](https://www.facebook.com/kjt7889)\n\n김예찬 : [Github](https://github.com/suhoy901), [Facebook](https://www.facebook.com/suhoy90)\n","source":"_posts/0_lets-do-irl-guide.md","raw":"---\ntitle: Let's do Inverse RL Guide\ndate: 2019-01-22\ntags: [\"프로젝트\", \"GAIL하자!\"]\ncategories: 프로젝트\nauthor: 이동민, 이승현\nsubtitle: Let's do Inverse RL Guide\n---\n\n---\n\n# 0. Inverse RL의 세계로\n\n반갑습니다! 저희는 Inverse RL을 흐름을 살펴보기 위해 모인 IRL 프로젝트 팀입니다.\n\n강화학습에서 reward라는 요소는 굉장히 중요합니다. 왜냐하면 agent라는 아이가 유일하게 학습할 수 있는 요소이기 때문입니다. 일반적으로 강화학습에서는 사람이 reward를 일일히 정해주지만, 실제로 그 reward에 따라 \"desirable\"  action이 나오지 않을 수도 있습니다. 여기서 생각해볼 수 있는 것이 바로 \"expert\"의 행동을 통해 reward를 찾는 것입니다.\n\n저희는 Andrew Ng의 논문인 Linear IRL과 Pieter Abbeel의 논문인 APP를 필두로 하여 MMP, MaxEnt, 그리고 보통 IRL을 통해 얻어진 reward로 다시 RL을 풀어서 policy를 얻어야하지만, 이 과정을 한번에 풀어버리는 GAIL, 최근 들어 GAIL을 뛰어넘는 VAIL까지 살펴보고자 합니다.\n\n<center> <img src=\"../../../../img/irl/lets-do-irl-guide_2.png\" width=\"900\"> </center>\n\n논문의 순서는 다음과 같습니다.\n\n1. [Linear_IRL](http://ai.stanford.edu/~ang/papers/icml00-irl.pdf)\n2. [APP](http://people.eecs.berkeley.edu/~russell/classes/cs294/s11/readings/Abbeel+Ng:2004.pdf)\n3. [MMP](https://www.ri.cmu.edu/pub_files/pub4/ratliff_nathan_2006_1/ratliff_nathan_2006_1.pdf)\n4. [MaxEnt](http://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf)\n5. [GAIL](https://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf)\n6. [VAIL](https://arxiv.org/pdf/1810.00821.pdf)\n\n위와 같이 총 6가지 논문들을 리뷰하여 블로그로 정리하였습니다. 각 순서에 맞춰 보시는 것을 권장해드립니다.\n\n<br><br>\n\n# 1. \\[Linear_IRL\\] Algorithms for Inverse Reinforcement Learning\n\n[Linear_IRL 여행하기](https://reinforcement-learning-kr.github.io/2019/01/28/1_linear-irl/)\n\n짧은 글 소개, 요약\n\n[Linear_IRL 여행하기](https://reinforcement-learning-kr.github.io/2019/01/28/1_linear-irl/)\n\n<br><br>\n\n# 2. \\[APP\\] Apprenticeship Learning via Inverse Reinforcement Learning\n\n[APP 여행하기](https://reinforcement-learning-kr.github.io/2019/02/01/2_app/)\n[APP Code]()\n\n짧은 글 소개, 요약\n\n[APP 여행하기](https://reinforcement-learning-kr.github.io/2019/02/01/2_app/)\n[APP Code]()\n\n<br><br>\n\n# 3. \\[MMP\\] Maximum Margin Planning\n\n[MMP 여행하기]()\n\n짧은 글 소개, 요약\n\n[MMP 여행하기]()\n\n<br><br>\n\n# 4. \\[MaxEnt\\] Maximum Entropy Inverse Reinforcement Learning\n\n[MaxEnt 여행하기]()\n[MaxEnt Code]()\n\n짧은 글 소개, 요약\n\n[MaxEnt 여행하기]()\n[MaxEnt Code]()\n\n<br><br>\n\n# 5. \\[GAIL\\] Generative Adversarial Imitation Learning\n\n[GAIL 여행하기]()\n[GAIL Code]()\n\n짧은 글 소개, 요약\n\n[GAIL 여행하기]()\n[GAIL Code]()\n\n<br><br>\n\n# 6. \\[VAIL\\] Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow\n\n[VAIL 여행하기]()\n[VAIL Code]()\n\n짧은 글 소개, 요약\n\n[VAIL 여행하기]()\n[VAIL Code]()\n\n<br><br>\n\n# Team\n\n이동민 : [Github](https://github.com/dongminleeai), [Facebook](https://www.facebook.com/dongminleeai)\n\n윤승제 : [Github](https://github.com/sjYoondeltar), [Facebook](https://www.facebook.com/seungje.yoon)\n\n이승현 : [Github](https://github.com/Clyde21c), [Facebook](https://www.facebook.com/Clyde21c)\n\n이건희 : [Github](https://github.com/Geonhee-LEE), [Facebook](https://www.facebook.com/Geonheeee)\n\n김준태 : [Github](https://github.com/OPAYA), [Facebook](https://www.facebook.com/kjt7889)\n\n김예찬 : [Github](https://github.com/suhoy901), [Facebook](https://www.facebook.com/suhoy90)\n","slug":"0_lets-do-irl-guide","published":1,"updated":"2019-02-07T11:21:31.904Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjrujltyd00005wfe5q61p70e","content":"<hr>\n<h1 id=\"0-Inverse-RL의-세계로\"><a href=\"#0-Inverse-RL의-세계로\" class=\"headerlink\" title=\"0. Inverse RL의 세계로\"></a>0. Inverse RL의 세계로</h1><p>반갑습니다! 저희는 Inverse RL을 흐름을 살펴보기 위해 모인 IRL 프로젝트 팀입니다.</p>\n<p>강화학습에서 reward라는 요소는 굉장히 중요합니다. 왜냐하면 agent라는 아이가 유일하게 학습할 수 있는 요소이기 때문입니다. 일반적으로 강화학습에서는 사람이 reward를 일일히 정해주지만, 실제로 그 reward에 따라 “desirable”  action이 나오지 않을 수도 있습니다. 여기서 생각해볼 수 있는 것이 바로 “expert”의 행동을 통해 reward를 찾는 것입니다.</p>\n<p>저희는 Andrew Ng의 논문인 Linear IRL과 Pieter Abbeel의 논문인 APP를 필두로 하여 MMP, MaxEnt, 그리고 보통 IRL을 통해 얻어진 reward로 다시 RL을 풀어서 policy를 얻어야하지만, 이 과정을 한번에 풀어버리는 GAIL, 최근 들어 GAIL을 뛰어넘는 VAIL까지 살펴보고자 합니다.</p>\n<center> <img src=\"../../../../img/irl/lets-do-irl-guide_2.png\" width=\"900\"> </center>\n\n<p>논문의 순서는 다음과 같습니다.</p>\n<ol>\n<li><a href=\"http://ai.stanford.edu/~ang/papers/icml00-irl.pdf\" target=\"_blank\" rel=\"noopener\">Linear_IRL</a></li>\n<li><a href=\"http://people.eecs.berkeley.edu/~russell/classes/cs294/s11/readings/Abbeel+Ng:2004.pdf\" target=\"_blank\" rel=\"noopener\">APP</a></li>\n<li><a href=\"https://www.ri.cmu.edu/pub_files/pub4/ratliff_nathan_2006_1/ratliff_nathan_2006_1.pdf\" target=\"_blank\" rel=\"noopener\">MMP</a></li>\n<li><a href=\"http://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf\" target=\"_blank\" rel=\"noopener\">MaxEnt</a></li>\n<li><a href=\"https://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf\" target=\"_blank\" rel=\"noopener\">GAIL</a></li>\n<li><a href=\"https://arxiv.org/pdf/1810.00821.pdf\" target=\"_blank\" rel=\"noopener\">VAIL</a></li>\n</ol>\n<p>위와 같이 총 6가지 논문들을 리뷰하여 블로그로 정리하였습니다. 각 순서에 맞춰 보시는 것을 권장해드립니다.</p>\n<p><br><br></p>\n<h1 id=\"1-Linear-IRL-Algorithms-for-Inverse-Reinforcement-Learning\"><a href=\"#1-Linear-IRL-Algorithms-for-Inverse-Reinforcement-Learning\" class=\"headerlink\" title=\"1. [Linear_IRL] Algorithms for Inverse Reinforcement Learning\"></a>1. [Linear_IRL] Algorithms for Inverse Reinforcement Learning</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2019/01/28/1_linear-irl/\">Linear_IRL 여행하기</a></p>\n<p>짧은 글 소개, 요약</p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2019/01/28/1_linear-irl/\">Linear_IRL 여행하기</a></p>\n<p><br><br></p>\n<h1 id=\"2-APP-Apprenticeship-Learning-via-Inverse-Reinforcement-Learning\"><a href=\"#2-APP-Apprenticeship-Learning-via-Inverse-Reinforcement-Learning\" class=\"headerlink\" title=\"2. [APP] Apprenticeship Learning via Inverse Reinforcement Learning\"></a>2. [APP] Apprenticeship Learning via Inverse Reinforcement Learning</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2019/02/01/2_app/\">APP 여행하기</a><br><a href=\"\">APP Code</a></p>\n<p>짧은 글 소개, 요약</p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2019/02/01/2_app/\">APP 여행하기</a><br><a href=\"\">APP Code</a></p>\n<p><br><br></p>\n<h1 id=\"3-MMP-Maximum-Margin-Planning\"><a href=\"#3-MMP-Maximum-Margin-Planning\" class=\"headerlink\" title=\"3. [MMP] Maximum Margin Planning\"></a>3. [MMP] Maximum Margin Planning</h1><p><a href=\"\">MMP 여행하기</a></p>\n<p>짧은 글 소개, 요약</p>\n<p><a href=\"\">MMP 여행하기</a></p>\n<p><br><br></p>\n<h1 id=\"4-MaxEnt-Maximum-Entropy-Inverse-Reinforcement-Learning\"><a href=\"#4-MaxEnt-Maximum-Entropy-Inverse-Reinforcement-Learning\" class=\"headerlink\" title=\"4. [MaxEnt] Maximum Entropy Inverse Reinforcement Learning\"></a>4. [MaxEnt] Maximum Entropy Inverse Reinforcement Learning</h1><p><a href=\"\">MaxEnt 여행하기</a><br><a href=\"\">MaxEnt Code</a></p>\n<p>짧은 글 소개, 요약</p>\n<p><a href=\"\">MaxEnt 여행하기</a><br><a href=\"\">MaxEnt Code</a></p>\n<p><br><br></p>\n<h1 id=\"5-GAIL-Generative-Adversarial-Imitation-Learning\"><a href=\"#5-GAIL-Generative-Adversarial-Imitation-Learning\" class=\"headerlink\" title=\"5. [GAIL] Generative Adversarial Imitation Learning\"></a>5. [GAIL] Generative Adversarial Imitation Learning</h1><p><a href=\"\">GAIL 여행하기</a><br><a href=\"\">GAIL Code</a></p>\n<p>짧은 글 소개, 요약</p>\n<p><a href=\"\">GAIL 여행하기</a><br><a href=\"\">GAIL Code</a></p>\n<p><br><br></p>\n<h1 id=\"6-VAIL-Variational-Discriminator-Bottleneck-Improving-Imitation-Learning-Inverse-RL-and-GANs-by-Constraining-Information-Flow\"><a href=\"#6-VAIL-Variational-Discriminator-Bottleneck-Improving-Imitation-Learning-Inverse-RL-and-GANs-by-Constraining-Information-Flow\" class=\"headerlink\" title=\"6. [VAIL] Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow\"></a>6. [VAIL] Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow</h1><p><a href=\"\">VAIL 여행하기</a><br><a href=\"\">VAIL Code</a></p>\n<p>짧은 글 소개, 요약</p>\n<p><a href=\"\">VAIL 여행하기</a><br><a href=\"\">VAIL Code</a></p>\n<p><br><br></p>\n<h1 id=\"Team\"><a href=\"#Team\" class=\"headerlink\" title=\"Team\"></a>Team</h1><p>이동민 : <a href=\"https://github.com/dongminleeai\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/dongminleeai\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>윤승제 : <a href=\"https://github.com/sjYoondeltar\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/seungje.yoon\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>이승현 : <a href=\"https://github.com/Clyde21c\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/Clyde21c\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>이건희 : <a href=\"https://github.com/Geonhee-LEE\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/Geonheeee\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>김준태 : <a href=\"https://github.com/OPAYA\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/kjt7889\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>김예찬 : <a href=\"https://github.com/suhoy901\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/suhoy90\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n","site":{"data":{}},"excerpt":"","more":"<hr>\n<h1 id=\"0-Inverse-RL의-세계로\"><a href=\"#0-Inverse-RL의-세계로\" class=\"headerlink\" title=\"0. Inverse RL의 세계로\"></a>0. Inverse RL의 세계로</h1><p>반갑습니다! 저희는 Inverse RL을 흐름을 살펴보기 위해 모인 IRL 프로젝트 팀입니다.</p>\n<p>강화학습에서 reward라는 요소는 굉장히 중요합니다. 왜냐하면 agent라는 아이가 유일하게 학습할 수 있는 요소이기 때문입니다. 일반적으로 강화학습에서는 사람이 reward를 일일히 정해주지만, 실제로 그 reward에 따라 “desirable”  action이 나오지 않을 수도 있습니다. 여기서 생각해볼 수 있는 것이 바로 “expert”의 행동을 통해 reward를 찾는 것입니다.</p>\n<p>저희는 Andrew Ng의 논문인 Linear IRL과 Pieter Abbeel의 논문인 APP를 필두로 하여 MMP, MaxEnt, 그리고 보통 IRL을 통해 얻어진 reward로 다시 RL을 풀어서 policy를 얻어야하지만, 이 과정을 한번에 풀어버리는 GAIL, 최근 들어 GAIL을 뛰어넘는 VAIL까지 살펴보고자 합니다.</p>\n<center> <img src=\"../../../../img/irl/lets-do-irl-guide_2.png\" width=\"900\"> </center>\n\n<p>논문의 순서는 다음과 같습니다.</p>\n<ol>\n<li><a href=\"http://ai.stanford.edu/~ang/papers/icml00-irl.pdf\" target=\"_blank\" rel=\"noopener\">Linear_IRL</a></li>\n<li><a href=\"http://people.eecs.berkeley.edu/~russell/classes/cs294/s11/readings/Abbeel+Ng:2004.pdf\" target=\"_blank\" rel=\"noopener\">APP</a></li>\n<li><a href=\"https://www.ri.cmu.edu/pub_files/pub4/ratliff_nathan_2006_1/ratliff_nathan_2006_1.pdf\" target=\"_blank\" rel=\"noopener\">MMP</a></li>\n<li><a href=\"http://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf\" target=\"_blank\" rel=\"noopener\">MaxEnt</a></li>\n<li><a href=\"https://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf\" target=\"_blank\" rel=\"noopener\">GAIL</a></li>\n<li><a href=\"https://arxiv.org/pdf/1810.00821.pdf\" target=\"_blank\" rel=\"noopener\">VAIL</a></li>\n</ol>\n<p>위와 같이 총 6가지 논문들을 리뷰하여 블로그로 정리하였습니다. 각 순서에 맞춰 보시는 것을 권장해드립니다.</p>\n<p><br><br></p>\n<h1 id=\"1-Linear-IRL-Algorithms-for-Inverse-Reinforcement-Learning\"><a href=\"#1-Linear-IRL-Algorithms-for-Inverse-Reinforcement-Learning\" class=\"headerlink\" title=\"1. [Linear_IRL] Algorithms for Inverse Reinforcement Learning\"></a>1. [Linear_IRL] Algorithms for Inverse Reinforcement Learning</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2019/01/28/1_linear-irl/\">Linear_IRL 여행하기</a></p>\n<p>짧은 글 소개, 요약</p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2019/01/28/1_linear-irl/\">Linear_IRL 여행하기</a></p>\n<p><br><br></p>\n<h1 id=\"2-APP-Apprenticeship-Learning-via-Inverse-Reinforcement-Learning\"><a href=\"#2-APP-Apprenticeship-Learning-via-Inverse-Reinforcement-Learning\" class=\"headerlink\" title=\"2. [APP] Apprenticeship Learning via Inverse Reinforcement Learning\"></a>2. [APP] Apprenticeship Learning via Inverse Reinforcement Learning</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2019/02/01/2_app/\">APP 여행하기</a><br><a href=\"\">APP Code</a></p>\n<p>짧은 글 소개, 요약</p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2019/02/01/2_app/\">APP 여행하기</a><br><a href=\"\">APP Code</a></p>\n<p><br><br></p>\n<h1 id=\"3-MMP-Maximum-Margin-Planning\"><a href=\"#3-MMP-Maximum-Margin-Planning\" class=\"headerlink\" title=\"3. [MMP] Maximum Margin Planning\"></a>3. [MMP] Maximum Margin Planning</h1><p><a href=\"\">MMP 여행하기</a></p>\n<p>짧은 글 소개, 요약</p>\n<p><a href=\"\">MMP 여행하기</a></p>\n<p><br><br></p>\n<h1 id=\"4-MaxEnt-Maximum-Entropy-Inverse-Reinforcement-Learning\"><a href=\"#4-MaxEnt-Maximum-Entropy-Inverse-Reinforcement-Learning\" class=\"headerlink\" title=\"4. [MaxEnt] Maximum Entropy Inverse Reinforcement Learning\"></a>4. [MaxEnt] Maximum Entropy Inverse Reinforcement Learning</h1><p><a href=\"\">MaxEnt 여행하기</a><br><a href=\"\">MaxEnt Code</a></p>\n<p>짧은 글 소개, 요약</p>\n<p><a href=\"\">MaxEnt 여행하기</a><br><a href=\"\">MaxEnt Code</a></p>\n<p><br><br></p>\n<h1 id=\"5-GAIL-Generative-Adversarial-Imitation-Learning\"><a href=\"#5-GAIL-Generative-Adversarial-Imitation-Learning\" class=\"headerlink\" title=\"5. [GAIL] Generative Adversarial Imitation Learning\"></a>5. [GAIL] Generative Adversarial Imitation Learning</h1><p><a href=\"\">GAIL 여행하기</a><br><a href=\"\">GAIL Code</a></p>\n<p>짧은 글 소개, 요약</p>\n<p><a href=\"\">GAIL 여행하기</a><br><a href=\"\">GAIL Code</a></p>\n<p><br><br></p>\n<h1 id=\"6-VAIL-Variational-Discriminator-Bottleneck-Improving-Imitation-Learning-Inverse-RL-and-GANs-by-Constraining-Information-Flow\"><a href=\"#6-VAIL-Variational-Discriminator-Bottleneck-Improving-Imitation-Learning-Inverse-RL-and-GANs-by-Constraining-Information-Flow\" class=\"headerlink\" title=\"6. [VAIL] Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow\"></a>6. [VAIL] Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow</h1><p><a href=\"\">VAIL 여행하기</a><br><a href=\"\">VAIL Code</a></p>\n<p>짧은 글 소개, 요약</p>\n<p><a href=\"\">VAIL 여행하기</a><br><a href=\"\">VAIL Code</a></p>\n<p><br><br></p>\n<h1 id=\"Team\"><a href=\"#Team\" class=\"headerlink\" title=\"Team\"></a>Team</h1><p>이동민 : <a href=\"https://github.com/dongminleeai\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/dongminleeai\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>윤승제 : <a href=\"https://github.com/sjYoondeltar\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/seungje.yoon\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>이승현 : <a href=\"https://github.com/Clyde21c\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/Clyde21c\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>이건희 : <a href=\"https://github.com/Geonhee-LEE\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/Geonheeee\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>김준태 : <a href=\"https://github.com/OPAYA\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/kjt7889\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>김예찬 : <a href=\"https://github.com/suhoy901\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/suhoy90\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n"},{"title":"각잡고 로봇팔 프로젝트 소개","date":"2018-11-20T08:22:40.000Z","author":"유지원","subtitle":"각잡고 로봇팔 프로젝트 소개","_content":"\n\n## 프로젝트 소개\n본 게시글은 **Reinforcement Learning Korea** 커뮤니티의 2회 프로젝트인 **각잡고 로봇팔** 을 소개하는 글입니다\n\n<br>\n\n## 프로젝트 목표\n\n강화학습을 로봇 컨트롤에 적용함\n\n강화학습의 시작은 게임의 승리 혹은 discrete한 상황의 goal 달성을 목표로 한 task가 주를 이루었습니다. 하지만 절대적인 승패가 존재하지 않는 일반적인 상황에서는 판단하기가 힘듭니다. 이를 극복하기 위해, 정책 자체를 근사화 하는 PG(Policy Gradient)가 고안되었습니다. 현재는 이 기법을 기본으로한 연속적인 동작 제어에 관한 연구가 활발히 진행되고 있습니다. 특히, 우리는 Open AI와 BAIR의 놀라운 연구성과를 토대로 로봇팔 제어에 강화학습을 적용하고자 합니다.\n\n현재는, 하기 2개의 논문의 알고리즘을 변형시켜 Pytorch로 구현 할 예정입니다.\n\n- [Data-Efficient HRL(Data-Efficient Hierarchical Reinforcement Learning)](https://arxiv.org/abs/1805.08296)\n- [Deepmimic](https://arxiv.org/abs/1804.02717)\n\n또한, 구현을 위해 하기 논문을 리뷰하였습니다.\n \n - [HER(Handsight Experiece Replay)](https://arxiv.org/abs/1707.01495)\n\n<br>\n\n## 프로젝트 설립 취지\n\n강화학습을 연구하는데 있어, 단순히 강화학습 자체를 연구하기보다 특정 산업 혹은 실물에 적용하는 노력은 학문적인 탐개와 별개로 지속적으로 진행되어야 한다고 생각합니다. 그 가운데, 로봇팔을 제어하는 것을 목표로 삼은 이유는 사람에게 가장 도움이 될 기술이라고 생각하였기 때문입니다. 사람의 팔을 대신할 로봇 혹은 자신의 생각을 말로 표현하는데 어려움이 있는 사람들을 위해 로봇팔의 자연스러움 움직임을 제어하고자 합니다.\n\n<br>\n\n## 프로젝트 연구 일정\n**2018.10.27 ~ 2019.1.20 진행**\n\n- (첫째달)첫 보름은 환경과 Task 선정을 합니다.\n- (첫째달)동시에, 각자 1개의 논문(강화학습 혹은 로봇팔 제어관련)을 리뷰합니다. -여기까지 왔습니다!\n- (첫째달)1주일에 1번씩 행아웃을 통해 함께 논문을 리뷰합니다.\n- (둘째달)리뷰한 논문 중 직접 구현할 2개의 논문을 추립니다. \n- (둘째달)2명당 하나의 논문을 담당하여 함께 구현합니다 - 강화학습과 제어에 대한 이해가 더 필요하다면 하나의 논문을 다같이 구현합니다.\n- (셋째달)구현을 완료하고, 튜토리얼을 작성합니다.\n  - 코드 구현은 github을 통해 협업할 것입니다. \n  - 자료 정리는 ppt 혹은 markdown 형태로 공유할 예정입니다.\n\n<br>\n\n## 프로젝트 현황\n\n- 환경 선정 및 셋업 : [Mujuco](http://www.mujoco.org/)\n- 리뷰할 논문 선정\n- 현재 두 가지의 논문으로 의견을 모아 논문을 리뷰중입니다.\n\n<br>\n\n## 프로젝트 team\n\n**김재윤** [github](https://github.com/jangikim2) [facebook](https://www.facebook.com/jangikim)\n**류연훈** [github](https://github.com/yhryu0409) [facebook](https://www.facebook.com/yeonhun.ryu)\n**류(유)지원** [github](https://github.com/AshleyRyu) [facebook](https://www.facebook.com/profile.php?id=100001622442143)\n**전준형** [github](https://github.com/junhyeongjeon) [facebook](https://www.facebook.com/Jsobu)\n**정의진** [github](https://github.com/jinPrelude) [facebook](https://www.facebook.com/profile.php?id=100011176712221&fref=gs&dti=1890180054554559&hc_location=group_dialog)\n","source":"_posts/robot_arm_intro.md","raw":"---\ntitle: 각잡고 로봇팔 프로젝트 소개\ndate: 2018-11-20 17:22:40\ntags: [\"프로젝트\", \"각잡고로봇팔\"]\ncategories: 프로젝트\nauthor: 유지원\nsubtitle: 각잡고 로봇팔 프로젝트 소개\n---\n\n\n## 프로젝트 소개\n본 게시글은 **Reinforcement Learning Korea** 커뮤니티의 2회 프로젝트인 **각잡고 로봇팔** 을 소개하는 글입니다\n\n<br>\n\n## 프로젝트 목표\n\n강화학습을 로봇 컨트롤에 적용함\n\n강화학습의 시작은 게임의 승리 혹은 discrete한 상황의 goal 달성을 목표로 한 task가 주를 이루었습니다. 하지만 절대적인 승패가 존재하지 않는 일반적인 상황에서는 판단하기가 힘듭니다. 이를 극복하기 위해, 정책 자체를 근사화 하는 PG(Policy Gradient)가 고안되었습니다. 현재는 이 기법을 기본으로한 연속적인 동작 제어에 관한 연구가 활발히 진행되고 있습니다. 특히, 우리는 Open AI와 BAIR의 놀라운 연구성과를 토대로 로봇팔 제어에 강화학습을 적용하고자 합니다.\n\n현재는, 하기 2개의 논문의 알고리즘을 변형시켜 Pytorch로 구현 할 예정입니다.\n\n- [Data-Efficient HRL(Data-Efficient Hierarchical Reinforcement Learning)](https://arxiv.org/abs/1805.08296)\n- [Deepmimic](https://arxiv.org/abs/1804.02717)\n\n또한, 구현을 위해 하기 논문을 리뷰하였습니다.\n \n - [HER(Handsight Experiece Replay)](https://arxiv.org/abs/1707.01495)\n\n<br>\n\n## 프로젝트 설립 취지\n\n강화학습을 연구하는데 있어, 단순히 강화학습 자체를 연구하기보다 특정 산업 혹은 실물에 적용하는 노력은 학문적인 탐개와 별개로 지속적으로 진행되어야 한다고 생각합니다. 그 가운데, 로봇팔을 제어하는 것을 목표로 삼은 이유는 사람에게 가장 도움이 될 기술이라고 생각하였기 때문입니다. 사람의 팔을 대신할 로봇 혹은 자신의 생각을 말로 표현하는데 어려움이 있는 사람들을 위해 로봇팔의 자연스러움 움직임을 제어하고자 합니다.\n\n<br>\n\n## 프로젝트 연구 일정\n**2018.10.27 ~ 2019.1.20 진행**\n\n- (첫째달)첫 보름은 환경과 Task 선정을 합니다.\n- (첫째달)동시에, 각자 1개의 논문(강화학습 혹은 로봇팔 제어관련)을 리뷰합니다. -여기까지 왔습니다!\n- (첫째달)1주일에 1번씩 행아웃을 통해 함께 논문을 리뷰합니다.\n- (둘째달)리뷰한 논문 중 직접 구현할 2개의 논문을 추립니다. \n- (둘째달)2명당 하나의 논문을 담당하여 함께 구현합니다 - 강화학습과 제어에 대한 이해가 더 필요하다면 하나의 논문을 다같이 구현합니다.\n- (셋째달)구현을 완료하고, 튜토리얼을 작성합니다.\n  - 코드 구현은 github을 통해 협업할 것입니다. \n  - 자료 정리는 ppt 혹은 markdown 형태로 공유할 예정입니다.\n\n<br>\n\n## 프로젝트 현황\n\n- 환경 선정 및 셋업 : [Mujuco](http://www.mujoco.org/)\n- 리뷰할 논문 선정\n- 현재 두 가지의 논문으로 의견을 모아 논문을 리뷰중입니다.\n\n<br>\n\n## 프로젝트 team\n\n**김재윤** [github](https://github.com/jangikim2) [facebook](https://www.facebook.com/jangikim)\n**류연훈** [github](https://github.com/yhryu0409) [facebook](https://www.facebook.com/yeonhun.ryu)\n**류(유)지원** [github](https://github.com/AshleyRyu) [facebook](https://www.facebook.com/profile.php?id=100001622442143)\n**전준형** [github](https://github.com/junhyeongjeon) [facebook](https://www.facebook.com/Jsobu)\n**정의진** [github](https://github.com/jinPrelude) [facebook](https://www.facebook.com/profile.php?id=100011176712221&fref=gs&dti=1890180054554559&hc_location=group_dialog)\n","slug":"robot_arm_intro","published":1,"updated":"2019-02-07T11:21:32.003Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjrujltyj00015wfe3i3o2d0k","content":"<h2 id=\"프로젝트-소개\"><a href=\"#프로젝트-소개\" class=\"headerlink\" title=\"프로젝트 소개\"></a>프로젝트 소개</h2><p>본 게시글은 <strong>Reinforcement Learning Korea</strong> 커뮤니티의 2회 프로젝트인 <strong>각잡고 로봇팔</strong> 을 소개하는 글입니다</p>\n<p><br></p>\n<h2 id=\"프로젝트-목표\"><a href=\"#프로젝트-목표\" class=\"headerlink\" title=\"프로젝트 목표\"></a>프로젝트 목표</h2><p>강화학습을 로봇 컨트롤에 적용함</p>\n<p>강화학습의 시작은 게임의 승리 혹은 discrete한 상황의 goal 달성을 목표로 한 task가 주를 이루었습니다. 하지만 절대적인 승패가 존재하지 않는 일반적인 상황에서는 판단하기가 힘듭니다. 이를 극복하기 위해, 정책 자체를 근사화 하는 PG(Policy Gradient)가 고안되었습니다. 현재는 이 기법을 기본으로한 연속적인 동작 제어에 관한 연구가 활발히 진행되고 있습니다. 특히, 우리는 Open AI와 BAIR의 놀라운 연구성과를 토대로 로봇팔 제어에 강화학습을 적용하고자 합니다.</p>\n<p>현재는, 하기 2개의 논문의 알고리즘을 변형시켜 Pytorch로 구현 할 예정입니다.</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1805.08296\" target=\"_blank\" rel=\"noopener\">Data-Efficient HRL(Data-Efficient Hierarchical Reinforcement Learning)</a></li>\n<li><a href=\"https://arxiv.org/abs/1804.02717\" target=\"_blank\" rel=\"noopener\">Deepmimic</a></li>\n</ul>\n<p>또한, 구현을 위해 하기 논문을 리뷰하였습니다.</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1707.01495\" target=\"_blank\" rel=\"noopener\">HER(Handsight Experiece Replay)</a></li>\n</ul>\n<p><br></p>\n<h2 id=\"프로젝트-설립-취지\"><a href=\"#프로젝트-설립-취지\" class=\"headerlink\" title=\"프로젝트 설립 취지\"></a>프로젝트 설립 취지</h2><p>강화학습을 연구하는데 있어, 단순히 강화학습 자체를 연구하기보다 특정 산업 혹은 실물에 적용하는 노력은 학문적인 탐개와 별개로 지속적으로 진행되어야 한다고 생각합니다. 그 가운데, 로봇팔을 제어하는 것을 목표로 삼은 이유는 사람에게 가장 도움이 될 기술이라고 생각하였기 때문입니다. 사람의 팔을 대신할 로봇 혹은 자신의 생각을 말로 표현하는데 어려움이 있는 사람들을 위해 로봇팔의 자연스러움 움직임을 제어하고자 합니다.</p>\n<p><br></p>\n<h2 id=\"프로젝트-연구-일정\"><a href=\"#프로젝트-연구-일정\" class=\"headerlink\" title=\"프로젝트 연구 일정\"></a>프로젝트 연구 일정</h2><p><strong>2018.10.27 ~ 2019.1.20 진행</strong></p>\n<ul>\n<li>(첫째달)첫 보름은 환경과 Task 선정을 합니다.</li>\n<li>(첫째달)동시에, 각자 1개의 논문(강화학습 혹은 로봇팔 제어관련)을 리뷰합니다. -여기까지 왔습니다!</li>\n<li>(첫째달)1주일에 1번씩 행아웃을 통해 함께 논문을 리뷰합니다.</li>\n<li>(둘째달)리뷰한 논문 중 직접 구현할 2개의 논문을 추립니다. </li>\n<li>(둘째달)2명당 하나의 논문을 담당하여 함께 구현합니다 - 강화학습과 제어에 대한 이해가 더 필요하다면 하나의 논문을 다같이 구현합니다.</li>\n<li>(셋째달)구현을 완료하고, 튜토리얼을 작성합니다.<ul>\n<li>코드 구현은 github을 통해 협업할 것입니다. </li>\n<li>자료 정리는 ppt 혹은 markdown 형태로 공유할 예정입니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"프로젝트-현황\"><a href=\"#프로젝트-현황\" class=\"headerlink\" title=\"프로젝트 현황\"></a>프로젝트 현황</h2><ul>\n<li>환경 선정 및 셋업 : <a href=\"http://www.mujoco.org/\" target=\"_blank\" rel=\"noopener\">Mujuco</a></li>\n<li>리뷰할 논문 선정</li>\n<li>현재 두 가지의 논문으로 의견을 모아 논문을 리뷰중입니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"프로젝트-team\"><a href=\"#프로젝트-team\" class=\"headerlink\" title=\"프로젝트 team\"></a>프로젝트 team</h2><p><strong>김재윤</strong> <a href=\"https://github.com/jangikim2\" target=\"_blank\" rel=\"noopener\">github</a> <a href=\"https://www.facebook.com/jangikim\" target=\"_blank\" rel=\"noopener\">facebook</a><br><strong>류연훈</strong> <a href=\"https://github.com/yhryu0409\" target=\"_blank\" rel=\"noopener\">github</a> <a href=\"https://www.facebook.com/yeonhun.ryu\" target=\"_blank\" rel=\"noopener\">facebook</a><br><strong>류(유)지원</strong> <a href=\"https://github.com/AshleyRyu\" target=\"_blank\" rel=\"noopener\">github</a> <a href=\"https://www.facebook.com/profile.php?id=100001622442143\" target=\"_blank\" rel=\"noopener\">facebook</a><br><strong>전준형</strong> <a href=\"https://github.com/junhyeongjeon\" target=\"_blank\" rel=\"noopener\">github</a> <a href=\"https://www.facebook.com/Jsobu\" target=\"_blank\" rel=\"noopener\">facebook</a><br><strong>정의진</strong> <a href=\"https://github.com/jinPrelude\" target=\"_blank\" rel=\"noopener\">github</a> <a href=\"https://www.facebook.com/profile.php?id=100011176712221&amp;fref=gs&amp;dti=1890180054554559&amp;hc_location=group_dialog\" target=\"_blank\" rel=\"noopener\">facebook</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"프로젝트-소개\"><a href=\"#프로젝트-소개\" class=\"headerlink\" title=\"프로젝트 소개\"></a>프로젝트 소개</h2><p>본 게시글은 <strong>Reinforcement Learning Korea</strong> 커뮤니티의 2회 프로젝트인 <strong>각잡고 로봇팔</strong> 을 소개하는 글입니다</p>\n<p><br></p>\n<h2 id=\"프로젝트-목표\"><a href=\"#프로젝트-목표\" class=\"headerlink\" title=\"프로젝트 목표\"></a>프로젝트 목표</h2><p>강화학습을 로봇 컨트롤에 적용함</p>\n<p>강화학습의 시작은 게임의 승리 혹은 discrete한 상황의 goal 달성을 목표로 한 task가 주를 이루었습니다. 하지만 절대적인 승패가 존재하지 않는 일반적인 상황에서는 판단하기가 힘듭니다. 이를 극복하기 위해, 정책 자체를 근사화 하는 PG(Policy Gradient)가 고안되었습니다. 현재는 이 기법을 기본으로한 연속적인 동작 제어에 관한 연구가 활발히 진행되고 있습니다. 특히, 우리는 Open AI와 BAIR의 놀라운 연구성과를 토대로 로봇팔 제어에 강화학습을 적용하고자 합니다.</p>\n<p>현재는, 하기 2개의 논문의 알고리즘을 변형시켜 Pytorch로 구현 할 예정입니다.</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1805.08296\" target=\"_blank\" rel=\"noopener\">Data-Efficient HRL(Data-Efficient Hierarchical Reinforcement Learning)</a></li>\n<li><a href=\"https://arxiv.org/abs/1804.02717\" target=\"_blank\" rel=\"noopener\">Deepmimic</a></li>\n</ul>\n<p>또한, 구현을 위해 하기 논문을 리뷰하였습니다.</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1707.01495\" target=\"_blank\" rel=\"noopener\">HER(Handsight Experiece Replay)</a></li>\n</ul>\n<p><br></p>\n<h2 id=\"프로젝트-설립-취지\"><a href=\"#프로젝트-설립-취지\" class=\"headerlink\" title=\"프로젝트 설립 취지\"></a>프로젝트 설립 취지</h2><p>강화학습을 연구하는데 있어, 단순히 강화학습 자체를 연구하기보다 특정 산업 혹은 실물에 적용하는 노력은 학문적인 탐개와 별개로 지속적으로 진행되어야 한다고 생각합니다. 그 가운데, 로봇팔을 제어하는 것을 목표로 삼은 이유는 사람에게 가장 도움이 될 기술이라고 생각하였기 때문입니다. 사람의 팔을 대신할 로봇 혹은 자신의 생각을 말로 표현하는데 어려움이 있는 사람들을 위해 로봇팔의 자연스러움 움직임을 제어하고자 합니다.</p>\n<p><br></p>\n<h2 id=\"프로젝트-연구-일정\"><a href=\"#프로젝트-연구-일정\" class=\"headerlink\" title=\"프로젝트 연구 일정\"></a>프로젝트 연구 일정</h2><p><strong>2018.10.27 ~ 2019.1.20 진행</strong></p>\n<ul>\n<li>(첫째달)첫 보름은 환경과 Task 선정을 합니다.</li>\n<li>(첫째달)동시에, 각자 1개의 논문(강화학습 혹은 로봇팔 제어관련)을 리뷰합니다. -여기까지 왔습니다!</li>\n<li>(첫째달)1주일에 1번씩 행아웃을 통해 함께 논문을 리뷰합니다.</li>\n<li>(둘째달)리뷰한 논문 중 직접 구현할 2개의 논문을 추립니다. </li>\n<li>(둘째달)2명당 하나의 논문을 담당하여 함께 구현합니다 - 강화학습과 제어에 대한 이해가 더 필요하다면 하나의 논문을 다같이 구현합니다.</li>\n<li>(셋째달)구현을 완료하고, 튜토리얼을 작성합니다.<ul>\n<li>코드 구현은 github을 통해 협업할 것입니다. </li>\n<li>자료 정리는 ppt 혹은 markdown 형태로 공유할 예정입니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"프로젝트-현황\"><a href=\"#프로젝트-현황\" class=\"headerlink\" title=\"프로젝트 현황\"></a>프로젝트 현황</h2><ul>\n<li>환경 선정 및 셋업 : <a href=\"http://www.mujoco.org/\" target=\"_blank\" rel=\"noopener\">Mujuco</a></li>\n<li>리뷰할 논문 선정</li>\n<li>현재 두 가지의 논문으로 의견을 모아 논문을 리뷰중입니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"프로젝트-team\"><a href=\"#프로젝트-team\" class=\"headerlink\" title=\"프로젝트 team\"></a>프로젝트 team</h2><p><strong>김재윤</strong> <a href=\"https://github.com/jangikim2\" target=\"_blank\" rel=\"noopener\">github</a> <a href=\"https://www.facebook.com/jangikim\" target=\"_blank\" rel=\"noopener\">facebook</a><br><strong>류연훈</strong> <a href=\"https://github.com/yhryu0409\" target=\"_blank\" rel=\"noopener\">github</a> <a href=\"https://www.facebook.com/yeonhun.ryu\" target=\"_blank\" rel=\"noopener\">facebook</a><br><strong>류(유)지원</strong> <a href=\"https://github.com/AshleyRyu\" target=\"_blank\" rel=\"noopener\">github</a> <a href=\"https://www.facebook.com/profile.php?id=100001622442143\" target=\"_blank\" rel=\"noopener\">facebook</a><br><strong>전준형</strong> <a href=\"https://github.com/junhyeongjeon\" target=\"_blank\" rel=\"noopener\">github</a> <a href=\"https://www.facebook.com/Jsobu\" target=\"_blank\" rel=\"noopener\">facebook</a><br><strong>정의진</strong> <a href=\"https://github.com/jinPrelude\" target=\"_blank\" rel=\"noopener\">github</a> <a href=\"https://www.facebook.com/profile.php?id=100011176712221&amp;fref=gs&amp;dti=1890180054554559&amp;hc_location=group_dialog\" target=\"_blank\" rel=\"noopener\">facebook</a></p>\n"},{"title":"Introduction to Distributional RL","date":"2018-09-27T05:18:32.000Z","author":"민규식","subtitle":"Introduction to Distributional RL","comments":1,"_content":"\n# Distributional RL\n## Introduction\n\n본 게시글은 강화학습의 한 종류 중 하나인 **Distributional Reinforcement Learning**에 대해서 설명하고 [Deepmind](https://deepmind.com/)에서 발표한 몇가지 알고리즘을 설명할 예정입니다.   \n\n우선 일반적인 강화학습 알고리즘과 distributional RL 알고리즘을 간단하게 그림으로 비교한 것이 다음과 같습니다. \n\n<p align= \"center\">\n  <img src=\"/img/Fig0.png\" width=\"600\" alt=\"Comparison\" />\n</p>\n\n\n일반적인 강화학습은 다양하게 task를 시도해보고 그 경험을 바탕으로 미래에 받을 평균 reward를 하나의 숫자(scalar)로 예측하도록 학습합니다. 그리고 이 예측을 통해 미래에 많은 reward를 받을 것이라 예측되는 action을 선택하게 됩니다. \n\n하지만!!! 환경이 **랜덤성**을 포함하고 있는 경우 상황에 따라 동일한 state에서 동일한 action을 취하더라도 다음 state에서 받는 reward가 변할 수 있습니다. 이런 상황에 대한 예시를 한번 들어보겠습니다! 아래의 경우처럼 자동차가 운전을 하고 있다고 해보겠습니다.\n\n<p align= \"center\">\n  <img src=\"/img/Fig1.png\" width=\"600\" alt=\"Driving example\" />\n</p>\n\n\n**빨간색 자동차**가 **agent** 이며 하얀색 자동차는 랜덤하게 가속, 감속, 차선변경을 하는 차량이라고 가정해보겠습니다. 이에 따라 해당 환경은 랜덤성을 포함한 환경이라 할 수 있습니다. 가장 위쪽 그림들을 보면 두 상황은 동일한 상황이고 빨간 차량은 모두 빠른 속도로 직진하는 action을 선택했습니다. 하지만 다음 state에서는 서로 다른 결과가 발생하였습니다. 왼쪽의 경우 하얀 차량이 갑자기 차선 변경을 하여 충돌이 발생하였고, 오른쪽의 경우 하얀차량이 agent보다 느린 속도로 직진하였으므로 agent가 하얀 차량을 추월하여 주행하였습니다. \n\n이렇게 확률적인 상황에서는 예측되는 reward를 분포로 나타내면 아래와 같이 나타낼 수 있습니다. \n\n<p align= \"center\">\n  <img src=\"/img/Fig2.png\" width=\"500\" alt=\"Distribution\" />\n</p>\n\n\n위의 분포를 보면 확률적으로 약 1점의 reward를 받을수도, 약 -9점의 reward를 받을 수 있는 상황입니다. 확률적인 환경의 경우 상황에 따라서는 받을 확률이 높은 reward가 더 다양할 수도 있습니다. 이런 분포를 [Multimodal Distribution](https://en.wikipedia.org/wiki/Multimodal_distribution)이라고 합니다. \n\n이렇게 확률적인 환경에서는 하나의 숫자로 미래의 reward를 예측하는 것보다 위와 같이 분포로 미래의 reward를 예측하는 것이 더 정확한 예측이라고 할 수 있습니다. 이렇게 미래의 보상을 분포를 통해 예측하여 확률적인 환경에서 더 정확한 예측을 수행하는 강화학습 알고리즘이 바로 **Distributional Reinforcement Learning** 알고리즘입니다!! \n\n<br>\n\n## Papers\n\n최고의 AI관련 연구 기관 중 하나인 [Deepmind](https://deepmind.com/)가 최근 distributional RL에 대한 몇 가지 논문들을 발표하였으며 해당 기술들을 계속 발전시켜 다양하게 적용 중에 있습니다. 본 블로그에서 리뷰할 논문들은 다음과 같습니다. \n\n- [A Distributional Perspective on Reinforcement Learning (C51)](https://arxiv.org/abs/1707.06887)\n- [Distributional Reinforcement Learning with Quantile Regression (QR-DQN)](https://arxiv.org/abs/1710.10044)\n- [Implicit Quantile Networks for Distributional Reinforcement Learning (IQN)](https://arxiv.org/abs/1806.06923)\n\n<br>\n\n## Description of the algorithms\n\n각 알고리즘들에 대한 설명은 다음의 링크들을 따라가시면 됩니다 \n\n- [C51](https://reinforcement-learning-kr.github.io/2018/10/02/C51/)\n- [QR-DQN](https://reinforcement-learning-kr.github.io/2018/10/22/QR-DQN/)\n- [IQN](https://reinforcement-learning-kr.github.io/2018/10/30/IQN/)\n\n\n<br>\n\n## Github\n\n해당 알고리즘의 코드들은 아래의 Github에 정리되어있습니다. \n\n[RL Korea Distibutional RL Github](https://github.com/reinforcement-learning-kr/distributional_rl)\n\n\n\n## Team\n\n민규식: [Github](https://github.com/Kyushik), [Facebook](https://www.facebook.com/kyushik.min)\n\n차금강: [Github](https://github.com/chagmgang), [Facebook](https://www.facebook.com/profile.php?id=100002147815509)\n\n윤승제: [Github](https://github.com/sjYoondeltar), [Facebook](https://www.facebook.com/seungje.yoon)\n\n김하영: [Github](https://github.com/hayoung-kim), [Facebook](https://www.facebook.com/altairyoung)\n\n김정대: [Github](https://github.com/kekmodel), [Facebook](https://www.facebook.com/kekmodel)\n","source":"_posts/Distributional_intro.md","raw":"---\ntitle: Introduction to Distributional RL\ndate: 2018-09-27 14:18:32\ntags: [\"프로젝트\", \"DistRL\"]\ncategories: 프로젝트\nauthor: 민규식\nsubtitle: Introduction to Distributional RL\ncomments: true\n---\n\n# Distributional RL\n## Introduction\n\n본 게시글은 강화학습의 한 종류 중 하나인 **Distributional Reinforcement Learning**에 대해서 설명하고 [Deepmind](https://deepmind.com/)에서 발표한 몇가지 알고리즘을 설명할 예정입니다.   \n\n우선 일반적인 강화학습 알고리즘과 distributional RL 알고리즘을 간단하게 그림으로 비교한 것이 다음과 같습니다. \n\n<p align= \"center\">\n  <img src=\"/img/Fig0.png\" width=\"600\" alt=\"Comparison\" />\n</p>\n\n\n일반적인 강화학습은 다양하게 task를 시도해보고 그 경험을 바탕으로 미래에 받을 평균 reward를 하나의 숫자(scalar)로 예측하도록 학습합니다. 그리고 이 예측을 통해 미래에 많은 reward를 받을 것이라 예측되는 action을 선택하게 됩니다. \n\n하지만!!! 환경이 **랜덤성**을 포함하고 있는 경우 상황에 따라 동일한 state에서 동일한 action을 취하더라도 다음 state에서 받는 reward가 변할 수 있습니다. 이런 상황에 대한 예시를 한번 들어보겠습니다! 아래의 경우처럼 자동차가 운전을 하고 있다고 해보겠습니다.\n\n<p align= \"center\">\n  <img src=\"/img/Fig1.png\" width=\"600\" alt=\"Driving example\" />\n</p>\n\n\n**빨간색 자동차**가 **agent** 이며 하얀색 자동차는 랜덤하게 가속, 감속, 차선변경을 하는 차량이라고 가정해보겠습니다. 이에 따라 해당 환경은 랜덤성을 포함한 환경이라 할 수 있습니다. 가장 위쪽 그림들을 보면 두 상황은 동일한 상황이고 빨간 차량은 모두 빠른 속도로 직진하는 action을 선택했습니다. 하지만 다음 state에서는 서로 다른 결과가 발생하였습니다. 왼쪽의 경우 하얀 차량이 갑자기 차선 변경을 하여 충돌이 발생하였고, 오른쪽의 경우 하얀차량이 agent보다 느린 속도로 직진하였으므로 agent가 하얀 차량을 추월하여 주행하였습니다. \n\n이렇게 확률적인 상황에서는 예측되는 reward를 분포로 나타내면 아래와 같이 나타낼 수 있습니다. \n\n<p align= \"center\">\n  <img src=\"/img/Fig2.png\" width=\"500\" alt=\"Distribution\" />\n</p>\n\n\n위의 분포를 보면 확률적으로 약 1점의 reward를 받을수도, 약 -9점의 reward를 받을 수 있는 상황입니다. 확률적인 환경의 경우 상황에 따라서는 받을 확률이 높은 reward가 더 다양할 수도 있습니다. 이런 분포를 [Multimodal Distribution](https://en.wikipedia.org/wiki/Multimodal_distribution)이라고 합니다. \n\n이렇게 확률적인 환경에서는 하나의 숫자로 미래의 reward를 예측하는 것보다 위와 같이 분포로 미래의 reward를 예측하는 것이 더 정확한 예측이라고 할 수 있습니다. 이렇게 미래의 보상을 분포를 통해 예측하여 확률적인 환경에서 더 정확한 예측을 수행하는 강화학습 알고리즘이 바로 **Distributional Reinforcement Learning** 알고리즘입니다!! \n\n<br>\n\n## Papers\n\n최고의 AI관련 연구 기관 중 하나인 [Deepmind](https://deepmind.com/)가 최근 distributional RL에 대한 몇 가지 논문들을 발표하였으며 해당 기술들을 계속 발전시켜 다양하게 적용 중에 있습니다. 본 블로그에서 리뷰할 논문들은 다음과 같습니다. \n\n- [A Distributional Perspective on Reinforcement Learning (C51)](https://arxiv.org/abs/1707.06887)\n- [Distributional Reinforcement Learning with Quantile Regression (QR-DQN)](https://arxiv.org/abs/1710.10044)\n- [Implicit Quantile Networks for Distributional Reinforcement Learning (IQN)](https://arxiv.org/abs/1806.06923)\n\n<br>\n\n## Description of the algorithms\n\n각 알고리즘들에 대한 설명은 다음의 링크들을 따라가시면 됩니다 \n\n- [C51](https://reinforcement-learning-kr.github.io/2018/10/02/C51/)\n- [QR-DQN](https://reinforcement-learning-kr.github.io/2018/10/22/QR-DQN/)\n- [IQN](https://reinforcement-learning-kr.github.io/2018/10/30/IQN/)\n\n\n<br>\n\n## Github\n\n해당 알고리즘의 코드들은 아래의 Github에 정리되어있습니다. \n\n[RL Korea Distibutional RL Github](https://github.com/reinforcement-learning-kr/distributional_rl)\n\n\n\n## Team\n\n민규식: [Github](https://github.com/Kyushik), [Facebook](https://www.facebook.com/kyushik.min)\n\n차금강: [Github](https://github.com/chagmgang), [Facebook](https://www.facebook.com/profile.php?id=100002147815509)\n\n윤승제: [Github](https://github.com/sjYoondeltar), [Facebook](https://www.facebook.com/seungje.yoon)\n\n김하영: [Github](https://github.com/hayoung-kim), [Facebook](https://www.facebook.com/altairyoung)\n\n김정대: [Github](https://github.com/kekmodel), [Facebook](https://www.facebook.com/kekmodel)\n","slug":"Distributional_intro","published":1,"updated":"2019-02-07T11:21:31.987Z","layout":"post","photos":[],"link":"","_id":"cjrujltyo00045wfeol83b6z5","content":"<h1 id=\"Distributional-RL\"><a href=\"#Distributional-RL\" class=\"headerlink\" title=\"Distributional RL\"></a>Distributional RL</h1><h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>본 게시글은 강화학습의 한 종류 중 하나인 <strong>Distributional Reinforcement Learning</strong>에 대해서 설명하고 <a href=\"https://deepmind.com/\" target=\"_blank\" rel=\"noopener\">Deepmind</a>에서 발표한 몇가지 알고리즘을 설명할 예정입니다.   </p>\n<p>우선 일반적인 강화학습 알고리즘과 distributional RL 알고리즘을 간단하게 그림으로 비교한 것이 다음과 같습니다. </p>\n<p align=\"center\"><br>  <img src=\"/img/Fig0.png\" width=\"600\" alt=\"Comparison\"><br></p>\n\n\n<p>일반적인 강화학습은 다양하게 task를 시도해보고 그 경험을 바탕으로 미래에 받을 평균 reward를 하나의 숫자(scalar)로 예측하도록 학습합니다. 그리고 이 예측을 통해 미래에 많은 reward를 받을 것이라 예측되는 action을 선택하게 됩니다. </p>\n<p>하지만!!! 환경이 <strong>랜덤성</strong>을 포함하고 있는 경우 상황에 따라 동일한 state에서 동일한 action을 취하더라도 다음 state에서 받는 reward가 변할 수 있습니다. 이런 상황에 대한 예시를 한번 들어보겠습니다! 아래의 경우처럼 자동차가 운전을 하고 있다고 해보겠습니다.</p>\n<p align=\"center\"><br>  <img src=\"/img/Fig1.png\" width=\"600\" alt=\"Driving example\"><br></p>\n\n\n<p><strong>빨간색 자동차</strong>가 <strong>agent</strong> 이며 하얀색 자동차는 랜덤하게 가속, 감속, 차선변경을 하는 차량이라고 가정해보겠습니다. 이에 따라 해당 환경은 랜덤성을 포함한 환경이라 할 수 있습니다. 가장 위쪽 그림들을 보면 두 상황은 동일한 상황이고 빨간 차량은 모두 빠른 속도로 직진하는 action을 선택했습니다. 하지만 다음 state에서는 서로 다른 결과가 발생하였습니다. 왼쪽의 경우 하얀 차량이 갑자기 차선 변경을 하여 충돌이 발생하였고, 오른쪽의 경우 하얀차량이 agent보다 느린 속도로 직진하였으므로 agent가 하얀 차량을 추월하여 주행하였습니다. </p>\n<p>이렇게 확률적인 상황에서는 예측되는 reward를 분포로 나타내면 아래와 같이 나타낼 수 있습니다. </p>\n<p align=\"center\"><br>  <img src=\"/img/Fig2.png\" width=\"500\" alt=\"Distribution\"><br></p>\n\n\n<p>위의 분포를 보면 확률적으로 약 1점의 reward를 받을수도, 약 -9점의 reward를 받을 수 있는 상황입니다. 확률적인 환경의 경우 상황에 따라서는 받을 확률이 높은 reward가 더 다양할 수도 있습니다. 이런 분포를 <a href=\"https://en.wikipedia.org/wiki/Multimodal_distribution\" target=\"_blank\" rel=\"noopener\">Multimodal Distribution</a>이라고 합니다. </p>\n<p>이렇게 확률적인 환경에서는 하나의 숫자로 미래의 reward를 예측하는 것보다 위와 같이 분포로 미래의 reward를 예측하는 것이 더 정확한 예측이라고 할 수 있습니다. 이렇게 미래의 보상을 분포를 통해 예측하여 확률적인 환경에서 더 정확한 예측을 수행하는 강화학습 알고리즘이 바로 <strong>Distributional Reinforcement Learning</strong> 알고리즘입니다!! </p>\n<p><br></p>\n<h2 id=\"Papers\"><a href=\"#Papers\" class=\"headerlink\" title=\"Papers\"></a>Papers</h2><p>최고의 AI관련 연구 기관 중 하나인 <a href=\"https://deepmind.com/\" target=\"_blank\" rel=\"noopener\">Deepmind</a>가 최근 distributional RL에 대한 몇 가지 논문들을 발표하였으며 해당 기술들을 계속 발전시켜 다양하게 적용 중에 있습니다. 본 블로그에서 리뷰할 논문들은 다음과 같습니다. </p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1707.06887\" target=\"_blank\" rel=\"noopener\">A Distributional Perspective on Reinforcement Learning (C51)</a></li>\n<li><a href=\"https://arxiv.org/abs/1710.10044\" target=\"_blank\" rel=\"noopener\">Distributional Reinforcement Learning with Quantile Regression (QR-DQN)</a></li>\n<li><a href=\"https://arxiv.org/abs/1806.06923\" target=\"_blank\" rel=\"noopener\">Implicit Quantile Networks for Distributional Reinforcement Learning (IQN)</a></li>\n</ul>\n<p><br></p>\n<h2 id=\"Description-of-the-algorithms\"><a href=\"#Description-of-the-algorithms\" class=\"headerlink\" title=\"Description of the algorithms\"></a>Description of the algorithms</h2><p>각 알고리즘들에 대한 설명은 다음의 링크들을 따라가시면 됩니다 </p>\n<ul>\n<li><a href=\"https://reinforcement-learning-kr.github.io/2018/10/02/C51/\">C51</a></li>\n<li><a href=\"https://reinforcement-learning-kr.github.io/2018/10/22/QR-DQN/\">QR-DQN</a></li>\n<li><a href=\"https://reinforcement-learning-kr.github.io/2018/10/30/IQN/\">IQN</a></li>\n</ul>\n<p><br></p>\n<h2 id=\"Github\"><a href=\"#Github\" class=\"headerlink\" title=\"Github\"></a>Github</h2><p>해당 알고리즘의 코드들은 아래의 Github에 정리되어있습니다. </p>\n<p><a href=\"https://github.com/reinforcement-learning-kr/distributional_rl\" target=\"_blank\" rel=\"noopener\">RL Korea Distibutional RL Github</a></p>\n<h2 id=\"Team\"><a href=\"#Team\" class=\"headerlink\" title=\"Team\"></a>Team</h2><p>민규식: <a href=\"https://github.com/Kyushik\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/kyushik.min\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>차금강: <a href=\"https://github.com/chagmgang\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/profile.php?id=100002147815509\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>윤승제: <a href=\"https://github.com/sjYoondeltar\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/seungje.yoon\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>김하영: <a href=\"https://github.com/hayoung-kim\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/altairyoung\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>김정대: <a href=\"https://github.com/kekmodel\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/kekmodel\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Distributional-RL\"><a href=\"#Distributional-RL\" class=\"headerlink\" title=\"Distributional RL\"></a>Distributional RL</h1><h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>본 게시글은 강화학습의 한 종류 중 하나인 <strong>Distributional Reinforcement Learning</strong>에 대해서 설명하고 <a href=\"https://deepmind.com/\" target=\"_blank\" rel=\"noopener\">Deepmind</a>에서 발표한 몇가지 알고리즘을 설명할 예정입니다.   </p>\n<p>우선 일반적인 강화학습 알고리즘과 distributional RL 알고리즘을 간단하게 그림으로 비교한 것이 다음과 같습니다. </p>\n<p align=\"center\"><br>  <img src=\"/img/Fig0.png\" width=\"600\" alt=\"Comparison\"><br></p>\n\n\n<p>일반적인 강화학습은 다양하게 task를 시도해보고 그 경험을 바탕으로 미래에 받을 평균 reward를 하나의 숫자(scalar)로 예측하도록 학습합니다. 그리고 이 예측을 통해 미래에 많은 reward를 받을 것이라 예측되는 action을 선택하게 됩니다. </p>\n<p>하지만!!! 환경이 <strong>랜덤성</strong>을 포함하고 있는 경우 상황에 따라 동일한 state에서 동일한 action을 취하더라도 다음 state에서 받는 reward가 변할 수 있습니다. 이런 상황에 대한 예시를 한번 들어보겠습니다! 아래의 경우처럼 자동차가 운전을 하고 있다고 해보겠습니다.</p>\n<p align=\"center\"><br>  <img src=\"/img/Fig1.png\" width=\"600\" alt=\"Driving example\"><br></p>\n\n\n<p><strong>빨간색 자동차</strong>가 <strong>agent</strong> 이며 하얀색 자동차는 랜덤하게 가속, 감속, 차선변경을 하는 차량이라고 가정해보겠습니다. 이에 따라 해당 환경은 랜덤성을 포함한 환경이라 할 수 있습니다. 가장 위쪽 그림들을 보면 두 상황은 동일한 상황이고 빨간 차량은 모두 빠른 속도로 직진하는 action을 선택했습니다. 하지만 다음 state에서는 서로 다른 결과가 발생하였습니다. 왼쪽의 경우 하얀 차량이 갑자기 차선 변경을 하여 충돌이 발생하였고, 오른쪽의 경우 하얀차량이 agent보다 느린 속도로 직진하였으므로 agent가 하얀 차량을 추월하여 주행하였습니다. </p>\n<p>이렇게 확률적인 상황에서는 예측되는 reward를 분포로 나타내면 아래와 같이 나타낼 수 있습니다. </p>\n<p align=\"center\"><br>  <img src=\"/img/Fig2.png\" width=\"500\" alt=\"Distribution\"><br></p>\n\n\n<p>위의 분포를 보면 확률적으로 약 1점의 reward를 받을수도, 약 -9점의 reward를 받을 수 있는 상황입니다. 확률적인 환경의 경우 상황에 따라서는 받을 확률이 높은 reward가 더 다양할 수도 있습니다. 이런 분포를 <a href=\"https://en.wikipedia.org/wiki/Multimodal_distribution\" target=\"_blank\" rel=\"noopener\">Multimodal Distribution</a>이라고 합니다. </p>\n<p>이렇게 확률적인 환경에서는 하나의 숫자로 미래의 reward를 예측하는 것보다 위와 같이 분포로 미래의 reward를 예측하는 것이 더 정확한 예측이라고 할 수 있습니다. 이렇게 미래의 보상을 분포를 통해 예측하여 확률적인 환경에서 더 정확한 예측을 수행하는 강화학습 알고리즘이 바로 <strong>Distributional Reinforcement Learning</strong> 알고리즘입니다!! </p>\n<p><br></p>\n<h2 id=\"Papers\"><a href=\"#Papers\" class=\"headerlink\" title=\"Papers\"></a>Papers</h2><p>최고의 AI관련 연구 기관 중 하나인 <a href=\"https://deepmind.com/\" target=\"_blank\" rel=\"noopener\">Deepmind</a>가 최근 distributional RL에 대한 몇 가지 논문들을 발표하였으며 해당 기술들을 계속 발전시켜 다양하게 적용 중에 있습니다. 본 블로그에서 리뷰할 논문들은 다음과 같습니다. </p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1707.06887\" target=\"_blank\" rel=\"noopener\">A Distributional Perspective on Reinforcement Learning (C51)</a></li>\n<li><a href=\"https://arxiv.org/abs/1710.10044\" target=\"_blank\" rel=\"noopener\">Distributional Reinforcement Learning with Quantile Regression (QR-DQN)</a></li>\n<li><a href=\"https://arxiv.org/abs/1806.06923\" target=\"_blank\" rel=\"noopener\">Implicit Quantile Networks for Distributional Reinforcement Learning (IQN)</a></li>\n</ul>\n<p><br></p>\n<h2 id=\"Description-of-the-algorithms\"><a href=\"#Description-of-the-algorithms\" class=\"headerlink\" title=\"Description of the algorithms\"></a>Description of the algorithms</h2><p>각 알고리즘들에 대한 설명은 다음의 링크들을 따라가시면 됩니다 </p>\n<ul>\n<li><a href=\"https://reinforcement-learning-kr.github.io/2018/10/02/C51/\">C51</a></li>\n<li><a href=\"https://reinforcement-learning-kr.github.io/2018/10/22/QR-DQN/\">QR-DQN</a></li>\n<li><a href=\"https://reinforcement-learning-kr.github.io/2018/10/30/IQN/\">IQN</a></li>\n</ul>\n<p><br></p>\n<h2 id=\"Github\"><a href=\"#Github\" class=\"headerlink\" title=\"Github\"></a>Github</h2><p>해당 알고리즘의 코드들은 아래의 Github에 정리되어있습니다. </p>\n<p><a href=\"https://github.com/reinforcement-learning-kr/distributional_rl\" target=\"_blank\" rel=\"noopener\">RL Korea Distibutional RL Github</a></p>\n<h2 id=\"Team\"><a href=\"#Team\" class=\"headerlink\" title=\"Team\"></a>Team</h2><p>민규식: <a href=\"https://github.com/Kyushik\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/kyushik.min\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>차금강: <a href=\"https://github.com/chagmgang\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/profile.php?id=100002147815509\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>윤승제: <a href=\"https://github.com/sjYoondeltar\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/seungje.yoon\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>김하영: <a href=\"https://github.com/hayoung-kim\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/altairyoung\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>김정대: <a href=\"https://github.com/kekmodel\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/kekmodel\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n"},{"title":"Deep Determinstic Policy Gradient (DDPG)","date":"2018-06-26T02:20:45.000Z","author":"양혁렬, 이동민, 차금강","subtitle":"피지여행 3번째 논문","_content":"\n<center > <img src=\"https://www.dropbox.com/s/zv8rk0uf87ipiaj/Screenshot%202018-06-23%2010.01.48.png?dl=1\" width=\"600\"> </center>\n\n논문 저자 : Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver & Daan Wierstra\n논문 링크 : https://arxiv.org/pdf/1509.02971.pdf\nProceeding : International Conference on Learning Representations (ICLR) 2016\n정리 : 양혁렬, 이동민, 차금강\n\n---\n\n# 1. 들어가며...\n\n<br>\n## 1.1 Success & Limination of DQN\n\n-  Success\n    - sensor로부터 나오는 전처리를 거친 input 대신에 raw pixel input을 사용합니다. 이렇게 함으로써 High dimensional observation space 문제를 풀어냅니다.\n- Limitation\n    - discrete & low dimensional action space만 다룰 수 있습니다. Continuous action space를 다루기 위해서는 매 스텝 이를 위한 iterative optimization process를 거쳐야 합니다.\n\n<br>\n## 1.2 Problems of discritization\n\n<center> <img src=\"https://www.dropbox.com/s/nulhzxs8bak2fn6/Screenshot%202018-06-23%2012.22.20.png?dl=1\" width=\"450px\"> </center>\n\n- 만약 7개의 관절을 가진 로봇 팔이 있다면, 가장 간단한 discretization은 각 관절을 다음과 같이 $a_{i}\\in \\\\{ -k, 0, k \\\\}$ 3개의 값을 가지도록 하는 것입니다.\n- 그렇다면 $3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 = 3^{7} = 2187$ 가지의 dimension을 가진 action space가 만들어집니다. 이와같이 Discretization을 하면 action space가 exponential하게 늘어납니다.\n- 충분히 큰 action space 임에도 discretization으로 인한 정보의 손실이 있을 수 있고, 섬세한 Control을 할 수 없습니다.\n\n<br>\n## 1.3 New approach for continuous control\n\n- Model-free, Off-policy, Actor-critic algorithm을 제안합니다.\n- Deep Deterministic Policy(이하 DPG)를 기반으로 합니다.\n- Actor-Critic approach와 DQN의 성공적이었던 부분을 합칩니다.\n    - Replay buffer : 샘플들 사이의 상관관계를 줄여줍니다.\n    - target Q Network : Update 동안 target을 안정적으로 만듭니다.\n\n<br><br>\n\n# 2.Background\n\n<br>\n## 2.1 Notation\n\n- Observation : $x_{t}$\n- Action : $a_t \\in {\\rm IR}^N $\n- Reward : $r_t$\n- Discount factor : $\\gamma$\n- Environment : $E$\n- Policy : $\\pi : S \\rightarrow P(A)  $\n- Transition dynamics : $p(s_{t+1} \\vert s_t, a_t) $\n- Reward function : $r(s_t, a_t)$\n- Return : $ \\sum_{ i=t }^{ T } \\gamma^{(i-t)}r(s_i, a_i) $\n- Discounted state visitation distribution for a policy : $\\rho^\\pi $\n\n<br>\n## 2.2 Bellman Equation\n\n- 상태 $s_t$에서 행동 $a_t$를 취했을 때 Expected return은 다음과 같습니다.\n\n$$Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{i \\geqq t},s_{i \\geqq t} \\backsim E, a_{i \\geqq t} \\backsim \\pi } [R_{t} \\vert s_t, a_t  ]$$\n\n- 벨만 방정식을 사용하여 위의 식을 변형합니다.\n\n$$ Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } \\[r(s_t,a_t)+\\gamma {\\rm E}_{a_{t+1} \\backsim \\pi } \\[ Q^{\\pi}(s_{t+1}, a_{t+1}) \\] \\]$$\n\n- Determinsitc policy를 가정합니다.\n\n$$Q^{\\mu}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } \\[r(s_t,a_t)+\\gamma Q^{\\mu}(s_{t+1}, \\mu (s_{t+1})) \\]$$\n\n- 위의 수식에 대한 추가설명\n    - 두 번째 수식에서 위의 수식으로 내려오면서 policy가 determinstic하기 때문에 policy에 dependent한 Expectation이 빠진 것을 알 수 있습니다.\n    - Deterministic policy를 가정하기 전의 수식에서는 $a_{t+1}$을 골랐던 순간의 policy로 Q에 대한 Expection을 원래 구해야하기 때문에 off-policy가 아니지만, Determinsitic policy를 가정한다면 update 할 당시의 policy로 $a_{t+1}$를 구할 수 있기 때문에 off-policy가 됩니다.\n\n- Q learning\n$$L(\\theta^{Q}) = {\\rm E}_{s_t \\backsim \\rho^\\beta , a_t \\backsim \\beta , r_t \\backsim E} [(Q(s_t, a_t \\vert \\theta^Q)-y_t)^2]$$ \n\n- 위의 수식에 대한 추가설명\n    - $\\beta$는 behavior policy를 의미합니다.\n    - $ y_t = r(s_t, a_t) + \\gamma Q^{\\mu}(s_{t+1},\\mu(s_{t+1}))$  \n    - $\\mu(s) = argmax_{a}Q(s,a)$\n        - Q learning은 위와 같이 $argmax$라는 deterministic policy를 사용하기 때문에 off policy로 사용할 수 있습니다. \n\n<br>\n## 2.3 DPG\n\n$$\\nabla_{\\theta^\\mu} J \\approx  {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{\\theta^\\mu} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)}] = {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{a} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)} \\nabla_{\\theta^\\mu} \\mu(s \\vert Q^{\\mu})\\vert_{s=s_t}]$$\n\n- 위의 수식은 피지여행 DPG 글 4-2.Q-learning을 이용한 off-policy actor-critic에서 이미 정리 한 바 있습니다. [DPG](http://localhost:4000/2018/06/27/2_dpg/)를 참고해주세요.\n\n<br><br>\n\n# 3.Algorithm\n\nContinous control을 위한 새로운 알고리즘을 제안합니다. 제안하는 알고리즘의 특징은 다음과 같습니다.\n\n- Replay buffer를 사용합니다.\n- \"soft\" target update를 사용합니다.\n- 각 차원의 scale이 다른 low dimension vector로 부터 학습할 때 Batch Normalization을 사용합니다. \n- 탐험을 위해 action에 Noise를 추가합니다.\n\n<br>\n## 3.1 Replay buffer\n\n<center> <img src=\"https://www.dropbox.com/s/lc61b8nas1clqme/Screenshot%202018-06-23%2016.32.53.png?dl=1\" width=\"700px\"> </center>\n\n- 큰 State space를 학습하고 일반화 하기위해서는 Neural Network와 같은 non-linear approximator가 필수적이지만 수렴한다는 보장이 없습니다.\n- NFQCA에서는 수렴의 안정성을 위해서 batch learning을 도입합니다. 하지만 NFQCA에서는 업데이트시에 policy를 reset하지 않습니다.\n- DDPG는 DQN에서 사용된 Replay buffer를 사용하여 online batch update를 가능하게 합니다.\n\n<br>\n## 3.2 Soft target update\n\n$$ \\theta^{Q^{'}} \\leftarrow \\tau \\theta^{Q} + (1-\\tau) \\theta^{Q^{'}}$$\n\n$$ \\theta^{\\mu^{'}} \\leftarrow \\tau \\theta^{\\mu} + (1-\\tau) \\theta^{\\mu^{'}}$$\n\n- DQN에서는 일정 주기마다 origin network의 weight를 target network로 직접 복사해서 사용합니다.\n- DDPG에서는 exponential moving average(지수이동평균) 식으로 대체합니다.\n- soft update가 DQN에서 사용했던 방식에 비해 어떤 장점이 있는지는 명확하게 설명되어있지 않지만 stochatic gradient descent와 같이 급격하게 학습이 진행되는 것을 막기 위해 사용하는 것 같습니다.\n\n<br>\n## 3.3 Batch Normalization\n\n<center> <img src=\"https://www.dropbox.com/s/1erxzrgk69x04j8/Screenshot%202018-06-23%2016.51.56.png?dl=1\" width=\"500px\"> </center>\n\n- 서로 scale이 다른 feature를 state로 사용할 때에 Neural Net이 일반화에서 어려움을 겪습니다.\n    - 이걸 해결하기 위해서는 원래 직접 스케일을 조정해주었습니다.\n\n- 하지만 각 layer의 Input을 Unit Gaussian이 되도록 강제하는 BatchNormalization을 사용하여 이 문제를 해결합니다.\n\n<br>\n## 3.4 Noise Process\n\nDDPG 에서는 Exploration을 위해서 output으로 나온 행동에 노이즈를 추가해줍니다.\n\nORNSTEIN UHLENBECK PROCESS(OU)\n\n$$dx_t = \\theta (\\mu - x_t) dt + \\sigma dW_t$$\n\n- OU Process는 평균으로 회귀하는 random process입니다.\n- $\\theta$는 얼마나 빨리 평균으로 회귀할 지를 나타내는 파라미터이며 $\\mu$는 평균을 의미합니다.\n- $\\sigma$는 process의 변동성을 의미하며 $W_t$는 Wiener process를 의미합니다.\n- 따라서 이전의 noise들과 temporally correlated입니다.\n- 위와 같은 temporally correlated noise process를 사용하는 이유는 physical control과 같은 관성이 있는 환경에서 학습 시킬 때 보다 효과적이기 때문입니다.\n\n<br>\n## 3.5 Diagram & Pseudocode \n\n- DDPG의 학습 과정을 간단히 도식화 해본 다이어그램입니다.\n\n<img src=\"https://www.dropbox.com/s/0ffb2c9irctjx2n/Screenshot%202018-06-23%2017.58.57.png?dl=1\"> \n\n- DDPG의 알고리즘 수도코드입니다. 모든 항목을 위에서 설명했으니 순서대로 보시면 이해에 도움이 될것입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/fd0nj7goixfnd6z/Screenshot%202018-06-23%2018.02.13.png?dl=1\" width=\"700px\"> </center>\n\n<br><br>\n\n# 4. Results\n\n<br>\n## 4.1 Variants of DPG\n\n<center> <img src=\"https://www.dropbox.com/s/k8q6tih85lgow5l/Screenshot%202018-06-23%2018.08.05.png?dl=1\" width=\"800px\"> </center>\n\n- original DPG에 batchnorm만 추가(연한 회색), target network만 추가(진한 회색), 둘 다 추가(초록), pixel로만 학습(파랑). Target network가 성능을 가장 좌지우지합니다.\n\n<br>\n## 4.2 Q estimation of DDPG\n\n<center> <img src=\"https://www.dropbox.com/s/vvcfoni0lqrisst/Screenshot%202018-06-23%2018.10.23.png?dl=1\" width=\"650px\"> </center>\n\n- DQN은 Q value를 Over-estimate하는 경향이 있었지만, DDPG는 simple task에 대해서는 잘한다. 복잡한 문제에 대해서는 estimation을 잘 못했지만, 여전히 좋은 Policy를 찾았습니다.\n\n<br>\n## 4.3 Performance Comparison\n\n<center> <img src=\"https://www.dropbox.com/s/u8ibmz9q4kfxh6l/Screenshot%202018-06-23%2018.11.40.png?dl=1\" width=\"800px\"> </center>\n\n- Score는 naive policy를 0, ILQG (planning algorithm)의 mean score를 1점으로 놓았을 때의 점수 Torcs 환경에 대해서만 raw reward를 score로 사용합니다.\n\n<br><br>\n\n# 5. Implementation Details\n\n<br>\n## 5.1 Hyper parameters\n\n- Optimizer : Adam\n    - actor lr : 0.0001, critic lr : 0.001\n- Weight decay(L2) for critic(Q) : 0.001\n- Discount factor : $\\gamma = 0.99 $\n- Soft target updates : $\\tau = 0.001 $\n- Size of replay buffer : 1,000,000\n- Orstein Uhlenbeck Process : $\\theta = 0.15$, $\\sigma = 0.2$\n\n<br>\n## 5.2 Etc.\n\n- Final output layer of actor : tanh (행동의 최소 최대를 맞춰주기 위해서)\n- low-dimentional 문제에서 네트워크는 2개의 hidden layer (1st layer 400 units, 2nd layer 300 units)를 가집니다.\n- 이미지를 통해서 학습시킬 때 : 3 convolutional layers (no pooling) with 32 filters at each layer.\n- actor와 critic 각각의 final layer(weight, bias 모두)는 다음 범위의 uniform distribution에서 샘플링합니다. [ - 0.003, 0.003], [ - 0.0003 , 0.0003]. 이렇게 하는 이유는 가장 처음의 policy와 value의 output이 0에 가깝게 나오도록 하기 위합니다.\n\n<br><br>\n\n# 6.Conclusion\n\n- 이 연구는 최근 딥러닝의 발전과 강화학습을 엮은 것으로 Continuous action space를 가지는 문제를 robust하게 풀어냅니다.\n- non-linear function approximators을 쓰는 것은 수렴을 보장하지 않지만, 여러 환경에 대해서 특별한 조작 없이 안정적으로 수렴하는 것을 실험으로 보여냅니다.\n- Atari 도메인에서 DQN보다 상당히 적은 step만에 수렴하는 것을 실험을 통해서 알아냅니다.\n- model-free 알고리즘은 좋은 solution을 찾기 위해서는 많은 sample을 필요로 한다는 한계가 있지만 더 큰 시스템에서는 이러한 한계를 물리칠 정도로 중요한 역할을 하게 될 것이라고 합니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [DPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/)\n\n<br>\n\n# 다음으로\n\n## [NPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/)","source":"_posts/3_ddpg.md","raw":"---\ntitle: Deep Determinstic Policy Gradient (DDPG)\ndate: 2018-06-26 11:20:45\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 양혁렬, 이동민, 차금강\nsubtitle: 피지여행 3번째 논문\n---\n\n<center > <img src=\"https://www.dropbox.com/s/zv8rk0uf87ipiaj/Screenshot%202018-06-23%2010.01.48.png?dl=1\" width=\"600\"> </center>\n\n논문 저자 : Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver & Daan Wierstra\n논문 링크 : https://arxiv.org/pdf/1509.02971.pdf\nProceeding : International Conference on Learning Representations (ICLR) 2016\n정리 : 양혁렬, 이동민, 차금강\n\n---\n\n# 1. 들어가며...\n\n<br>\n## 1.1 Success & Limination of DQN\n\n-  Success\n    - sensor로부터 나오는 전처리를 거친 input 대신에 raw pixel input을 사용합니다. 이렇게 함으로써 High dimensional observation space 문제를 풀어냅니다.\n- Limitation\n    - discrete & low dimensional action space만 다룰 수 있습니다. Continuous action space를 다루기 위해서는 매 스텝 이를 위한 iterative optimization process를 거쳐야 합니다.\n\n<br>\n## 1.2 Problems of discritization\n\n<center> <img src=\"https://www.dropbox.com/s/nulhzxs8bak2fn6/Screenshot%202018-06-23%2012.22.20.png?dl=1\" width=\"450px\"> </center>\n\n- 만약 7개의 관절을 가진 로봇 팔이 있다면, 가장 간단한 discretization은 각 관절을 다음과 같이 $a_{i}\\in \\\\{ -k, 0, k \\\\}$ 3개의 값을 가지도록 하는 것입니다.\n- 그렇다면 $3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 = 3^{7} = 2187$ 가지의 dimension을 가진 action space가 만들어집니다. 이와같이 Discretization을 하면 action space가 exponential하게 늘어납니다.\n- 충분히 큰 action space 임에도 discretization으로 인한 정보의 손실이 있을 수 있고, 섬세한 Control을 할 수 없습니다.\n\n<br>\n## 1.3 New approach for continuous control\n\n- Model-free, Off-policy, Actor-critic algorithm을 제안합니다.\n- Deep Deterministic Policy(이하 DPG)를 기반으로 합니다.\n- Actor-Critic approach와 DQN의 성공적이었던 부분을 합칩니다.\n    - Replay buffer : 샘플들 사이의 상관관계를 줄여줍니다.\n    - target Q Network : Update 동안 target을 안정적으로 만듭니다.\n\n<br><br>\n\n# 2.Background\n\n<br>\n## 2.1 Notation\n\n- Observation : $x_{t}$\n- Action : $a_t \\in {\\rm IR}^N $\n- Reward : $r_t$\n- Discount factor : $\\gamma$\n- Environment : $E$\n- Policy : $\\pi : S \\rightarrow P(A)  $\n- Transition dynamics : $p(s_{t+1} \\vert s_t, a_t) $\n- Reward function : $r(s_t, a_t)$\n- Return : $ \\sum_{ i=t }^{ T } \\gamma^{(i-t)}r(s_i, a_i) $\n- Discounted state visitation distribution for a policy : $\\rho^\\pi $\n\n<br>\n## 2.2 Bellman Equation\n\n- 상태 $s_t$에서 행동 $a_t$를 취했을 때 Expected return은 다음과 같습니다.\n\n$$Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{i \\geqq t},s_{i \\geqq t} \\backsim E, a_{i \\geqq t} \\backsim \\pi } [R_{t} \\vert s_t, a_t  ]$$\n\n- 벨만 방정식을 사용하여 위의 식을 변형합니다.\n\n$$ Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } \\[r(s_t,a_t)+\\gamma {\\rm E}_{a_{t+1} \\backsim \\pi } \\[ Q^{\\pi}(s_{t+1}, a_{t+1}) \\] \\]$$\n\n- Determinsitc policy를 가정합니다.\n\n$$Q^{\\mu}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } \\[r(s_t,a_t)+\\gamma Q^{\\mu}(s_{t+1}, \\mu (s_{t+1})) \\]$$\n\n- 위의 수식에 대한 추가설명\n    - 두 번째 수식에서 위의 수식으로 내려오면서 policy가 determinstic하기 때문에 policy에 dependent한 Expectation이 빠진 것을 알 수 있습니다.\n    - Deterministic policy를 가정하기 전의 수식에서는 $a_{t+1}$을 골랐던 순간의 policy로 Q에 대한 Expection을 원래 구해야하기 때문에 off-policy가 아니지만, Determinsitic policy를 가정한다면 update 할 당시의 policy로 $a_{t+1}$를 구할 수 있기 때문에 off-policy가 됩니다.\n\n- Q learning\n$$L(\\theta^{Q}) = {\\rm E}_{s_t \\backsim \\rho^\\beta , a_t \\backsim \\beta , r_t \\backsim E} [(Q(s_t, a_t \\vert \\theta^Q)-y_t)^2]$$ \n\n- 위의 수식에 대한 추가설명\n    - $\\beta$는 behavior policy를 의미합니다.\n    - $ y_t = r(s_t, a_t) + \\gamma Q^{\\mu}(s_{t+1},\\mu(s_{t+1}))$  \n    - $\\mu(s) = argmax_{a}Q(s,a)$\n        - Q learning은 위와 같이 $argmax$라는 deterministic policy를 사용하기 때문에 off policy로 사용할 수 있습니다. \n\n<br>\n## 2.3 DPG\n\n$$\\nabla_{\\theta^\\mu} J \\approx  {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{\\theta^\\mu} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)}] = {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{a} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)} \\nabla_{\\theta^\\mu} \\mu(s \\vert Q^{\\mu})\\vert_{s=s_t}]$$\n\n- 위의 수식은 피지여행 DPG 글 4-2.Q-learning을 이용한 off-policy actor-critic에서 이미 정리 한 바 있습니다. [DPG](http://localhost:4000/2018/06/27/2_dpg/)를 참고해주세요.\n\n<br><br>\n\n# 3.Algorithm\n\nContinous control을 위한 새로운 알고리즘을 제안합니다. 제안하는 알고리즘의 특징은 다음과 같습니다.\n\n- Replay buffer를 사용합니다.\n- \"soft\" target update를 사용합니다.\n- 각 차원의 scale이 다른 low dimension vector로 부터 학습할 때 Batch Normalization을 사용합니다. \n- 탐험을 위해 action에 Noise를 추가합니다.\n\n<br>\n## 3.1 Replay buffer\n\n<center> <img src=\"https://www.dropbox.com/s/lc61b8nas1clqme/Screenshot%202018-06-23%2016.32.53.png?dl=1\" width=\"700px\"> </center>\n\n- 큰 State space를 학습하고 일반화 하기위해서는 Neural Network와 같은 non-linear approximator가 필수적이지만 수렴한다는 보장이 없습니다.\n- NFQCA에서는 수렴의 안정성을 위해서 batch learning을 도입합니다. 하지만 NFQCA에서는 업데이트시에 policy를 reset하지 않습니다.\n- DDPG는 DQN에서 사용된 Replay buffer를 사용하여 online batch update를 가능하게 합니다.\n\n<br>\n## 3.2 Soft target update\n\n$$ \\theta^{Q^{'}} \\leftarrow \\tau \\theta^{Q} + (1-\\tau) \\theta^{Q^{'}}$$\n\n$$ \\theta^{\\mu^{'}} \\leftarrow \\tau \\theta^{\\mu} + (1-\\tau) \\theta^{\\mu^{'}}$$\n\n- DQN에서는 일정 주기마다 origin network의 weight를 target network로 직접 복사해서 사용합니다.\n- DDPG에서는 exponential moving average(지수이동평균) 식으로 대체합니다.\n- soft update가 DQN에서 사용했던 방식에 비해 어떤 장점이 있는지는 명확하게 설명되어있지 않지만 stochatic gradient descent와 같이 급격하게 학습이 진행되는 것을 막기 위해 사용하는 것 같습니다.\n\n<br>\n## 3.3 Batch Normalization\n\n<center> <img src=\"https://www.dropbox.com/s/1erxzrgk69x04j8/Screenshot%202018-06-23%2016.51.56.png?dl=1\" width=\"500px\"> </center>\n\n- 서로 scale이 다른 feature를 state로 사용할 때에 Neural Net이 일반화에서 어려움을 겪습니다.\n    - 이걸 해결하기 위해서는 원래 직접 스케일을 조정해주었습니다.\n\n- 하지만 각 layer의 Input을 Unit Gaussian이 되도록 강제하는 BatchNormalization을 사용하여 이 문제를 해결합니다.\n\n<br>\n## 3.4 Noise Process\n\nDDPG 에서는 Exploration을 위해서 output으로 나온 행동에 노이즈를 추가해줍니다.\n\nORNSTEIN UHLENBECK PROCESS(OU)\n\n$$dx_t = \\theta (\\mu - x_t) dt + \\sigma dW_t$$\n\n- OU Process는 평균으로 회귀하는 random process입니다.\n- $\\theta$는 얼마나 빨리 평균으로 회귀할 지를 나타내는 파라미터이며 $\\mu$는 평균을 의미합니다.\n- $\\sigma$는 process의 변동성을 의미하며 $W_t$는 Wiener process를 의미합니다.\n- 따라서 이전의 noise들과 temporally correlated입니다.\n- 위와 같은 temporally correlated noise process를 사용하는 이유는 physical control과 같은 관성이 있는 환경에서 학습 시킬 때 보다 효과적이기 때문입니다.\n\n<br>\n## 3.5 Diagram & Pseudocode \n\n- DDPG의 학습 과정을 간단히 도식화 해본 다이어그램입니다.\n\n<img src=\"https://www.dropbox.com/s/0ffb2c9irctjx2n/Screenshot%202018-06-23%2017.58.57.png?dl=1\"> \n\n- DDPG의 알고리즘 수도코드입니다. 모든 항목을 위에서 설명했으니 순서대로 보시면 이해에 도움이 될것입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/fd0nj7goixfnd6z/Screenshot%202018-06-23%2018.02.13.png?dl=1\" width=\"700px\"> </center>\n\n<br><br>\n\n# 4. Results\n\n<br>\n## 4.1 Variants of DPG\n\n<center> <img src=\"https://www.dropbox.com/s/k8q6tih85lgow5l/Screenshot%202018-06-23%2018.08.05.png?dl=1\" width=\"800px\"> </center>\n\n- original DPG에 batchnorm만 추가(연한 회색), target network만 추가(진한 회색), 둘 다 추가(초록), pixel로만 학습(파랑). Target network가 성능을 가장 좌지우지합니다.\n\n<br>\n## 4.2 Q estimation of DDPG\n\n<center> <img src=\"https://www.dropbox.com/s/vvcfoni0lqrisst/Screenshot%202018-06-23%2018.10.23.png?dl=1\" width=\"650px\"> </center>\n\n- DQN은 Q value를 Over-estimate하는 경향이 있었지만, DDPG는 simple task에 대해서는 잘한다. 복잡한 문제에 대해서는 estimation을 잘 못했지만, 여전히 좋은 Policy를 찾았습니다.\n\n<br>\n## 4.3 Performance Comparison\n\n<center> <img src=\"https://www.dropbox.com/s/u8ibmz9q4kfxh6l/Screenshot%202018-06-23%2018.11.40.png?dl=1\" width=\"800px\"> </center>\n\n- Score는 naive policy를 0, ILQG (planning algorithm)의 mean score를 1점으로 놓았을 때의 점수 Torcs 환경에 대해서만 raw reward를 score로 사용합니다.\n\n<br><br>\n\n# 5. Implementation Details\n\n<br>\n## 5.1 Hyper parameters\n\n- Optimizer : Adam\n    - actor lr : 0.0001, critic lr : 0.001\n- Weight decay(L2) for critic(Q) : 0.001\n- Discount factor : $\\gamma = 0.99 $\n- Soft target updates : $\\tau = 0.001 $\n- Size of replay buffer : 1,000,000\n- Orstein Uhlenbeck Process : $\\theta = 0.15$, $\\sigma = 0.2$\n\n<br>\n## 5.2 Etc.\n\n- Final output layer of actor : tanh (행동의 최소 최대를 맞춰주기 위해서)\n- low-dimentional 문제에서 네트워크는 2개의 hidden layer (1st layer 400 units, 2nd layer 300 units)를 가집니다.\n- 이미지를 통해서 학습시킬 때 : 3 convolutional layers (no pooling) with 32 filters at each layer.\n- actor와 critic 각각의 final layer(weight, bias 모두)는 다음 범위의 uniform distribution에서 샘플링합니다. [ - 0.003, 0.003], [ - 0.0003 , 0.0003]. 이렇게 하는 이유는 가장 처음의 policy와 value의 output이 0에 가깝게 나오도록 하기 위합니다.\n\n<br><br>\n\n# 6.Conclusion\n\n- 이 연구는 최근 딥러닝의 발전과 강화학습을 엮은 것으로 Continuous action space를 가지는 문제를 robust하게 풀어냅니다.\n- non-linear function approximators을 쓰는 것은 수렴을 보장하지 않지만, 여러 환경에 대해서 특별한 조작 없이 안정적으로 수렴하는 것을 실험으로 보여냅니다.\n- Atari 도메인에서 DQN보다 상당히 적은 step만에 수렴하는 것을 실험을 통해서 알아냅니다.\n- model-free 알고리즘은 좋은 solution을 찾기 위해서는 많은 sample을 필요로 한다는 한계가 있지만 더 큰 시스템에서는 이러한 한계를 물리칠 정도로 중요한 역할을 하게 될 것이라고 합니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [DPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/)\n\n<br>\n\n# 다음으로\n\n## [NPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/)","slug":"3_ddpg","published":1,"updated":"2019-02-07T11:21:31.934Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjrujltzn000l5wfeljlyo9o0","content":"<center> <img src=\"https://www.dropbox.com/s/zv8rk0uf87ipiaj/Screenshot%202018-06-23%2010.01.48.png?dl=1\" width=\"600\"> </center>\n\n<p>논문 저자 : Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver &amp; Daan Wierstra<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1509.02971.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1509.02971.pdf</a><br>Proceeding : International Conference on Learning Representations (ICLR) 2016<br>정리 : 양혁렬, 이동민, 차금강</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p><br></p>\n<h2 id=\"1-1-Success-amp-Limination-of-DQN\"><a href=\"#1-1-Success-amp-Limination-of-DQN\" class=\"headerlink\" title=\"1.1 Success &amp; Limination of DQN\"></a>1.1 Success &amp; Limination of DQN</h2><ul>\n<li>Success<ul>\n<li>sensor로부터 나오는 전처리를 거친 input 대신에 raw pixel input을 사용합니다. 이렇게 함으로써 High dimensional observation space 문제를 풀어냅니다.</li>\n</ul>\n</li>\n<li>Limitation<ul>\n<li>discrete &amp; low dimensional action space만 다룰 수 있습니다. Continuous action space를 다루기 위해서는 매 스텝 이를 위한 iterative optimization process를 거쳐야 합니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"1-2-Problems-of-discritization\"><a href=\"#1-2-Problems-of-discritization\" class=\"headerlink\" title=\"1.2 Problems of discritization\"></a>1.2 Problems of discritization</h2><center> <img src=\"https://www.dropbox.com/s/nulhzxs8bak2fn6/Screenshot%202018-06-23%2012.22.20.png?dl=1\" width=\"450px\"> </center>\n\n<ul>\n<li>만약 7개의 관절을 가진 로봇 팔이 있다면, 가장 간단한 discretization은 각 관절을 다음과 같이 $a_{i}\\in \\{ -k, 0, k \\}$ 3개의 값을 가지도록 하는 것입니다.</li>\n<li>그렇다면 $3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 = 3^{7} = 2187$ 가지의 dimension을 가진 action space가 만들어집니다. 이와같이 Discretization을 하면 action space가 exponential하게 늘어납니다.</li>\n<li>충분히 큰 action space 임에도 discretization으로 인한 정보의 손실이 있을 수 있고, 섬세한 Control을 할 수 없습니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"1-3-New-approach-for-continuous-control\"><a href=\"#1-3-New-approach-for-continuous-control\" class=\"headerlink\" title=\"1.3 New approach for continuous control\"></a>1.3 New approach for continuous control</h2><ul>\n<li>Model-free, Off-policy, Actor-critic algorithm을 제안합니다.</li>\n<li>Deep Deterministic Policy(이하 DPG)를 기반으로 합니다.</li>\n<li>Actor-Critic approach와 DQN의 성공적이었던 부분을 합칩니다.<ul>\n<li>Replay buffer : 샘플들 사이의 상관관계를 줄여줍니다.</li>\n<li>target Q Network : Update 동안 target을 안정적으로 만듭니다.</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"2-Background\"><a href=\"#2-Background\" class=\"headerlink\" title=\"2.Background\"></a>2.Background</h1><p><br></p>\n<h2 id=\"2-1-Notation\"><a href=\"#2-1-Notation\" class=\"headerlink\" title=\"2.1 Notation\"></a>2.1 Notation</h2><ul>\n<li>Observation : $x_{t}$</li>\n<li>Action : $a_t \\in {\\rm IR}^N $</li>\n<li>Reward : $r_t$</li>\n<li>Discount factor : $\\gamma$</li>\n<li>Environment : $E$</li>\n<li>Policy : $\\pi : S \\rightarrow P(A)  $</li>\n<li>Transition dynamics : $p(s_{t+1} \\vert s_t, a_t) $</li>\n<li>Reward function : $r(s_t, a_t)$</li>\n<li>Return : $ \\sum_{ i=t }^{ T } \\gamma^{(i-t)}r(s_i, a_i) $</li>\n<li>Discounted state visitation distribution for a policy : $\\rho^\\pi $</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-2-Bellman-Equation\"><a href=\"#2-2-Bellman-Equation\" class=\"headerlink\" title=\"2.2 Bellman Equation\"></a>2.2 Bellman Equation</h2><ul>\n<li>상태 $s_t$에서 행동 $a_t$를 취했을 때 Expected return은 다음과 같습니다.</li>\n</ul>\n<p>$$Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{i \\geqq t},s_{i \\geqq t} \\backsim E, a_{i \\geqq t} \\backsim \\pi } [R_{t} \\vert s_t, a_t  ]$$</p>\n<ul>\n<li>벨만 방정식을 사용하여 위의 식을 변형합니다.</li>\n</ul>\n<p>$$ Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } [r(s_t,a_t)+\\gamma {\\rm E}_{a_{t+1} \\backsim \\pi } [ Q^{\\pi}(s_{t+1}, a_{t+1}) ] ]$$</p>\n<ul>\n<li>Determinsitc policy를 가정합니다.</li>\n</ul>\n<p>$$Q^{\\mu}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } [r(s_t,a_t)+\\gamma Q^{\\mu}(s_{t+1}, \\mu (s_{t+1})) ]$$</p>\n<ul>\n<li><p>위의 수식에 대한 추가설명</p>\n<ul>\n<li>두 번째 수식에서 위의 수식으로 내려오면서 policy가 determinstic하기 때문에 policy에 dependent한 Expectation이 빠진 것을 알 수 있습니다.</li>\n<li>Deterministic policy를 가정하기 전의 수식에서는 $a_{t+1}$을 골랐던 순간의 policy로 Q에 대한 Expection을 원래 구해야하기 때문에 off-policy가 아니지만, Determinsitic policy를 가정한다면 update 할 당시의 policy로 $a_{t+1}$를 구할 수 있기 때문에 off-policy가 됩니다.</li>\n</ul>\n</li>\n<li><p>Q learning<br>$$L(\\theta^{Q}) = {\\rm E}_{s_t \\backsim \\rho^\\beta , a_t \\backsim \\beta , r_t \\backsim E} [(Q(s_t, a_t \\vert \\theta^Q)-y_t)^2]$$ </p>\n</li>\n<li><p>위의 수식에 대한 추가설명</p>\n<ul>\n<li>$\\beta$는 behavior policy를 의미합니다.</li>\n<li>$ y_t = r(s_t, a_t) + \\gamma Q^{\\mu}(s_{t+1},\\mu(s_{t+1}))$  </li>\n<li>$\\mu(s) = argmax_{a}Q(s,a)$<ul>\n<li>Q learning은 위와 같이 $argmax$라는 deterministic policy를 사용하기 때문에 off policy로 사용할 수 있습니다. </li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-3-DPG\"><a href=\"#2-3-DPG\" class=\"headerlink\" title=\"2.3 DPG\"></a>2.3 DPG</h2><p>$$\\nabla_{\\theta^\\mu} J \\approx  {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{\\theta^\\mu} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)}] = {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{a} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)} \\nabla_{\\theta^\\mu} \\mu(s \\vert Q^{\\mu})\\vert_{s=s_t}]$$</p>\n<ul>\n<li>위의 수식은 피지여행 DPG 글 4-2.Q-learning을 이용한 off-policy actor-critic에서 이미 정리 한 바 있습니다. <a href=\"http://localhost:4000/2018/06/27/2_dpg/\" target=\"_blank\" rel=\"noopener\">DPG</a>를 참고해주세요.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"3-Algorithm\"><a href=\"#3-Algorithm\" class=\"headerlink\" title=\"3.Algorithm\"></a>3.Algorithm</h1><p>Continous control을 위한 새로운 알고리즘을 제안합니다. 제안하는 알고리즘의 특징은 다음과 같습니다.</p>\n<ul>\n<li>Replay buffer를 사용합니다.</li>\n<li>“soft” target update를 사용합니다.</li>\n<li>각 차원의 scale이 다른 low dimension vector로 부터 학습할 때 Batch Normalization을 사용합니다. </li>\n<li>탐험을 위해 action에 Noise를 추가합니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-1-Replay-buffer\"><a href=\"#3-1-Replay-buffer\" class=\"headerlink\" title=\"3.1 Replay buffer\"></a>3.1 Replay buffer</h2><center> <img src=\"https://www.dropbox.com/s/lc61b8nas1clqme/Screenshot%202018-06-23%2016.32.53.png?dl=1\" width=\"700px\"> </center>\n\n<ul>\n<li>큰 State space를 학습하고 일반화 하기위해서는 Neural Network와 같은 non-linear approximator가 필수적이지만 수렴한다는 보장이 없습니다.</li>\n<li>NFQCA에서는 수렴의 안정성을 위해서 batch learning을 도입합니다. 하지만 NFQCA에서는 업데이트시에 policy를 reset하지 않습니다.</li>\n<li>DDPG는 DQN에서 사용된 Replay buffer를 사용하여 online batch update를 가능하게 합니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-2-Soft-target-update\"><a href=\"#3-2-Soft-target-update\" class=\"headerlink\" title=\"3.2 Soft target update\"></a>3.2 Soft target update</h2><p>$$ \\theta^{Q^{‘}} \\leftarrow \\tau \\theta^{Q} + (1-\\tau) \\theta^{Q^{‘}}$$</p>\n<p>$$ \\theta^{\\mu^{‘}} \\leftarrow \\tau \\theta^{\\mu} + (1-\\tau) \\theta^{\\mu^{‘}}$$</p>\n<ul>\n<li>DQN에서는 일정 주기마다 origin network의 weight를 target network로 직접 복사해서 사용합니다.</li>\n<li>DDPG에서는 exponential moving average(지수이동평균) 식으로 대체합니다.</li>\n<li>soft update가 DQN에서 사용했던 방식에 비해 어떤 장점이 있는지는 명확하게 설명되어있지 않지만 stochatic gradient descent와 같이 급격하게 학습이 진행되는 것을 막기 위해 사용하는 것 같습니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-3-Batch-Normalization\"><a href=\"#3-3-Batch-Normalization\" class=\"headerlink\" title=\"3.3 Batch Normalization\"></a>3.3 Batch Normalization</h2><center> <img src=\"https://www.dropbox.com/s/1erxzrgk69x04j8/Screenshot%202018-06-23%2016.51.56.png?dl=1\" width=\"500px\"> </center>\n\n<ul>\n<li><p>서로 scale이 다른 feature를 state로 사용할 때에 Neural Net이 일반화에서 어려움을 겪습니다.</p>\n<ul>\n<li>이걸 해결하기 위해서는 원래 직접 스케일을 조정해주었습니다.</li>\n</ul>\n</li>\n<li><p>하지만 각 layer의 Input을 Unit Gaussian이 되도록 강제하는 BatchNormalization을 사용하여 이 문제를 해결합니다.</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-4-Noise-Process\"><a href=\"#3-4-Noise-Process\" class=\"headerlink\" title=\"3.4 Noise Process\"></a>3.4 Noise Process</h2><p>DDPG 에서는 Exploration을 위해서 output으로 나온 행동에 노이즈를 추가해줍니다.</p>\n<p>ORNSTEIN UHLENBECK PROCESS(OU)</p>\n<p>$$dx_t = \\theta (\\mu - x_t) dt + \\sigma dW_t$$</p>\n<ul>\n<li>OU Process는 평균으로 회귀하는 random process입니다.</li>\n<li>$\\theta$는 얼마나 빨리 평균으로 회귀할 지를 나타내는 파라미터이며 $\\mu$는 평균을 의미합니다.</li>\n<li>$\\sigma$는 process의 변동성을 의미하며 $W_t$는 Wiener process를 의미합니다.</li>\n<li>따라서 이전의 noise들과 temporally correlated입니다.</li>\n<li>위와 같은 temporally correlated noise process를 사용하는 이유는 physical control과 같은 관성이 있는 환경에서 학습 시킬 때 보다 효과적이기 때문입니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-5-Diagram-amp-Pseudocode\"><a href=\"#3-5-Diagram-amp-Pseudocode\" class=\"headerlink\" title=\"3.5 Diagram &amp; Pseudocode\"></a>3.5 Diagram &amp; Pseudocode</h2><ul>\n<li>DDPG의 학습 과정을 간단히 도식화 해본 다이어그램입니다.</li>\n</ul>\n<p><img src=\"https://www.dropbox.com/s/0ffb2c9irctjx2n/Screenshot%202018-06-23%2017.58.57.png?dl=1\"> </p>\n<ul>\n<li>DDPG의 알고리즘 수도코드입니다. 모든 항목을 위에서 설명했으니 순서대로 보시면 이해에 도움이 될것입니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/fd0nj7goixfnd6z/Screenshot%202018-06-23%2018.02.13.png?dl=1\" width=\"700px\"> </center>\n\n<p><br><br></p>\n<h1 id=\"4-Results\"><a href=\"#4-Results\" class=\"headerlink\" title=\"4. Results\"></a>4. Results</h1><p><br></p>\n<h2 id=\"4-1-Variants-of-DPG\"><a href=\"#4-1-Variants-of-DPG\" class=\"headerlink\" title=\"4.1 Variants of DPG\"></a>4.1 Variants of DPG</h2><center> <img src=\"https://www.dropbox.com/s/k8q6tih85lgow5l/Screenshot%202018-06-23%2018.08.05.png?dl=1\" width=\"800px\"> </center>\n\n<ul>\n<li>original DPG에 batchnorm만 추가(연한 회색), target network만 추가(진한 회색), 둘 다 추가(초록), pixel로만 학습(파랑). Target network가 성능을 가장 좌지우지합니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"4-2-Q-estimation-of-DDPG\"><a href=\"#4-2-Q-estimation-of-DDPG\" class=\"headerlink\" title=\"4.2 Q estimation of DDPG\"></a>4.2 Q estimation of DDPG</h2><center> <img src=\"https://www.dropbox.com/s/vvcfoni0lqrisst/Screenshot%202018-06-23%2018.10.23.png?dl=1\" width=\"650px\"> </center>\n\n<ul>\n<li>DQN은 Q value를 Over-estimate하는 경향이 있었지만, DDPG는 simple task에 대해서는 잘한다. 복잡한 문제에 대해서는 estimation을 잘 못했지만, 여전히 좋은 Policy를 찾았습니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"4-3-Performance-Comparison\"><a href=\"#4-3-Performance-Comparison\" class=\"headerlink\" title=\"4.3 Performance Comparison\"></a>4.3 Performance Comparison</h2><center> <img src=\"https://www.dropbox.com/s/u8ibmz9q4kfxh6l/Screenshot%202018-06-23%2018.11.40.png?dl=1\" width=\"800px\"> </center>\n\n<ul>\n<li>Score는 naive policy를 0, ILQG (planning algorithm)의 mean score를 1점으로 놓았을 때의 점수 Torcs 환경에 대해서만 raw reward를 score로 사용합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"5-Implementation-Details\"><a href=\"#5-Implementation-Details\" class=\"headerlink\" title=\"5. Implementation Details\"></a>5. Implementation Details</h1><p><br></p>\n<h2 id=\"5-1-Hyper-parameters\"><a href=\"#5-1-Hyper-parameters\" class=\"headerlink\" title=\"5.1 Hyper parameters\"></a>5.1 Hyper parameters</h2><ul>\n<li>Optimizer : Adam<ul>\n<li>actor lr : 0.0001, critic lr : 0.001</li>\n</ul>\n</li>\n<li>Weight decay(L2) for critic(Q) : 0.001</li>\n<li>Discount factor : $\\gamma = 0.99 $</li>\n<li>Soft target updates : $\\tau = 0.001 $</li>\n<li>Size of replay buffer : 1,000,000</li>\n<li>Orstein Uhlenbeck Process : $\\theta = 0.15$, $\\sigma = 0.2$</li>\n</ul>\n<p><br></p>\n<h2 id=\"5-2-Etc\"><a href=\"#5-2-Etc\" class=\"headerlink\" title=\"5.2 Etc.\"></a>5.2 Etc.</h2><ul>\n<li>Final output layer of actor : tanh (행동의 최소 최대를 맞춰주기 위해서)</li>\n<li>low-dimentional 문제에서 네트워크는 2개의 hidden layer (1st layer 400 units, 2nd layer 300 units)를 가집니다.</li>\n<li>이미지를 통해서 학습시킬 때 : 3 convolutional layers (no pooling) with 32 filters at each layer.</li>\n<li>actor와 critic 각각의 final layer(weight, bias 모두)는 다음 범위의 uniform distribution에서 샘플링합니다. [ - 0.003, 0.003], [ - 0.0003 , 0.0003]. 이렇게 하는 이유는 가장 처음의 policy와 value의 output이 0에 가깝게 나오도록 하기 위합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"6-Conclusion\"><a href=\"#6-Conclusion\" class=\"headerlink\" title=\"6.Conclusion\"></a>6.Conclusion</h1><ul>\n<li>이 연구는 최근 딥러닝의 발전과 강화학습을 엮은 것으로 Continuous action space를 가지는 문제를 robust하게 풀어냅니다.</li>\n<li>non-linear function approximators을 쓰는 것은 수렴을 보장하지 않지만, 여러 환경에 대해서 특별한 조작 없이 안정적으로 수렴하는 것을 실험으로 보여냅니다.</li>\n<li>Atari 도메인에서 DQN보다 상당히 적은 step만에 수렴하는 것을 실험을 통해서 알아냅니다.</li>\n<li>model-free 알고리즘은 좋은 solution을 찾기 위해서는 많은 sample을 필요로 한다는 한계가 있지만 더 큰 시스템에서는 이러한 한계를 물리칠 정도로 중요한 역할을 하게 될 것이라고 합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"DPG-여행하기\"><a href=\"#DPG-여행하기\" class=\"headerlink\" title=\"DPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/\">DPG 여행하기</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"NPG-여행하기\"><a href=\"#NPG-여행하기\" class=\"headerlink\" title=\"NPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/\">NPG 여행하기</a></h2>","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"https://www.dropbox.com/s/zv8rk0uf87ipiaj/Screenshot%202018-06-23%2010.01.48.png?dl=1\" width=\"600\"> </center>\n\n<p>논문 저자 : Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver &amp; Daan Wierstra<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1509.02971.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1509.02971.pdf</a><br>Proceeding : International Conference on Learning Representations (ICLR) 2016<br>정리 : 양혁렬, 이동민, 차금강</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p><br></p>\n<h2 id=\"1-1-Success-amp-Limination-of-DQN\"><a href=\"#1-1-Success-amp-Limination-of-DQN\" class=\"headerlink\" title=\"1.1 Success &amp; Limination of DQN\"></a>1.1 Success &amp; Limination of DQN</h2><ul>\n<li>Success<ul>\n<li>sensor로부터 나오는 전처리를 거친 input 대신에 raw pixel input을 사용합니다. 이렇게 함으로써 High dimensional observation space 문제를 풀어냅니다.</li>\n</ul>\n</li>\n<li>Limitation<ul>\n<li>discrete &amp; low dimensional action space만 다룰 수 있습니다. Continuous action space를 다루기 위해서는 매 스텝 이를 위한 iterative optimization process를 거쳐야 합니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"1-2-Problems-of-discritization\"><a href=\"#1-2-Problems-of-discritization\" class=\"headerlink\" title=\"1.2 Problems of discritization\"></a>1.2 Problems of discritization</h2><center> <img src=\"https://www.dropbox.com/s/nulhzxs8bak2fn6/Screenshot%202018-06-23%2012.22.20.png?dl=1\" width=\"450px\"> </center>\n\n<ul>\n<li>만약 7개의 관절을 가진 로봇 팔이 있다면, 가장 간단한 discretization은 각 관절을 다음과 같이 $a_{i}\\in \\{ -k, 0, k \\}$ 3개의 값을 가지도록 하는 것입니다.</li>\n<li>그렇다면 $3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 \\times 3 = 3^{7} = 2187$ 가지의 dimension을 가진 action space가 만들어집니다. 이와같이 Discretization을 하면 action space가 exponential하게 늘어납니다.</li>\n<li>충분히 큰 action space 임에도 discretization으로 인한 정보의 손실이 있을 수 있고, 섬세한 Control을 할 수 없습니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"1-3-New-approach-for-continuous-control\"><a href=\"#1-3-New-approach-for-continuous-control\" class=\"headerlink\" title=\"1.3 New approach for continuous control\"></a>1.3 New approach for continuous control</h2><ul>\n<li>Model-free, Off-policy, Actor-critic algorithm을 제안합니다.</li>\n<li>Deep Deterministic Policy(이하 DPG)를 기반으로 합니다.</li>\n<li>Actor-Critic approach와 DQN의 성공적이었던 부분을 합칩니다.<ul>\n<li>Replay buffer : 샘플들 사이의 상관관계를 줄여줍니다.</li>\n<li>target Q Network : Update 동안 target을 안정적으로 만듭니다.</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"2-Background\"><a href=\"#2-Background\" class=\"headerlink\" title=\"2.Background\"></a>2.Background</h1><p><br></p>\n<h2 id=\"2-1-Notation\"><a href=\"#2-1-Notation\" class=\"headerlink\" title=\"2.1 Notation\"></a>2.1 Notation</h2><ul>\n<li>Observation : $x_{t}$</li>\n<li>Action : $a_t \\in {\\rm IR}^N $</li>\n<li>Reward : $r_t$</li>\n<li>Discount factor : $\\gamma$</li>\n<li>Environment : $E$</li>\n<li>Policy : $\\pi : S \\rightarrow P(A)  $</li>\n<li>Transition dynamics : $p(s_{t+1} \\vert s_t, a_t) $</li>\n<li>Reward function : $r(s_t, a_t)$</li>\n<li>Return : $ \\sum_{ i=t }^{ T } \\gamma^{(i-t)}r(s_i, a_i) $</li>\n<li>Discounted state visitation distribution for a policy : $\\rho^\\pi $</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-2-Bellman-Equation\"><a href=\"#2-2-Bellman-Equation\" class=\"headerlink\" title=\"2.2 Bellman Equation\"></a>2.2 Bellman Equation</h2><ul>\n<li>상태 $s_t$에서 행동 $a_t$를 취했을 때 Expected return은 다음과 같습니다.</li>\n</ul>\n<p>$$Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{i \\geqq t},s_{i \\geqq t} \\backsim E, a_{i \\geqq t} \\backsim \\pi } [R_{t} \\vert s_t, a_t  ]$$</p>\n<ul>\n<li>벨만 방정식을 사용하여 위의 식을 변형합니다.</li>\n</ul>\n<p>$$ Q^{\\pi}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } [r(s_t,a_t)+\\gamma {\\rm E}_{a_{t+1} \\backsim \\pi } [ Q^{\\pi}(s_{t+1}, a_{t+1}) ] ]$$</p>\n<ul>\n<li>Determinsitc policy를 가정합니다.</li>\n</ul>\n<p>$$Q^{\\mu}(s_t, a_t)={\\rm E}_{r_{t},s_{t} \\backsim E } [r(s_t,a_t)+\\gamma Q^{\\mu}(s_{t+1}, \\mu (s_{t+1})) ]$$</p>\n<ul>\n<li><p>위의 수식에 대한 추가설명</p>\n<ul>\n<li>두 번째 수식에서 위의 수식으로 내려오면서 policy가 determinstic하기 때문에 policy에 dependent한 Expectation이 빠진 것을 알 수 있습니다.</li>\n<li>Deterministic policy를 가정하기 전의 수식에서는 $a_{t+1}$을 골랐던 순간의 policy로 Q에 대한 Expection을 원래 구해야하기 때문에 off-policy가 아니지만, Determinsitic policy를 가정한다면 update 할 당시의 policy로 $a_{t+1}$를 구할 수 있기 때문에 off-policy가 됩니다.</li>\n</ul>\n</li>\n<li><p>Q learning<br>$$L(\\theta^{Q}) = {\\rm E}_{s_t \\backsim \\rho^\\beta , a_t \\backsim \\beta , r_t \\backsim E} [(Q(s_t, a_t \\vert \\theta^Q)-y_t)^2]$$ </p>\n</li>\n<li><p>위의 수식에 대한 추가설명</p>\n<ul>\n<li>$\\beta$는 behavior policy를 의미합니다.</li>\n<li>$ y_t = r(s_t, a_t) + \\gamma Q^{\\mu}(s_{t+1},\\mu(s_{t+1}))$  </li>\n<li>$\\mu(s) = argmax_{a}Q(s,a)$<ul>\n<li>Q learning은 위와 같이 $argmax$라는 deterministic policy를 사용하기 때문에 off policy로 사용할 수 있습니다. </li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-3-DPG\"><a href=\"#2-3-DPG\" class=\"headerlink\" title=\"2.3 DPG\"></a>2.3 DPG</h2><p>$$\\nabla_{\\theta^\\mu} J \\approx  {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{\\theta^\\mu} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)}] = {\\rm E}_{s_t \\backsim \\rho^\\beta} [ \\nabla_{a} Q(s, a \\vert \\theta ^ Q) \\vert_{s=s_t, a=\\mu(s_t)} \\nabla_{\\theta^\\mu} \\mu(s \\vert Q^{\\mu})\\vert_{s=s_t}]$$</p>\n<ul>\n<li>위의 수식은 피지여행 DPG 글 4-2.Q-learning을 이용한 off-policy actor-critic에서 이미 정리 한 바 있습니다. <a href=\"http://localhost:4000/2018/06/27/2_dpg/\" target=\"_blank\" rel=\"noopener\">DPG</a>를 참고해주세요.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"3-Algorithm\"><a href=\"#3-Algorithm\" class=\"headerlink\" title=\"3.Algorithm\"></a>3.Algorithm</h1><p>Continous control을 위한 새로운 알고리즘을 제안합니다. 제안하는 알고리즘의 특징은 다음과 같습니다.</p>\n<ul>\n<li>Replay buffer를 사용합니다.</li>\n<li>“soft” target update를 사용합니다.</li>\n<li>각 차원의 scale이 다른 low dimension vector로 부터 학습할 때 Batch Normalization을 사용합니다. </li>\n<li>탐험을 위해 action에 Noise를 추가합니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-1-Replay-buffer\"><a href=\"#3-1-Replay-buffer\" class=\"headerlink\" title=\"3.1 Replay buffer\"></a>3.1 Replay buffer</h2><center> <img src=\"https://www.dropbox.com/s/lc61b8nas1clqme/Screenshot%202018-06-23%2016.32.53.png?dl=1\" width=\"700px\"> </center>\n\n<ul>\n<li>큰 State space를 학습하고 일반화 하기위해서는 Neural Network와 같은 non-linear approximator가 필수적이지만 수렴한다는 보장이 없습니다.</li>\n<li>NFQCA에서는 수렴의 안정성을 위해서 batch learning을 도입합니다. 하지만 NFQCA에서는 업데이트시에 policy를 reset하지 않습니다.</li>\n<li>DDPG는 DQN에서 사용된 Replay buffer를 사용하여 online batch update를 가능하게 합니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-2-Soft-target-update\"><a href=\"#3-2-Soft-target-update\" class=\"headerlink\" title=\"3.2 Soft target update\"></a>3.2 Soft target update</h2><p>$$ \\theta^{Q^{‘}} \\leftarrow \\tau \\theta^{Q} + (1-\\tau) \\theta^{Q^{‘}}$$</p>\n<p>$$ \\theta^{\\mu^{‘}} \\leftarrow \\tau \\theta^{\\mu} + (1-\\tau) \\theta^{\\mu^{‘}}$$</p>\n<ul>\n<li>DQN에서는 일정 주기마다 origin network의 weight를 target network로 직접 복사해서 사용합니다.</li>\n<li>DDPG에서는 exponential moving average(지수이동평균) 식으로 대체합니다.</li>\n<li>soft update가 DQN에서 사용했던 방식에 비해 어떤 장점이 있는지는 명확하게 설명되어있지 않지만 stochatic gradient descent와 같이 급격하게 학습이 진행되는 것을 막기 위해 사용하는 것 같습니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-3-Batch-Normalization\"><a href=\"#3-3-Batch-Normalization\" class=\"headerlink\" title=\"3.3 Batch Normalization\"></a>3.3 Batch Normalization</h2><center> <img src=\"https://www.dropbox.com/s/1erxzrgk69x04j8/Screenshot%202018-06-23%2016.51.56.png?dl=1\" width=\"500px\"> </center>\n\n<ul>\n<li><p>서로 scale이 다른 feature를 state로 사용할 때에 Neural Net이 일반화에서 어려움을 겪습니다.</p>\n<ul>\n<li>이걸 해결하기 위해서는 원래 직접 스케일을 조정해주었습니다.</li>\n</ul>\n</li>\n<li><p>하지만 각 layer의 Input을 Unit Gaussian이 되도록 강제하는 BatchNormalization을 사용하여 이 문제를 해결합니다.</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-4-Noise-Process\"><a href=\"#3-4-Noise-Process\" class=\"headerlink\" title=\"3.4 Noise Process\"></a>3.4 Noise Process</h2><p>DDPG 에서는 Exploration을 위해서 output으로 나온 행동에 노이즈를 추가해줍니다.</p>\n<p>ORNSTEIN UHLENBECK PROCESS(OU)</p>\n<p>$$dx_t = \\theta (\\mu - x_t) dt + \\sigma dW_t$$</p>\n<ul>\n<li>OU Process는 평균으로 회귀하는 random process입니다.</li>\n<li>$\\theta$는 얼마나 빨리 평균으로 회귀할 지를 나타내는 파라미터이며 $\\mu$는 평균을 의미합니다.</li>\n<li>$\\sigma$는 process의 변동성을 의미하며 $W_t$는 Wiener process를 의미합니다.</li>\n<li>따라서 이전의 noise들과 temporally correlated입니다.</li>\n<li>위와 같은 temporally correlated noise process를 사용하는 이유는 physical control과 같은 관성이 있는 환경에서 학습 시킬 때 보다 효과적이기 때문입니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-5-Diagram-amp-Pseudocode\"><a href=\"#3-5-Diagram-amp-Pseudocode\" class=\"headerlink\" title=\"3.5 Diagram &amp; Pseudocode\"></a>3.5 Diagram &amp; Pseudocode</h2><ul>\n<li>DDPG의 학습 과정을 간단히 도식화 해본 다이어그램입니다.</li>\n</ul>\n<p><img src=\"https://www.dropbox.com/s/0ffb2c9irctjx2n/Screenshot%202018-06-23%2017.58.57.png?dl=1\"> </p>\n<ul>\n<li>DDPG의 알고리즘 수도코드입니다. 모든 항목을 위에서 설명했으니 순서대로 보시면 이해에 도움이 될것입니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/fd0nj7goixfnd6z/Screenshot%202018-06-23%2018.02.13.png?dl=1\" width=\"700px\"> </center>\n\n<p><br><br></p>\n<h1 id=\"4-Results\"><a href=\"#4-Results\" class=\"headerlink\" title=\"4. Results\"></a>4. Results</h1><p><br></p>\n<h2 id=\"4-1-Variants-of-DPG\"><a href=\"#4-1-Variants-of-DPG\" class=\"headerlink\" title=\"4.1 Variants of DPG\"></a>4.1 Variants of DPG</h2><center> <img src=\"https://www.dropbox.com/s/k8q6tih85lgow5l/Screenshot%202018-06-23%2018.08.05.png?dl=1\" width=\"800px\"> </center>\n\n<ul>\n<li>original DPG에 batchnorm만 추가(연한 회색), target network만 추가(진한 회색), 둘 다 추가(초록), pixel로만 학습(파랑). Target network가 성능을 가장 좌지우지합니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"4-2-Q-estimation-of-DDPG\"><a href=\"#4-2-Q-estimation-of-DDPG\" class=\"headerlink\" title=\"4.2 Q estimation of DDPG\"></a>4.2 Q estimation of DDPG</h2><center> <img src=\"https://www.dropbox.com/s/vvcfoni0lqrisst/Screenshot%202018-06-23%2018.10.23.png?dl=1\" width=\"650px\"> </center>\n\n<ul>\n<li>DQN은 Q value를 Over-estimate하는 경향이 있었지만, DDPG는 simple task에 대해서는 잘한다. 복잡한 문제에 대해서는 estimation을 잘 못했지만, 여전히 좋은 Policy를 찾았습니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"4-3-Performance-Comparison\"><a href=\"#4-3-Performance-Comparison\" class=\"headerlink\" title=\"4.3 Performance Comparison\"></a>4.3 Performance Comparison</h2><center> <img src=\"https://www.dropbox.com/s/u8ibmz9q4kfxh6l/Screenshot%202018-06-23%2018.11.40.png?dl=1\" width=\"800px\"> </center>\n\n<ul>\n<li>Score는 naive policy를 0, ILQG (planning algorithm)의 mean score를 1점으로 놓았을 때의 점수 Torcs 환경에 대해서만 raw reward를 score로 사용합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"5-Implementation-Details\"><a href=\"#5-Implementation-Details\" class=\"headerlink\" title=\"5. Implementation Details\"></a>5. Implementation Details</h1><p><br></p>\n<h2 id=\"5-1-Hyper-parameters\"><a href=\"#5-1-Hyper-parameters\" class=\"headerlink\" title=\"5.1 Hyper parameters\"></a>5.1 Hyper parameters</h2><ul>\n<li>Optimizer : Adam<ul>\n<li>actor lr : 0.0001, critic lr : 0.001</li>\n</ul>\n</li>\n<li>Weight decay(L2) for critic(Q) : 0.001</li>\n<li>Discount factor : $\\gamma = 0.99 $</li>\n<li>Soft target updates : $\\tau = 0.001 $</li>\n<li>Size of replay buffer : 1,000,000</li>\n<li>Orstein Uhlenbeck Process : $\\theta = 0.15$, $\\sigma = 0.2$</li>\n</ul>\n<p><br></p>\n<h2 id=\"5-2-Etc\"><a href=\"#5-2-Etc\" class=\"headerlink\" title=\"5.2 Etc.\"></a>5.2 Etc.</h2><ul>\n<li>Final output layer of actor : tanh (행동의 최소 최대를 맞춰주기 위해서)</li>\n<li>low-dimentional 문제에서 네트워크는 2개의 hidden layer (1st layer 400 units, 2nd layer 300 units)를 가집니다.</li>\n<li>이미지를 통해서 학습시킬 때 : 3 convolutional layers (no pooling) with 32 filters at each layer.</li>\n<li>actor와 critic 각각의 final layer(weight, bias 모두)는 다음 범위의 uniform distribution에서 샘플링합니다. [ - 0.003, 0.003], [ - 0.0003 , 0.0003]. 이렇게 하는 이유는 가장 처음의 policy와 value의 output이 0에 가깝게 나오도록 하기 위합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"6-Conclusion\"><a href=\"#6-Conclusion\" class=\"headerlink\" title=\"6.Conclusion\"></a>6.Conclusion</h1><ul>\n<li>이 연구는 최근 딥러닝의 발전과 강화학습을 엮은 것으로 Continuous action space를 가지는 문제를 robust하게 풀어냅니다.</li>\n<li>non-linear function approximators을 쓰는 것은 수렴을 보장하지 않지만, 여러 환경에 대해서 특별한 조작 없이 안정적으로 수렴하는 것을 실험으로 보여냅니다.</li>\n<li>Atari 도메인에서 DQN보다 상당히 적은 step만에 수렴하는 것을 실험을 통해서 알아냅니다.</li>\n<li>model-free 알고리즘은 좋은 solution을 찾기 위해서는 많은 sample을 필요로 한다는 한계가 있지만 더 큰 시스템에서는 이러한 한계를 물리칠 정도로 중요한 역할을 하게 될 것이라고 합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"DPG-여행하기\"><a href=\"#DPG-여행하기\" class=\"headerlink\" title=\"DPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/\">DPG 여행하기</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"NPG-여행하기\"><a href=\"#NPG-여행하기\" class=\"headerlink\" title=\"NPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/\">NPG 여행하기</a></h2>"},{"title":"A Distributional Perspective on Reinforcement Learning","date":"2018-10-02T05:18:32.000Z","author":"민규식","subtitle":"Distributional RL 1번째 논문","_content":"\n<center> <img src=\"/img/paper_c51.png\" width=\"800\"> </center>\n\n논문 저자 : [Marc G. Bellemare](https://arxiv.org/search/cs?searchtype=author&query=Bellemare%2C+M+G), [Will Dabney](https://arxiv.org/search/cs?searchtype=author&query=Dabney%2C+W), [Rémi Munos](https://arxiv.org/search/cs?searchtype=author&query=Munos%2C+R)    \n논문 링크 : [ArXiv](https://arxiv.org/abs/1707.06887)\nProceeding : International Conference on Machine Learning (ICML) 2017        \n정리 : 민규식\n\n\n\n## Introduction\n\n본 게시물은 2017년에 발표된 논문 [A Distributional Perspective on Reinforcement Learning](https://arxiv.org/abs/1707.06887) 의 내용에 대해 설명합니다.\n\nDistributional RL에 대해 설명한 게시물에서도 언급했듯이 distributional RL 알고리즘은 value를 하나의 scalar 값이 아닌 distribution으로 예측합니다.\n\n<p align=\"center\">\n\n<img src=\"/img/distributionalRL.png\" alt=\"distributional RL\" width=\"600\"/>\n\n</p>\n\n 이에 따라 일반적인 강화학습에서 이용하는 `bellman equation`의 value Q 대신 distribution Z를 사용합니다. 이 bellman equation을 `distributional bellman equation` 이라고 합니다. 해당 식들은 다음과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"/img/bellman_equation.png\" alt=\"distributional RL\" width=\"600\"/>\n\n</p>\n\n<br>\n\n이에 따라 network의 output도 각 알고리즘에 따라 차이가 있습니다. \n\n[DQN](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning/)에서 network의 output이 각 action에 대한 Q-value였다면 distributional RL에서 network의 output은 아래 그림과 같이 각 action에 대한 value distribution 입니다. \n\n<p align=\"center\">\n\n <img src=\"/img/network_output.png\" alt=\"Network\" width=\"800\"/>\n\n</p>\n\n<br>\n\n## Value Distribution\n\nAction에 대한 Distribution에 대해서 살펴보도록 하겠습니다. \n\n<p align=\"center\">\n\n <img src=\"/img/support_and_prob.png\" alt=\"Distribution\" width=\"800\"/>\n\n</p>\n\nNetwork의 output은 위와 같은 이산 확률분포(Discrete Probability Distribution) 입니다. 여기서 가로축은 **Support** 혹은 **atom**이라고 부르며 value값을 나타냅니다. 세로축은 확률을 나타냅니다. 즉, 이 분포는 각각의 value와 그 value를 받을 확률을 나타내는 분포입니다.\n\n\n\nC51의 경우 분포를 결정해주기 위해서 몇가지 파라미터들이 필요합니다. 해당 파라미터들은 다음과 같습니다.\n\n- Support의 수\n- Support의 최대값\n- Support의 최소값\n\n위 그림에서도 볼 수 있듯이 support값은 최소값부터 최대값까지 support의 수에 맞게 일정한 간격으로 나누게 됩니다. 즉, supports는 이렇게 미리 결정된 파라미터들에 의해 그 값이 정해지게 됩니다. 네트워크는 바로 이 support들에 대한 확률을 구해주게 됩니다. 각 action에 대해서 하나의 distribution이 필요하기 때문에 **network output의 크기는 [support의 수 * action의 수]** 가 됩니다.  \n\n<br>\n\n## Algorithm\n\n알고리즘의 진행은 DQN과 거의 동일합니다. `Experience Replay` 나 `Target Network`  같은 기법도 그대로 이용합니다. 차이점은 아래 3가지 정도입니다. \n\n1. Q-value 계산\n2. Loss\n3. Target distribution 구하기\n\n각각 어떻게 차이가 있는지 한번 살펴보도록 하겠습니다. \n\n<br>\n\n### 1. Q-value 계산\n\n\n\nC51 알고리즘에서 Q-value를 계산하는 방법은 이산확률분포의 기대값을 구하는 것입니다. 이에 따라 각 action에 대한 Q-value를 계산하는 식은 다음과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"/img/Equation_Q_c51.png\" alt=\"Equation of Q-value\" width=\"400\"/>\n\n</p>\n\n 수식에서 사용된 각 기호는 위에 있는 Value distribution 그림을 참고해주세요! 각 action의 distribution마다 위의 식에 따라 연산을 수행하고 최대의 Q-value를 가지는 action을 선택하면 됩니다. \n\n 위 수식처럼 분포의 기대값을 구하는 연산을 하면 결과적으로 Q-value는 하나의 scalar값을 가지게 됩니다. 이 경우 \"뭐지... 결국 scalar값으로 action 선택하자나...\" 라고 생각하실수도 있지만!! 단순히 하나의 기대값을 추정하는 것보다 정확하게 분포를 추정하고 그 기대값을 구하는 것이 더 정확한 예측이 될 것이라 생각할 수 있습니다. \n\n<br>\n\n### 2. Loss 구하기 \n\n\n\n Distributional RL의 경우 정확한 분포를 예측하는 것이 목적입니다. 이에 따라 target value가 아닌 target distribution을 통해 학습을 수행합니다. Target distribution과 추정된 distribution간의 차이를 줄이는 방향으로 학습을 수행해야 할텐데! 그럼 distribution간의 차이는 어떻게 구할까요? 본 논문에서는 [Cross Entropy](https://en.wikipedia.org/wiki/Cross_entropy) 를 이용합니다. 그 식은 다음과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"/img/Equation_loss_c51.png\" alt=\"Equation of Loss\" width=\"400\"/>\n\n</p>\n\n마지막으로 위의 식에 필요한 target distribution을 구하는 과정에 대해 살펴보도록 하겠습니다. \n\n<br>\n\n### 3. Target Distribution\n\n\n\nTarget distribution을 만드는 이 과정이 C51 알고리즘의 구현에 있어 가장 귀찮고 까다로운 부분입니다. 우선 target distribution을 만들기 위해서 우선 value값과 직접적으로 연관이 있는 support에 대한 연산을 수행합니다. 해당 연산은 DQN에서 target value를 구하는 식과 같습니다. \n\n<center> <img src=\"/img/Equation_target1_c51.png\" width=\"400\"/> </center>\n\nTarget distribution의 경우 supports에 대해 위와 같은 연산을 해줍니다. 각 support에 discount factor를 곱하고 reward를 더해줍니다. (단, terminal이 false일 때, 즉 게임이 끝나지 않은 경우에만 이렇게 연산을 하고 게임이 끝난 경우에는 모든 support 값들을 reward 값으로 사용합니다 -> supports = [r_t]) 이 값은 support의 최대값보다 큰 경우 최대값과 같도록, 최소값보다 작은 경우 최소값과 같도록 설정해줍니다. \n\n그런데 이 경우에 문제가 생길 수 있습니다! 예를 들어 supports가 [1, 2, 3, 4, 5] 인 경우 reward = 0.1, discount factor = 0.9라고 해보겠습니다. 이 경우 위의 식에 따라 연산을 해주면 supports가 [1, 1.9, 2.8, 3.7, 4.6]이 됩니다. Loss인 cross entropy 연산을 하기 위해서는 두 distribution의 supports가 일치해야되는데 support가 달라져 버렸습니다! 이 경우 때문에 target distribution의 support를 원래의 support와 같이 분배해주는 **Projection** 이라는 과정이 추가적으로 필요합니다! \n\n위의 과정을 그림으로 표현한 것이 다음과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"/img/bellman_operation.png\" alt=\"Bellman Operation\" class =\"center\" width=\"800\"/>\n\n</p>\n\n Projection의 경우 다음의 예시를 통해 설명을 하도록 하겠습니다. \n\n아래와 같은 상황이 있다고 해보겠습니다. \n\n<p align=\"center\">\n\n <img src=\"/img/projection1.png\" alt=\"Projection1\" class =\"center\" width=\"700\"/>\n\n</p>\n\n변경된  support가 3.7인 경우를 예시로 하여 Projection을 해보도록 하겠습니다. 이 경우 3.7에 해당하는 확률값인 0.25를 support 3과 4로부터의 거리에 비례하여 분배합니다. 그 과정이 아래 그림과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"/img/projection2.png\" alt=\"Projection2\" class =\"center\" width=\"500\"/>\n\n</p>\n\n위와 같이 3.7은 기존의 support 3과 4 사이에 존재하는 값입니다. 3.7이 가진 확률값 0.25를 3과 4에 분배해야합니다. 이때 거리를 기반으로 값을 분배합니다. 3.7은 3보다는 4에 더 가깝죠. 그렇기 때문에 4에 더 많은 비율을 분배해줘야 합니다. 3과 4에서부터 3.7까지 거리의 비율은 각각 7:3 입니다. 이 3:7의 비율 중 3을 support값 3에, 7을 support값 4에 분배해줍니다. 그렇기 때문에 support 3의 확률값은 0.25 * 0.3 = 0.075을, support 4의 확률값은 0.25 * 0.7 = 0.175를 각각 할당받게 됩니다. 이 과정을 위의 예시에서 모든 supports에 적용한 결과가 다음과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"/img/projection3.png\" alt=\"Projection2\" class =\"center\" width=\"800\"/>\n\n</p>\n\n위와 같이 supports [1, 1.9, 2.8, 3.7, 4.6] 일때 확률값이 [0.1, 0.15, 0.2, 0.25, 0.3] 였던 것이 **Projection** 과정 이후에는 supports [1, 2, 3, 4, 5] 이고 각각에 대한 확률값이 [0.115, 0.175, 0.235, 0.295, 0.18]이 되었습니다. 이것이 최종적으로 구한 target distribution이며 이제 Projection을 통해 supports가 기존의 supports와 같아졌기 때문에 **Cross entropy** 연산이 가능하게 되었습니다!! \n\n여기까지가 Projection을 수행하는 과정입니다. \n\n<br>\n\n지금까지의 과정을 잘 이해하셨다면 논문에 나온 아래의 알고리즘을 더 수월하게 이해할 수 있을 것입니다. \n\n<p align=\"center\">\n\n <img src=\"/img/algorithm_c51.png\" alt=\"algorithm\" class =\"center\" width=\"800\"/>\n\n</p>\n\n<br>\n\n## Result\n\n본 알고리즘의 성능은 Atari 환경에서 검증되었으며 이때 사용한 parameters는 다음과 같습니다. \n\n- V(max) = 10 -> support의 최대값\n- V(min) = -10 -> support의 최소값\n- Epsilon (Adam) = 0.01 / (batch size) -> Adam optimizer의 epsilon 값\n- Learning rate = 0.00025\n- Batch size = 32\n\n\n\nAtari 환경에서 테스트한 결과는 다음과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"/img/result_c51.png\" alt=\"algorithm\" class =\"center\" width=\"800\"/>\n\n</p>\n\n위의 결과에서 볼 수 있듯이 support의 수가 많아질수록 성능은 좋아지는 편입니다. 특히 51개의 support를 이용했을 때 `SeaQuest` 게임에서 아주 좋은 성능을 보입니다. 그래서 일반적으로 이 알고리즘에서는 51개의 support를 이용하며 이에 따라 **C51**이라고 알고리즘을 부르게 되었습니다. \n\n\n\n<p align=\"center\">\n\n <img src=\"/img/sparse_good_result_c51.png\" alt=\"algorithm\" class =\"center\" width=\"800\"/>\n\n</p>\n\n위에서 볼 수 있듯이 sparse한 reward 환경인 Private Eye나 Venture 같은 게임에서도 다른 알고리즘들에 비해 좋은 결과를 보였다고 합니다. \n\n\n\n<p align=\"center\">\n\n <img src=\"/img/result_c51_2.png\" alt=\"algorithm\" class =\"center\" width=\"400\"/>\n\n</p>\n\n또한 위에서 볼 수 있듯이 DQN, DDQN, Dueling, PER, PER + Dueling 과 비교했을 때에도 매우 좋은 성능을 보입니다. \n\n\n\n<p align=\"center\">\n\n <img src=\"/img/result_c51_3.png\" alt=\"algorithm\" class =\"center\" width=\"400\"/>\n\n</p>\n\n마지막으로 위의 결과는 C51 vs DQN, C51 vs Human, DQN vs Human 을 했을 때 더 좋은 성능을 보인 게임의 수 입니다. 총 57개의 게임이 있으니 C51의 경우 약 절반 이상의 게임에서 사람보다 우수한 성능을 보인 알고리즘이라 할 수 있습니다. \n\n<br>\n\n## Conclusion\n\nC51의 경우 distributional RL 3형제 중 첫번째 알고리즘으로 distribution을 통해 value를 예측하는 알고리즘입니다. Distribution을 이용하여 value를 잘 예측하며 좋은 성능을 보이지만 이 알고리즘에서는 몇가지 아쉬운 점이 있습니다. \n\n1.  support에 관련된 파라미터들을 결정해줘야하며 게임의 reward 범위에 따라 이 값들을 따로 설정해야 할 수 있습니다. 이런 점에 조금 귀찮은 점이 있습니다. \n2. 알고리즘에 굉장히 번거로운 projection 과정이 포함되어 있습니다. \n3. 마지막으로 해당 알고리즘의 경우 수학적으로 수렴성을 보장하지 못하는 알고리즘입니다. \n\n<p align=\"center\">\n\n <img src=\"/img/gamma_contraction.png\" alt=\"gamma contraction\" class =\"center\" width=\"400\"/>\n\n</p>\n\nDistributional RL이 수렴하기 위해서는 위와 같은 gamma-contraction 조건을 만족해야합니다. value distribution간 거리를 측정하는 distance metric (d_p)가 **Wasserstein distance**인 경우 위 조건을 **만족**하지만 **Cross entropy** 의 경우 수학적으로 위의 조건을 만족한다는 보장이 없다고 합니다. 하지만 C51 논문은 wasserstein distance를 감소시킬 방법을 찾지 못한 관계로 Cross entropy를 loss로 설정하고 이를 줄이는 방향으로 학습을 수행하는 알고리즘이기 때문에 수학적으로 distributional RL의 수렴성을 증명하지는 못하는 논문입니다. \n\n<br>\n\n이 위의 3가지 문제점들을 해결한 논문이 C51의 후속으로 발표된 [Distributional Reinforcement Learning with Quantile Regression (QR-DQN)](https://arxiv.org/abs/1710.10044) 논문입니다. 다음 게시물에서는 이 QR-DQN 논문의 내용에 대해 살펴보도록 하겠습니다!! :smile:\n\n<br>\n\n## Implementation\n\n본 논문의 코드는 다음의 Github를 참고해주세요. \n\n[Github](https://github.com/reinforcement-learning-kr/distributional_rl)\n\n<br>\n\n## Other Posts\n\n[Distributional RL 개요](https://reinforcement-learning-kr.github.io/2018/09/27/Distributional_intro/)\n\n[QR-DQN](https://reinforcement-learning-kr.github.io/2018/10/22/QR-DQN/)\n\n[IQN](https://reinforcement-learning-kr.github.io/2018/10/30/IQN/)\n\n<br>\n\n## Reference\n\n- [A Distributional Perspective on Reinforcement Learning](https://arxiv.org/abs/1707.06887)\n- [Blog: Distributional Bellman and the C51 Algorithm](https://flyyufelix.github.io/2017/10/24/distributional-bellman.html)\n- [Blog: Distributional RL](https://mtomassoli.github.io/2017/12/08/distributional_rl/) \n- [Deepmind Blog: Going beyond average for reinforcement learning](https://deepmind.com/blog/going-beyond-average-reinforcement-learning/)\n\n\n\n<br>\n\n## Team\n\n민규식: [Github](https://github.com/Kyushik), [Facebook](https://www.facebook.com/kyushik.min)\n\n차금강: [Github](https://github.com/chagmgang), [Facebook](https://www.facebook.com/profile.php?id=100002147815509)\n\n윤승제: [Github](https://github.com/sjYoondeltar), [Facebook](https://www.facebook.com/seungje.yoon)\n\n김하영: [Github](https://github.com/hayoung-kim), [Facebook](https://www.facebook.com/altairyoung)\n\n김정대: [Github](https://github.com/kekmodel), [Facebook](https://www.facebook.com/kekmodel)\n\n\n\n","source":"_posts/C51.md","raw":"---\ntitle: A Distributional Perspective on Reinforcement Learning\ndate: 2018-10-02 14:18:32\ntags: [\"논문\", \"Distributional RL\", \"C51\"]\ncategories: 논문 정리\nauthor: 민규식\nsubtitle: Distributional RL 1번째 논문\n\n---\n\n<center> <img src=\"/img/paper_c51.png\" width=\"800\"> </center>\n\n논문 저자 : [Marc G. Bellemare](https://arxiv.org/search/cs?searchtype=author&query=Bellemare%2C+M+G), [Will Dabney](https://arxiv.org/search/cs?searchtype=author&query=Dabney%2C+W), [Rémi Munos](https://arxiv.org/search/cs?searchtype=author&query=Munos%2C+R)    \n논문 링크 : [ArXiv](https://arxiv.org/abs/1707.06887)\nProceeding : International Conference on Machine Learning (ICML) 2017        \n정리 : 민규식\n\n\n\n## Introduction\n\n본 게시물은 2017년에 발표된 논문 [A Distributional Perspective on Reinforcement Learning](https://arxiv.org/abs/1707.06887) 의 내용에 대해 설명합니다.\n\nDistributional RL에 대해 설명한 게시물에서도 언급했듯이 distributional RL 알고리즘은 value를 하나의 scalar 값이 아닌 distribution으로 예측합니다.\n\n<p align=\"center\">\n\n<img src=\"/img/distributionalRL.png\" alt=\"distributional RL\" width=\"600\"/>\n\n</p>\n\n 이에 따라 일반적인 강화학습에서 이용하는 `bellman equation`의 value Q 대신 distribution Z를 사용합니다. 이 bellman equation을 `distributional bellman equation` 이라고 합니다. 해당 식들은 다음과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"/img/bellman_equation.png\" alt=\"distributional RL\" width=\"600\"/>\n\n</p>\n\n<br>\n\n이에 따라 network의 output도 각 알고리즘에 따라 차이가 있습니다. \n\n[DQN](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning/)에서 network의 output이 각 action에 대한 Q-value였다면 distributional RL에서 network의 output은 아래 그림과 같이 각 action에 대한 value distribution 입니다. \n\n<p align=\"center\">\n\n <img src=\"/img/network_output.png\" alt=\"Network\" width=\"800\"/>\n\n</p>\n\n<br>\n\n## Value Distribution\n\nAction에 대한 Distribution에 대해서 살펴보도록 하겠습니다. \n\n<p align=\"center\">\n\n <img src=\"/img/support_and_prob.png\" alt=\"Distribution\" width=\"800\"/>\n\n</p>\n\nNetwork의 output은 위와 같은 이산 확률분포(Discrete Probability Distribution) 입니다. 여기서 가로축은 **Support** 혹은 **atom**이라고 부르며 value값을 나타냅니다. 세로축은 확률을 나타냅니다. 즉, 이 분포는 각각의 value와 그 value를 받을 확률을 나타내는 분포입니다.\n\n\n\nC51의 경우 분포를 결정해주기 위해서 몇가지 파라미터들이 필요합니다. 해당 파라미터들은 다음과 같습니다.\n\n- Support의 수\n- Support의 최대값\n- Support의 최소값\n\n위 그림에서도 볼 수 있듯이 support값은 최소값부터 최대값까지 support의 수에 맞게 일정한 간격으로 나누게 됩니다. 즉, supports는 이렇게 미리 결정된 파라미터들에 의해 그 값이 정해지게 됩니다. 네트워크는 바로 이 support들에 대한 확률을 구해주게 됩니다. 각 action에 대해서 하나의 distribution이 필요하기 때문에 **network output의 크기는 [support의 수 * action의 수]** 가 됩니다.  \n\n<br>\n\n## Algorithm\n\n알고리즘의 진행은 DQN과 거의 동일합니다. `Experience Replay` 나 `Target Network`  같은 기법도 그대로 이용합니다. 차이점은 아래 3가지 정도입니다. \n\n1. Q-value 계산\n2. Loss\n3. Target distribution 구하기\n\n각각 어떻게 차이가 있는지 한번 살펴보도록 하겠습니다. \n\n<br>\n\n### 1. Q-value 계산\n\n\n\nC51 알고리즘에서 Q-value를 계산하는 방법은 이산확률분포의 기대값을 구하는 것입니다. 이에 따라 각 action에 대한 Q-value를 계산하는 식은 다음과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"/img/Equation_Q_c51.png\" alt=\"Equation of Q-value\" width=\"400\"/>\n\n</p>\n\n 수식에서 사용된 각 기호는 위에 있는 Value distribution 그림을 참고해주세요! 각 action의 distribution마다 위의 식에 따라 연산을 수행하고 최대의 Q-value를 가지는 action을 선택하면 됩니다. \n\n 위 수식처럼 분포의 기대값을 구하는 연산을 하면 결과적으로 Q-value는 하나의 scalar값을 가지게 됩니다. 이 경우 \"뭐지... 결국 scalar값으로 action 선택하자나...\" 라고 생각하실수도 있지만!! 단순히 하나의 기대값을 추정하는 것보다 정확하게 분포를 추정하고 그 기대값을 구하는 것이 더 정확한 예측이 될 것이라 생각할 수 있습니다. \n\n<br>\n\n### 2. Loss 구하기 \n\n\n\n Distributional RL의 경우 정확한 분포를 예측하는 것이 목적입니다. 이에 따라 target value가 아닌 target distribution을 통해 학습을 수행합니다. Target distribution과 추정된 distribution간의 차이를 줄이는 방향으로 학습을 수행해야 할텐데! 그럼 distribution간의 차이는 어떻게 구할까요? 본 논문에서는 [Cross Entropy](https://en.wikipedia.org/wiki/Cross_entropy) 를 이용합니다. 그 식은 다음과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"/img/Equation_loss_c51.png\" alt=\"Equation of Loss\" width=\"400\"/>\n\n</p>\n\n마지막으로 위의 식에 필요한 target distribution을 구하는 과정에 대해 살펴보도록 하겠습니다. \n\n<br>\n\n### 3. Target Distribution\n\n\n\nTarget distribution을 만드는 이 과정이 C51 알고리즘의 구현에 있어 가장 귀찮고 까다로운 부분입니다. 우선 target distribution을 만들기 위해서 우선 value값과 직접적으로 연관이 있는 support에 대한 연산을 수행합니다. 해당 연산은 DQN에서 target value를 구하는 식과 같습니다. \n\n<center> <img src=\"/img/Equation_target1_c51.png\" width=\"400\"/> </center>\n\nTarget distribution의 경우 supports에 대해 위와 같은 연산을 해줍니다. 각 support에 discount factor를 곱하고 reward를 더해줍니다. (단, terminal이 false일 때, 즉 게임이 끝나지 않은 경우에만 이렇게 연산을 하고 게임이 끝난 경우에는 모든 support 값들을 reward 값으로 사용합니다 -> supports = [r_t]) 이 값은 support의 최대값보다 큰 경우 최대값과 같도록, 최소값보다 작은 경우 최소값과 같도록 설정해줍니다. \n\n그런데 이 경우에 문제가 생길 수 있습니다! 예를 들어 supports가 [1, 2, 3, 4, 5] 인 경우 reward = 0.1, discount factor = 0.9라고 해보겠습니다. 이 경우 위의 식에 따라 연산을 해주면 supports가 [1, 1.9, 2.8, 3.7, 4.6]이 됩니다. Loss인 cross entropy 연산을 하기 위해서는 두 distribution의 supports가 일치해야되는데 support가 달라져 버렸습니다! 이 경우 때문에 target distribution의 support를 원래의 support와 같이 분배해주는 **Projection** 이라는 과정이 추가적으로 필요합니다! \n\n위의 과정을 그림으로 표현한 것이 다음과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"/img/bellman_operation.png\" alt=\"Bellman Operation\" class =\"center\" width=\"800\"/>\n\n</p>\n\n Projection의 경우 다음의 예시를 통해 설명을 하도록 하겠습니다. \n\n아래와 같은 상황이 있다고 해보겠습니다. \n\n<p align=\"center\">\n\n <img src=\"/img/projection1.png\" alt=\"Projection1\" class =\"center\" width=\"700\"/>\n\n</p>\n\n변경된  support가 3.7인 경우를 예시로 하여 Projection을 해보도록 하겠습니다. 이 경우 3.7에 해당하는 확률값인 0.25를 support 3과 4로부터의 거리에 비례하여 분배합니다. 그 과정이 아래 그림과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"/img/projection2.png\" alt=\"Projection2\" class =\"center\" width=\"500\"/>\n\n</p>\n\n위와 같이 3.7은 기존의 support 3과 4 사이에 존재하는 값입니다. 3.7이 가진 확률값 0.25를 3과 4에 분배해야합니다. 이때 거리를 기반으로 값을 분배합니다. 3.7은 3보다는 4에 더 가깝죠. 그렇기 때문에 4에 더 많은 비율을 분배해줘야 합니다. 3과 4에서부터 3.7까지 거리의 비율은 각각 7:3 입니다. 이 3:7의 비율 중 3을 support값 3에, 7을 support값 4에 분배해줍니다. 그렇기 때문에 support 3의 확률값은 0.25 * 0.3 = 0.075을, support 4의 확률값은 0.25 * 0.7 = 0.175를 각각 할당받게 됩니다. 이 과정을 위의 예시에서 모든 supports에 적용한 결과가 다음과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"/img/projection3.png\" alt=\"Projection2\" class =\"center\" width=\"800\"/>\n\n</p>\n\n위와 같이 supports [1, 1.9, 2.8, 3.7, 4.6] 일때 확률값이 [0.1, 0.15, 0.2, 0.25, 0.3] 였던 것이 **Projection** 과정 이후에는 supports [1, 2, 3, 4, 5] 이고 각각에 대한 확률값이 [0.115, 0.175, 0.235, 0.295, 0.18]이 되었습니다. 이것이 최종적으로 구한 target distribution이며 이제 Projection을 통해 supports가 기존의 supports와 같아졌기 때문에 **Cross entropy** 연산이 가능하게 되었습니다!! \n\n여기까지가 Projection을 수행하는 과정입니다. \n\n<br>\n\n지금까지의 과정을 잘 이해하셨다면 논문에 나온 아래의 알고리즘을 더 수월하게 이해할 수 있을 것입니다. \n\n<p align=\"center\">\n\n <img src=\"/img/algorithm_c51.png\" alt=\"algorithm\" class =\"center\" width=\"800\"/>\n\n</p>\n\n<br>\n\n## Result\n\n본 알고리즘의 성능은 Atari 환경에서 검증되었으며 이때 사용한 parameters는 다음과 같습니다. \n\n- V(max) = 10 -> support의 최대값\n- V(min) = -10 -> support의 최소값\n- Epsilon (Adam) = 0.01 / (batch size) -> Adam optimizer의 epsilon 값\n- Learning rate = 0.00025\n- Batch size = 32\n\n\n\nAtari 환경에서 테스트한 결과는 다음과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"/img/result_c51.png\" alt=\"algorithm\" class =\"center\" width=\"800\"/>\n\n</p>\n\n위의 결과에서 볼 수 있듯이 support의 수가 많아질수록 성능은 좋아지는 편입니다. 특히 51개의 support를 이용했을 때 `SeaQuest` 게임에서 아주 좋은 성능을 보입니다. 그래서 일반적으로 이 알고리즘에서는 51개의 support를 이용하며 이에 따라 **C51**이라고 알고리즘을 부르게 되었습니다. \n\n\n\n<p align=\"center\">\n\n <img src=\"/img/sparse_good_result_c51.png\" alt=\"algorithm\" class =\"center\" width=\"800\"/>\n\n</p>\n\n위에서 볼 수 있듯이 sparse한 reward 환경인 Private Eye나 Venture 같은 게임에서도 다른 알고리즘들에 비해 좋은 결과를 보였다고 합니다. \n\n\n\n<p align=\"center\">\n\n <img src=\"/img/result_c51_2.png\" alt=\"algorithm\" class =\"center\" width=\"400\"/>\n\n</p>\n\n또한 위에서 볼 수 있듯이 DQN, DDQN, Dueling, PER, PER + Dueling 과 비교했을 때에도 매우 좋은 성능을 보입니다. \n\n\n\n<p align=\"center\">\n\n <img src=\"/img/result_c51_3.png\" alt=\"algorithm\" class =\"center\" width=\"400\"/>\n\n</p>\n\n마지막으로 위의 결과는 C51 vs DQN, C51 vs Human, DQN vs Human 을 했을 때 더 좋은 성능을 보인 게임의 수 입니다. 총 57개의 게임이 있으니 C51의 경우 약 절반 이상의 게임에서 사람보다 우수한 성능을 보인 알고리즘이라 할 수 있습니다. \n\n<br>\n\n## Conclusion\n\nC51의 경우 distributional RL 3형제 중 첫번째 알고리즘으로 distribution을 통해 value를 예측하는 알고리즘입니다. Distribution을 이용하여 value를 잘 예측하며 좋은 성능을 보이지만 이 알고리즘에서는 몇가지 아쉬운 점이 있습니다. \n\n1.  support에 관련된 파라미터들을 결정해줘야하며 게임의 reward 범위에 따라 이 값들을 따로 설정해야 할 수 있습니다. 이런 점에 조금 귀찮은 점이 있습니다. \n2. 알고리즘에 굉장히 번거로운 projection 과정이 포함되어 있습니다. \n3. 마지막으로 해당 알고리즘의 경우 수학적으로 수렴성을 보장하지 못하는 알고리즘입니다. \n\n<p align=\"center\">\n\n <img src=\"/img/gamma_contraction.png\" alt=\"gamma contraction\" class =\"center\" width=\"400\"/>\n\n</p>\n\nDistributional RL이 수렴하기 위해서는 위와 같은 gamma-contraction 조건을 만족해야합니다. value distribution간 거리를 측정하는 distance metric (d_p)가 **Wasserstein distance**인 경우 위 조건을 **만족**하지만 **Cross entropy** 의 경우 수학적으로 위의 조건을 만족한다는 보장이 없다고 합니다. 하지만 C51 논문은 wasserstein distance를 감소시킬 방법을 찾지 못한 관계로 Cross entropy를 loss로 설정하고 이를 줄이는 방향으로 학습을 수행하는 알고리즘이기 때문에 수학적으로 distributional RL의 수렴성을 증명하지는 못하는 논문입니다. \n\n<br>\n\n이 위의 3가지 문제점들을 해결한 논문이 C51의 후속으로 발표된 [Distributional Reinforcement Learning with Quantile Regression (QR-DQN)](https://arxiv.org/abs/1710.10044) 논문입니다. 다음 게시물에서는 이 QR-DQN 논문의 내용에 대해 살펴보도록 하겠습니다!! :smile:\n\n<br>\n\n## Implementation\n\n본 논문의 코드는 다음의 Github를 참고해주세요. \n\n[Github](https://github.com/reinforcement-learning-kr/distributional_rl)\n\n<br>\n\n## Other Posts\n\n[Distributional RL 개요](https://reinforcement-learning-kr.github.io/2018/09/27/Distributional_intro/)\n\n[QR-DQN](https://reinforcement-learning-kr.github.io/2018/10/22/QR-DQN/)\n\n[IQN](https://reinforcement-learning-kr.github.io/2018/10/30/IQN/)\n\n<br>\n\n## Reference\n\n- [A Distributional Perspective on Reinforcement Learning](https://arxiv.org/abs/1707.06887)\n- [Blog: Distributional Bellman and the C51 Algorithm](https://flyyufelix.github.io/2017/10/24/distributional-bellman.html)\n- [Blog: Distributional RL](https://mtomassoli.github.io/2017/12/08/distributional_rl/) \n- [Deepmind Blog: Going beyond average for reinforcement learning](https://deepmind.com/blog/going-beyond-average-reinforcement-learning/)\n\n\n\n<br>\n\n## Team\n\n민규식: [Github](https://github.com/Kyushik), [Facebook](https://www.facebook.com/kyushik.min)\n\n차금강: [Github](https://github.com/chagmgang), [Facebook](https://www.facebook.com/profile.php?id=100002147815509)\n\n윤승제: [Github](https://github.com/sjYoondeltar), [Facebook](https://www.facebook.com/seungje.yoon)\n\n김하영: [Github](https://github.com/hayoung-kim), [Facebook](https://www.facebook.com/altairyoung)\n\n김정대: [Github](https://github.com/kekmodel), [Facebook](https://www.facebook.com/kekmodel)\n\n\n\n","slug":"C51","published":1,"updated":"2019-02-07T11:21:31.983Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjrujltzp000m5wfery95psnv","content":"<center> <img src=\"/img/paper_c51.png\" width=\"800\"> </center>\n\n<p>논문 저자 : <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Bellemare%2C+M+G\" target=\"_blank\" rel=\"noopener\">Marc G. Bellemare</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Dabney%2C+W\" target=\"_blank\" rel=\"noopener\">Will Dabney</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Munos%2C+R\" target=\"_blank\" rel=\"noopener\">Rémi Munos</a><br>논문 링크 : <a href=\"https://arxiv.org/abs/1707.06887\" target=\"_blank\" rel=\"noopener\">ArXiv</a><br>Proceeding : International Conference on Machine Learning (ICML) 2017<br>정리 : 민규식</p>\n<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>본 게시물은 2017년에 발표된 논문 <a href=\"https://arxiv.org/abs/1707.06887\" target=\"_blank\" rel=\"noopener\">A Distributional Perspective on Reinforcement Learning</a> 의 내용에 대해 설명합니다.</p>\n<p>Distributional RL에 대해 설명한 게시물에서도 언급했듯이 distributional RL 알고리즘은 value를 하나의 scalar 값이 아닌 distribution으로 예측합니다.</p>\n<p align=\"center\"><br><br><img src=\"/img/distributionalRL.png\" alt=\"distributional RL\" width=\"600\"><br><br></p>\n\n<p> 이에 따라 일반적인 강화학습에서 이용하는 <code>bellman equation</code>의 value Q 대신 distribution Z를 사용합니다. 이 bellman equation을 <code>distributional bellman equation</code> 이라고 합니다. 해당 식들은 다음과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/bellman_equation.png\" alt=\"distributional RL\" width=\"600\"><br><br></p>\n\n<p><br></p>\n<p>이에 따라 network의 output도 각 알고리즘에 따라 차이가 있습니다. </p>\n<p><a href=\"https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning/\" target=\"_blank\" rel=\"noopener\">DQN</a>에서 network의 output이 각 action에 대한 Q-value였다면 distributional RL에서 network의 output은 아래 그림과 같이 각 action에 대한 value distribution 입니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/network_output.png\" alt=\"Network\" width=\"800\"><br><br></p>\n\n<p><br></p>\n<h2 id=\"Value-Distribution\"><a href=\"#Value-Distribution\" class=\"headerlink\" title=\"Value Distribution\"></a>Value Distribution</h2><p>Action에 대한 Distribution에 대해서 살펴보도록 하겠습니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/support_and_prob.png\" alt=\"Distribution\" width=\"800\"><br><br></p>\n\n<p>Network의 output은 위와 같은 이산 확률분포(Discrete Probability Distribution) 입니다. 여기서 가로축은 <strong>Support</strong> 혹은 <strong>atom</strong>이라고 부르며 value값을 나타냅니다. 세로축은 확률을 나타냅니다. 즉, 이 분포는 각각의 value와 그 value를 받을 확률을 나타내는 분포입니다.</p>\n<p>C51의 경우 분포를 결정해주기 위해서 몇가지 파라미터들이 필요합니다. 해당 파라미터들은 다음과 같습니다.</p>\n<ul>\n<li>Support의 수</li>\n<li>Support의 최대값</li>\n<li>Support의 최소값</li>\n</ul>\n<p>위 그림에서도 볼 수 있듯이 support값은 최소값부터 최대값까지 support의 수에 맞게 일정한 간격으로 나누게 됩니다. 즉, supports는 이렇게 미리 결정된 파라미터들에 의해 그 값이 정해지게 됩니다. 네트워크는 바로 이 support들에 대한 확률을 구해주게 됩니다. 각 action에 대해서 하나의 distribution이 필요하기 때문에 <strong>network output의 크기는 [support의 수 * action의 수]</strong> 가 됩니다.  </p>\n<p><br></p>\n<h2 id=\"Algorithm\"><a href=\"#Algorithm\" class=\"headerlink\" title=\"Algorithm\"></a>Algorithm</h2><p>알고리즘의 진행은 DQN과 거의 동일합니다. <code>Experience Replay</code> 나 <code>Target Network</code>  같은 기법도 그대로 이용합니다. 차이점은 아래 3가지 정도입니다. </p>\n<ol>\n<li>Q-value 계산</li>\n<li>Loss</li>\n<li>Target distribution 구하기</li>\n</ol>\n<p>각각 어떻게 차이가 있는지 한번 살펴보도록 하겠습니다. </p>\n<p><br></p>\n<h3 id=\"1-Q-value-계산\"><a href=\"#1-Q-value-계산\" class=\"headerlink\" title=\"1. Q-value 계산\"></a>1. Q-value 계산</h3><p>C51 알고리즘에서 Q-value를 계산하는 방법은 이산확률분포의 기대값을 구하는 것입니다. 이에 따라 각 action에 대한 Q-value를 계산하는 식은 다음과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/Equation_Q_c51.png\" alt=\"Equation of Q-value\" width=\"400\"><br><br></p>\n\n<p> 수식에서 사용된 각 기호는 위에 있는 Value distribution 그림을 참고해주세요! 각 action의 distribution마다 위의 식에 따라 연산을 수행하고 최대의 Q-value를 가지는 action을 선택하면 됩니다. </p>\n<p> 위 수식처럼 분포의 기대값을 구하는 연산을 하면 결과적으로 Q-value는 하나의 scalar값을 가지게 됩니다. 이 경우 “뭐지… 결국 scalar값으로 action 선택하자나…” 라고 생각하실수도 있지만!! 단순히 하나의 기대값을 추정하는 것보다 정확하게 분포를 추정하고 그 기대값을 구하는 것이 더 정확한 예측이 될 것이라 생각할 수 있습니다. </p>\n<p><br></p>\n<h3 id=\"2-Loss-구하기\"><a href=\"#2-Loss-구하기\" class=\"headerlink\" title=\"2. Loss 구하기\"></a>2. Loss 구하기</h3><p> Distributional RL의 경우 정확한 분포를 예측하는 것이 목적입니다. 이에 따라 target value가 아닌 target distribution을 통해 학습을 수행합니다. Target distribution과 추정된 distribution간의 차이를 줄이는 방향으로 학습을 수행해야 할텐데! 그럼 distribution간의 차이는 어떻게 구할까요? 본 논문에서는 <a href=\"https://en.wikipedia.org/wiki/Cross_entropy\" target=\"_blank\" rel=\"noopener\">Cross Entropy</a> 를 이용합니다. 그 식은 다음과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/Equation_loss_c51.png\" alt=\"Equation of Loss\" width=\"400\"><br><br></p>\n\n<p>마지막으로 위의 식에 필요한 target distribution을 구하는 과정에 대해 살펴보도록 하겠습니다. </p>\n<p><br></p>\n<h3 id=\"3-Target-Distribution\"><a href=\"#3-Target-Distribution\" class=\"headerlink\" title=\"3. Target Distribution\"></a>3. Target Distribution</h3><p>Target distribution을 만드는 이 과정이 C51 알고리즘의 구현에 있어 가장 귀찮고 까다로운 부분입니다. 우선 target distribution을 만들기 위해서 우선 value값과 직접적으로 연관이 있는 support에 대한 연산을 수행합니다. 해당 연산은 DQN에서 target value를 구하는 식과 같습니다. </p>\n<center> <img src=\"/img/Equation_target1_c51.png\" width=\"400\"> </center>\n\n<p>Target distribution의 경우 supports에 대해 위와 같은 연산을 해줍니다. 각 support에 discount factor를 곱하고 reward를 더해줍니다. (단, terminal이 false일 때, 즉 게임이 끝나지 않은 경우에만 이렇게 연산을 하고 게임이 끝난 경우에는 모든 support 값들을 reward 값으로 사용합니다 -&gt; supports = [r_t]) 이 값은 support의 최대값보다 큰 경우 최대값과 같도록, 최소값보다 작은 경우 최소값과 같도록 설정해줍니다. </p>\n<p>그런데 이 경우에 문제가 생길 수 있습니다! 예를 들어 supports가 [1, 2, 3, 4, 5] 인 경우 reward = 0.1, discount factor = 0.9라고 해보겠습니다. 이 경우 위의 식에 따라 연산을 해주면 supports가 [1, 1.9, 2.8, 3.7, 4.6]이 됩니다. Loss인 cross entropy 연산을 하기 위해서는 두 distribution의 supports가 일치해야되는데 support가 달라져 버렸습니다! 이 경우 때문에 target distribution의 support를 원래의 support와 같이 분배해주는 <strong>Projection</strong> 이라는 과정이 추가적으로 필요합니다! </p>\n<p>위의 과정을 그림으로 표현한 것이 다음과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/bellman_operation.png\" alt=\"Bellman Operation\" class=\"center\" width=\"800\"><br><br></p>\n\n<p> Projection의 경우 다음의 예시를 통해 설명을 하도록 하겠습니다. </p>\n<p>아래와 같은 상황이 있다고 해보겠습니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/projection1.png\" alt=\"Projection1\" class=\"center\" width=\"700\"><br><br></p>\n\n<p>변경된  support가 3.7인 경우를 예시로 하여 Projection을 해보도록 하겠습니다. 이 경우 3.7에 해당하는 확률값인 0.25를 support 3과 4로부터의 거리에 비례하여 분배합니다. 그 과정이 아래 그림과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/projection2.png\" alt=\"Projection2\" class=\"center\" width=\"500\"><br><br></p>\n\n<p>위와 같이 3.7은 기존의 support 3과 4 사이에 존재하는 값입니다. 3.7이 가진 확률값 0.25를 3과 4에 분배해야합니다. 이때 거리를 기반으로 값을 분배합니다. 3.7은 3보다는 4에 더 가깝죠. 그렇기 때문에 4에 더 많은 비율을 분배해줘야 합니다. 3과 4에서부터 3.7까지 거리의 비율은 각각 7:3 입니다. 이 3:7의 비율 중 3을 support값 3에, 7을 support값 4에 분배해줍니다. 그렇기 때문에 support 3의 확률값은 0.25 <em> 0.3 = 0.075을, support 4의 확률값은 0.25 </em> 0.7 = 0.175를 각각 할당받게 됩니다. 이 과정을 위의 예시에서 모든 supports에 적용한 결과가 다음과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/projection3.png\" alt=\"Projection2\" class=\"center\" width=\"800\"><br><br></p>\n\n<p>위와 같이 supports [1, 1.9, 2.8, 3.7, 4.6] 일때 확률값이 [0.1, 0.15, 0.2, 0.25, 0.3] 였던 것이 <strong>Projection</strong> 과정 이후에는 supports [1, 2, 3, 4, 5] 이고 각각에 대한 확률값이 [0.115, 0.175, 0.235, 0.295, 0.18]이 되었습니다. 이것이 최종적으로 구한 target distribution이며 이제 Projection을 통해 supports가 기존의 supports와 같아졌기 때문에 <strong>Cross entropy</strong> 연산이 가능하게 되었습니다!! </p>\n<p>여기까지가 Projection을 수행하는 과정입니다. </p>\n<p><br></p>\n<p>지금까지의 과정을 잘 이해하셨다면 논문에 나온 아래의 알고리즘을 더 수월하게 이해할 수 있을 것입니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/algorithm_c51.png\" alt=\"algorithm\" class=\"center\" width=\"800\"><br><br></p>\n\n<p><br></p>\n<h2 id=\"Result\"><a href=\"#Result\" class=\"headerlink\" title=\"Result\"></a>Result</h2><p>본 알고리즘의 성능은 Atari 환경에서 검증되었으며 이때 사용한 parameters는 다음과 같습니다. </p>\n<ul>\n<li>V(max) = 10 -&gt; support의 최대값</li>\n<li>V(min) = -10 -&gt; support의 최소값</li>\n<li>Epsilon (Adam) = 0.01 / (batch size) -&gt; Adam optimizer의 epsilon 값</li>\n<li>Learning rate = 0.00025</li>\n<li>Batch size = 32</li>\n</ul>\n<p>Atari 환경에서 테스트한 결과는 다음과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/result_c51.png\" alt=\"algorithm\" class=\"center\" width=\"800\"><br><br></p>\n\n<p>위의 결과에서 볼 수 있듯이 support의 수가 많아질수록 성능은 좋아지는 편입니다. 특히 51개의 support를 이용했을 때 <code>SeaQuest</code> 게임에서 아주 좋은 성능을 보입니다. 그래서 일반적으로 이 알고리즘에서는 51개의 support를 이용하며 이에 따라 <strong>C51</strong>이라고 알고리즘을 부르게 되었습니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/sparse_good_result_c51.png\" alt=\"algorithm\" class=\"center\" width=\"800\"><br><br></p>\n\n<p>위에서 볼 수 있듯이 sparse한 reward 환경인 Private Eye나 Venture 같은 게임에서도 다른 알고리즘들에 비해 좋은 결과를 보였다고 합니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/result_c51_2.png\" alt=\"algorithm\" class=\"center\" width=\"400\"><br><br></p>\n\n<p>또한 위에서 볼 수 있듯이 DQN, DDQN, Dueling, PER, PER + Dueling 과 비교했을 때에도 매우 좋은 성능을 보입니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/result_c51_3.png\" alt=\"algorithm\" class=\"center\" width=\"400\"><br><br></p>\n\n<p>마지막으로 위의 결과는 C51 vs DQN, C51 vs Human, DQN vs Human 을 했을 때 더 좋은 성능을 보인 게임의 수 입니다. 총 57개의 게임이 있으니 C51의 경우 약 절반 이상의 게임에서 사람보다 우수한 성능을 보인 알고리즘이라 할 수 있습니다. </p>\n<p><br></p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>C51의 경우 distributional RL 3형제 중 첫번째 알고리즘으로 distribution을 통해 value를 예측하는 알고리즘입니다. Distribution을 이용하여 value를 잘 예측하며 좋은 성능을 보이지만 이 알고리즘에서는 몇가지 아쉬운 점이 있습니다. </p>\n<ol>\n<li>support에 관련된 파라미터들을 결정해줘야하며 게임의 reward 범위에 따라 이 값들을 따로 설정해야 할 수 있습니다. 이런 점에 조금 귀찮은 점이 있습니다. </li>\n<li>알고리즘에 굉장히 번거로운 projection 과정이 포함되어 있습니다. </li>\n<li>마지막으로 해당 알고리즘의 경우 수학적으로 수렴성을 보장하지 못하는 알고리즘입니다. </li>\n</ol>\n<p align=\"center\"><br><br> <img src=\"/img/gamma_contraction.png\" alt=\"gamma contraction\" class=\"center\" width=\"400\"><br><br></p>\n\n<p>Distributional RL이 수렴하기 위해서는 위와 같은 gamma-contraction 조건을 만족해야합니다. value distribution간 거리를 측정하는 distance metric (d_p)가 <strong>Wasserstein distance</strong>인 경우 위 조건을 <strong>만족</strong>하지만 <strong>Cross entropy</strong> 의 경우 수학적으로 위의 조건을 만족한다는 보장이 없다고 합니다. 하지만 C51 논문은 wasserstein distance를 감소시킬 방법을 찾지 못한 관계로 Cross entropy를 loss로 설정하고 이를 줄이는 방향으로 학습을 수행하는 알고리즘이기 때문에 수학적으로 distributional RL의 수렴성을 증명하지는 못하는 논문입니다. </p>\n<p><br></p>\n<p>이 위의 3가지 문제점들을 해결한 논문이 C51의 후속으로 발표된 <a href=\"https://arxiv.org/abs/1710.10044\" target=\"_blank\" rel=\"noopener\">Distributional Reinforcement Learning with Quantile Regression (QR-DQN)</a> 논문입니다. 다음 게시물에서는 이 QR-DQN 논문의 내용에 대해 살펴보도록 하겠습니다!! :smile:</p>\n<p><br></p>\n<h2 id=\"Implementation\"><a href=\"#Implementation\" class=\"headerlink\" title=\"Implementation\"></a>Implementation</h2><p>본 논문의 코드는 다음의 Github를 참고해주세요. </p>\n<p><a href=\"https://github.com/reinforcement-learning-kr/distributional_rl\" target=\"_blank\" rel=\"noopener\">Github</a></p>\n<p><br></p>\n<h2 id=\"Other-Posts\"><a href=\"#Other-Posts\" class=\"headerlink\" title=\"Other Posts\"></a>Other Posts</h2><p><a href=\"https://reinforcement-learning-kr.github.io/2018/09/27/Distributional_intro/\">Distributional RL 개요</a></p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/10/22/QR-DQN/\">QR-DQN</a></p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/10/30/IQN/\">IQN</a></p>\n<p><br></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://arxiv.org/abs/1707.06887\" target=\"_blank\" rel=\"noopener\">A Distributional Perspective on Reinforcement Learning</a></li>\n<li><a href=\"https://flyyufelix.github.io/2017/10/24/distributional-bellman.html\" target=\"_blank\" rel=\"noopener\">Blog: Distributional Bellman and the C51 Algorithm</a></li>\n<li><a href=\"https://mtomassoli.github.io/2017/12/08/distributional_rl/\" target=\"_blank\" rel=\"noopener\">Blog: Distributional RL</a> </li>\n<li><a href=\"https://deepmind.com/blog/going-beyond-average-reinforcement-learning/\" target=\"_blank\" rel=\"noopener\">Deepmind Blog: Going beyond average for reinforcement learning</a></li>\n</ul>\n<p><br></p>\n<h2 id=\"Team\"><a href=\"#Team\" class=\"headerlink\" title=\"Team\"></a>Team</h2><p>민규식: <a href=\"https://github.com/Kyushik\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/kyushik.min\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>차금강: <a href=\"https://github.com/chagmgang\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/profile.php?id=100002147815509\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>윤승제: <a href=\"https://github.com/sjYoondeltar\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/seungje.yoon\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>김하영: <a href=\"https://github.com/hayoung-kim\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/altairyoung\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>김정대: <a href=\"https://github.com/kekmodel\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/kekmodel\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"/img/paper_c51.png\" width=\"800\"> </center>\n\n<p>논문 저자 : <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Bellemare%2C+M+G\" target=\"_blank\" rel=\"noopener\">Marc G. Bellemare</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Dabney%2C+W\" target=\"_blank\" rel=\"noopener\">Will Dabney</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Munos%2C+R\" target=\"_blank\" rel=\"noopener\">Rémi Munos</a><br>논문 링크 : <a href=\"https://arxiv.org/abs/1707.06887\" target=\"_blank\" rel=\"noopener\">ArXiv</a><br>Proceeding : International Conference on Machine Learning (ICML) 2017<br>정리 : 민규식</p>\n<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>본 게시물은 2017년에 발표된 논문 <a href=\"https://arxiv.org/abs/1707.06887\" target=\"_blank\" rel=\"noopener\">A Distributional Perspective on Reinforcement Learning</a> 의 내용에 대해 설명합니다.</p>\n<p>Distributional RL에 대해 설명한 게시물에서도 언급했듯이 distributional RL 알고리즘은 value를 하나의 scalar 값이 아닌 distribution으로 예측합니다.</p>\n<p align=\"center\"><br><br><img src=\"/img/distributionalRL.png\" alt=\"distributional RL\" width=\"600\"><br><br></p>\n\n<p> 이에 따라 일반적인 강화학습에서 이용하는 <code>bellman equation</code>의 value Q 대신 distribution Z를 사용합니다. 이 bellman equation을 <code>distributional bellman equation</code> 이라고 합니다. 해당 식들은 다음과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/bellman_equation.png\" alt=\"distributional RL\" width=\"600\"><br><br></p>\n\n<p><br></p>\n<p>이에 따라 network의 output도 각 알고리즘에 따라 차이가 있습니다. </p>\n<p><a href=\"https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning/\" target=\"_blank\" rel=\"noopener\">DQN</a>에서 network의 output이 각 action에 대한 Q-value였다면 distributional RL에서 network의 output은 아래 그림과 같이 각 action에 대한 value distribution 입니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/network_output.png\" alt=\"Network\" width=\"800\"><br><br></p>\n\n<p><br></p>\n<h2 id=\"Value-Distribution\"><a href=\"#Value-Distribution\" class=\"headerlink\" title=\"Value Distribution\"></a>Value Distribution</h2><p>Action에 대한 Distribution에 대해서 살펴보도록 하겠습니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/support_and_prob.png\" alt=\"Distribution\" width=\"800\"><br><br></p>\n\n<p>Network의 output은 위와 같은 이산 확률분포(Discrete Probability Distribution) 입니다. 여기서 가로축은 <strong>Support</strong> 혹은 <strong>atom</strong>이라고 부르며 value값을 나타냅니다. 세로축은 확률을 나타냅니다. 즉, 이 분포는 각각의 value와 그 value를 받을 확률을 나타내는 분포입니다.</p>\n<p>C51의 경우 분포를 결정해주기 위해서 몇가지 파라미터들이 필요합니다. 해당 파라미터들은 다음과 같습니다.</p>\n<ul>\n<li>Support의 수</li>\n<li>Support의 최대값</li>\n<li>Support의 최소값</li>\n</ul>\n<p>위 그림에서도 볼 수 있듯이 support값은 최소값부터 최대값까지 support의 수에 맞게 일정한 간격으로 나누게 됩니다. 즉, supports는 이렇게 미리 결정된 파라미터들에 의해 그 값이 정해지게 됩니다. 네트워크는 바로 이 support들에 대한 확률을 구해주게 됩니다. 각 action에 대해서 하나의 distribution이 필요하기 때문에 <strong>network output의 크기는 [support의 수 * action의 수]</strong> 가 됩니다.  </p>\n<p><br></p>\n<h2 id=\"Algorithm\"><a href=\"#Algorithm\" class=\"headerlink\" title=\"Algorithm\"></a>Algorithm</h2><p>알고리즘의 진행은 DQN과 거의 동일합니다. <code>Experience Replay</code> 나 <code>Target Network</code>  같은 기법도 그대로 이용합니다. 차이점은 아래 3가지 정도입니다. </p>\n<ol>\n<li>Q-value 계산</li>\n<li>Loss</li>\n<li>Target distribution 구하기</li>\n</ol>\n<p>각각 어떻게 차이가 있는지 한번 살펴보도록 하겠습니다. </p>\n<p><br></p>\n<h3 id=\"1-Q-value-계산\"><a href=\"#1-Q-value-계산\" class=\"headerlink\" title=\"1. Q-value 계산\"></a>1. Q-value 계산</h3><p>C51 알고리즘에서 Q-value를 계산하는 방법은 이산확률분포의 기대값을 구하는 것입니다. 이에 따라 각 action에 대한 Q-value를 계산하는 식은 다음과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/Equation_Q_c51.png\" alt=\"Equation of Q-value\" width=\"400\"><br><br></p>\n\n<p> 수식에서 사용된 각 기호는 위에 있는 Value distribution 그림을 참고해주세요! 각 action의 distribution마다 위의 식에 따라 연산을 수행하고 최대의 Q-value를 가지는 action을 선택하면 됩니다. </p>\n<p> 위 수식처럼 분포의 기대값을 구하는 연산을 하면 결과적으로 Q-value는 하나의 scalar값을 가지게 됩니다. 이 경우 “뭐지… 결국 scalar값으로 action 선택하자나…” 라고 생각하실수도 있지만!! 단순히 하나의 기대값을 추정하는 것보다 정확하게 분포를 추정하고 그 기대값을 구하는 것이 더 정확한 예측이 될 것이라 생각할 수 있습니다. </p>\n<p><br></p>\n<h3 id=\"2-Loss-구하기\"><a href=\"#2-Loss-구하기\" class=\"headerlink\" title=\"2. Loss 구하기\"></a>2. Loss 구하기</h3><p> Distributional RL의 경우 정확한 분포를 예측하는 것이 목적입니다. 이에 따라 target value가 아닌 target distribution을 통해 학습을 수행합니다. Target distribution과 추정된 distribution간의 차이를 줄이는 방향으로 학습을 수행해야 할텐데! 그럼 distribution간의 차이는 어떻게 구할까요? 본 논문에서는 <a href=\"https://en.wikipedia.org/wiki/Cross_entropy\" target=\"_blank\" rel=\"noopener\">Cross Entropy</a> 를 이용합니다. 그 식은 다음과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/Equation_loss_c51.png\" alt=\"Equation of Loss\" width=\"400\"><br><br></p>\n\n<p>마지막으로 위의 식에 필요한 target distribution을 구하는 과정에 대해 살펴보도록 하겠습니다. </p>\n<p><br></p>\n<h3 id=\"3-Target-Distribution\"><a href=\"#3-Target-Distribution\" class=\"headerlink\" title=\"3. Target Distribution\"></a>3. Target Distribution</h3><p>Target distribution을 만드는 이 과정이 C51 알고리즘의 구현에 있어 가장 귀찮고 까다로운 부분입니다. 우선 target distribution을 만들기 위해서 우선 value값과 직접적으로 연관이 있는 support에 대한 연산을 수행합니다. 해당 연산은 DQN에서 target value를 구하는 식과 같습니다. </p>\n<center> <img src=\"/img/Equation_target1_c51.png\" width=\"400\"> </center>\n\n<p>Target distribution의 경우 supports에 대해 위와 같은 연산을 해줍니다. 각 support에 discount factor를 곱하고 reward를 더해줍니다. (단, terminal이 false일 때, 즉 게임이 끝나지 않은 경우에만 이렇게 연산을 하고 게임이 끝난 경우에는 모든 support 값들을 reward 값으로 사용합니다 -&gt; supports = [r_t]) 이 값은 support의 최대값보다 큰 경우 최대값과 같도록, 최소값보다 작은 경우 최소값과 같도록 설정해줍니다. </p>\n<p>그런데 이 경우에 문제가 생길 수 있습니다! 예를 들어 supports가 [1, 2, 3, 4, 5] 인 경우 reward = 0.1, discount factor = 0.9라고 해보겠습니다. 이 경우 위의 식에 따라 연산을 해주면 supports가 [1, 1.9, 2.8, 3.7, 4.6]이 됩니다. Loss인 cross entropy 연산을 하기 위해서는 두 distribution의 supports가 일치해야되는데 support가 달라져 버렸습니다! 이 경우 때문에 target distribution의 support를 원래의 support와 같이 분배해주는 <strong>Projection</strong> 이라는 과정이 추가적으로 필요합니다! </p>\n<p>위의 과정을 그림으로 표현한 것이 다음과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/bellman_operation.png\" alt=\"Bellman Operation\" class=\"center\" width=\"800\"><br><br></p>\n\n<p> Projection의 경우 다음의 예시를 통해 설명을 하도록 하겠습니다. </p>\n<p>아래와 같은 상황이 있다고 해보겠습니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/projection1.png\" alt=\"Projection1\" class=\"center\" width=\"700\"><br><br></p>\n\n<p>변경된  support가 3.7인 경우를 예시로 하여 Projection을 해보도록 하겠습니다. 이 경우 3.7에 해당하는 확률값인 0.25를 support 3과 4로부터의 거리에 비례하여 분배합니다. 그 과정이 아래 그림과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/projection2.png\" alt=\"Projection2\" class=\"center\" width=\"500\"><br><br></p>\n\n<p>위와 같이 3.7은 기존의 support 3과 4 사이에 존재하는 값입니다. 3.7이 가진 확률값 0.25를 3과 4에 분배해야합니다. 이때 거리를 기반으로 값을 분배합니다. 3.7은 3보다는 4에 더 가깝죠. 그렇기 때문에 4에 더 많은 비율을 분배해줘야 합니다. 3과 4에서부터 3.7까지 거리의 비율은 각각 7:3 입니다. 이 3:7의 비율 중 3을 support값 3에, 7을 support값 4에 분배해줍니다. 그렇기 때문에 support 3의 확률값은 0.25 <em> 0.3 = 0.075을, support 4의 확률값은 0.25 </em> 0.7 = 0.175를 각각 할당받게 됩니다. 이 과정을 위의 예시에서 모든 supports에 적용한 결과가 다음과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/projection3.png\" alt=\"Projection2\" class=\"center\" width=\"800\"><br><br></p>\n\n<p>위와 같이 supports [1, 1.9, 2.8, 3.7, 4.6] 일때 확률값이 [0.1, 0.15, 0.2, 0.25, 0.3] 였던 것이 <strong>Projection</strong> 과정 이후에는 supports [1, 2, 3, 4, 5] 이고 각각에 대한 확률값이 [0.115, 0.175, 0.235, 0.295, 0.18]이 되었습니다. 이것이 최종적으로 구한 target distribution이며 이제 Projection을 통해 supports가 기존의 supports와 같아졌기 때문에 <strong>Cross entropy</strong> 연산이 가능하게 되었습니다!! </p>\n<p>여기까지가 Projection을 수행하는 과정입니다. </p>\n<p><br></p>\n<p>지금까지의 과정을 잘 이해하셨다면 논문에 나온 아래의 알고리즘을 더 수월하게 이해할 수 있을 것입니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/algorithm_c51.png\" alt=\"algorithm\" class=\"center\" width=\"800\"><br><br></p>\n\n<p><br></p>\n<h2 id=\"Result\"><a href=\"#Result\" class=\"headerlink\" title=\"Result\"></a>Result</h2><p>본 알고리즘의 성능은 Atari 환경에서 검증되었으며 이때 사용한 parameters는 다음과 같습니다. </p>\n<ul>\n<li>V(max) = 10 -&gt; support의 최대값</li>\n<li>V(min) = -10 -&gt; support의 최소값</li>\n<li>Epsilon (Adam) = 0.01 / (batch size) -&gt; Adam optimizer의 epsilon 값</li>\n<li>Learning rate = 0.00025</li>\n<li>Batch size = 32</li>\n</ul>\n<p>Atari 환경에서 테스트한 결과는 다음과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/result_c51.png\" alt=\"algorithm\" class=\"center\" width=\"800\"><br><br></p>\n\n<p>위의 결과에서 볼 수 있듯이 support의 수가 많아질수록 성능은 좋아지는 편입니다. 특히 51개의 support를 이용했을 때 <code>SeaQuest</code> 게임에서 아주 좋은 성능을 보입니다. 그래서 일반적으로 이 알고리즘에서는 51개의 support를 이용하며 이에 따라 <strong>C51</strong>이라고 알고리즘을 부르게 되었습니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/sparse_good_result_c51.png\" alt=\"algorithm\" class=\"center\" width=\"800\"><br><br></p>\n\n<p>위에서 볼 수 있듯이 sparse한 reward 환경인 Private Eye나 Venture 같은 게임에서도 다른 알고리즘들에 비해 좋은 결과를 보였다고 합니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/result_c51_2.png\" alt=\"algorithm\" class=\"center\" width=\"400\"><br><br></p>\n\n<p>또한 위에서 볼 수 있듯이 DQN, DDQN, Dueling, PER, PER + Dueling 과 비교했을 때에도 매우 좋은 성능을 보입니다. </p>\n<p align=\"center\"><br><br> <img src=\"/img/result_c51_3.png\" alt=\"algorithm\" class=\"center\" width=\"400\"><br><br></p>\n\n<p>마지막으로 위의 결과는 C51 vs DQN, C51 vs Human, DQN vs Human 을 했을 때 더 좋은 성능을 보인 게임의 수 입니다. 총 57개의 게임이 있으니 C51의 경우 약 절반 이상의 게임에서 사람보다 우수한 성능을 보인 알고리즘이라 할 수 있습니다. </p>\n<p><br></p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>C51의 경우 distributional RL 3형제 중 첫번째 알고리즘으로 distribution을 통해 value를 예측하는 알고리즘입니다. Distribution을 이용하여 value를 잘 예측하며 좋은 성능을 보이지만 이 알고리즘에서는 몇가지 아쉬운 점이 있습니다. </p>\n<ol>\n<li>support에 관련된 파라미터들을 결정해줘야하며 게임의 reward 범위에 따라 이 값들을 따로 설정해야 할 수 있습니다. 이런 점에 조금 귀찮은 점이 있습니다. </li>\n<li>알고리즘에 굉장히 번거로운 projection 과정이 포함되어 있습니다. </li>\n<li>마지막으로 해당 알고리즘의 경우 수학적으로 수렴성을 보장하지 못하는 알고리즘입니다. </li>\n</ol>\n<p align=\"center\"><br><br> <img src=\"/img/gamma_contraction.png\" alt=\"gamma contraction\" class=\"center\" width=\"400\"><br><br></p>\n\n<p>Distributional RL이 수렴하기 위해서는 위와 같은 gamma-contraction 조건을 만족해야합니다. value distribution간 거리를 측정하는 distance metric (d_p)가 <strong>Wasserstein distance</strong>인 경우 위 조건을 <strong>만족</strong>하지만 <strong>Cross entropy</strong> 의 경우 수학적으로 위의 조건을 만족한다는 보장이 없다고 합니다. 하지만 C51 논문은 wasserstein distance를 감소시킬 방법을 찾지 못한 관계로 Cross entropy를 loss로 설정하고 이를 줄이는 방향으로 학습을 수행하는 알고리즘이기 때문에 수학적으로 distributional RL의 수렴성을 증명하지는 못하는 논문입니다. </p>\n<p><br></p>\n<p>이 위의 3가지 문제점들을 해결한 논문이 C51의 후속으로 발표된 <a href=\"https://arxiv.org/abs/1710.10044\" target=\"_blank\" rel=\"noopener\">Distributional Reinforcement Learning with Quantile Regression (QR-DQN)</a> 논문입니다. 다음 게시물에서는 이 QR-DQN 논문의 내용에 대해 살펴보도록 하겠습니다!! :smile:</p>\n<p><br></p>\n<h2 id=\"Implementation\"><a href=\"#Implementation\" class=\"headerlink\" title=\"Implementation\"></a>Implementation</h2><p>본 논문의 코드는 다음의 Github를 참고해주세요. </p>\n<p><a href=\"https://github.com/reinforcement-learning-kr/distributional_rl\" target=\"_blank\" rel=\"noopener\">Github</a></p>\n<p><br></p>\n<h2 id=\"Other-Posts\"><a href=\"#Other-Posts\" class=\"headerlink\" title=\"Other Posts\"></a>Other Posts</h2><p><a href=\"https://reinforcement-learning-kr.github.io/2018/09/27/Distributional_intro/\">Distributional RL 개요</a></p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/10/22/QR-DQN/\">QR-DQN</a></p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/10/30/IQN/\">IQN</a></p>\n<p><br></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://arxiv.org/abs/1707.06887\" target=\"_blank\" rel=\"noopener\">A Distributional Perspective on Reinforcement Learning</a></li>\n<li><a href=\"https://flyyufelix.github.io/2017/10/24/distributional-bellman.html\" target=\"_blank\" rel=\"noopener\">Blog: Distributional Bellman and the C51 Algorithm</a></li>\n<li><a href=\"https://mtomassoli.github.io/2017/12/08/distributional_rl/\" target=\"_blank\" rel=\"noopener\">Blog: Distributional RL</a> </li>\n<li><a href=\"https://deepmind.com/blog/going-beyond-average-reinforcement-learning/\" target=\"_blank\" rel=\"noopener\">Deepmind Blog: Going beyond average for reinforcement learning</a></li>\n</ul>\n<p><br></p>\n<h2 id=\"Team\"><a href=\"#Team\" class=\"headerlink\" title=\"Team\"></a>Team</h2><p>민규식: <a href=\"https://github.com/Kyushik\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/kyushik.min\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>차금강: <a href=\"https://github.com/chagmgang\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/profile.php?id=100002147815509\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>윤승제: <a href=\"https://github.com/sjYoondeltar\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/seungje.yoon\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>김하영: <a href=\"https://github.com/hayoung-kim\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/altairyoung\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>김정대: <a href=\"https://github.com/kekmodel\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/kekmodel\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n"},{"title":"PG Travel Guide","date":"2018-06-28T16:11:26.000Z","author":"김동민, 이동민, 차금강","subtitle":"피지여행에 관한 개략적 기록","_content":"\n---\n\n# 0. Policy Gradient의 세계로\n\n반갑습니다! 저희는 PG여행을 위해 모인 PG탐험대입니다. 강화학습하면 보통 Q-learning을 떠올립니다. 그렇지만 오래전부터 Policy Gradient라는 Q-learning 못지 않은 훌륭한 방법론이 연구되어 왔고, 최근에는 강화학습의 최정점의 기술로 자리매김하고 있습니다. 강화학습의 아버지인 Sutton의 논문을 필두로 하여 기존의 DQN보다 뛰어난 성능을 내는 DPG와 DDPG, 그리고 현재 가장 주목받는 강화학습 연구자인 John Schulmann의 TRPO, GAE, PPO와 이를 이해하기 위해 필요한 Natural Policy Gardient까지 더불어 살펴보고자 합니다.\n\n<center> <img src=\"https://www.dropbox.com/s/tbcyhvilaqy4ra0/Policy%20Optimization%20in%20the%20RL%20Algorithm%20Landscape.png?dl=1\" width=\"800\"> </center>\n\n위의 그림은 강화학습 알고리즘 landscape에서 Policy Optimization의 관점을 중점적으로 하여 나타낸 그림입니다. 위의 그림에서 빨간색 작은 숫자로 나타낸 것이 저희 PG여행에서 다룰 논문들입니다. 순서는 다음과 같습니다.\n\n1. [Sutton_PG](http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)\n2. [DPG](http://proceedings.mlr.press/v32/silver14.pdf)\n3. [DDPG](https://arxiv.org/pdf/1509.02971.pdf)\n4. [NPG](https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf)\n5. [TRPO](https://arxiv.org/pdf/1502.05477.pdf)\n6. [GAE](https://arxiv.org/pdf/1506.02438.pdf)\n7. [PPO](https://arxiv.org/pdf/1707.06347.pdf)\n\n위와 같이 총 7가지 논문들을 리뷰하여 블로그로 정리하였습니다. 각 순서에 맞춰 보시는 것을 권장해드립니다.\n\n이 블로그에는 각각의 기술을 제안한 논문을 PG탐험대분들이 자세하게 리뷰한 포스트들이 있습니다. 우리나라에서 PG에 대해서 이렇게 자세하게 리뷰한 포스트들은 없었다고 감히 말씀드리고 싶습니다. 본 글에서는 이 포스트들을 읽기 전에 전체 내용을 개략적으로 소개하고 각각의 포스트들로 안내하고자 합니다. 자, 저희와 함께 PG여행을 즐겨보시겠습니까?\n\n<br><br>\n\n# 1. \\[Sutton PG\\] Policy gradient methods for reinforcement learning with function approximation\n\n[Sutton PG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/)\n\npolicy gradient (PG)는 expected reward를 policy의 파라미터에 대한 함수로 모델링하고 이 reward를 최대화하는 policy를 gradient ascent 기법을 이용해서 찾는 기법입니다. 강화학습의 대표격이라고 할 수 있는 Q-learning이라는 훌륭한 방법론이 이미 존재하고 있었지만 Q값의 작은 변화에도 policy가 크게 변할 수도 있다는 단점이 있기 때문에 policy의 점진적인 변화를 통해 더 나은 policy를 찾아가는 PG기법이 개발되었습니다.\n\n이 PG기법은 먼저 개발되었던 [REINFORCE](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf)라는 기법과 관련이 아주 많습니다. 서튼의 PG기법은 REINFORCE 기법을 [actor-critic algorithm](http://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf)을 사용하여 개선시킨 것이라고 볼 수도 있습니다. 저희 PG여행 팀도 처음에는 REINFORCE를 출발지로 삼으려고 했었지만 예전 논문이다보니 논문의 가독성이 너무 떨어져서 강화학습의 아버지라고 할 수 있는 서튼의 논문을 출발지로 삼았습니다. 하지만 이 논문도 만만치 않았습니다. 이 논문을 읽으시려는 분들께 저희의 여행기가 도움이 될 것입니다. PG기법에 대해서 먼저 감을 잡고 시작하시려면 [Andre Karpathy의 PG에 대한 블로그](http://karpathy.github.io/2016/05/31/rl/)를 먼저 한 번 읽어보세요.  한글번역도 있습니다! 1) http://keunwoochoi.blogspot.com/2016/06/andrej-karpathy.html 2) https://tensorflow.blog/2016/07/13/reinforce-pong-at-gym/\n\n[Sutton PG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/)\n\n<br><br>\n\n# 2. \\[DPG\\] Deterministic policy gradient algorithms\n\n[DPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/)\n\ndeterministic policy gradeint (DPG)는 어찌보면 상당히 도전적인 아이디어였던 것 같습니다. Sutton PG 논문에서는 DPG 스타일의 기법이 가진 단점에 대해서 언급하면서 stochastic policy gradient (SPG)를 써야 optimal을 찾을 수 있다고 기술하고 있었기 때문입니다.\n\n그런데 이 논문에서 높은 차원의 action space를 가지는 문제들에(예를 들면 문어발 제어) 대해서는 DPG가 상당히 좋은 성능을 내는 것을 저자들이 보였습니다. 그리고 DPG는 SPG와 대척점에 있는 기술이 아니고 SPG의 special case 중 하나임을 증명하면서 SPG를 가정하고 만들어진 기술들을 DPG에서도 그대로 이용할 수 있음을 보였습니다. David Silver의 [동영상 강의](http://techtalks.tv/talks/deterministic-policy-gradient-algorithms/61098/)를 한 번 보시길 추천드립니다. 짧은 강의지만 랩을 하듯이 쉴새없이 설명하는 Silver의 모습에서 천재성이 엿보이는 것을 확인하실 수 있습니다. \n\n[DPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/)\n\n<br><br>\n\n# 3. \\[DDPG\\] Continuous control with deep reinforcement learning\n\n[DDPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/)\n\nDPG의 후속 연구로 DPG보다 더 큰 주목을 받은 논문입니다. 소위 말하는 deep deterministic policy gradient (DDPG)로 불리는 기술을 제안한 논문입니다. 이 논문의 저자 중 일부는 그 유명한 DQN 논문의 저자이기도 합니다. Q-learning과 deep neural network를 접목시켰던 DQN처럼 이 논문도 DPG와 deep neural network를 접목시킨 논문입니다.\n\n이 논문은 DQN으로는 좋은 성능을 내지 못했던 continuous action을 가지는 상황들에 대해서 상당히 훌륭한 결과를 보이면서 큰 주목을 받았습니다. 소위 말하는 deep reinforcement learning (DRL)에서 Q-learning 계열의 DQN, PG 계열의 DDPG로 양대산맥을 이루는 논문이라고 할 수 있습니다. 두 논문 모두 Deepmind에서 나왔다는 것은 Deepmind 기술력이 DRL 분야에서 최정점에 있음을 보여주는 상징이 아닌가 싶습니다. 논문 자체는 그리 어렵지 않습니다. 새로운 아이디어를 제시했다기보다는 딥러닝을 활용한 강화학습의 가능성을 보여주는 논문이라는 점에서 큰 의의를 가지는 것 같습니다. 여러분도 한번 코딩에 도전해보시는게 어떨까요?\n\n[DDPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/)\n\n<br><br>\n\n# 4. \\[NPG\\] A natural policy gradient\n\n[NPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/)\n[NPG Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py)\n\n이 논문은 뒤이어 나오는 TRPO를 더 잘 이해하기 위해서 보는 논문입니다. 이번 논문부터 내용이 상당히 어려워집니다. 다소 생소한 수학 개념들이 많이 나오기 때문입니다. 하지만 이 블로그를 보시면 많은 부분들이 채워질 것이라고 믿습니다.\n\n2002년 당시에도 많은 연구자들이 objective function의 gradient 값을 따라서 좋은 policy $\\pi$를 찾고자 하였습니다. 하지만 기존의 우리가 알던 gradient descent method는 steepest descent direction이 아닐 수 있기 때문에(쉽게 말해 가장 가파른 방향을 따라서 내려가야 하는데 그러지 못할 수도 있다는 것입니다.) 이 논문에서 steepest descent direction를 나타내는 natural gradient method를 policy gradient에 적용하여 좋은 policy $\\pi$를 찾습니다. gradient descent는 parameter를 한 번에 많이 update 할 수 없는 반면, natural gradient는 가장 좋은 action을 고르도록 학습이 됩니다.\n\n또한 natural gradient method는 Line search 기법과 함께 사용하면 더 Policy Iteration과 비슷해집니다. Greedy Policy Iteration에서와 비교하면 Performance Improvement가 보장됩니다. 하지만 Fisher Information Matrix(FIM)이 asymtotically Hessian으로 수렴하지 않습니다. 그렇기 때문에 Conjugate Gradient Method로 구하는 방법이 더 좋을수 있습니다.\n\n위의 요약한 문장들만 봤을 때는 생소한 용어들이 많이 나와서 무슨 말인지 감이 안잡히실 수 있습니다. 저희가 포스팅한 블로그 글에는 다음과 같은 추가적인 내용이 나옵니다. 반드시 알고 가야 TRPO를 이해하는 것은 아닙니다. 다만 NPG를 이해하면 할 수록 TRPO를 접하기가 더 쉬울 수 있습니다.\n\n- Euclidean space와 Riemannian space의 차이\n- Natural Gradient 증명\n- Fisher Information Matrix(FIM)\n- Line Search\n- FIM과 Hessian 방법의 차이\n- Conjugate Gradient Method\n\n아래의 NPG Code는 Hessian 방법이 아닌 Conjugate Gradient Method를 사용한 \"Truncated Natural Policy Gradient(TNPG)\"라고 하는 방법의 코드입니다.\n\n마지막으로 프로젝트 내에 있는 한 팀원의 문장을 인용하겠습니다. \"머리가 아프고 힘들수록 잘하고 있는겁니다.\" NPG 논문을 보시는 분들 화이팅입니다!\n\n[NPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/)\n[NPG Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py)\n\n<br><br>\n\n# 5. \\[TRPO\\] Trust region policy optimization\n\n[TRPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/)\n[TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py)\n\nPG기법이 각광을 받게 된 계기는 아마도 TRPO 때문이 아닌가 싶습니다. DQN이라는 인공지능이 아타리게임을 사람보다 잘 플레이했던 영상을 보고 충격을 받았던 사람들이 많았던 것처럼 TRPO가 난제로 여겨졌던 서기, 걷기, 뛰기 같은 로봇에 적용할 수 있는 continuous action을 잘 수행하는 것을 보고 많은 사람들이 놀라워했습니다. (솔직히 처음 아타리게임을 봤을 때 만큼은 아니지만...) 이것들은 DQN으로는 달성할 수 없었던 난제였습니다. \n\n그렇지만 거인의 어깨 위에서 세상을 바라보았던 많은 과학자들이 그랬던 것처럼 Schulmann도 TRPO를 갑자기 생각낸 것은 아닙니다. policy를 업데이트할 때 기존의 policy와 너무 많이 달라지면 문제가 생깁니다. 일단, state distribution이 변합니다. state distribution은 간단히 얘기하면 각각의 state를 방문하는 빈도를 의미한다고 할 수 있습니다. policy가 많이 바뀌면 기존의 state distribution을 그대로 이용할 수 없고 이렇게 되면 policy gradient를 구하기 어려워집니다. \n\n그래서 착안한 것이 policy의 변화 정도를 제한하는 것입니다. 여기서 우리는 stochastic policy, 즉, 확률적 policy를 이용한다고 전제하고 있기 때문에, policy의 변화는 policy 확률 분포의 변화를 의미합니다. 현재 policy와 새로운 policy 사이의 확률 분포의 변화를 어떻게 측정할 수 있을까요? Schulmann은 [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)라는 척도를 이용하였습니다. 이 변화량을 특정값 이하로 제한하고 이것을 만족하는 영역을 trust region이라고 불렀습니다. \n\n그렇지만 문제는 풀기에는 복잡했습니다. 이것을 풀기 위해서 여러 번의 근사화를 적용시켰습니다. objective function에는 linear approximation을 적용하고 trust region constraint에는 second-order approximation을 적용하면 문제가 좀 더 풀기 쉬운 형태로 바뀐다는 것을 알아냈습니다. 이것은 사실 natural gradient를 이용하는 것이었습니다. 이러한 이유로 TRPO를 이해하기 위해서 natural gradient도 살펴보았던 것입니다. \n\nSchulmann이 너무 똑똑해서 일까요? 훌륭한 아이디어로 policy gradient 기법의 르네상스를 열은 Schulmann의 역작인 TRPO 논문은 이해하기 쉽게 쓰여지지 않은 것 같습니다. (더 잘 쓸 수 있었잖아 Schulmann...) 저희의 포스트와 함께 보다 편하게 여행하시길 바랍니다. 이 [유투브 영상](https://youtu.be/CKaN5PgkSBc)도 무조건 보세요~\n\n[TRPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/)\n[TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py)\n\n<br><br>\n\n# 6. \\[GAE\\] High-Dimensional Continuous Control Using Generalized Advantage Estimation\n\n[GAE 여행하기](https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/)\n\nTRPO가 나오고 난 뒤로도 복잡하고 어려운 control problem에서 Reinforcement Learning (RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 \"variance reduction\"에 대해 연구하였습니다.\n\n\"Generalized Advantage Estimator (GAE)\"라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.\n또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는지를 보였습니다.\n\n이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.\n\nGAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.\n\n추가적으로 앞으로 연구되어야할 부분은 만약 value function estimation error와 policy gradient estimation error 사이의 관계를 알아낸다면, value function fitting에 더 잘 맞는 error metric(policy gradient estimation의 정확성과 더 잘 맞는 value function)을 사용할 수 있을 것입니다. 여기서 policy와 value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.\n\n[GAE 여행하기](https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/)\n\n<br><br>\n\n# 7. \\[PPO\\] Proximal policy optimization algorithms\n\n[PPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/)\n[PPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/ppo_gae.py)\n\n이 논문에서는 Reinforcement Learning에서 Policy Gradient Method의 새로운 방법인 PPO를 제안합니다. 이 방법은 agent가 환경과의 상호작용을 통해 data를 sampling하는 것과 stochastic gradient ascent를 사용하여 \"surrogate\" objective function을 optimizing하는 것을 번갈아가면서 하는 방법입니다. data sample마다 one gradient update를 수행하는 기존의 방법과는 달리, minibatch update의 multiple epochs를 가능하게 하는 새로운 objective function을 말합니다.\n\n또한 PPO는 TRPO의 연장선상에 있는 알고리즘이라고 할 수 있습니다. TRPO에서는 문제를 단순하게 만들기 위해서 최적화 문제를 여러 번 변형시켰지만, PPO는 단순하게 clip이라는 개념을 사용합니다. TRPO에서 이용했던 surrogate objective function을 reward가 특정값 이상이거나 이하가 될 때 더 이상 변화시키지 않는 것입니다.\n\n이 알고리즘의 장점으로는\n\n- TRPO의 장점만을 가집니다. 다시 말해 알고리즘으로 학습하기에 훨씬 더 간단하고, 더 일반적이고, 더 좋은 sample complexity (empirically)를 가집니다.\n- 또한 다른 online policy gradient method들을 능가했고, 전반적으로 sample complexity(특히 computational complexity), simplicity, wall-time가 좋다고 합니다.\n\n[PPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/)\n[PPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/ppo_gae.py)\n","source":"_posts/0_pg-travel-guide.md","raw":"---\ntitle: PG Travel Guide\ndate: 2018-06-29 01:11:26\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 김동민, 이동민, 차금강\nsubtitle: 피지여행에 관한 개략적 기록\n---\n\n---\n\n# 0. Policy Gradient의 세계로\n\n반갑습니다! 저희는 PG여행을 위해 모인 PG탐험대입니다. 강화학습하면 보통 Q-learning을 떠올립니다. 그렇지만 오래전부터 Policy Gradient라는 Q-learning 못지 않은 훌륭한 방법론이 연구되어 왔고, 최근에는 강화학습의 최정점의 기술로 자리매김하고 있습니다. 강화학습의 아버지인 Sutton의 논문을 필두로 하여 기존의 DQN보다 뛰어난 성능을 내는 DPG와 DDPG, 그리고 현재 가장 주목받는 강화학습 연구자인 John Schulmann의 TRPO, GAE, PPO와 이를 이해하기 위해 필요한 Natural Policy Gardient까지 더불어 살펴보고자 합니다.\n\n<center> <img src=\"https://www.dropbox.com/s/tbcyhvilaqy4ra0/Policy%20Optimization%20in%20the%20RL%20Algorithm%20Landscape.png?dl=1\" width=\"800\"> </center>\n\n위의 그림은 강화학습 알고리즘 landscape에서 Policy Optimization의 관점을 중점적으로 하여 나타낸 그림입니다. 위의 그림에서 빨간색 작은 숫자로 나타낸 것이 저희 PG여행에서 다룰 논문들입니다. 순서는 다음과 같습니다.\n\n1. [Sutton_PG](http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)\n2. [DPG](http://proceedings.mlr.press/v32/silver14.pdf)\n3. [DDPG](https://arxiv.org/pdf/1509.02971.pdf)\n4. [NPG](https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf)\n5. [TRPO](https://arxiv.org/pdf/1502.05477.pdf)\n6. [GAE](https://arxiv.org/pdf/1506.02438.pdf)\n7. [PPO](https://arxiv.org/pdf/1707.06347.pdf)\n\n위와 같이 총 7가지 논문들을 리뷰하여 블로그로 정리하였습니다. 각 순서에 맞춰 보시는 것을 권장해드립니다.\n\n이 블로그에는 각각의 기술을 제안한 논문을 PG탐험대분들이 자세하게 리뷰한 포스트들이 있습니다. 우리나라에서 PG에 대해서 이렇게 자세하게 리뷰한 포스트들은 없었다고 감히 말씀드리고 싶습니다. 본 글에서는 이 포스트들을 읽기 전에 전체 내용을 개략적으로 소개하고 각각의 포스트들로 안내하고자 합니다. 자, 저희와 함께 PG여행을 즐겨보시겠습니까?\n\n<br><br>\n\n# 1. \\[Sutton PG\\] Policy gradient methods for reinforcement learning with function approximation\n\n[Sutton PG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/)\n\npolicy gradient (PG)는 expected reward를 policy의 파라미터에 대한 함수로 모델링하고 이 reward를 최대화하는 policy를 gradient ascent 기법을 이용해서 찾는 기법입니다. 강화학습의 대표격이라고 할 수 있는 Q-learning이라는 훌륭한 방법론이 이미 존재하고 있었지만 Q값의 작은 변화에도 policy가 크게 변할 수도 있다는 단점이 있기 때문에 policy의 점진적인 변화를 통해 더 나은 policy를 찾아가는 PG기법이 개발되었습니다.\n\n이 PG기법은 먼저 개발되었던 [REINFORCE](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf)라는 기법과 관련이 아주 많습니다. 서튼의 PG기법은 REINFORCE 기법을 [actor-critic algorithm](http://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf)을 사용하여 개선시킨 것이라고 볼 수도 있습니다. 저희 PG여행 팀도 처음에는 REINFORCE를 출발지로 삼으려고 했었지만 예전 논문이다보니 논문의 가독성이 너무 떨어져서 강화학습의 아버지라고 할 수 있는 서튼의 논문을 출발지로 삼았습니다. 하지만 이 논문도 만만치 않았습니다. 이 논문을 읽으시려는 분들께 저희의 여행기가 도움이 될 것입니다. PG기법에 대해서 먼저 감을 잡고 시작하시려면 [Andre Karpathy의 PG에 대한 블로그](http://karpathy.github.io/2016/05/31/rl/)를 먼저 한 번 읽어보세요.  한글번역도 있습니다! 1) http://keunwoochoi.blogspot.com/2016/06/andrej-karpathy.html 2) https://tensorflow.blog/2016/07/13/reinforce-pong-at-gym/\n\n[Sutton PG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/)\n\n<br><br>\n\n# 2. \\[DPG\\] Deterministic policy gradient algorithms\n\n[DPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/)\n\ndeterministic policy gradeint (DPG)는 어찌보면 상당히 도전적인 아이디어였던 것 같습니다. Sutton PG 논문에서는 DPG 스타일의 기법이 가진 단점에 대해서 언급하면서 stochastic policy gradient (SPG)를 써야 optimal을 찾을 수 있다고 기술하고 있었기 때문입니다.\n\n그런데 이 논문에서 높은 차원의 action space를 가지는 문제들에(예를 들면 문어발 제어) 대해서는 DPG가 상당히 좋은 성능을 내는 것을 저자들이 보였습니다. 그리고 DPG는 SPG와 대척점에 있는 기술이 아니고 SPG의 special case 중 하나임을 증명하면서 SPG를 가정하고 만들어진 기술들을 DPG에서도 그대로 이용할 수 있음을 보였습니다. David Silver의 [동영상 강의](http://techtalks.tv/talks/deterministic-policy-gradient-algorithms/61098/)를 한 번 보시길 추천드립니다. 짧은 강의지만 랩을 하듯이 쉴새없이 설명하는 Silver의 모습에서 천재성이 엿보이는 것을 확인하실 수 있습니다. \n\n[DPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/)\n\n<br><br>\n\n# 3. \\[DDPG\\] Continuous control with deep reinforcement learning\n\n[DDPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/)\n\nDPG의 후속 연구로 DPG보다 더 큰 주목을 받은 논문입니다. 소위 말하는 deep deterministic policy gradient (DDPG)로 불리는 기술을 제안한 논문입니다. 이 논문의 저자 중 일부는 그 유명한 DQN 논문의 저자이기도 합니다. Q-learning과 deep neural network를 접목시켰던 DQN처럼 이 논문도 DPG와 deep neural network를 접목시킨 논문입니다.\n\n이 논문은 DQN으로는 좋은 성능을 내지 못했던 continuous action을 가지는 상황들에 대해서 상당히 훌륭한 결과를 보이면서 큰 주목을 받았습니다. 소위 말하는 deep reinforcement learning (DRL)에서 Q-learning 계열의 DQN, PG 계열의 DDPG로 양대산맥을 이루는 논문이라고 할 수 있습니다. 두 논문 모두 Deepmind에서 나왔다는 것은 Deepmind 기술력이 DRL 분야에서 최정점에 있음을 보여주는 상징이 아닌가 싶습니다. 논문 자체는 그리 어렵지 않습니다. 새로운 아이디어를 제시했다기보다는 딥러닝을 활용한 강화학습의 가능성을 보여주는 논문이라는 점에서 큰 의의를 가지는 것 같습니다. 여러분도 한번 코딩에 도전해보시는게 어떨까요?\n\n[DDPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/)\n\n<br><br>\n\n# 4. \\[NPG\\] A natural policy gradient\n\n[NPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/)\n[NPG Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py)\n\n이 논문은 뒤이어 나오는 TRPO를 더 잘 이해하기 위해서 보는 논문입니다. 이번 논문부터 내용이 상당히 어려워집니다. 다소 생소한 수학 개념들이 많이 나오기 때문입니다. 하지만 이 블로그를 보시면 많은 부분들이 채워질 것이라고 믿습니다.\n\n2002년 당시에도 많은 연구자들이 objective function의 gradient 값을 따라서 좋은 policy $\\pi$를 찾고자 하였습니다. 하지만 기존의 우리가 알던 gradient descent method는 steepest descent direction이 아닐 수 있기 때문에(쉽게 말해 가장 가파른 방향을 따라서 내려가야 하는데 그러지 못할 수도 있다는 것입니다.) 이 논문에서 steepest descent direction를 나타내는 natural gradient method를 policy gradient에 적용하여 좋은 policy $\\pi$를 찾습니다. gradient descent는 parameter를 한 번에 많이 update 할 수 없는 반면, natural gradient는 가장 좋은 action을 고르도록 학습이 됩니다.\n\n또한 natural gradient method는 Line search 기법과 함께 사용하면 더 Policy Iteration과 비슷해집니다. Greedy Policy Iteration에서와 비교하면 Performance Improvement가 보장됩니다. 하지만 Fisher Information Matrix(FIM)이 asymtotically Hessian으로 수렴하지 않습니다. 그렇기 때문에 Conjugate Gradient Method로 구하는 방법이 더 좋을수 있습니다.\n\n위의 요약한 문장들만 봤을 때는 생소한 용어들이 많이 나와서 무슨 말인지 감이 안잡히실 수 있습니다. 저희가 포스팅한 블로그 글에는 다음과 같은 추가적인 내용이 나옵니다. 반드시 알고 가야 TRPO를 이해하는 것은 아닙니다. 다만 NPG를 이해하면 할 수록 TRPO를 접하기가 더 쉬울 수 있습니다.\n\n- Euclidean space와 Riemannian space의 차이\n- Natural Gradient 증명\n- Fisher Information Matrix(FIM)\n- Line Search\n- FIM과 Hessian 방법의 차이\n- Conjugate Gradient Method\n\n아래의 NPG Code는 Hessian 방법이 아닌 Conjugate Gradient Method를 사용한 \"Truncated Natural Policy Gradient(TNPG)\"라고 하는 방법의 코드입니다.\n\n마지막으로 프로젝트 내에 있는 한 팀원의 문장을 인용하겠습니다. \"머리가 아프고 힘들수록 잘하고 있는겁니다.\" NPG 논문을 보시는 분들 화이팅입니다!\n\n[NPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/)\n[NPG Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py)\n\n<br><br>\n\n# 5. \\[TRPO\\] Trust region policy optimization\n\n[TRPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/)\n[TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py)\n\nPG기법이 각광을 받게 된 계기는 아마도 TRPO 때문이 아닌가 싶습니다. DQN이라는 인공지능이 아타리게임을 사람보다 잘 플레이했던 영상을 보고 충격을 받았던 사람들이 많았던 것처럼 TRPO가 난제로 여겨졌던 서기, 걷기, 뛰기 같은 로봇에 적용할 수 있는 continuous action을 잘 수행하는 것을 보고 많은 사람들이 놀라워했습니다. (솔직히 처음 아타리게임을 봤을 때 만큼은 아니지만...) 이것들은 DQN으로는 달성할 수 없었던 난제였습니다. \n\n그렇지만 거인의 어깨 위에서 세상을 바라보았던 많은 과학자들이 그랬던 것처럼 Schulmann도 TRPO를 갑자기 생각낸 것은 아닙니다. policy를 업데이트할 때 기존의 policy와 너무 많이 달라지면 문제가 생깁니다. 일단, state distribution이 변합니다. state distribution은 간단히 얘기하면 각각의 state를 방문하는 빈도를 의미한다고 할 수 있습니다. policy가 많이 바뀌면 기존의 state distribution을 그대로 이용할 수 없고 이렇게 되면 policy gradient를 구하기 어려워집니다. \n\n그래서 착안한 것이 policy의 변화 정도를 제한하는 것입니다. 여기서 우리는 stochastic policy, 즉, 확률적 policy를 이용한다고 전제하고 있기 때문에, policy의 변화는 policy 확률 분포의 변화를 의미합니다. 현재 policy와 새로운 policy 사이의 확률 분포의 변화를 어떻게 측정할 수 있을까요? Schulmann은 [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)라는 척도를 이용하였습니다. 이 변화량을 특정값 이하로 제한하고 이것을 만족하는 영역을 trust region이라고 불렀습니다. \n\n그렇지만 문제는 풀기에는 복잡했습니다. 이것을 풀기 위해서 여러 번의 근사화를 적용시켰습니다. objective function에는 linear approximation을 적용하고 trust region constraint에는 second-order approximation을 적용하면 문제가 좀 더 풀기 쉬운 형태로 바뀐다는 것을 알아냈습니다. 이것은 사실 natural gradient를 이용하는 것이었습니다. 이러한 이유로 TRPO를 이해하기 위해서 natural gradient도 살펴보았던 것입니다. \n\nSchulmann이 너무 똑똑해서 일까요? 훌륭한 아이디어로 policy gradient 기법의 르네상스를 열은 Schulmann의 역작인 TRPO 논문은 이해하기 쉽게 쓰여지지 않은 것 같습니다. (더 잘 쓸 수 있었잖아 Schulmann...) 저희의 포스트와 함께 보다 편하게 여행하시길 바랍니다. 이 [유투브 영상](https://youtu.be/CKaN5PgkSBc)도 무조건 보세요~\n\n[TRPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/)\n[TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py)\n\n<br><br>\n\n# 6. \\[GAE\\] High-Dimensional Continuous Control Using Generalized Advantage Estimation\n\n[GAE 여행하기](https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/)\n\nTRPO가 나오고 난 뒤로도 복잡하고 어려운 control problem에서 Reinforcement Learning (RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 \"variance reduction\"에 대해 연구하였습니다.\n\n\"Generalized Advantage Estimator (GAE)\"라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.\n또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는지를 보였습니다.\n\n이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.\n\nGAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.\n\n추가적으로 앞으로 연구되어야할 부분은 만약 value function estimation error와 policy gradient estimation error 사이의 관계를 알아낸다면, value function fitting에 더 잘 맞는 error metric(policy gradient estimation의 정확성과 더 잘 맞는 value function)을 사용할 수 있을 것입니다. 여기서 policy와 value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.\n\n[GAE 여행하기](https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/)\n\n<br><br>\n\n# 7. \\[PPO\\] Proximal policy optimization algorithms\n\n[PPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/)\n[PPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/ppo_gae.py)\n\n이 논문에서는 Reinforcement Learning에서 Policy Gradient Method의 새로운 방법인 PPO를 제안합니다. 이 방법은 agent가 환경과의 상호작용을 통해 data를 sampling하는 것과 stochastic gradient ascent를 사용하여 \"surrogate\" objective function을 optimizing하는 것을 번갈아가면서 하는 방법입니다. data sample마다 one gradient update를 수행하는 기존의 방법과는 달리, minibatch update의 multiple epochs를 가능하게 하는 새로운 objective function을 말합니다.\n\n또한 PPO는 TRPO의 연장선상에 있는 알고리즘이라고 할 수 있습니다. TRPO에서는 문제를 단순하게 만들기 위해서 최적화 문제를 여러 번 변형시켰지만, PPO는 단순하게 clip이라는 개념을 사용합니다. TRPO에서 이용했던 surrogate objective function을 reward가 특정값 이상이거나 이하가 될 때 더 이상 변화시키지 않는 것입니다.\n\n이 알고리즘의 장점으로는\n\n- TRPO의 장점만을 가집니다. 다시 말해 알고리즘으로 학습하기에 훨씬 더 간단하고, 더 일반적이고, 더 좋은 sample complexity (empirically)를 가집니다.\n- 또한 다른 online policy gradient method들을 능가했고, 전반적으로 sample complexity(특히 computational complexity), simplicity, wall-time가 좋다고 합니다.\n\n[PPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/)\n[PPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/ppo_gae.py)\n","slug":"0_pg-travel-guide","published":1,"updated":"2019-02-07T11:21:31.912Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjrujlu05000z5wfew0licp76","content":"<hr>\n<h1 id=\"0-Policy-Gradient의-세계로\"><a href=\"#0-Policy-Gradient의-세계로\" class=\"headerlink\" title=\"0. Policy Gradient의 세계로\"></a>0. Policy Gradient의 세계로</h1><p>반갑습니다! 저희는 PG여행을 위해 모인 PG탐험대입니다. 강화학습하면 보통 Q-learning을 떠올립니다. 그렇지만 오래전부터 Policy Gradient라는 Q-learning 못지 않은 훌륭한 방법론이 연구되어 왔고, 최근에는 강화학습의 최정점의 기술로 자리매김하고 있습니다. 강화학습의 아버지인 Sutton의 논문을 필두로 하여 기존의 DQN보다 뛰어난 성능을 내는 DPG와 DDPG, 그리고 현재 가장 주목받는 강화학습 연구자인 John Schulmann의 TRPO, GAE, PPO와 이를 이해하기 위해 필요한 Natural Policy Gardient까지 더불어 살펴보고자 합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/tbcyhvilaqy4ra0/Policy%20Optimization%20in%20the%20RL%20Algorithm%20Landscape.png?dl=1\" width=\"800\"> </center>\n\n<p>위의 그림은 강화학습 알고리즘 landscape에서 Policy Optimization의 관점을 중점적으로 하여 나타낸 그림입니다. 위의 그림에서 빨간색 작은 숫자로 나타낸 것이 저희 PG여행에서 다룰 논문들입니다. 순서는 다음과 같습니다.</p>\n<ol>\n<li><a href=\"http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf\" target=\"_blank\" rel=\"noopener\">Sutton_PG</a></li>\n<li><a href=\"http://proceedings.mlr.press/v32/silver14.pdf\" target=\"_blank\" rel=\"noopener\">DPG</a></li>\n<li><a href=\"https://arxiv.org/pdf/1509.02971.pdf\" target=\"_blank\" rel=\"noopener\">DDPG</a></li>\n<li><a href=\"https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf\" target=\"_blank\" rel=\"noopener\">NPG</a></li>\n<li><a href=\"https://arxiv.org/pdf/1502.05477.pdf\" target=\"_blank\" rel=\"noopener\">TRPO</a></li>\n<li><a href=\"https://arxiv.org/pdf/1506.02438.pdf\" target=\"_blank\" rel=\"noopener\">GAE</a></li>\n<li><a href=\"https://arxiv.org/pdf/1707.06347.pdf\" target=\"_blank\" rel=\"noopener\">PPO</a></li>\n</ol>\n<p>위와 같이 총 7가지 논문들을 리뷰하여 블로그로 정리하였습니다. 각 순서에 맞춰 보시는 것을 권장해드립니다.</p>\n<p>이 블로그에는 각각의 기술을 제안한 논문을 PG탐험대분들이 자세하게 리뷰한 포스트들이 있습니다. 우리나라에서 PG에 대해서 이렇게 자세하게 리뷰한 포스트들은 없었다고 감히 말씀드리고 싶습니다. 본 글에서는 이 포스트들을 읽기 전에 전체 내용을 개략적으로 소개하고 각각의 포스트들로 안내하고자 합니다. 자, 저희와 함께 PG여행을 즐겨보시겠습니까?</p>\n<p><br><br></p>\n<h1 id=\"1-Sutton-PG-Policy-gradient-methods-for-reinforcement-learning-with-function-approximation\"><a href=\"#1-Sutton-PG-Policy-gradient-methods-for-reinforcement-learning-with-function-approximation\" class=\"headerlink\" title=\"1. [Sutton PG] Policy gradient methods for reinforcement learning with function approximation\"></a>1. [Sutton PG] Policy gradient methods for reinforcement learning with function approximation</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/\">Sutton PG 여행하기</a></p>\n<p>policy gradient (PG)는 expected reward를 policy의 파라미터에 대한 함수로 모델링하고 이 reward를 최대화하는 policy를 gradient ascent 기법을 이용해서 찾는 기법입니다. 강화학습의 대표격이라고 할 수 있는 Q-learning이라는 훌륭한 방법론이 이미 존재하고 있었지만 Q값의 작은 변화에도 policy가 크게 변할 수도 있다는 단점이 있기 때문에 policy의 점진적인 변화를 통해 더 나은 policy를 찾아가는 PG기법이 개발되었습니다.</p>\n<p>이 PG기법은 먼저 개발되었던 <a href=\"https://link.springer.com/content/pdf/10.1007/BF00992696.pdf\" target=\"_blank\" rel=\"noopener\">REINFORCE</a>라는 기법과 관련이 아주 많습니다. 서튼의 PG기법은 REINFORCE 기법을 <a href=\"http://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf\" target=\"_blank\" rel=\"noopener\">actor-critic algorithm</a>을 사용하여 개선시킨 것이라고 볼 수도 있습니다. 저희 PG여행 팀도 처음에는 REINFORCE를 출발지로 삼으려고 했었지만 예전 논문이다보니 논문의 가독성이 너무 떨어져서 강화학습의 아버지라고 할 수 있는 서튼의 논문을 출발지로 삼았습니다. 하지만 이 논문도 만만치 않았습니다. 이 논문을 읽으시려는 분들께 저희의 여행기가 도움이 될 것입니다. PG기법에 대해서 먼저 감을 잡고 시작하시려면 <a href=\"http://karpathy.github.io/2016/05/31/rl/\" target=\"_blank\" rel=\"noopener\">Andre Karpathy의 PG에 대한 블로그</a>를 먼저 한 번 읽어보세요.  한글번역도 있습니다! 1) <a href=\"http://keunwoochoi.blogspot.com/2016/06/andrej-karpathy.html\" target=\"_blank\" rel=\"noopener\">http://keunwoochoi.blogspot.com/2016/06/andrej-karpathy.html</a> 2) <a href=\"https://tensorflow.blog/2016/07/13/reinforce-pong-at-gym/\" target=\"_blank\" rel=\"noopener\">https://tensorflow.blog/2016/07/13/reinforce-pong-at-gym/</a></p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/\">Sutton PG 여행하기</a></p>\n<p><br><br></p>\n<h1 id=\"2-DPG-Deterministic-policy-gradient-algorithms\"><a href=\"#2-DPG-Deterministic-policy-gradient-algorithms\" class=\"headerlink\" title=\"2. [DPG] Deterministic policy gradient algorithms\"></a>2. [DPG] Deterministic policy gradient algorithms</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/\">DPG 여행하기</a></p>\n<p>deterministic policy gradeint (DPG)는 어찌보면 상당히 도전적인 아이디어였던 것 같습니다. Sutton PG 논문에서는 DPG 스타일의 기법이 가진 단점에 대해서 언급하면서 stochastic policy gradient (SPG)를 써야 optimal을 찾을 수 있다고 기술하고 있었기 때문입니다.</p>\n<p>그런데 이 논문에서 높은 차원의 action space를 가지는 문제들에(예를 들면 문어발 제어) 대해서는 DPG가 상당히 좋은 성능을 내는 것을 저자들이 보였습니다. 그리고 DPG는 SPG와 대척점에 있는 기술이 아니고 SPG의 special case 중 하나임을 증명하면서 SPG를 가정하고 만들어진 기술들을 DPG에서도 그대로 이용할 수 있음을 보였습니다. David Silver의 <a href=\"http://techtalks.tv/talks/deterministic-policy-gradient-algorithms/61098/\" target=\"_blank\" rel=\"noopener\">동영상 강의</a>를 한 번 보시길 추천드립니다. 짧은 강의지만 랩을 하듯이 쉴새없이 설명하는 Silver의 모습에서 천재성이 엿보이는 것을 확인하실 수 있습니다. </p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/\">DPG 여행하기</a></p>\n<p><br><br></p>\n<h1 id=\"3-DDPG-Continuous-control-with-deep-reinforcement-learning\"><a href=\"#3-DDPG-Continuous-control-with-deep-reinforcement-learning\" class=\"headerlink\" title=\"3. [DDPG] Continuous control with deep reinforcement learning\"></a>3. [DDPG] Continuous control with deep reinforcement learning</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/\">DDPG 여행하기</a></p>\n<p>DPG의 후속 연구로 DPG보다 더 큰 주목을 받은 논문입니다. 소위 말하는 deep deterministic policy gradient (DDPG)로 불리는 기술을 제안한 논문입니다. 이 논문의 저자 중 일부는 그 유명한 DQN 논문의 저자이기도 합니다. Q-learning과 deep neural network를 접목시켰던 DQN처럼 이 논문도 DPG와 deep neural network를 접목시킨 논문입니다.</p>\n<p>이 논문은 DQN으로는 좋은 성능을 내지 못했던 continuous action을 가지는 상황들에 대해서 상당히 훌륭한 결과를 보이면서 큰 주목을 받았습니다. 소위 말하는 deep reinforcement learning (DRL)에서 Q-learning 계열의 DQN, PG 계열의 DDPG로 양대산맥을 이루는 논문이라고 할 수 있습니다. 두 논문 모두 Deepmind에서 나왔다는 것은 Deepmind 기술력이 DRL 분야에서 최정점에 있음을 보여주는 상징이 아닌가 싶습니다. 논문 자체는 그리 어렵지 않습니다. 새로운 아이디어를 제시했다기보다는 딥러닝을 활용한 강화학습의 가능성을 보여주는 논문이라는 점에서 큰 의의를 가지는 것 같습니다. 여러분도 한번 코딩에 도전해보시는게 어떨까요?</p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/\">DDPG 여행하기</a></p>\n<p><br><br></p>\n<h1 id=\"4-NPG-A-natural-policy-gradient\"><a href=\"#4-NPG-A-natural-policy-gradient\" class=\"headerlink\" title=\"4. [NPG] A natural policy gradient\"></a>4. [NPG] A natural policy gradient</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/\">NPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py\" target=\"_blank\" rel=\"noopener\">NPG Code</a></p>\n<p>이 논문은 뒤이어 나오는 TRPO를 더 잘 이해하기 위해서 보는 논문입니다. 이번 논문부터 내용이 상당히 어려워집니다. 다소 생소한 수학 개념들이 많이 나오기 때문입니다. 하지만 이 블로그를 보시면 많은 부분들이 채워질 것이라고 믿습니다.</p>\n<p>2002년 당시에도 많은 연구자들이 objective function의 gradient 값을 따라서 좋은 policy $\\pi$를 찾고자 하였습니다. 하지만 기존의 우리가 알던 gradient descent method는 steepest descent direction이 아닐 수 있기 때문에(쉽게 말해 가장 가파른 방향을 따라서 내려가야 하는데 그러지 못할 수도 있다는 것입니다.) 이 논문에서 steepest descent direction를 나타내는 natural gradient method를 policy gradient에 적용하여 좋은 policy $\\pi$를 찾습니다. gradient descent는 parameter를 한 번에 많이 update 할 수 없는 반면, natural gradient는 가장 좋은 action을 고르도록 학습이 됩니다.</p>\n<p>또한 natural gradient method는 Line search 기법과 함께 사용하면 더 Policy Iteration과 비슷해집니다. Greedy Policy Iteration에서와 비교하면 Performance Improvement가 보장됩니다. 하지만 Fisher Information Matrix(FIM)이 asymtotically Hessian으로 수렴하지 않습니다. 그렇기 때문에 Conjugate Gradient Method로 구하는 방법이 더 좋을수 있습니다.</p>\n<p>위의 요약한 문장들만 봤을 때는 생소한 용어들이 많이 나와서 무슨 말인지 감이 안잡히실 수 있습니다. 저희가 포스팅한 블로그 글에는 다음과 같은 추가적인 내용이 나옵니다. 반드시 알고 가야 TRPO를 이해하는 것은 아닙니다. 다만 NPG를 이해하면 할 수록 TRPO를 접하기가 더 쉬울 수 있습니다.</p>\n<ul>\n<li>Euclidean space와 Riemannian space의 차이</li>\n<li>Natural Gradient 증명</li>\n<li>Fisher Information Matrix(FIM)</li>\n<li>Line Search</li>\n<li>FIM과 Hessian 방법의 차이</li>\n<li>Conjugate Gradient Method</li>\n</ul>\n<p>아래의 NPG Code는 Hessian 방법이 아닌 Conjugate Gradient Method를 사용한 “Truncated Natural Policy Gradient(TNPG)”라고 하는 방법의 코드입니다.</p>\n<p>마지막으로 프로젝트 내에 있는 한 팀원의 문장을 인용하겠습니다. “머리가 아프고 힘들수록 잘하고 있는겁니다.” NPG 논문을 보시는 분들 화이팅입니다!</p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/\">NPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py\" target=\"_blank\" rel=\"noopener\">NPG Code</a></p>\n<p><br><br></p>\n<h1 id=\"5-TRPO-Trust-region-policy-optimization\"><a href=\"#5-TRPO-Trust-region-policy-optimization\" class=\"headerlink\" title=\"5. [TRPO] Trust region policy optimization\"></a>5. [TRPO] Trust region policy optimization</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/\">TRPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></p>\n<p>PG기법이 각광을 받게 된 계기는 아마도 TRPO 때문이 아닌가 싶습니다. DQN이라는 인공지능이 아타리게임을 사람보다 잘 플레이했던 영상을 보고 충격을 받았던 사람들이 많았던 것처럼 TRPO가 난제로 여겨졌던 서기, 걷기, 뛰기 같은 로봇에 적용할 수 있는 continuous action을 잘 수행하는 것을 보고 많은 사람들이 놀라워했습니다. (솔직히 처음 아타리게임을 봤을 때 만큼은 아니지만…) 이것들은 DQN으로는 달성할 수 없었던 난제였습니다. </p>\n<p>그렇지만 거인의 어깨 위에서 세상을 바라보았던 많은 과학자들이 그랬던 것처럼 Schulmann도 TRPO를 갑자기 생각낸 것은 아닙니다. policy를 업데이트할 때 기존의 policy와 너무 많이 달라지면 문제가 생깁니다. 일단, state distribution이 변합니다. state distribution은 간단히 얘기하면 각각의 state를 방문하는 빈도를 의미한다고 할 수 있습니다. policy가 많이 바뀌면 기존의 state distribution을 그대로 이용할 수 없고 이렇게 되면 policy gradient를 구하기 어려워집니다. </p>\n<p>그래서 착안한 것이 policy의 변화 정도를 제한하는 것입니다. 여기서 우리는 stochastic policy, 즉, 확률적 policy를 이용한다고 전제하고 있기 때문에, policy의 변화는 policy 확률 분포의 변화를 의미합니다. 현재 policy와 새로운 policy 사이의 확률 분포의 변화를 어떻게 측정할 수 있을까요? Schulmann은 <a href=\"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\" target=\"_blank\" rel=\"noopener\">KL divergence</a>라는 척도를 이용하였습니다. 이 변화량을 특정값 이하로 제한하고 이것을 만족하는 영역을 trust region이라고 불렀습니다. </p>\n<p>그렇지만 문제는 풀기에는 복잡했습니다. 이것을 풀기 위해서 여러 번의 근사화를 적용시켰습니다. objective function에는 linear approximation을 적용하고 trust region constraint에는 second-order approximation을 적용하면 문제가 좀 더 풀기 쉬운 형태로 바뀐다는 것을 알아냈습니다. 이것은 사실 natural gradient를 이용하는 것이었습니다. 이러한 이유로 TRPO를 이해하기 위해서 natural gradient도 살펴보았던 것입니다. </p>\n<p>Schulmann이 너무 똑똑해서 일까요? 훌륭한 아이디어로 policy gradient 기법의 르네상스를 열은 Schulmann의 역작인 TRPO 논문은 이해하기 쉽게 쓰여지지 않은 것 같습니다. (더 잘 쓸 수 있었잖아 Schulmann…) 저희의 포스트와 함께 보다 편하게 여행하시길 바랍니다. 이 <a href=\"https://youtu.be/CKaN5PgkSBc\" target=\"_blank\" rel=\"noopener\">유투브 영상</a>도 무조건 보세요~</p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/\">TRPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></p>\n<p><br><br></p>\n<h1 id=\"6-GAE-High-Dimensional-Continuous-Control-Using-Generalized-Advantage-Estimation\"><a href=\"#6-GAE-High-Dimensional-Continuous-Control-Using-Generalized-Advantage-Estimation\" class=\"headerlink\" title=\"6. [GAE] High-Dimensional Continuous Control Using Generalized Advantage Estimation\"></a>6. [GAE] High-Dimensional Continuous Control Using Generalized Advantage Estimation</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/\">GAE 여행하기</a></p>\n<p>TRPO가 나오고 난 뒤로도 복잡하고 어려운 control problem에서 Reinforcement Learning (RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 “variance reduction”에 대해 연구하였습니다.</p>\n<p>“Generalized Advantage Estimator (GAE)”라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.<br>또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는지를 보였습니다.</p>\n<p>이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.</p>\n<p>GAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.</p>\n<p>추가적으로 앞으로 연구되어야할 부분은 만약 value function estimation error와 policy gradient estimation error 사이의 관계를 알아낸다면, value function fitting에 더 잘 맞는 error metric(policy gradient estimation의 정확성과 더 잘 맞는 value function)을 사용할 수 있을 것입니다. 여기서 policy와 value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.</p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/\">GAE 여행하기</a></p>\n<p><br><br></p>\n<h1 id=\"7-PPO-Proximal-policy-optimization-algorithms\"><a href=\"#7-PPO-Proximal-policy-optimization-algorithms\" class=\"headerlink\" title=\"7. [PPO] Proximal policy optimization algorithms\"></a>7. [PPO] Proximal policy optimization algorithms</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/\">PPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/ppo_gae.py\" target=\"_blank\" rel=\"noopener\">PPO Code</a></p>\n<p>이 논문에서는 Reinforcement Learning에서 Policy Gradient Method의 새로운 방법인 PPO를 제안합니다. 이 방법은 agent가 환경과의 상호작용을 통해 data를 sampling하는 것과 stochastic gradient ascent를 사용하여 “surrogate” objective function을 optimizing하는 것을 번갈아가면서 하는 방법입니다. data sample마다 one gradient update를 수행하는 기존의 방법과는 달리, minibatch update의 multiple epochs를 가능하게 하는 새로운 objective function을 말합니다.</p>\n<p>또한 PPO는 TRPO의 연장선상에 있는 알고리즘이라고 할 수 있습니다. TRPO에서는 문제를 단순하게 만들기 위해서 최적화 문제를 여러 번 변형시켰지만, PPO는 단순하게 clip이라는 개념을 사용합니다. TRPO에서 이용했던 surrogate objective function을 reward가 특정값 이상이거나 이하가 될 때 더 이상 변화시키지 않는 것입니다.</p>\n<p>이 알고리즘의 장점으로는</p>\n<ul>\n<li>TRPO의 장점만을 가집니다. 다시 말해 알고리즘으로 학습하기에 훨씬 더 간단하고, 더 일반적이고, 더 좋은 sample complexity (empirically)를 가집니다.</li>\n<li>또한 다른 online policy gradient method들을 능가했고, 전반적으로 sample complexity(특히 computational complexity), simplicity, wall-time가 좋다고 합니다.</li>\n</ul>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/\">PPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/ppo_gae.py\" target=\"_blank\" rel=\"noopener\">PPO Code</a></p>\n","site":{"data":{}},"excerpt":"","more":"<hr>\n<h1 id=\"0-Policy-Gradient의-세계로\"><a href=\"#0-Policy-Gradient의-세계로\" class=\"headerlink\" title=\"0. Policy Gradient의 세계로\"></a>0. Policy Gradient의 세계로</h1><p>반갑습니다! 저희는 PG여행을 위해 모인 PG탐험대입니다. 강화학습하면 보통 Q-learning을 떠올립니다. 그렇지만 오래전부터 Policy Gradient라는 Q-learning 못지 않은 훌륭한 방법론이 연구되어 왔고, 최근에는 강화학습의 최정점의 기술로 자리매김하고 있습니다. 강화학습의 아버지인 Sutton의 논문을 필두로 하여 기존의 DQN보다 뛰어난 성능을 내는 DPG와 DDPG, 그리고 현재 가장 주목받는 강화학습 연구자인 John Schulmann의 TRPO, GAE, PPO와 이를 이해하기 위해 필요한 Natural Policy Gardient까지 더불어 살펴보고자 합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/tbcyhvilaqy4ra0/Policy%20Optimization%20in%20the%20RL%20Algorithm%20Landscape.png?dl=1\" width=\"800\"> </center>\n\n<p>위의 그림은 강화학습 알고리즘 landscape에서 Policy Optimization의 관점을 중점적으로 하여 나타낸 그림입니다. 위의 그림에서 빨간색 작은 숫자로 나타낸 것이 저희 PG여행에서 다룰 논문들입니다. 순서는 다음과 같습니다.</p>\n<ol>\n<li><a href=\"http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf\" target=\"_blank\" rel=\"noopener\">Sutton_PG</a></li>\n<li><a href=\"http://proceedings.mlr.press/v32/silver14.pdf\" target=\"_blank\" rel=\"noopener\">DPG</a></li>\n<li><a href=\"https://arxiv.org/pdf/1509.02971.pdf\" target=\"_blank\" rel=\"noopener\">DDPG</a></li>\n<li><a href=\"https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf\" target=\"_blank\" rel=\"noopener\">NPG</a></li>\n<li><a href=\"https://arxiv.org/pdf/1502.05477.pdf\" target=\"_blank\" rel=\"noopener\">TRPO</a></li>\n<li><a href=\"https://arxiv.org/pdf/1506.02438.pdf\" target=\"_blank\" rel=\"noopener\">GAE</a></li>\n<li><a href=\"https://arxiv.org/pdf/1707.06347.pdf\" target=\"_blank\" rel=\"noopener\">PPO</a></li>\n</ol>\n<p>위와 같이 총 7가지 논문들을 리뷰하여 블로그로 정리하였습니다. 각 순서에 맞춰 보시는 것을 권장해드립니다.</p>\n<p>이 블로그에는 각각의 기술을 제안한 논문을 PG탐험대분들이 자세하게 리뷰한 포스트들이 있습니다. 우리나라에서 PG에 대해서 이렇게 자세하게 리뷰한 포스트들은 없었다고 감히 말씀드리고 싶습니다. 본 글에서는 이 포스트들을 읽기 전에 전체 내용을 개략적으로 소개하고 각각의 포스트들로 안내하고자 합니다. 자, 저희와 함께 PG여행을 즐겨보시겠습니까?</p>\n<p><br><br></p>\n<h1 id=\"1-Sutton-PG-Policy-gradient-methods-for-reinforcement-learning-with-function-approximation\"><a href=\"#1-Sutton-PG-Policy-gradient-methods-for-reinforcement-learning-with-function-approximation\" class=\"headerlink\" title=\"1. [Sutton PG] Policy gradient methods for reinforcement learning with function approximation\"></a>1. [Sutton PG] Policy gradient methods for reinforcement learning with function approximation</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/\">Sutton PG 여행하기</a></p>\n<p>policy gradient (PG)는 expected reward를 policy의 파라미터에 대한 함수로 모델링하고 이 reward를 최대화하는 policy를 gradient ascent 기법을 이용해서 찾는 기법입니다. 강화학습의 대표격이라고 할 수 있는 Q-learning이라는 훌륭한 방법론이 이미 존재하고 있었지만 Q값의 작은 변화에도 policy가 크게 변할 수도 있다는 단점이 있기 때문에 policy의 점진적인 변화를 통해 더 나은 policy를 찾아가는 PG기법이 개발되었습니다.</p>\n<p>이 PG기법은 먼저 개발되었던 <a href=\"https://link.springer.com/content/pdf/10.1007/BF00992696.pdf\" target=\"_blank\" rel=\"noopener\">REINFORCE</a>라는 기법과 관련이 아주 많습니다. 서튼의 PG기법은 REINFORCE 기법을 <a href=\"http://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf\" target=\"_blank\" rel=\"noopener\">actor-critic algorithm</a>을 사용하여 개선시킨 것이라고 볼 수도 있습니다. 저희 PG여행 팀도 처음에는 REINFORCE를 출발지로 삼으려고 했었지만 예전 논문이다보니 논문의 가독성이 너무 떨어져서 강화학습의 아버지라고 할 수 있는 서튼의 논문을 출발지로 삼았습니다. 하지만 이 논문도 만만치 않았습니다. 이 논문을 읽으시려는 분들께 저희의 여행기가 도움이 될 것입니다. PG기법에 대해서 먼저 감을 잡고 시작하시려면 <a href=\"http://karpathy.github.io/2016/05/31/rl/\" target=\"_blank\" rel=\"noopener\">Andre Karpathy의 PG에 대한 블로그</a>를 먼저 한 번 읽어보세요.  한글번역도 있습니다! 1) <a href=\"http://keunwoochoi.blogspot.com/2016/06/andrej-karpathy.html\" target=\"_blank\" rel=\"noopener\">http://keunwoochoi.blogspot.com/2016/06/andrej-karpathy.html</a> 2) <a href=\"https://tensorflow.blog/2016/07/13/reinforce-pong-at-gym/\" target=\"_blank\" rel=\"noopener\">https://tensorflow.blog/2016/07/13/reinforce-pong-at-gym/</a></p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/\">Sutton PG 여행하기</a></p>\n<p><br><br></p>\n<h1 id=\"2-DPG-Deterministic-policy-gradient-algorithms\"><a href=\"#2-DPG-Deterministic-policy-gradient-algorithms\" class=\"headerlink\" title=\"2. [DPG] Deterministic policy gradient algorithms\"></a>2. [DPG] Deterministic policy gradient algorithms</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/\">DPG 여행하기</a></p>\n<p>deterministic policy gradeint (DPG)는 어찌보면 상당히 도전적인 아이디어였던 것 같습니다. Sutton PG 논문에서는 DPG 스타일의 기법이 가진 단점에 대해서 언급하면서 stochastic policy gradient (SPG)를 써야 optimal을 찾을 수 있다고 기술하고 있었기 때문입니다.</p>\n<p>그런데 이 논문에서 높은 차원의 action space를 가지는 문제들에(예를 들면 문어발 제어) 대해서는 DPG가 상당히 좋은 성능을 내는 것을 저자들이 보였습니다. 그리고 DPG는 SPG와 대척점에 있는 기술이 아니고 SPG의 special case 중 하나임을 증명하면서 SPG를 가정하고 만들어진 기술들을 DPG에서도 그대로 이용할 수 있음을 보였습니다. David Silver의 <a href=\"http://techtalks.tv/talks/deterministic-policy-gradient-algorithms/61098/\" target=\"_blank\" rel=\"noopener\">동영상 강의</a>를 한 번 보시길 추천드립니다. 짧은 강의지만 랩을 하듯이 쉴새없이 설명하는 Silver의 모습에서 천재성이 엿보이는 것을 확인하실 수 있습니다. </p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/\">DPG 여행하기</a></p>\n<p><br><br></p>\n<h1 id=\"3-DDPG-Continuous-control-with-deep-reinforcement-learning\"><a href=\"#3-DDPG-Continuous-control-with-deep-reinforcement-learning\" class=\"headerlink\" title=\"3. [DDPG] Continuous control with deep reinforcement learning\"></a>3. [DDPG] Continuous control with deep reinforcement learning</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/\">DDPG 여행하기</a></p>\n<p>DPG의 후속 연구로 DPG보다 더 큰 주목을 받은 논문입니다. 소위 말하는 deep deterministic policy gradient (DDPG)로 불리는 기술을 제안한 논문입니다. 이 논문의 저자 중 일부는 그 유명한 DQN 논문의 저자이기도 합니다. Q-learning과 deep neural network를 접목시켰던 DQN처럼 이 논문도 DPG와 deep neural network를 접목시킨 논문입니다.</p>\n<p>이 논문은 DQN으로는 좋은 성능을 내지 못했던 continuous action을 가지는 상황들에 대해서 상당히 훌륭한 결과를 보이면서 큰 주목을 받았습니다. 소위 말하는 deep reinforcement learning (DRL)에서 Q-learning 계열의 DQN, PG 계열의 DDPG로 양대산맥을 이루는 논문이라고 할 수 있습니다. 두 논문 모두 Deepmind에서 나왔다는 것은 Deepmind 기술력이 DRL 분야에서 최정점에 있음을 보여주는 상징이 아닌가 싶습니다. 논문 자체는 그리 어렵지 않습니다. 새로운 아이디어를 제시했다기보다는 딥러닝을 활용한 강화학습의 가능성을 보여주는 논문이라는 점에서 큰 의의를 가지는 것 같습니다. 여러분도 한번 코딩에 도전해보시는게 어떨까요?</p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/\">DDPG 여행하기</a></p>\n<p><br><br></p>\n<h1 id=\"4-NPG-A-natural-policy-gradient\"><a href=\"#4-NPG-A-natural-policy-gradient\" class=\"headerlink\" title=\"4. [NPG] A natural policy gradient\"></a>4. [NPG] A natural policy gradient</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/\">NPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py\" target=\"_blank\" rel=\"noopener\">NPG Code</a></p>\n<p>이 논문은 뒤이어 나오는 TRPO를 더 잘 이해하기 위해서 보는 논문입니다. 이번 논문부터 내용이 상당히 어려워집니다. 다소 생소한 수학 개념들이 많이 나오기 때문입니다. 하지만 이 블로그를 보시면 많은 부분들이 채워질 것이라고 믿습니다.</p>\n<p>2002년 당시에도 많은 연구자들이 objective function의 gradient 값을 따라서 좋은 policy $\\pi$를 찾고자 하였습니다. 하지만 기존의 우리가 알던 gradient descent method는 steepest descent direction이 아닐 수 있기 때문에(쉽게 말해 가장 가파른 방향을 따라서 내려가야 하는데 그러지 못할 수도 있다는 것입니다.) 이 논문에서 steepest descent direction를 나타내는 natural gradient method를 policy gradient에 적용하여 좋은 policy $\\pi$를 찾습니다. gradient descent는 parameter를 한 번에 많이 update 할 수 없는 반면, natural gradient는 가장 좋은 action을 고르도록 학습이 됩니다.</p>\n<p>또한 natural gradient method는 Line search 기법과 함께 사용하면 더 Policy Iteration과 비슷해집니다. Greedy Policy Iteration에서와 비교하면 Performance Improvement가 보장됩니다. 하지만 Fisher Information Matrix(FIM)이 asymtotically Hessian으로 수렴하지 않습니다. 그렇기 때문에 Conjugate Gradient Method로 구하는 방법이 더 좋을수 있습니다.</p>\n<p>위의 요약한 문장들만 봤을 때는 생소한 용어들이 많이 나와서 무슨 말인지 감이 안잡히실 수 있습니다. 저희가 포스팅한 블로그 글에는 다음과 같은 추가적인 내용이 나옵니다. 반드시 알고 가야 TRPO를 이해하는 것은 아닙니다. 다만 NPG를 이해하면 할 수록 TRPO를 접하기가 더 쉬울 수 있습니다.</p>\n<ul>\n<li>Euclidean space와 Riemannian space의 차이</li>\n<li>Natural Gradient 증명</li>\n<li>Fisher Information Matrix(FIM)</li>\n<li>Line Search</li>\n<li>FIM과 Hessian 방법의 차이</li>\n<li>Conjugate Gradient Method</li>\n</ul>\n<p>아래의 NPG Code는 Hessian 방법이 아닌 Conjugate Gradient Method를 사용한 “Truncated Natural Policy Gradient(TNPG)”라고 하는 방법의 코드입니다.</p>\n<p>마지막으로 프로젝트 내에 있는 한 팀원의 문장을 인용하겠습니다. “머리가 아프고 힘들수록 잘하고 있는겁니다.” NPG 논문을 보시는 분들 화이팅입니다!</p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/\">NPG 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py\" target=\"_blank\" rel=\"noopener\">NPG Code</a></p>\n<p><br><br></p>\n<h1 id=\"5-TRPO-Trust-region-policy-optimization\"><a href=\"#5-TRPO-Trust-region-policy-optimization\" class=\"headerlink\" title=\"5. [TRPO] Trust region policy optimization\"></a>5. [TRPO] Trust region policy optimization</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/\">TRPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></p>\n<p>PG기법이 각광을 받게 된 계기는 아마도 TRPO 때문이 아닌가 싶습니다. DQN이라는 인공지능이 아타리게임을 사람보다 잘 플레이했던 영상을 보고 충격을 받았던 사람들이 많았던 것처럼 TRPO가 난제로 여겨졌던 서기, 걷기, 뛰기 같은 로봇에 적용할 수 있는 continuous action을 잘 수행하는 것을 보고 많은 사람들이 놀라워했습니다. (솔직히 처음 아타리게임을 봤을 때 만큼은 아니지만…) 이것들은 DQN으로는 달성할 수 없었던 난제였습니다. </p>\n<p>그렇지만 거인의 어깨 위에서 세상을 바라보았던 많은 과학자들이 그랬던 것처럼 Schulmann도 TRPO를 갑자기 생각낸 것은 아닙니다. policy를 업데이트할 때 기존의 policy와 너무 많이 달라지면 문제가 생깁니다. 일단, state distribution이 변합니다. state distribution은 간단히 얘기하면 각각의 state를 방문하는 빈도를 의미한다고 할 수 있습니다. policy가 많이 바뀌면 기존의 state distribution을 그대로 이용할 수 없고 이렇게 되면 policy gradient를 구하기 어려워집니다. </p>\n<p>그래서 착안한 것이 policy의 변화 정도를 제한하는 것입니다. 여기서 우리는 stochastic policy, 즉, 확률적 policy를 이용한다고 전제하고 있기 때문에, policy의 변화는 policy 확률 분포의 변화를 의미합니다. 현재 policy와 새로운 policy 사이의 확률 분포의 변화를 어떻게 측정할 수 있을까요? Schulmann은 <a href=\"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\" target=\"_blank\" rel=\"noopener\">KL divergence</a>라는 척도를 이용하였습니다. 이 변화량을 특정값 이하로 제한하고 이것을 만족하는 영역을 trust region이라고 불렀습니다. </p>\n<p>그렇지만 문제는 풀기에는 복잡했습니다. 이것을 풀기 위해서 여러 번의 근사화를 적용시켰습니다. objective function에는 linear approximation을 적용하고 trust region constraint에는 second-order approximation을 적용하면 문제가 좀 더 풀기 쉬운 형태로 바뀐다는 것을 알아냈습니다. 이것은 사실 natural gradient를 이용하는 것이었습니다. 이러한 이유로 TRPO를 이해하기 위해서 natural gradient도 살펴보았던 것입니다. </p>\n<p>Schulmann이 너무 똑똑해서 일까요? 훌륭한 아이디어로 policy gradient 기법의 르네상스를 열은 Schulmann의 역작인 TRPO 논문은 이해하기 쉽게 쓰여지지 않은 것 같습니다. (더 잘 쓸 수 있었잖아 Schulmann…) 저희의 포스트와 함께 보다 편하게 여행하시길 바랍니다. 이 <a href=\"https://youtu.be/CKaN5PgkSBc\" target=\"_blank\" rel=\"noopener\">유투브 영상</a>도 무조건 보세요~</p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/\">TRPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></p>\n<p><br><br></p>\n<h1 id=\"6-GAE-High-Dimensional-Continuous-Control-Using-Generalized-Advantage-Estimation\"><a href=\"#6-GAE-High-Dimensional-Continuous-Control-Using-Generalized-Advantage-Estimation\" class=\"headerlink\" title=\"6. [GAE] High-Dimensional Continuous Control Using Generalized Advantage Estimation\"></a>6. [GAE] High-Dimensional Continuous Control Using Generalized Advantage Estimation</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/\">GAE 여행하기</a></p>\n<p>TRPO가 나오고 난 뒤로도 복잡하고 어려운 control problem에서 Reinforcement Learning (RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 “variance reduction”에 대해 연구하였습니다.</p>\n<p>“Generalized Advantage Estimator (GAE)”라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.<br>또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는지를 보였습니다.</p>\n<p>이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.</p>\n<p>GAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.</p>\n<p>추가적으로 앞으로 연구되어야할 부분은 만약 value function estimation error와 policy gradient estimation error 사이의 관계를 알아낸다면, value function fitting에 더 잘 맞는 error metric(policy gradient estimation의 정확성과 더 잘 맞는 value function)을 사용할 수 있을 것입니다. 여기서 policy와 value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.</p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/\">GAE 여행하기</a></p>\n<p><br><br></p>\n<h1 id=\"7-PPO-Proximal-policy-optimization-algorithms\"><a href=\"#7-PPO-Proximal-policy-optimization-algorithms\" class=\"headerlink\" title=\"7. [PPO] Proximal policy optimization algorithms\"></a>7. [PPO] Proximal policy optimization algorithms</h1><p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/\">PPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/ppo_gae.py\" target=\"_blank\" rel=\"noopener\">PPO Code</a></p>\n<p>이 논문에서는 Reinforcement Learning에서 Policy Gradient Method의 새로운 방법인 PPO를 제안합니다. 이 방법은 agent가 환경과의 상호작용을 통해 data를 sampling하는 것과 stochastic gradient ascent를 사용하여 “surrogate” objective function을 optimizing하는 것을 번갈아가면서 하는 방법입니다. data sample마다 one gradient update를 수행하는 기존의 방법과는 달리, minibatch update의 multiple epochs를 가능하게 하는 새로운 objective function을 말합니다.</p>\n<p>또한 PPO는 TRPO의 연장선상에 있는 알고리즘이라고 할 수 있습니다. TRPO에서는 문제를 단순하게 만들기 위해서 최적화 문제를 여러 번 변형시켰지만, PPO는 단순하게 clip이라는 개념을 사용합니다. TRPO에서 이용했던 surrogate objective function을 reward가 특정값 이상이거나 이하가 될 때 더 이상 변화시키지 않는 것입니다.</p>\n<p>이 알고리즘의 장점으로는</p>\n<ul>\n<li>TRPO의 장점만을 가집니다. 다시 말해 알고리즘으로 학습하기에 훨씬 더 간단하고, 더 일반적이고, 더 좋은 sample complexity (empirically)를 가집니다.</li>\n<li>또한 다른 online policy gradient method들을 능가했고, 전반적으로 sample complexity(특히 computational complexity), simplicity, wall-time가 좋다고 합니다.</li>\n</ul>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/\">PPO 여행하기</a><br><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/ppo_gae.py\" target=\"_blank\" rel=\"noopener\">PPO Code</a></p>\n"},{"title":"Proximal Policy Optimization","date":"2018-06-22T07:53:12.000Z","author":"이동민, 장수영, 차금강","subtitle":"피지여행 7번째 논문","_content":"\n<center> <img src=\"https://www.dropbox.com/s/145van5kldfvvd5/Screen%20Shot%202018-07-18%20at%201.19.30%20AM.png?dl=1\" width=\"700\"> </center>\n\n논문 저자 : John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov\n논문 링크 : https://arxiv.org/pdf/1707.06347.pdf\nProceeding : unpublished.\n정리 : 이동민, 장수영, 차금강\n\n---\n\n# 1. 들어가며...\n\nSutton_PG부터 시작하여 TRPO, GAE를 거쳐 PPO까지 대단히 고생많으셨습니다. 먼저 이 논문은 TRPO보다는 쉽습니다. cliping이라는 새로운 개념이 나오지만 크게 어렵진 않습니다.\n\n이 논문에서는 Reinforcement Learning에서 Policy Gradient Method의 새로운 방법인 PPO를 제안합니다. 이 방법은 agent가 환경과의 상호작용을 통해 data를 sampling하는 것과 stochastic gradient ascent를 사용하여 \"surrogate\" objective function을 optimizing하는 것을 번갈아가면서 하는 방법입니다. data sample마다 one gradient update를 수행하는 기존의 방법과는 달리, minibatch update의 multiple epochs를 가능하게 하는 새로운 objective function을 말합니다.\n\n이 알고리즘의 장점으로는\n\n- Trust Region Policy Optimization(TRPO)의 장점만을 가집니다. 다시 말해 알고리즘으로 학습하기에 훨씬 더 간단하고, 더 일반적이고, 더 좋은 sample complexity (empirically)를 가집니다. \n- 또한 다른 online policy gradient method들을 능가했고, 전반적으로 sample complexity(특히 computational complexity), simplicity, wall-time가 좋다고 합니다.\n\n<br><br>\n\n# 2. Introduction\n\n<br>\n## 2.1 대표적인 방법들\n\n- DQN\n    - Discrete action space를 가지는 문제들에는 효과적으로 적용 가능하지만, continuous control에도 잘 작동하는지는 검증되지 않았습니다.\n- A3C - \"Vanilla\" policy gradient methods\n    - Data efficiency와 robustness 측면이 좋지 않습니다. A3C의 data efficiency의 경우 on-policy로서 한 번 쓴 data는 바로 버리기 때문에 data efficiency가 좋지 않다는 것입니다.\n- TRPO\n    - 간단히 말해 복잡합니다.\n    - 또한 noise(ex. dropout)나 parameter sharing(policy와 value function 간 혹은 auxiliary tasks와의)를 포함하는 architecture와의 호환성 없습니다.\n\n<br>\n## 2.2 대표적인 방법들 대비 개선사항\n\n- Scalability\n    - large models and parallel implementations\n- Data Efficiency\n- Robustness\n    - hyperparameter tuning없이 다양한 문제들에 적용되어 해결\n\n<br>\n## 2.3 제안하는 알고리즘\n\n이 논문에는 Clipped probability ratios를 포함하는 objective function 제안하였습니다.\n\n- TRPO의 data efficiency와 robustness를 유지하면서도 first-order approximation만 사용합니다.\n- Policy 성능에 대한 lower bound를 제공합니다.\n\n따라서 Policy로부터의 data sampling과 sampled data를 이용한 최적화를 번갈아가면서 수행합니다.\n\n<br>\n## 2.4 실험 결과\n\n다양한 버전의 surrogate objectives 중에는 clipped probability ratio가 가장 성능이 좋았습니다.\n\n- Continuous control tasks에서 기존 알고리즘 대비 성능이 좋습니다.\n- Atari에서는 A2C 대비 sampling efficiency 측면에서는 성능이 월등히 좋으며, ACER 대비 훨씬 간단하지만 성능은 비슷합니다.\n\n<br><br>\n\n# 3. Backgroud: Policy Optimization\n\n<br>\n## 3.1 Policy Gradient Methods\n\n일반적인 PG Method들은 실험적으로 destructive large policy updates가 발생합니다.\n\n- 더 자세히 말하자면, PG Method가 수행하는 parameter space에서의 gradual update가 policy space에서는 큰 변화를 유발할 수 있다는 의미입니다.\n\n<br>\n## 3.2 Trust Region Methods\n\nPolicy update 크기에 대한 contraint하에 objective function(\"surrogate\" function)을 최대화하는 것이 목표입니다. 수식은 아래와 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/gx6udoz5upswyf9/Screen%20Shot%202018-07-31%20at%2011.11.49%20PM.png?dl=1\" width=\"400\"> </center>\n\n위의 수식은 contraint로 인해 excessive large policy update가 방지됩니다.\n\nTRPO에서는 constrained optimization problem을 풀기 위해서는 다음과 같은 방법들이 필요합니다.\n\n1. Fisher Information Matrix인 second-order derivative of KL divergence를 사용하거나,\n    - 여기서 second-order matrixes를 구하기 위해서는 많은 계산량 필요합니다.\n2. Conjugate Gradient를 사용합니다.\n    - Conjugate Gradient는 구현하기가 어렵습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/6xpw9igndl3dmb9/Screen%20Shot%202018-07-31%20at%2011.15.13%20PM.png?dl=1\" width=\"500\"> </center>\n\n원래 이론적으로 위의 수식과 같이 \"contraint\"가 아니라 objective에 \"penalty\"를 부여하는 형태입니다. 하지만 다양한 문제들(혹은 학습 중에 특성이 변하는 문제)에서 모두 잘 동작하는 single value $\\beta$를 찾는 것(robustness)이 어렵기 때문에, TRPO에서는 penalty대신 contraint를 취하는 방식을 택한 것입니다.\n\n<br><br>\n\n# 4. Clipped Surrogate Objective\n\n이번 section에서는 TRPO의 surrogate obejctive function을 강제적으로 clipping하는 방법에 대하여 말합니다.\n\n먼저 기존의 TRPO의 surrogate function을 다음과 같이 표현합니다. \n$$r_t(\\theta)=\\dfrac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta old}(a_t|s_t)}, \\, \\, r_t(\\theta old) = 1$$\n\n위의 수식을 이용하여 TRPO의 surrogate object를 최대화합니다.\n<center> <img src=\"https://www.dropbox.com/s/e524vwsyolp6h9n/Screen%20Shot%202018-07-26%20at%2010.12.22%20AM.png?dl=1\" width=\"350\"> </center>\n\n위의 수식을 그대로 사용한다면 excessively large policy update가 됩니다. 따라서 penalty를 이용하여 필요 이상의 policy update를 방지합니다. TRPO에서는 KL-Divergence를 이용하여 penalty를 적용하지만 PPO에서는 computation적으로 효율적인 penalty를 적용하고 excessively large policy update를 방지하기 위해 아래와 같은 clipping 기법을 사용합니다.\n\n$$L^{CLIP}(\\theta) = \\hat{E}_t [min(r_t(\\theta) \\, \\hat{A}_t,  clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\, \\hat{A}_t)]$$\n\n- 위의 수식에 대한 추가적인 설명\n    - $\\epsilon$은 hyperparameter로서 continuous control에서는 0.2일 때 성능이 가장 좋았으며, Atari Game에서는 0.1 x $\\alpha$ 값을 사용합니다. (여기서 $\\alpha$ 는 학습률로 1 에서 시작하여 학습이 진행됨에 따라 0 으로 감소합니다.)\n    - Clipped 와 unclipped objective 중 min 값을 택함으로써 $L^{CLIP} (\\theta)$는 unclipped objective에 대한 lowerbound가 됩니다.\n\n이어서 아래의 그림을 보겠습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/3wcuf2tq7q24fy6/Screen%20Shot%202018-07-26%20at%2010.25.08%20AM.png?dl=1\" width=\"600\"> </center>\n\n위의 그림을 보면 두 가지의 그래프(Advantage Function $\\hat{A}_t$의 부호에 따라)로 clip에 대해서 설명하고 있습니다.\n\n- Advantage Function $\\hat{A}_t$가 양수일 때\n    - Advantage가 현재보다 높다라는 뜻이며 파라미터를 +의 방향으로 업데이트 하여야 합니다. 다시 말해 어떠한 상태 $s$에서 행동 $a$가 평균보다 좋다는 의미입니다. 따라서 이를 취할 확률이 증가하게 되고, $r_t (\\theta)$를 clip하여 $\\epsilon$보다 커지지 않도록 유도하는 것입니다.\n    - 추가적으로, TRPO에서 다뤘던 constraint가 아니라 단순히 clip하는 것이기 때문에 실제로 $\\pi_\\theta (a_t|s_t)$의 증가량이 $\\epsilon$보다 더 커질 수도 있습니다. 하지만 증가량이 더 커졌다고 하더라도 objective function을 업데이트 할 때 효과적이지 않을 수 있기 때문에 대부분 clip을 통해서 $\\epsilon$ 이하로 유지합니다.\n    - 또한 만약 $r_t (\\theta)$가 objective function의 값을 감소시키는 방향으로 움직이는 경우더라도 $1-\\epsilon$보다 작아져도 됩니다. 여기서의 목적은 최대한 lowerbound를 구하는 것이 목적이기 때문입니다. 그러니까 쉽게 말해서 왼쪽 그림에서도 볼 수 있듯이 $1+\\epsilon$만 구하는 데에 포커스를 맞추고 있고, $1-\\epsilon$은 신경쓰지 않고 있는 것입니다. (이 부분은 Advantage Function $\\hat{A}_t$가 음수일 때도 동일합니다.)\n- Advantage Function $\\hat{A}_t$가 음수일 때 \n    - Advantage가 현재보다 좋지 않다라는 뜻이며 그의 반대 방향으로 업데이트 하여야 합니다. 다시 말해 어떠한 상태 $s$에서 행동 $a$가 평균보다 좋지 않다는 의미입니다. 따라서 이를 취할 확률이 감소하게 되고, $r_t (\\theta)$를 clip하여 $\\epsilon$보다 작아지지 않도록 유도하는 것입니다.  \n    - 또한 $r_t(\\theta)$은 확률을 뜻하는 두 개의 함수를 분자 분모로 가지고 있으며, 분수로 구성되어 있기 때문에 무조건 양수로 이루어져 있습니다. Advantage function인 $\\hat{A}_t$와 곱해져 Objective function인 $L^{CLIP}$은 Advantage function과 방향이 같아집니다.\n\n위의 설명으로 인하여 나오는 그림이 아래의 그림입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/f7i97geiligfej9/Screen%20Shot%202018-07-26%20at%2011.02.17%20AM.png?dl=1\" width=\"700\"> </center>\n\n위의 그림에서 빨간색 그래프를 보면,\n\n- $L^{CLIP}$은 min 값들의 평균이기 때문에 평균들의 min 값보다는 더 작아집니다. 즉, 주황색 그래프와 초록색 그래프 중 작은값보다 더 작아지는 것을 볼 수 있습니다.\n- $L^{CLIP}$을 최대화하는 $\\theta$가 다음 $\\pi_\\theta$가 됩니다. 다시 말해 PPO는 기존 PG 방법들처럼 parameter space에서 parameter $\\theta$를 점진적으로 업데이트하는 것이 아니라, 매번 policy를 maximize하는 방향으로 업데이트하는 것으로 볼 수 있는 것입니다.\n\n\n<br><br>\n\n# 5. Adaptive KL Penalty Coefficient\n\n이전까지 설명했던 Clipped Surrogate Objective 방법과 달리 기존의 TRPO에서 Adaptive한 파라미터를 적용한 또 다른 방법에 대해서 알아보겠습니다. 사실 이 방법은 앞서본 clip을 사용한 방법보다는 성능이 좋지 않습니다. 하지만 baseline으로써 알아볼 필요가 있습니다.\n\nclip에서 다뤘던 Probability ratio $r_t(\\theta)$ 대신 KL divergence를 이용하여 penalty를 줍니다. 그 결과 각각의 policy update에 대해 KL divergence $d_{targ}$의 target value를 얻습니다.\n\n- clipped surrogate objective 대신하여 or 추가적으로 사용할 수 있다고 합니다. \n- 자체 실험 결과에서는 clipped surrogate objective 보다는 성능이 안 좋았다고 합니다.\n- 둘 다 사용한 실험 결과는 없습니다.\n\n이 알고리즘에서 각각의 policy update에 적용하는 step은 다음과 같습니다. \n\n1. KL-penalized objective 최적화를 합니다. 기존의 TRPO의 Objective function에 $\\beta$를 적용하여 다음과 같이 $\\beta$를 Adaptive하게 조절하고 있습니다. 수식은 아래와 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ojq7kfobf0f8x9n/Screen%20Shot%202018-07-26%20at%2011.22.44%20AM.png?dl=1\" width=\"500\"> </center>\n\n2. <img src=\"https://www.dropbox.com/s/j9phkalrbgf45k0/Screen%20Shot%202018-07-26%20at%2011.34.25%20AM.png?dl=1\" width=\"230\">을 계산합니다.\n    - 만약 $d < d_{targ} \\, / \\, 1.5$라면, 그 때 $\\beta \\leftarrow \\beta \\, / 2$\n    - 만약 $d > d_{targ} \\, x \\, 1.5$라면, 그 때 $\\beta \\leftarrow \\beta \\, x 2$\n    - 즉, KL-divergence 값이 일정 이상 커지게 되면 objective function에 penalty를 더 크게 부과합니다.\n    - 갱신된 $\\beta$ 값은 다음 policy update 때 사용합니다. \n\n$\\beta$를 조절하는 방법은 만약 $\\theta old$와 $\\theta$간의 파라미터 차이가 크다면 penalty를 강하게 부여하여 차이를 작게하고, 파라미터 차이가 작다면 penalty를 완화시켜 주어 차이를 더 크게하는 것입니다.\n\n정리하자면, KL-divergence를 constraint로 둔 것이 아니기 때문에, 간혹 excessive large policy update가 발생할 수도 있지만, KL Penaly coefficienct인 $\\beta$가 KL-divergence에 따라 adpative 하게 조정됨으로써 excessive large policy update가 지속적으로 발생되는 것을 방지하는 것입니다.\n\n<br><br>\n\n# 6. Algorithm\n\n앞서 다뤘던 section들은 어떻게 policy만을 업데이트 하는지에 대해 설명하고 있습니다. 이번 section에서는 value, entropy-exploration과 합쳐서 어떻게 통합적으로 업데이트 하는지에 대해서 알아보겠습니다.\n\n먼저 Variance-reduced advantage function estimator을 계산하기 위해 learned state-value fucntion $V(s)$을 사용합니다. 여기에는 두 가지 방법이 있습니다.\n\n- Generalized advantage estimation\n- Finite horizontal estimators\n\n만약 policy와 value function 간 parameter sharing하는 neural network architecture를 사용한다면, policy surrogate와 value function error term을 combine한 loss function을 사용해야 합니다. 또한 이 loss function에 entropy bonus term을 추가하여, 충분한 exploration이 될 수 있도록 합니다.(exploration하는 부분은 [A3C 논문](https://arxiv.org/pdf/1602.01783.pdf)에 나와있습니다.)\n\n그래서 앞써 다뤘던 objective function을 $L^{CLIP}$라고 표현한다면 PPO에서 제시하는 통합적으로 최대화해야하는 objective function은 다음과 같이 표현할 수 있습니다.\n\n$$L_t^{CLIP+VF+S} (\\theta) = \\hat{E}_t [L^{CLIP} (\\theta) - c_1 L_t^{VF} (\\theta) + c_2 S[\\pi_\\theta] (s_t)]$$\n\n위의 수식에 대한 추가적인 설명은 다음과 같습니다.\n\n- $c_1, c_2$ : coefficients\n- $S$ : entropy bonus\n- $L_t^{VF}$ : squared-error loss $(V_\\theta (s_t) - V_t^{targ})^2$\n\n위의 objective function을 최대화하면 Reinforcement Learning을 통한 policy 학습, state-value function의 학습, exploration을 할 수 있습니다.\n\n추가적으로 A3C와 같은 요즘 인기있는 PG의 style에서는 T time steps(T는 episode length 보다 훨씬 작은 크기) 동안 policy에 따라서 sample들을 얻고, 이 sample들을 업데이트에 사용합니다. 이러한 방식은 time step T 까지만 고려하는 advantage estimator가 필요하며, A3C에서 다음과 같이 사용합니다.\n\n$$\\hat{A}_t = - V(s_t) + r_t + \\gamma r_{t+1} + \\cdots + \\gamma^{T-t+1} r_{T-1} + \\gamma^{T-t}V(s_T)$$\n\n- 여기서 $t$는 주어진 length T trajectory segment 내에서 $[0, T]$에 있는 time index입니다.\n\n위의 수식에 더하여 generalized version인 GAE의 truncated version(generalized advantage estimation)을 사용합니다.($\\lambda$가 1이면 위 식과 같아집니다.) 수식은 아래와 같습니다. \n\n$$\\hat{A}_t = \\delta_t + (\\gamma\\lambda)\\delta_{t+1} + \\cdots + (\\gamma\\lambda)^{T-t+1}\\delta_{T-1},$$$$where \\,\\,\\,\\, \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n\n그래서 고정된 length T trajectory segment를 사용한 PPO algorithm은 다음과 같은 pseudo code로 나타낼 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/q00y8x7eryl1ncd/Screen%20Shot%202018-07-26%20at%202.28.03%20PM.png?dl=1\" width=\"800\"> </center>\n\n- 여기서 actor는 A3C처럼 병렬로 두어도 상관없습니다.\n- advantage estimate 계산을 통해서 surrogate loss를 계산합니다.\n\n<br><br>\n\n# 7. Experimnet\n\n<br>\n## 7.1 Surrogate Objectives의 비교\n\n이번 section에서는 3가지 Surrogate Objectives를 비교분석 하고 있습니다.\n\n* No clipping or penalty: $L_t(\\theta) = r_t(\\theta)\\hat{A}_t$\n* Clipping: $L_t(\\theta) = min(r_t(\\theta) \\, \\hat{A}_t, \\, clip(r_t(\\theta), 1-\\epsilon,1+\\epsilon) \\, \\hat{A}_t)$\n* KL penalty(fixed or adaptive): <img src=\"https://www.dropbox.com/s/ksnhlxz2riuns1p/Screen%20Shot%202018-07-26%20at%202.42.50%20PM.png?dl=1\" width=\"270\">\n\n그리고 환경은 MuJoCo를 사용하며 OpenAI Gym 환경에서 실험을 진행하였습니다. 그리고 네트워크들은 2개의 hidden layer를 가지며 각각 64개의 unit들을 가지고 있습니다. 그리고 tanh를 활성화 함수로써 사용하고 있으며 네트워크는 Gaussian distribution을 출력하는 것으로 구성되어 있습니다. 그리고 Policy Network와 Value Network는 파라미터들을 공유하고 있지 않으며, 위에서 설명한 Exploration을 위한 entropy는 사용하지 않습니다.\n\n7개의 환경에서 실험하였으며 각 환경들은 HalfCheetah, Hopper, InvertedDoublePendulum, InvertedPerdulum, Reacher, Swimmer, Walker2d를 사용하였으며 모든 환경들은 v1 버전을 사용하였습니다.\n\n아래의 표는 실험 결과입니다. 세 가지 Surrogate Objective에 대해 각 파라미터들을 표에 표기된 방식대로 변화하며 실험하였습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/4xbkxh4m82mu1vo/Screen%20Shot%202018-07-26%20at%202.45.42%20PM.png?dl=1\" width=\"450\"> </center>\n\n<br>\n## 7.2 Comparison to other Algorithms in the Continuous Domain\n\n이번 section에서는 Clipping 버전의 PPO와 다른 알고리즘들 간의 비교를 연속적인 Action을 가지는 환경에서 실험을 한 결과를 보여주고 있습니다. 다른 알고리즘은 TRPO, cross-entropy method, vanilla policy gradient with adaptive stepsize, A2C, A2C with trust region를 사용하였습니다. 환경과 사용한 네트워크의 구성은 이전 section과 동일합니다.\n\n<center> <img src=\"https://www.dropbox.com/s/tf7djngioxnxtef/Screen%20Shot%202018-07-26%20at%202.50.23%20PM.png?dl=1\" width=\"900\"> </center>\n\n여타 다른 알고리즘보다 좋은 성능을 보이는 것을 알 수 있습니다.\n\n<br>\n## 7.3 Showcase in the Continuous Domain: Humanoid Running and Steering\n\n이번 section에서는 Humanoid-v0, HumanoidFlagrun-v0, HumanoidFlagrunHarder-v0의 환경에서 Roboschool을 사용하여 실험하였습니다.\n\n- Humanoid-v0는 단순히 앞으로 걸어나가는 환경\n- HumanoidFlagrun-v0은 200스텝마다 혹은 목적지에 도착할 때 마다 위치가 바뀌는 목적지에 걸어서 도달하는 환경입니다. \n- HumanoidFlagrunHarder-v0은 환경이 초기화될 때 마다 특정 경계안에 Humanoid가 위치하게 되며 특정 영역 밖으로 걸어나가는 것을 수행하는 환경입니다.\n\n\n<center> <img src=\"https://www.dropbox.com/s/y112ua7il6l0296/Screen%20Shot%202018-07-26%20at%202.53.01%20PM.png?dl=1\" width=\"900\"> </center>\n\n- Roboschool을 사용하여 3D humanoid control task에서 PPO 알고리즘으로 학습시킨 Learning curve입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/09scyk6zeviyswt/Screen%20Shot%202018-07-26%20at%202.53.10%20PM.png?dl=1\" width=\"900\"> </center>\n\n- Roboschool Humanoid Flagrun에서 학습된 policy의 frame들입니다. \n\n<br>\n## 7.4 Comparison to Other Algorithms on the Atari Domain\n\n이번 section에서는 Atari Domain에서 PPO, A2C, ACER(Actor Critic with Experience Replay)의 3가지 알고리즘을 비교합니다.\n\n<center> <img src=\"https://www.dropbox.com/s/jxuqdncxpnoyqgi/Screen%20Shot%202018-07-26%20at%203.00.00%20PM.png?dl=1\" width=\"600\"> </center>\n\n전체 training에 대해서는 PPO가 ACER보다 좋은 성능을 내고 있습니다. 하지만 마지막 100 에피소드만을 비교했을 때는 PPO보다 ACER이 더 좋은 성능을 보이고 있습니다. 이는 PPO가 더 빨리 최종 능력에 도달하지만, ACER이 가진 potential이 더 높다는 것입니다.\n\n<br><br>\n\n# 8. Conclusion\n\n이 논문에서는 policy update를 하기 위한 방법으로 stochastic gradient ascent의 multiple epchs를 사용하는 policy optimization method들의 하나의 알고리즘은 Proximal Policy Optimization(PPO)를 소개합니다.\n\n이 알고리즘은 trust region method의 stability와 reliability를 가집니다. 여기에 더하여 학습하기에 훨씬 더 간단하고, 더 일반적인 setting으로 적용하기에 편한 A3C로서 code를 구성하기에도 편하고, 계산량도 훨씬 덜합니다. 그리고 전반적으로 더 좋은 성능을 가집니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [GAE 여행하기](https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/)\n\n<br>\n\n# 다음으로\n\n## [PPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/ppo_gae.py)","source":"_posts/7_ppo.md","raw":"---\ntitle: Proximal Policy Optimization\ndate: 2018-06-22 16:53:12\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 이동민, 장수영, 차금강\nsubtitle: 피지여행 7번째 논문\n---\n\n<center> <img src=\"https://www.dropbox.com/s/145van5kldfvvd5/Screen%20Shot%202018-07-18%20at%201.19.30%20AM.png?dl=1\" width=\"700\"> </center>\n\n논문 저자 : John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov\n논문 링크 : https://arxiv.org/pdf/1707.06347.pdf\nProceeding : unpublished.\n정리 : 이동민, 장수영, 차금강\n\n---\n\n# 1. 들어가며...\n\nSutton_PG부터 시작하여 TRPO, GAE를 거쳐 PPO까지 대단히 고생많으셨습니다. 먼저 이 논문은 TRPO보다는 쉽습니다. cliping이라는 새로운 개념이 나오지만 크게 어렵진 않습니다.\n\n이 논문에서는 Reinforcement Learning에서 Policy Gradient Method의 새로운 방법인 PPO를 제안합니다. 이 방법은 agent가 환경과의 상호작용을 통해 data를 sampling하는 것과 stochastic gradient ascent를 사용하여 \"surrogate\" objective function을 optimizing하는 것을 번갈아가면서 하는 방법입니다. data sample마다 one gradient update를 수행하는 기존의 방법과는 달리, minibatch update의 multiple epochs를 가능하게 하는 새로운 objective function을 말합니다.\n\n이 알고리즘의 장점으로는\n\n- Trust Region Policy Optimization(TRPO)의 장점만을 가집니다. 다시 말해 알고리즘으로 학습하기에 훨씬 더 간단하고, 더 일반적이고, 더 좋은 sample complexity (empirically)를 가집니다. \n- 또한 다른 online policy gradient method들을 능가했고, 전반적으로 sample complexity(특히 computational complexity), simplicity, wall-time가 좋다고 합니다.\n\n<br><br>\n\n# 2. Introduction\n\n<br>\n## 2.1 대표적인 방법들\n\n- DQN\n    - Discrete action space를 가지는 문제들에는 효과적으로 적용 가능하지만, continuous control에도 잘 작동하는지는 검증되지 않았습니다.\n- A3C - \"Vanilla\" policy gradient methods\n    - Data efficiency와 robustness 측면이 좋지 않습니다. A3C의 data efficiency의 경우 on-policy로서 한 번 쓴 data는 바로 버리기 때문에 data efficiency가 좋지 않다는 것입니다.\n- TRPO\n    - 간단히 말해 복잡합니다.\n    - 또한 noise(ex. dropout)나 parameter sharing(policy와 value function 간 혹은 auxiliary tasks와의)를 포함하는 architecture와의 호환성 없습니다.\n\n<br>\n## 2.2 대표적인 방법들 대비 개선사항\n\n- Scalability\n    - large models and parallel implementations\n- Data Efficiency\n- Robustness\n    - hyperparameter tuning없이 다양한 문제들에 적용되어 해결\n\n<br>\n## 2.3 제안하는 알고리즘\n\n이 논문에는 Clipped probability ratios를 포함하는 objective function 제안하였습니다.\n\n- TRPO의 data efficiency와 robustness를 유지하면서도 first-order approximation만 사용합니다.\n- Policy 성능에 대한 lower bound를 제공합니다.\n\n따라서 Policy로부터의 data sampling과 sampled data를 이용한 최적화를 번갈아가면서 수행합니다.\n\n<br>\n## 2.4 실험 결과\n\n다양한 버전의 surrogate objectives 중에는 clipped probability ratio가 가장 성능이 좋았습니다.\n\n- Continuous control tasks에서 기존 알고리즘 대비 성능이 좋습니다.\n- Atari에서는 A2C 대비 sampling efficiency 측면에서는 성능이 월등히 좋으며, ACER 대비 훨씬 간단하지만 성능은 비슷합니다.\n\n<br><br>\n\n# 3. Backgroud: Policy Optimization\n\n<br>\n## 3.1 Policy Gradient Methods\n\n일반적인 PG Method들은 실험적으로 destructive large policy updates가 발생합니다.\n\n- 더 자세히 말하자면, PG Method가 수행하는 parameter space에서의 gradual update가 policy space에서는 큰 변화를 유발할 수 있다는 의미입니다.\n\n<br>\n## 3.2 Trust Region Methods\n\nPolicy update 크기에 대한 contraint하에 objective function(\"surrogate\" function)을 최대화하는 것이 목표입니다. 수식은 아래와 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/gx6udoz5upswyf9/Screen%20Shot%202018-07-31%20at%2011.11.49%20PM.png?dl=1\" width=\"400\"> </center>\n\n위의 수식은 contraint로 인해 excessive large policy update가 방지됩니다.\n\nTRPO에서는 constrained optimization problem을 풀기 위해서는 다음과 같은 방법들이 필요합니다.\n\n1. Fisher Information Matrix인 second-order derivative of KL divergence를 사용하거나,\n    - 여기서 second-order matrixes를 구하기 위해서는 많은 계산량 필요합니다.\n2. Conjugate Gradient를 사용합니다.\n    - Conjugate Gradient는 구현하기가 어렵습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/6xpw9igndl3dmb9/Screen%20Shot%202018-07-31%20at%2011.15.13%20PM.png?dl=1\" width=\"500\"> </center>\n\n원래 이론적으로 위의 수식과 같이 \"contraint\"가 아니라 objective에 \"penalty\"를 부여하는 형태입니다. 하지만 다양한 문제들(혹은 학습 중에 특성이 변하는 문제)에서 모두 잘 동작하는 single value $\\beta$를 찾는 것(robustness)이 어렵기 때문에, TRPO에서는 penalty대신 contraint를 취하는 방식을 택한 것입니다.\n\n<br><br>\n\n# 4. Clipped Surrogate Objective\n\n이번 section에서는 TRPO의 surrogate obejctive function을 강제적으로 clipping하는 방법에 대하여 말합니다.\n\n먼저 기존의 TRPO의 surrogate function을 다음과 같이 표현합니다. \n$$r_t(\\theta)=\\dfrac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta old}(a_t|s_t)}, \\, \\, r_t(\\theta old) = 1$$\n\n위의 수식을 이용하여 TRPO의 surrogate object를 최대화합니다.\n<center> <img src=\"https://www.dropbox.com/s/e524vwsyolp6h9n/Screen%20Shot%202018-07-26%20at%2010.12.22%20AM.png?dl=1\" width=\"350\"> </center>\n\n위의 수식을 그대로 사용한다면 excessively large policy update가 됩니다. 따라서 penalty를 이용하여 필요 이상의 policy update를 방지합니다. TRPO에서는 KL-Divergence를 이용하여 penalty를 적용하지만 PPO에서는 computation적으로 효율적인 penalty를 적용하고 excessively large policy update를 방지하기 위해 아래와 같은 clipping 기법을 사용합니다.\n\n$$L^{CLIP}(\\theta) = \\hat{E}_t [min(r_t(\\theta) \\, \\hat{A}_t,  clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\, \\hat{A}_t)]$$\n\n- 위의 수식에 대한 추가적인 설명\n    - $\\epsilon$은 hyperparameter로서 continuous control에서는 0.2일 때 성능이 가장 좋았으며, Atari Game에서는 0.1 x $\\alpha$ 값을 사용합니다. (여기서 $\\alpha$ 는 학습률로 1 에서 시작하여 학습이 진행됨에 따라 0 으로 감소합니다.)\n    - Clipped 와 unclipped objective 중 min 값을 택함으로써 $L^{CLIP} (\\theta)$는 unclipped objective에 대한 lowerbound가 됩니다.\n\n이어서 아래의 그림을 보겠습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/3wcuf2tq7q24fy6/Screen%20Shot%202018-07-26%20at%2010.25.08%20AM.png?dl=1\" width=\"600\"> </center>\n\n위의 그림을 보면 두 가지의 그래프(Advantage Function $\\hat{A}_t$의 부호에 따라)로 clip에 대해서 설명하고 있습니다.\n\n- Advantage Function $\\hat{A}_t$가 양수일 때\n    - Advantage가 현재보다 높다라는 뜻이며 파라미터를 +의 방향으로 업데이트 하여야 합니다. 다시 말해 어떠한 상태 $s$에서 행동 $a$가 평균보다 좋다는 의미입니다. 따라서 이를 취할 확률이 증가하게 되고, $r_t (\\theta)$를 clip하여 $\\epsilon$보다 커지지 않도록 유도하는 것입니다.\n    - 추가적으로, TRPO에서 다뤘던 constraint가 아니라 단순히 clip하는 것이기 때문에 실제로 $\\pi_\\theta (a_t|s_t)$의 증가량이 $\\epsilon$보다 더 커질 수도 있습니다. 하지만 증가량이 더 커졌다고 하더라도 objective function을 업데이트 할 때 효과적이지 않을 수 있기 때문에 대부분 clip을 통해서 $\\epsilon$ 이하로 유지합니다.\n    - 또한 만약 $r_t (\\theta)$가 objective function의 값을 감소시키는 방향으로 움직이는 경우더라도 $1-\\epsilon$보다 작아져도 됩니다. 여기서의 목적은 최대한 lowerbound를 구하는 것이 목적이기 때문입니다. 그러니까 쉽게 말해서 왼쪽 그림에서도 볼 수 있듯이 $1+\\epsilon$만 구하는 데에 포커스를 맞추고 있고, $1-\\epsilon$은 신경쓰지 않고 있는 것입니다. (이 부분은 Advantage Function $\\hat{A}_t$가 음수일 때도 동일합니다.)\n- Advantage Function $\\hat{A}_t$가 음수일 때 \n    - Advantage가 현재보다 좋지 않다라는 뜻이며 그의 반대 방향으로 업데이트 하여야 합니다. 다시 말해 어떠한 상태 $s$에서 행동 $a$가 평균보다 좋지 않다는 의미입니다. 따라서 이를 취할 확률이 감소하게 되고, $r_t (\\theta)$를 clip하여 $\\epsilon$보다 작아지지 않도록 유도하는 것입니다.  \n    - 또한 $r_t(\\theta)$은 확률을 뜻하는 두 개의 함수를 분자 분모로 가지고 있으며, 분수로 구성되어 있기 때문에 무조건 양수로 이루어져 있습니다. Advantage function인 $\\hat{A}_t$와 곱해져 Objective function인 $L^{CLIP}$은 Advantage function과 방향이 같아집니다.\n\n위의 설명으로 인하여 나오는 그림이 아래의 그림입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/f7i97geiligfej9/Screen%20Shot%202018-07-26%20at%2011.02.17%20AM.png?dl=1\" width=\"700\"> </center>\n\n위의 그림에서 빨간색 그래프를 보면,\n\n- $L^{CLIP}$은 min 값들의 평균이기 때문에 평균들의 min 값보다는 더 작아집니다. 즉, 주황색 그래프와 초록색 그래프 중 작은값보다 더 작아지는 것을 볼 수 있습니다.\n- $L^{CLIP}$을 최대화하는 $\\theta$가 다음 $\\pi_\\theta$가 됩니다. 다시 말해 PPO는 기존 PG 방법들처럼 parameter space에서 parameter $\\theta$를 점진적으로 업데이트하는 것이 아니라, 매번 policy를 maximize하는 방향으로 업데이트하는 것으로 볼 수 있는 것입니다.\n\n\n<br><br>\n\n# 5. Adaptive KL Penalty Coefficient\n\n이전까지 설명했던 Clipped Surrogate Objective 방법과 달리 기존의 TRPO에서 Adaptive한 파라미터를 적용한 또 다른 방법에 대해서 알아보겠습니다. 사실 이 방법은 앞서본 clip을 사용한 방법보다는 성능이 좋지 않습니다. 하지만 baseline으로써 알아볼 필요가 있습니다.\n\nclip에서 다뤘던 Probability ratio $r_t(\\theta)$ 대신 KL divergence를 이용하여 penalty를 줍니다. 그 결과 각각의 policy update에 대해 KL divergence $d_{targ}$의 target value를 얻습니다.\n\n- clipped surrogate objective 대신하여 or 추가적으로 사용할 수 있다고 합니다. \n- 자체 실험 결과에서는 clipped surrogate objective 보다는 성능이 안 좋았다고 합니다.\n- 둘 다 사용한 실험 결과는 없습니다.\n\n이 알고리즘에서 각각의 policy update에 적용하는 step은 다음과 같습니다. \n\n1. KL-penalized objective 최적화를 합니다. 기존의 TRPO의 Objective function에 $\\beta$를 적용하여 다음과 같이 $\\beta$를 Adaptive하게 조절하고 있습니다. 수식은 아래와 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ojq7kfobf0f8x9n/Screen%20Shot%202018-07-26%20at%2011.22.44%20AM.png?dl=1\" width=\"500\"> </center>\n\n2. <img src=\"https://www.dropbox.com/s/j9phkalrbgf45k0/Screen%20Shot%202018-07-26%20at%2011.34.25%20AM.png?dl=1\" width=\"230\">을 계산합니다.\n    - 만약 $d < d_{targ} \\, / \\, 1.5$라면, 그 때 $\\beta \\leftarrow \\beta \\, / 2$\n    - 만약 $d > d_{targ} \\, x \\, 1.5$라면, 그 때 $\\beta \\leftarrow \\beta \\, x 2$\n    - 즉, KL-divergence 값이 일정 이상 커지게 되면 objective function에 penalty를 더 크게 부과합니다.\n    - 갱신된 $\\beta$ 값은 다음 policy update 때 사용합니다. \n\n$\\beta$를 조절하는 방법은 만약 $\\theta old$와 $\\theta$간의 파라미터 차이가 크다면 penalty를 강하게 부여하여 차이를 작게하고, 파라미터 차이가 작다면 penalty를 완화시켜 주어 차이를 더 크게하는 것입니다.\n\n정리하자면, KL-divergence를 constraint로 둔 것이 아니기 때문에, 간혹 excessive large policy update가 발생할 수도 있지만, KL Penaly coefficienct인 $\\beta$가 KL-divergence에 따라 adpative 하게 조정됨으로써 excessive large policy update가 지속적으로 발생되는 것을 방지하는 것입니다.\n\n<br><br>\n\n# 6. Algorithm\n\n앞서 다뤘던 section들은 어떻게 policy만을 업데이트 하는지에 대해 설명하고 있습니다. 이번 section에서는 value, entropy-exploration과 합쳐서 어떻게 통합적으로 업데이트 하는지에 대해서 알아보겠습니다.\n\n먼저 Variance-reduced advantage function estimator을 계산하기 위해 learned state-value fucntion $V(s)$을 사용합니다. 여기에는 두 가지 방법이 있습니다.\n\n- Generalized advantage estimation\n- Finite horizontal estimators\n\n만약 policy와 value function 간 parameter sharing하는 neural network architecture를 사용한다면, policy surrogate와 value function error term을 combine한 loss function을 사용해야 합니다. 또한 이 loss function에 entropy bonus term을 추가하여, 충분한 exploration이 될 수 있도록 합니다.(exploration하는 부분은 [A3C 논문](https://arxiv.org/pdf/1602.01783.pdf)에 나와있습니다.)\n\n그래서 앞써 다뤘던 objective function을 $L^{CLIP}$라고 표현한다면 PPO에서 제시하는 통합적으로 최대화해야하는 objective function은 다음과 같이 표현할 수 있습니다.\n\n$$L_t^{CLIP+VF+S} (\\theta) = \\hat{E}_t [L^{CLIP} (\\theta) - c_1 L_t^{VF} (\\theta) + c_2 S[\\pi_\\theta] (s_t)]$$\n\n위의 수식에 대한 추가적인 설명은 다음과 같습니다.\n\n- $c_1, c_2$ : coefficients\n- $S$ : entropy bonus\n- $L_t^{VF}$ : squared-error loss $(V_\\theta (s_t) - V_t^{targ})^2$\n\n위의 objective function을 최대화하면 Reinforcement Learning을 통한 policy 학습, state-value function의 학습, exploration을 할 수 있습니다.\n\n추가적으로 A3C와 같은 요즘 인기있는 PG의 style에서는 T time steps(T는 episode length 보다 훨씬 작은 크기) 동안 policy에 따라서 sample들을 얻고, 이 sample들을 업데이트에 사용합니다. 이러한 방식은 time step T 까지만 고려하는 advantage estimator가 필요하며, A3C에서 다음과 같이 사용합니다.\n\n$$\\hat{A}_t = - V(s_t) + r_t + \\gamma r_{t+1} + \\cdots + \\gamma^{T-t+1} r_{T-1} + \\gamma^{T-t}V(s_T)$$\n\n- 여기서 $t$는 주어진 length T trajectory segment 내에서 $[0, T]$에 있는 time index입니다.\n\n위의 수식에 더하여 generalized version인 GAE의 truncated version(generalized advantage estimation)을 사용합니다.($\\lambda$가 1이면 위 식과 같아집니다.) 수식은 아래와 같습니다. \n\n$$\\hat{A}_t = \\delta_t + (\\gamma\\lambda)\\delta_{t+1} + \\cdots + (\\gamma\\lambda)^{T-t+1}\\delta_{T-1},$$$$where \\,\\,\\,\\, \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n\n그래서 고정된 length T trajectory segment를 사용한 PPO algorithm은 다음과 같은 pseudo code로 나타낼 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/q00y8x7eryl1ncd/Screen%20Shot%202018-07-26%20at%202.28.03%20PM.png?dl=1\" width=\"800\"> </center>\n\n- 여기서 actor는 A3C처럼 병렬로 두어도 상관없습니다.\n- advantage estimate 계산을 통해서 surrogate loss를 계산합니다.\n\n<br><br>\n\n# 7. Experimnet\n\n<br>\n## 7.1 Surrogate Objectives의 비교\n\n이번 section에서는 3가지 Surrogate Objectives를 비교분석 하고 있습니다.\n\n* No clipping or penalty: $L_t(\\theta) = r_t(\\theta)\\hat{A}_t$\n* Clipping: $L_t(\\theta) = min(r_t(\\theta) \\, \\hat{A}_t, \\, clip(r_t(\\theta), 1-\\epsilon,1+\\epsilon) \\, \\hat{A}_t)$\n* KL penalty(fixed or adaptive): <img src=\"https://www.dropbox.com/s/ksnhlxz2riuns1p/Screen%20Shot%202018-07-26%20at%202.42.50%20PM.png?dl=1\" width=\"270\">\n\n그리고 환경은 MuJoCo를 사용하며 OpenAI Gym 환경에서 실험을 진행하였습니다. 그리고 네트워크들은 2개의 hidden layer를 가지며 각각 64개의 unit들을 가지고 있습니다. 그리고 tanh를 활성화 함수로써 사용하고 있으며 네트워크는 Gaussian distribution을 출력하는 것으로 구성되어 있습니다. 그리고 Policy Network와 Value Network는 파라미터들을 공유하고 있지 않으며, 위에서 설명한 Exploration을 위한 entropy는 사용하지 않습니다.\n\n7개의 환경에서 실험하였으며 각 환경들은 HalfCheetah, Hopper, InvertedDoublePendulum, InvertedPerdulum, Reacher, Swimmer, Walker2d를 사용하였으며 모든 환경들은 v1 버전을 사용하였습니다.\n\n아래의 표는 실험 결과입니다. 세 가지 Surrogate Objective에 대해 각 파라미터들을 표에 표기된 방식대로 변화하며 실험하였습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/4xbkxh4m82mu1vo/Screen%20Shot%202018-07-26%20at%202.45.42%20PM.png?dl=1\" width=\"450\"> </center>\n\n<br>\n## 7.2 Comparison to other Algorithms in the Continuous Domain\n\n이번 section에서는 Clipping 버전의 PPO와 다른 알고리즘들 간의 비교를 연속적인 Action을 가지는 환경에서 실험을 한 결과를 보여주고 있습니다. 다른 알고리즘은 TRPO, cross-entropy method, vanilla policy gradient with adaptive stepsize, A2C, A2C with trust region를 사용하였습니다. 환경과 사용한 네트워크의 구성은 이전 section과 동일합니다.\n\n<center> <img src=\"https://www.dropbox.com/s/tf7djngioxnxtef/Screen%20Shot%202018-07-26%20at%202.50.23%20PM.png?dl=1\" width=\"900\"> </center>\n\n여타 다른 알고리즘보다 좋은 성능을 보이는 것을 알 수 있습니다.\n\n<br>\n## 7.3 Showcase in the Continuous Domain: Humanoid Running and Steering\n\n이번 section에서는 Humanoid-v0, HumanoidFlagrun-v0, HumanoidFlagrunHarder-v0의 환경에서 Roboschool을 사용하여 실험하였습니다.\n\n- Humanoid-v0는 단순히 앞으로 걸어나가는 환경\n- HumanoidFlagrun-v0은 200스텝마다 혹은 목적지에 도착할 때 마다 위치가 바뀌는 목적지에 걸어서 도달하는 환경입니다. \n- HumanoidFlagrunHarder-v0은 환경이 초기화될 때 마다 특정 경계안에 Humanoid가 위치하게 되며 특정 영역 밖으로 걸어나가는 것을 수행하는 환경입니다.\n\n\n<center> <img src=\"https://www.dropbox.com/s/y112ua7il6l0296/Screen%20Shot%202018-07-26%20at%202.53.01%20PM.png?dl=1\" width=\"900\"> </center>\n\n- Roboschool을 사용하여 3D humanoid control task에서 PPO 알고리즘으로 학습시킨 Learning curve입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/09scyk6zeviyswt/Screen%20Shot%202018-07-26%20at%202.53.10%20PM.png?dl=1\" width=\"900\"> </center>\n\n- Roboschool Humanoid Flagrun에서 학습된 policy의 frame들입니다. \n\n<br>\n## 7.4 Comparison to Other Algorithms on the Atari Domain\n\n이번 section에서는 Atari Domain에서 PPO, A2C, ACER(Actor Critic with Experience Replay)의 3가지 알고리즘을 비교합니다.\n\n<center> <img src=\"https://www.dropbox.com/s/jxuqdncxpnoyqgi/Screen%20Shot%202018-07-26%20at%203.00.00%20PM.png?dl=1\" width=\"600\"> </center>\n\n전체 training에 대해서는 PPO가 ACER보다 좋은 성능을 내고 있습니다. 하지만 마지막 100 에피소드만을 비교했을 때는 PPO보다 ACER이 더 좋은 성능을 보이고 있습니다. 이는 PPO가 더 빨리 최종 능력에 도달하지만, ACER이 가진 potential이 더 높다는 것입니다.\n\n<br><br>\n\n# 8. Conclusion\n\n이 논문에서는 policy update를 하기 위한 방법으로 stochastic gradient ascent의 multiple epchs를 사용하는 policy optimization method들의 하나의 알고리즘은 Proximal Policy Optimization(PPO)를 소개합니다.\n\n이 알고리즘은 trust region method의 stability와 reliability를 가집니다. 여기에 더하여 학습하기에 훨씬 더 간단하고, 더 일반적인 setting으로 적용하기에 편한 A3C로서 code를 구성하기에도 편하고, 계산량도 훨씬 덜합니다. 그리고 전반적으로 더 좋은 성능을 가집니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [GAE 여행하기](https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/)\n\n<br>\n\n# 다음으로\n\n## [PPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/ppo_gae.py)","slug":"7_ppo","published":1,"updated":"2019-02-07T11:21:31.972Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjrujlu0600105wfe1dn7jdsf","content":"<center> <img src=\"https://www.dropbox.com/s/145van5kldfvvd5/Screen%20Shot%202018-07-18%20at%201.19.30%20AM.png?dl=1\" width=\"700\"> </center>\n\n<p>논문 저자 : John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1707.06347.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1707.06347.pdf</a><br>Proceeding : unpublished.<br>정리 : 이동민, 장수영, 차금강</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p>Sutton_PG부터 시작하여 TRPO, GAE를 거쳐 PPO까지 대단히 고생많으셨습니다. 먼저 이 논문은 TRPO보다는 쉽습니다. cliping이라는 새로운 개념이 나오지만 크게 어렵진 않습니다.</p>\n<p>이 논문에서는 Reinforcement Learning에서 Policy Gradient Method의 새로운 방법인 PPO를 제안합니다. 이 방법은 agent가 환경과의 상호작용을 통해 data를 sampling하는 것과 stochastic gradient ascent를 사용하여 “surrogate” objective function을 optimizing하는 것을 번갈아가면서 하는 방법입니다. data sample마다 one gradient update를 수행하는 기존의 방법과는 달리, minibatch update의 multiple epochs를 가능하게 하는 새로운 objective function을 말합니다.</p>\n<p>이 알고리즘의 장점으로는</p>\n<ul>\n<li>Trust Region Policy Optimization(TRPO)의 장점만을 가집니다. 다시 말해 알고리즘으로 학습하기에 훨씬 더 간단하고, 더 일반적이고, 더 좋은 sample complexity (empirically)를 가집니다. </li>\n<li>또한 다른 online policy gradient method들을 능가했고, 전반적으로 sample complexity(특히 computational complexity), simplicity, wall-time가 좋다고 합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"2-Introduction\"><a href=\"#2-Introduction\" class=\"headerlink\" title=\"2. Introduction\"></a>2. Introduction</h1><p><br></p>\n<h2 id=\"2-1-대표적인-방법들\"><a href=\"#2-1-대표적인-방법들\" class=\"headerlink\" title=\"2.1 대표적인 방법들\"></a>2.1 대표적인 방법들</h2><ul>\n<li>DQN<ul>\n<li>Discrete action space를 가지는 문제들에는 효과적으로 적용 가능하지만, continuous control에도 잘 작동하는지는 검증되지 않았습니다.</li>\n</ul>\n</li>\n<li>A3C - “Vanilla” policy gradient methods<ul>\n<li>Data efficiency와 robustness 측면이 좋지 않습니다. A3C의 data efficiency의 경우 on-policy로서 한 번 쓴 data는 바로 버리기 때문에 data efficiency가 좋지 않다는 것입니다.</li>\n</ul>\n</li>\n<li>TRPO<ul>\n<li>간단히 말해 복잡합니다.</li>\n<li>또한 noise(ex. dropout)나 parameter sharing(policy와 value function 간 혹은 auxiliary tasks와의)를 포함하는 architecture와의 호환성 없습니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-2-대표적인-방법들-대비-개선사항\"><a href=\"#2-2-대표적인-방법들-대비-개선사항\" class=\"headerlink\" title=\"2.2 대표적인 방법들 대비 개선사항\"></a>2.2 대표적인 방법들 대비 개선사항</h2><ul>\n<li>Scalability<ul>\n<li>large models and parallel implementations</li>\n</ul>\n</li>\n<li>Data Efficiency</li>\n<li>Robustness<ul>\n<li>hyperparameter tuning없이 다양한 문제들에 적용되어 해결</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-3-제안하는-알고리즘\"><a href=\"#2-3-제안하는-알고리즘\" class=\"headerlink\" title=\"2.3 제안하는 알고리즘\"></a>2.3 제안하는 알고리즘</h2><p>이 논문에는 Clipped probability ratios를 포함하는 objective function 제안하였습니다.</p>\n<ul>\n<li>TRPO의 data efficiency와 robustness를 유지하면서도 first-order approximation만 사용합니다.</li>\n<li>Policy 성능에 대한 lower bound를 제공합니다.</li>\n</ul>\n<p>따라서 Policy로부터의 data sampling과 sampled data를 이용한 최적화를 번갈아가면서 수행합니다.</p>\n<p><br></p>\n<h2 id=\"2-4-실험-결과\"><a href=\"#2-4-실험-결과\" class=\"headerlink\" title=\"2.4 실험 결과\"></a>2.4 실험 결과</h2><p>다양한 버전의 surrogate objectives 중에는 clipped probability ratio가 가장 성능이 좋았습니다.</p>\n<ul>\n<li>Continuous control tasks에서 기존 알고리즘 대비 성능이 좋습니다.</li>\n<li>Atari에서는 A2C 대비 sampling efficiency 측면에서는 성능이 월등히 좋으며, ACER 대비 훨씬 간단하지만 성능은 비슷합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"3-Backgroud-Policy-Optimization\"><a href=\"#3-Backgroud-Policy-Optimization\" class=\"headerlink\" title=\"3. Backgroud: Policy Optimization\"></a>3. Backgroud: Policy Optimization</h1><p><br></p>\n<h2 id=\"3-1-Policy-Gradient-Methods\"><a href=\"#3-1-Policy-Gradient-Methods\" class=\"headerlink\" title=\"3.1 Policy Gradient Methods\"></a>3.1 Policy Gradient Methods</h2><p>일반적인 PG Method들은 실험적으로 destructive large policy updates가 발생합니다.</p>\n<ul>\n<li>더 자세히 말하자면, PG Method가 수행하는 parameter space에서의 gradual update가 policy space에서는 큰 변화를 유발할 수 있다는 의미입니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-2-Trust-Region-Methods\"><a href=\"#3-2-Trust-Region-Methods\" class=\"headerlink\" title=\"3.2 Trust Region Methods\"></a>3.2 Trust Region Methods</h2><p>Policy update 크기에 대한 contraint하에 objective function(“surrogate” function)을 최대화하는 것이 목표입니다. 수식은 아래와 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/gx6udoz5upswyf9/Screen%20Shot%202018-07-31%20at%2011.11.49%20PM.png?dl=1\" width=\"400\"> </center>\n\n<p>위의 수식은 contraint로 인해 excessive large policy update가 방지됩니다.</p>\n<p>TRPO에서는 constrained optimization problem을 풀기 위해서는 다음과 같은 방법들이 필요합니다.</p>\n<ol>\n<li>Fisher Information Matrix인 second-order derivative of KL divergence를 사용하거나,<ul>\n<li>여기서 second-order matrixes를 구하기 위해서는 많은 계산량 필요합니다.</li>\n</ul>\n</li>\n<li>Conjugate Gradient를 사용합니다.<ul>\n<li>Conjugate Gradient는 구현하기가 어렵습니다.</li>\n</ul>\n</li>\n</ol>\n<center> <img src=\"https://www.dropbox.com/s/6xpw9igndl3dmb9/Screen%20Shot%202018-07-31%20at%2011.15.13%20PM.png?dl=1\" width=\"500\"> </center>\n\n<p>원래 이론적으로 위의 수식과 같이 “contraint”가 아니라 objective에 “penalty”를 부여하는 형태입니다. 하지만 다양한 문제들(혹은 학습 중에 특성이 변하는 문제)에서 모두 잘 동작하는 single value $\\beta$를 찾는 것(robustness)이 어렵기 때문에, TRPO에서는 penalty대신 contraint를 취하는 방식을 택한 것입니다.</p>\n<p><br><br></p>\n<h1 id=\"4-Clipped-Surrogate-Objective\"><a href=\"#4-Clipped-Surrogate-Objective\" class=\"headerlink\" title=\"4. Clipped Surrogate Objective\"></a>4. Clipped Surrogate Objective</h1><p>이번 section에서는 TRPO의 surrogate obejctive function을 강제적으로 clipping하는 방법에 대하여 말합니다.</p>\n<p>먼저 기존의 TRPO의 surrogate function을 다음과 같이 표현합니다.<br>$$r_t(\\theta)=\\dfrac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta old}(a_t|s_t)}, \\, \\, r_t(\\theta old) = 1$$</p>\n<p>위의 수식을 이용하여 TRPO의 surrogate object를 최대화합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/e524vwsyolp6h9n/Screen%20Shot%202018-07-26%20at%2010.12.22%20AM.png?dl=1\" width=\"350\"> </center>\n\n<p>위의 수식을 그대로 사용한다면 excessively large policy update가 됩니다. 따라서 penalty를 이용하여 필요 이상의 policy update를 방지합니다. TRPO에서는 KL-Divergence를 이용하여 penalty를 적용하지만 PPO에서는 computation적으로 효율적인 penalty를 적용하고 excessively large policy update를 방지하기 위해 아래와 같은 clipping 기법을 사용합니다.</p>\n<p>$$L^{CLIP}(\\theta) = \\hat{E}_t [min(r_t(\\theta) \\, \\hat{A}_t,  clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\, \\hat{A}_t)]$$</p>\n<ul>\n<li>위의 수식에 대한 추가적인 설명<ul>\n<li>$\\epsilon$은 hyperparameter로서 continuous control에서는 0.2일 때 성능이 가장 좋았으며, Atari Game에서는 0.1 x $\\alpha$ 값을 사용합니다. (여기서 $\\alpha$ 는 학습률로 1 에서 시작하여 학습이 진행됨에 따라 0 으로 감소합니다.)</li>\n<li>Clipped 와 unclipped objective 중 min 값을 택함으로써 $L^{CLIP} (\\theta)$는 unclipped objective에 대한 lowerbound가 됩니다.</li>\n</ul>\n</li>\n</ul>\n<p>이어서 아래의 그림을 보겠습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/3wcuf2tq7q24fy6/Screen%20Shot%202018-07-26%20at%2010.25.08%20AM.png?dl=1\" width=\"600\"> </center>\n\n<p>위의 그림을 보면 두 가지의 그래프(Advantage Function $\\hat{A}_t$의 부호에 따라)로 clip에 대해서 설명하고 있습니다.</p>\n<ul>\n<li>Advantage Function $\\hat{A}_t$가 양수일 때<ul>\n<li>Advantage가 현재보다 높다라는 뜻이며 파라미터를 +의 방향으로 업데이트 하여야 합니다. 다시 말해 어떠한 상태 $s$에서 행동 $a$가 평균보다 좋다는 의미입니다. 따라서 이를 취할 확률이 증가하게 되고, $r_t (\\theta)$를 clip하여 $\\epsilon$보다 커지지 않도록 유도하는 것입니다.</li>\n<li>추가적으로, TRPO에서 다뤘던 constraint가 아니라 단순히 clip하는 것이기 때문에 실제로 $\\pi_\\theta (a_t|s_t)$의 증가량이 $\\epsilon$보다 더 커질 수도 있습니다. 하지만 증가량이 더 커졌다고 하더라도 objective function을 업데이트 할 때 효과적이지 않을 수 있기 때문에 대부분 clip을 통해서 $\\epsilon$ 이하로 유지합니다.</li>\n<li>또한 만약 $r_t (\\theta)$가 objective function의 값을 감소시키는 방향으로 움직이는 경우더라도 $1-\\epsilon$보다 작아져도 됩니다. 여기서의 목적은 최대한 lowerbound를 구하는 것이 목적이기 때문입니다. 그러니까 쉽게 말해서 왼쪽 그림에서도 볼 수 있듯이 $1+\\epsilon$만 구하는 데에 포커스를 맞추고 있고, $1-\\epsilon$은 신경쓰지 않고 있는 것입니다. (이 부분은 Advantage Function $\\hat{A}_t$가 음수일 때도 동일합니다.)</li>\n</ul>\n</li>\n<li>Advantage Function $\\hat{A}_t$가 음수일 때 <ul>\n<li>Advantage가 현재보다 좋지 않다라는 뜻이며 그의 반대 방향으로 업데이트 하여야 합니다. 다시 말해 어떠한 상태 $s$에서 행동 $a$가 평균보다 좋지 않다는 의미입니다. 따라서 이를 취할 확률이 감소하게 되고, $r_t (\\theta)$를 clip하여 $\\epsilon$보다 작아지지 않도록 유도하는 것입니다.  </li>\n<li>또한 $r_t(\\theta)$은 확률을 뜻하는 두 개의 함수를 분자 분모로 가지고 있으며, 분수로 구성되어 있기 때문에 무조건 양수로 이루어져 있습니다. Advantage function인 $\\hat{A}_t$와 곱해져 Objective function인 $L^{CLIP}$은 Advantage function과 방향이 같아집니다.</li>\n</ul>\n</li>\n</ul>\n<p>위의 설명으로 인하여 나오는 그림이 아래의 그림입니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/f7i97geiligfej9/Screen%20Shot%202018-07-26%20at%2011.02.17%20AM.png?dl=1\" width=\"700\"> </center>\n\n<p>위의 그림에서 빨간색 그래프를 보면,</p>\n<ul>\n<li>$L^{CLIP}$은 min 값들의 평균이기 때문에 평균들의 min 값보다는 더 작아집니다. 즉, 주황색 그래프와 초록색 그래프 중 작은값보다 더 작아지는 것을 볼 수 있습니다.</li>\n<li>$L^{CLIP}$을 최대화하는 $\\theta$가 다음 $\\pi_\\theta$가 됩니다. 다시 말해 PPO는 기존 PG 방법들처럼 parameter space에서 parameter $\\theta$를 점진적으로 업데이트하는 것이 아니라, 매번 policy를 maximize하는 방향으로 업데이트하는 것으로 볼 수 있는 것입니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"5-Adaptive-KL-Penalty-Coefficient\"><a href=\"#5-Adaptive-KL-Penalty-Coefficient\" class=\"headerlink\" title=\"5. Adaptive KL Penalty Coefficient\"></a>5. Adaptive KL Penalty Coefficient</h1><p>이전까지 설명했던 Clipped Surrogate Objective 방법과 달리 기존의 TRPO에서 Adaptive한 파라미터를 적용한 또 다른 방법에 대해서 알아보겠습니다. 사실 이 방법은 앞서본 clip을 사용한 방법보다는 성능이 좋지 않습니다. 하지만 baseline으로써 알아볼 필요가 있습니다.</p>\n<p>clip에서 다뤘던 Probability ratio $r_t(\\theta)$ 대신 KL divergence를 이용하여 penalty를 줍니다. 그 결과 각각의 policy update에 대해 KL divergence $d_{targ}$의 target value를 얻습니다.</p>\n<ul>\n<li>clipped surrogate objective 대신하여 or 추가적으로 사용할 수 있다고 합니다. </li>\n<li>자체 실험 결과에서는 clipped surrogate objective 보다는 성능이 안 좋았다고 합니다.</li>\n<li>둘 다 사용한 실험 결과는 없습니다.</li>\n</ul>\n<p>이 알고리즘에서 각각의 policy update에 적용하는 step은 다음과 같습니다. </p>\n<ol>\n<li>KL-penalized objective 최적화를 합니다. 기존의 TRPO의 Objective function에 $\\beta$를 적용하여 다음과 같이 $\\beta$를 Adaptive하게 조절하고 있습니다. 수식은 아래와 같습니다.</li>\n</ol>\n<center> <img src=\"https://www.dropbox.com/s/ojq7kfobf0f8x9n/Screen%20Shot%202018-07-26%20at%2011.22.44%20AM.png?dl=1\" width=\"500\"> </center>\n\n<ol start=\"2\">\n<li><img src=\"https://www.dropbox.com/s/j9phkalrbgf45k0/Screen%20Shot%202018-07-26%20at%2011.34.25%20AM.png?dl=1\" width=\"230\">을 계산합니다.<ul>\n<li>만약 $d &lt; d_{targ} \\, / \\, 1.5$라면, 그 때 $\\beta \\leftarrow \\beta \\, / 2$</li>\n<li>만약 $d &gt; d_{targ} \\, x \\, 1.5$라면, 그 때 $\\beta \\leftarrow \\beta \\, x 2$</li>\n<li>즉, KL-divergence 값이 일정 이상 커지게 되면 objective function에 penalty를 더 크게 부과합니다.</li>\n<li>갱신된 $\\beta$ 값은 다음 policy update 때 사용합니다. </li>\n</ul>\n</li>\n</ol>\n<p>$\\beta$를 조절하는 방법은 만약 $\\theta old$와 $\\theta$간의 파라미터 차이가 크다면 penalty를 강하게 부여하여 차이를 작게하고, 파라미터 차이가 작다면 penalty를 완화시켜 주어 차이를 더 크게하는 것입니다.</p>\n<p>정리하자면, KL-divergence를 constraint로 둔 것이 아니기 때문에, 간혹 excessive large policy update가 발생할 수도 있지만, KL Penaly coefficienct인 $\\beta$가 KL-divergence에 따라 adpative 하게 조정됨으로써 excessive large policy update가 지속적으로 발생되는 것을 방지하는 것입니다.</p>\n<p><br><br></p>\n<h1 id=\"6-Algorithm\"><a href=\"#6-Algorithm\" class=\"headerlink\" title=\"6. Algorithm\"></a>6. Algorithm</h1><p>앞서 다뤘던 section들은 어떻게 policy만을 업데이트 하는지에 대해 설명하고 있습니다. 이번 section에서는 value, entropy-exploration과 합쳐서 어떻게 통합적으로 업데이트 하는지에 대해서 알아보겠습니다.</p>\n<p>먼저 Variance-reduced advantage function estimator을 계산하기 위해 learned state-value fucntion $V(s)$을 사용합니다. 여기에는 두 가지 방법이 있습니다.</p>\n<ul>\n<li>Generalized advantage estimation</li>\n<li>Finite horizontal estimators</li>\n</ul>\n<p>만약 policy와 value function 간 parameter sharing하는 neural network architecture를 사용한다면, policy surrogate와 value function error term을 combine한 loss function을 사용해야 합니다. 또한 이 loss function에 entropy bonus term을 추가하여, 충분한 exploration이 될 수 있도록 합니다.(exploration하는 부분은 <a href=\"https://arxiv.org/pdf/1602.01783.pdf\" target=\"_blank\" rel=\"noopener\">A3C 논문</a>에 나와있습니다.)</p>\n<p>그래서 앞써 다뤘던 objective function을 $L^{CLIP}$라고 표현한다면 PPO에서 제시하는 통합적으로 최대화해야하는 objective function은 다음과 같이 표현할 수 있습니다.</p>\n<p>$$L_t^{CLIP+VF+S} (\\theta) = \\hat{E}_t [L^{CLIP} (\\theta) - c_1 L_t^{VF} (\\theta) + c_2 S[\\pi_\\theta] (s_t)]$$</p>\n<p>위의 수식에 대한 추가적인 설명은 다음과 같습니다.</p>\n<ul>\n<li>$c_1, c_2$ : coefficients</li>\n<li>$S$ : entropy bonus</li>\n<li>$L_t^{VF}$ : squared-error loss $(V_\\theta (s_t) - V_t^{targ})^2$</li>\n</ul>\n<p>위의 objective function을 최대화하면 Reinforcement Learning을 통한 policy 학습, state-value function의 학습, exploration을 할 수 있습니다.</p>\n<p>추가적으로 A3C와 같은 요즘 인기있는 PG의 style에서는 T time steps(T는 episode length 보다 훨씬 작은 크기) 동안 policy에 따라서 sample들을 얻고, 이 sample들을 업데이트에 사용합니다. 이러한 방식은 time step T 까지만 고려하는 advantage estimator가 필요하며, A3C에서 다음과 같이 사용합니다.</p>\n<p>$$\\hat{A}_t = - V(s_t) + r_t + \\gamma r_{t+1} + \\cdots + \\gamma^{T-t+1} r_{T-1} + \\gamma^{T-t}V(s_T)$$</p>\n<ul>\n<li>여기서 $t$는 주어진 length T trajectory segment 내에서 $[0, T]$에 있는 time index입니다.</li>\n</ul>\n<p>위의 수식에 더하여 generalized version인 GAE의 truncated version(generalized advantage estimation)을 사용합니다.($\\lambda$가 1이면 위 식과 같아집니다.) 수식은 아래와 같습니다. </p>\n<p>$$\\hat{A}_t = \\delta_t + (\\gamma\\lambda)\\delta_{t+1} + \\cdots + (\\gamma\\lambda)^{T-t+1}\\delta_{T-1},$$$$where \\,\\,\\,\\, \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$</p>\n<p>그래서 고정된 length T trajectory segment를 사용한 PPO algorithm은 다음과 같은 pseudo code로 나타낼 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/q00y8x7eryl1ncd/Screen%20Shot%202018-07-26%20at%202.28.03%20PM.png?dl=1\" width=\"800\"> </center>\n\n<ul>\n<li>여기서 actor는 A3C처럼 병렬로 두어도 상관없습니다.</li>\n<li>advantage estimate 계산을 통해서 surrogate loss를 계산합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"7-Experimnet\"><a href=\"#7-Experimnet\" class=\"headerlink\" title=\"7. Experimnet\"></a>7. Experimnet</h1><p><br></p>\n<h2 id=\"7-1-Surrogate-Objectives의-비교\"><a href=\"#7-1-Surrogate-Objectives의-비교\" class=\"headerlink\" title=\"7.1 Surrogate Objectives의 비교\"></a>7.1 Surrogate Objectives의 비교</h2><p>이번 section에서는 3가지 Surrogate Objectives를 비교분석 하고 있습니다.</p>\n<ul>\n<li>No clipping or penalty: $L_t(\\theta) = r_t(\\theta)\\hat{A}_t$</li>\n<li>Clipping: $L_t(\\theta) = min(r_t(\\theta) \\, \\hat{A}_t, \\, clip(r_t(\\theta), 1-\\epsilon,1+\\epsilon) \\, \\hat{A}_t)$</li>\n<li>KL penalty(fixed or adaptive): <img src=\"https://www.dropbox.com/s/ksnhlxz2riuns1p/Screen%20Shot%202018-07-26%20at%202.42.50%20PM.png?dl=1\" width=\"270\"></li>\n</ul>\n<p>그리고 환경은 MuJoCo를 사용하며 OpenAI Gym 환경에서 실험을 진행하였습니다. 그리고 네트워크들은 2개의 hidden layer를 가지며 각각 64개의 unit들을 가지고 있습니다. 그리고 tanh를 활성화 함수로써 사용하고 있으며 네트워크는 Gaussian distribution을 출력하는 것으로 구성되어 있습니다. 그리고 Policy Network와 Value Network는 파라미터들을 공유하고 있지 않으며, 위에서 설명한 Exploration을 위한 entropy는 사용하지 않습니다.</p>\n<p>7개의 환경에서 실험하였으며 각 환경들은 HalfCheetah, Hopper, InvertedDoublePendulum, InvertedPerdulum, Reacher, Swimmer, Walker2d를 사용하였으며 모든 환경들은 v1 버전을 사용하였습니다.</p>\n<p>아래의 표는 실험 결과입니다. 세 가지 Surrogate Objective에 대해 각 파라미터들을 표에 표기된 방식대로 변화하며 실험하였습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/4xbkxh4m82mu1vo/Screen%20Shot%202018-07-26%20at%202.45.42%20PM.png?dl=1\" width=\"450\"> </center>\n\n<p><br></p>\n<h2 id=\"7-2-Comparison-to-other-Algorithms-in-the-Continuous-Domain\"><a href=\"#7-2-Comparison-to-other-Algorithms-in-the-Continuous-Domain\" class=\"headerlink\" title=\"7.2 Comparison to other Algorithms in the Continuous Domain\"></a>7.2 Comparison to other Algorithms in the Continuous Domain</h2><p>이번 section에서는 Clipping 버전의 PPO와 다른 알고리즘들 간의 비교를 연속적인 Action을 가지는 환경에서 실험을 한 결과를 보여주고 있습니다. 다른 알고리즘은 TRPO, cross-entropy method, vanilla policy gradient with adaptive stepsize, A2C, A2C with trust region를 사용하였습니다. 환경과 사용한 네트워크의 구성은 이전 section과 동일합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/tf7djngioxnxtef/Screen%20Shot%202018-07-26%20at%202.50.23%20PM.png?dl=1\" width=\"900\"> </center>\n\n<p>여타 다른 알고리즘보다 좋은 성능을 보이는 것을 알 수 있습니다.</p>\n<p><br></p>\n<h2 id=\"7-3-Showcase-in-the-Continuous-Domain-Humanoid-Running-and-Steering\"><a href=\"#7-3-Showcase-in-the-Continuous-Domain-Humanoid-Running-and-Steering\" class=\"headerlink\" title=\"7.3 Showcase in the Continuous Domain: Humanoid Running and Steering\"></a>7.3 Showcase in the Continuous Domain: Humanoid Running and Steering</h2><p>이번 section에서는 Humanoid-v0, HumanoidFlagrun-v0, HumanoidFlagrunHarder-v0의 환경에서 Roboschool을 사용하여 실험하였습니다.</p>\n<ul>\n<li>Humanoid-v0는 단순히 앞으로 걸어나가는 환경</li>\n<li>HumanoidFlagrun-v0은 200스텝마다 혹은 목적지에 도착할 때 마다 위치가 바뀌는 목적지에 걸어서 도달하는 환경입니다. </li>\n<li>HumanoidFlagrunHarder-v0은 환경이 초기화될 때 마다 특정 경계안에 Humanoid가 위치하게 되며 특정 영역 밖으로 걸어나가는 것을 수행하는 환경입니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/y112ua7il6l0296/Screen%20Shot%202018-07-26%20at%202.53.01%20PM.png?dl=1\" width=\"900\"> </center>\n\n<ul>\n<li>Roboschool을 사용하여 3D humanoid control task에서 PPO 알고리즘으로 학습시킨 Learning curve입니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/09scyk6zeviyswt/Screen%20Shot%202018-07-26%20at%202.53.10%20PM.png?dl=1\" width=\"900\"> </center>\n\n<ul>\n<li>Roboschool Humanoid Flagrun에서 학습된 policy의 frame들입니다. </li>\n</ul>\n<p><br></p>\n<h2 id=\"7-4-Comparison-to-Other-Algorithms-on-the-Atari-Domain\"><a href=\"#7-4-Comparison-to-Other-Algorithms-on-the-Atari-Domain\" class=\"headerlink\" title=\"7.4 Comparison to Other Algorithms on the Atari Domain\"></a>7.4 Comparison to Other Algorithms on the Atari Domain</h2><p>이번 section에서는 Atari Domain에서 PPO, A2C, ACER(Actor Critic with Experience Replay)의 3가지 알고리즘을 비교합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/jxuqdncxpnoyqgi/Screen%20Shot%202018-07-26%20at%203.00.00%20PM.png?dl=1\" width=\"600\"> </center>\n\n<p>전체 training에 대해서는 PPO가 ACER보다 좋은 성능을 내고 있습니다. 하지만 마지막 100 에피소드만을 비교했을 때는 PPO보다 ACER이 더 좋은 성능을 보이고 있습니다. 이는 PPO가 더 빨리 최종 능력에 도달하지만, ACER이 가진 potential이 더 높다는 것입니다.</p>\n<p><br><br></p>\n<h1 id=\"8-Conclusion\"><a href=\"#8-Conclusion\" class=\"headerlink\" title=\"8. Conclusion\"></a>8. Conclusion</h1><p>이 논문에서는 policy update를 하기 위한 방법으로 stochastic gradient ascent의 multiple epchs를 사용하는 policy optimization method들의 하나의 알고리즘은 Proximal Policy Optimization(PPO)를 소개합니다.</p>\n<p>이 알고리즘은 trust region method의 stability와 reliability를 가집니다. 여기에 더하여 학습하기에 훨씬 더 간단하고, 더 일반적인 setting으로 적용하기에 편한 A3C로서 code를 구성하기에도 편하고, 계산량도 훨씬 덜합니다. 그리고 전반적으로 더 좋은 성능을 가집니다.</p>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"GAE-여행하기\"><a href=\"#GAE-여행하기\" class=\"headerlink\" title=\"GAE 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/\">GAE 여행하기</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"PPO-Code\"><a href=\"#PPO-Code\" class=\"headerlink\" title=\"PPO Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/ppo_gae.py\" target=\"_blank\" rel=\"noopener\">PPO Code</a></h2>","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"https://www.dropbox.com/s/145van5kldfvvd5/Screen%20Shot%202018-07-18%20at%201.19.30%20AM.png?dl=1\" width=\"700\"> </center>\n\n<p>논문 저자 : John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1707.06347.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1707.06347.pdf</a><br>Proceeding : unpublished.<br>정리 : 이동민, 장수영, 차금강</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p>Sutton_PG부터 시작하여 TRPO, GAE를 거쳐 PPO까지 대단히 고생많으셨습니다. 먼저 이 논문은 TRPO보다는 쉽습니다. cliping이라는 새로운 개념이 나오지만 크게 어렵진 않습니다.</p>\n<p>이 논문에서는 Reinforcement Learning에서 Policy Gradient Method의 새로운 방법인 PPO를 제안합니다. 이 방법은 agent가 환경과의 상호작용을 통해 data를 sampling하는 것과 stochastic gradient ascent를 사용하여 “surrogate” objective function을 optimizing하는 것을 번갈아가면서 하는 방법입니다. data sample마다 one gradient update를 수행하는 기존의 방법과는 달리, minibatch update의 multiple epochs를 가능하게 하는 새로운 objective function을 말합니다.</p>\n<p>이 알고리즘의 장점으로는</p>\n<ul>\n<li>Trust Region Policy Optimization(TRPO)의 장점만을 가집니다. 다시 말해 알고리즘으로 학습하기에 훨씬 더 간단하고, 더 일반적이고, 더 좋은 sample complexity (empirically)를 가집니다. </li>\n<li>또한 다른 online policy gradient method들을 능가했고, 전반적으로 sample complexity(특히 computational complexity), simplicity, wall-time가 좋다고 합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"2-Introduction\"><a href=\"#2-Introduction\" class=\"headerlink\" title=\"2. Introduction\"></a>2. Introduction</h1><p><br></p>\n<h2 id=\"2-1-대표적인-방법들\"><a href=\"#2-1-대표적인-방법들\" class=\"headerlink\" title=\"2.1 대표적인 방법들\"></a>2.1 대표적인 방법들</h2><ul>\n<li>DQN<ul>\n<li>Discrete action space를 가지는 문제들에는 효과적으로 적용 가능하지만, continuous control에도 잘 작동하는지는 검증되지 않았습니다.</li>\n</ul>\n</li>\n<li>A3C - “Vanilla” policy gradient methods<ul>\n<li>Data efficiency와 robustness 측면이 좋지 않습니다. A3C의 data efficiency의 경우 on-policy로서 한 번 쓴 data는 바로 버리기 때문에 data efficiency가 좋지 않다는 것입니다.</li>\n</ul>\n</li>\n<li>TRPO<ul>\n<li>간단히 말해 복잡합니다.</li>\n<li>또한 noise(ex. dropout)나 parameter sharing(policy와 value function 간 혹은 auxiliary tasks와의)를 포함하는 architecture와의 호환성 없습니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-2-대표적인-방법들-대비-개선사항\"><a href=\"#2-2-대표적인-방법들-대비-개선사항\" class=\"headerlink\" title=\"2.2 대표적인 방법들 대비 개선사항\"></a>2.2 대표적인 방법들 대비 개선사항</h2><ul>\n<li>Scalability<ul>\n<li>large models and parallel implementations</li>\n</ul>\n</li>\n<li>Data Efficiency</li>\n<li>Robustness<ul>\n<li>hyperparameter tuning없이 다양한 문제들에 적용되어 해결</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-3-제안하는-알고리즘\"><a href=\"#2-3-제안하는-알고리즘\" class=\"headerlink\" title=\"2.3 제안하는 알고리즘\"></a>2.3 제안하는 알고리즘</h2><p>이 논문에는 Clipped probability ratios를 포함하는 objective function 제안하였습니다.</p>\n<ul>\n<li>TRPO의 data efficiency와 robustness를 유지하면서도 first-order approximation만 사용합니다.</li>\n<li>Policy 성능에 대한 lower bound를 제공합니다.</li>\n</ul>\n<p>따라서 Policy로부터의 data sampling과 sampled data를 이용한 최적화를 번갈아가면서 수행합니다.</p>\n<p><br></p>\n<h2 id=\"2-4-실험-결과\"><a href=\"#2-4-실험-결과\" class=\"headerlink\" title=\"2.4 실험 결과\"></a>2.4 실험 결과</h2><p>다양한 버전의 surrogate objectives 중에는 clipped probability ratio가 가장 성능이 좋았습니다.</p>\n<ul>\n<li>Continuous control tasks에서 기존 알고리즘 대비 성능이 좋습니다.</li>\n<li>Atari에서는 A2C 대비 sampling efficiency 측면에서는 성능이 월등히 좋으며, ACER 대비 훨씬 간단하지만 성능은 비슷합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"3-Backgroud-Policy-Optimization\"><a href=\"#3-Backgroud-Policy-Optimization\" class=\"headerlink\" title=\"3. Backgroud: Policy Optimization\"></a>3. Backgroud: Policy Optimization</h1><p><br></p>\n<h2 id=\"3-1-Policy-Gradient-Methods\"><a href=\"#3-1-Policy-Gradient-Methods\" class=\"headerlink\" title=\"3.1 Policy Gradient Methods\"></a>3.1 Policy Gradient Methods</h2><p>일반적인 PG Method들은 실험적으로 destructive large policy updates가 발생합니다.</p>\n<ul>\n<li>더 자세히 말하자면, PG Method가 수행하는 parameter space에서의 gradual update가 policy space에서는 큰 변화를 유발할 수 있다는 의미입니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-2-Trust-Region-Methods\"><a href=\"#3-2-Trust-Region-Methods\" class=\"headerlink\" title=\"3.2 Trust Region Methods\"></a>3.2 Trust Region Methods</h2><p>Policy update 크기에 대한 contraint하에 objective function(“surrogate” function)을 최대화하는 것이 목표입니다. 수식은 아래와 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/gx6udoz5upswyf9/Screen%20Shot%202018-07-31%20at%2011.11.49%20PM.png?dl=1\" width=\"400\"> </center>\n\n<p>위의 수식은 contraint로 인해 excessive large policy update가 방지됩니다.</p>\n<p>TRPO에서는 constrained optimization problem을 풀기 위해서는 다음과 같은 방법들이 필요합니다.</p>\n<ol>\n<li>Fisher Information Matrix인 second-order derivative of KL divergence를 사용하거나,<ul>\n<li>여기서 second-order matrixes를 구하기 위해서는 많은 계산량 필요합니다.</li>\n</ul>\n</li>\n<li>Conjugate Gradient를 사용합니다.<ul>\n<li>Conjugate Gradient는 구현하기가 어렵습니다.</li>\n</ul>\n</li>\n</ol>\n<center> <img src=\"https://www.dropbox.com/s/6xpw9igndl3dmb9/Screen%20Shot%202018-07-31%20at%2011.15.13%20PM.png?dl=1\" width=\"500\"> </center>\n\n<p>원래 이론적으로 위의 수식과 같이 “contraint”가 아니라 objective에 “penalty”를 부여하는 형태입니다. 하지만 다양한 문제들(혹은 학습 중에 특성이 변하는 문제)에서 모두 잘 동작하는 single value $\\beta$를 찾는 것(robustness)이 어렵기 때문에, TRPO에서는 penalty대신 contraint를 취하는 방식을 택한 것입니다.</p>\n<p><br><br></p>\n<h1 id=\"4-Clipped-Surrogate-Objective\"><a href=\"#4-Clipped-Surrogate-Objective\" class=\"headerlink\" title=\"4. Clipped Surrogate Objective\"></a>4. Clipped Surrogate Objective</h1><p>이번 section에서는 TRPO의 surrogate obejctive function을 강제적으로 clipping하는 방법에 대하여 말합니다.</p>\n<p>먼저 기존의 TRPO의 surrogate function을 다음과 같이 표현합니다.<br>$$r_t(\\theta)=\\dfrac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta old}(a_t|s_t)}, \\, \\, r_t(\\theta old) = 1$$</p>\n<p>위의 수식을 이용하여 TRPO의 surrogate object를 최대화합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/e524vwsyolp6h9n/Screen%20Shot%202018-07-26%20at%2010.12.22%20AM.png?dl=1\" width=\"350\"> </center>\n\n<p>위의 수식을 그대로 사용한다면 excessively large policy update가 됩니다. 따라서 penalty를 이용하여 필요 이상의 policy update를 방지합니다. TRPO에서는 KL-Divergence를 이용하여 penalty를 적용하지만 PPO에서는 computation적으로 효율적인 penalty를 적용하고 excessively large policy update를 방지하기 위해 아래와 같은 clipping 기법을 사용합니다.</p>\n<p>$$L^{CLIP}(\\theta) = \\hat{E}_t [min(r_t(\\theta) \\, \\hat{A}_t,  clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\, \\hat{A}_t)]$$</p>\n<ul>\n<li>위의 수식에 대한 추가적인 설명<ul>\n<li>$\\epsilon$은 hyperparameter로서 continuous control에서는 0.2일 때 성능이 가장 좋았으며, Atari Game에서는 0.1 x $\\alpha$ 값을 사용합니다. (여기서 $\\alpha$ 는 학습률로 1 에서 시작하여 학습이 진행됨에 따라 0 으로 감소합니다.)</li>\n<li>Clipped 와 unclipped objective 중 min 값을 택함으로써 $L^{CLIP} (\\theta)$는 unclipped objective에 대한 lowerbound가 됩니다.</li>\n</ul>\n</li>\n</ul>\n<p>이어서 아래의 그림을 보겠습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/3wcuf2tq7q24fy6/Screen%20Shot%202018-07-26%20at%2010.25.08%20AM.png?dl=1\" width=\"600\"> </center>\n\n<p>위의 그림을 보면 두 가지의 그래프(Advantage Function $\\hat{A}_t$의 부호에 따라)로 clip에 대해서 설명하고 있습니다.</p>\n<ul>\n<li>Advantage Function $\\hat{A}_t$가 양수일 때<ul>\n<li>Advantage가 현재보다 높다라는 뜻이며 파라미터를 +의 방향으로 업데이트 하여야 합니다. 다시 말해 어떠한 상태 $s$에서 행동 $a$가 평균보다 좋다는 의미입니다. 따라서 이를 취할 확률이 증가하게 되고, $r_t (\\theta)$를 clip하여 $\\epsilon$보다 커지지 않도록 유도하는 것입니다.</li>\n<li>추가적으로, TRPO에서 다뤘던 constraint가 아니라 단순히 clip하는 것이기 때문에 실제로 $\\pi_\\theta (a_t|s_t)$의 증가량이 $\\epsilon$보다 더 커질 수도 있습니다. 하지만 증가량이 더 커졌다고 하더라도 objective function을 업데이트 할 때 효과적이지 않을 수 있기 때문에 대부분 clip을 통해서 $\\epsilon$ 이하로 유지합니다.</li>\n<li>또한 만약 $r_t (\\theta)$가 objective function의 값을 감소시키는 방향으로 움직이는 경우더라도 $1-\\epsilon$보다 작아져도 됩니다. 여기서의 목적은 최대한 lowerbound를 구하는 것이 목적이기 때문입니다. 그러니까 쉽게 말해서 왼쪽 그림에서도 볼 수 있듯이 $1+\\epsilon$만 구하는 데에 포커스를 맞추고 있고, $1-\\epsilon$은 신경쓰지 않고 있는 것입니다. (이 부분은 Advantage Function $\\hat{A}_t$가 음수일 때도 동일합니다.)</li>\n</ul>\n</li>\n<li>Advantage Function $\\hat{A}_t$가 음수일 때 <ul>\n<li>Advantage가 현재보다 좋지 않다라는 뜻이며 그의 반대 방향으로 업데이트 하여야 합니다. 다시 말해 어떠한 상태 $s$에서 행동 $a$가 평균보다 좋지 않다는 의미입니다. 따라서 이를 취할 확률이 감소하게 되고, $r_t (\\theta)$를 clip하여 $\\epsilon$보다 작아지지 않도록 유도하는 것입니다.  </li>\n<li>또한 $r_t(\\theta)$은 확률을 뜻하는 두 개의 함수를 분자 분모로 가지고 있으며, 분수로 구성되어 있기 때문에 무조건 양수로 이루어져 있습니다. Advantage function인 $\\hat{A}_t$와 곱해져 Objective function인 $L^{CLIP}$은 Advantage function과 방향이 같아집니다.</li>\n</ul>\n</li>\n</ul>\n<p>위의 설명으로 인하여 나오는 그림이 아래의 그림입니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/f7i97geiligfej9/Screen%20Shot%202018-07-26%20at%2011.02.17%20AM.png?dl=1\" width=\"700\"> </center>\n\n<p>위의 그림에서 빨간색 그래프를 보면,</p>\n<ul>\n<li>$L^{CLIP}$은 min 값들의 평균이기 때문에 평균들의 min 값보다는 더 작아집니다. 즉, 주황색 그래프와 초록색 그래프 중 작은값보다 더 작아지는 것을 볼 수 있습니다.</li>\n<li>$L^{CLIP}$을 최대화하는 $\\theta$가 다음 $\\pi_\\theta$가 됩니다. 다시 말해 PPO는 기존 PG 방법들처럼 parameter space에서 parameter $\\theta$를 점진적으로 업데이트하는 것이 아니라, 매번 policy를 maximize하는 방향으로 업데이트하는 것으로 볼 수 있는 것입니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"5-Adaptive-KL-Penalty-Coefficient\"><a href=\"#5-Adaptive-KL-Penalty-Coefficient\" class=\"headerlink\" title=\"5. Adaptive KL Penalty Coefficient\"></a>5. Adaptive KL Penalty Coefficient</h1><p>이전까지 설명했던 Clipped Surrogate Objective 방법과 달리 기존의 TRPO에서 Adaptive한 파라미터를 적용한 또 다른 방법에 대해서 알아보겠습니다. 사실 이 방법은 앞서본 clip을 사용한 방법보다는 성능이 좋지 않습니다. 하지만 baseline으로써 알아볼 필요가 있습니다.</p>\n<p>clip에서 다뤘던 Probability ratio $r_t(\\theta)$ 대신 KL divergence를 이용하여 penalty를 줍니다. 그 결과 각각의 policy update에 대해 KL divergence $d_{targ}$의 target value를 얻습니다.</p>\n<ul>\n<li>clipped surrogate objective 대신하여 or 추가적으로 사용할 수 있다고 합니다. </li>\n<li>자체 실험 결과에서는 clipped surrogate objective 보다는 성능이 안 좋았다고 합니다.</li>\n<li>둘 다 사용한 실험 결과는 없습니다.</li>\n</ul>\n<p>이 알고리즘에서 각각의 policy update에 적용하는 step은 다음과 같습니다. </p>\n<ol>\n<li>KL-penalized objective 최적화를 합니다. 기존의 TRPO의 Objective function에 $\\beta$를 적용하여 다음과 같이 $\\beta$를 Adaptive하게 조절하고 있습니다. 수식은 아래와 같습니다.</li>\n</ol>\n<center> <img src=\"https://www.dropbox.com/s/ojq7kfobf0f8x9n/Screen%20Shot%202018-07-26%20at%2011.22.44%20AM.png?dl=1\" width=\"500\"> </center>\n\n<ol start=\"2\">\n<li><img src=\"https://www.dropbox.com/s/j9phkalrbgf45k0/Screen%20Shot%202018-07-26%20at%2011.34.25%20AM.png?dl=1\" width=\"230\">을 계산합니다.<ul>\n<li>만약 $d &lt; d_{targ} \\, / \\, 1.5$라면, 그 때 $\\beta \\leftarrow \\beta \\, / 2$</li>\n<li>만약 $d &gt; d_{targ} \\, x \\, 1.5$라면, 그 때 $\\beta \\leftarrow \\beta \\, x 2$</li>\n<li>즉, KL-divergence 값이 일정 이상 커지게 되면 objective function에 penalty를 더 크게 부과합니다.</li>\n<li>갱신된 $\\beta$ 값은 다음 policy update 때 사용합니다. </li>\n</ul>\n</li>\n</ol>\n<p>$\\beta$를 조절하는 방법은 만약 $\\theta old$와 $\\theta$간의 파라미터 차이가 크다면 penalty를 강하게 부여하여 차이를 작게하고, 파라미터 차이가 작다면 penalty를 완화시켜 주어 차이를 더 크게하는 것입니다.</p>\n<p>정리하자면, KL-divergence를 constraint로 둔 것이 아니기 때문에, 간혹 excessive large policy update가 발생할 수도 있지만, KL Penaly coefficienct인 $\\beta$가 KL-divergence에 따라 adpative 하게 조정됨으로써 excessive large policy update가 지속적으로 발생되는 것을 방지하는 것입니다.</p>\n<p><br><br></p>\n<h1 id=\"6-Algorithm\"><a href=\"#6-Algorithm\" class=\"headerlink\" title=\"6. Algorithm\"></a>6. Algorithm</h1><p>앞서 다뤘던 section들은 어떻게 policy만을 업데이트 하는지에 대해 설명하고 있습니다. 이번 section에서는 value, entropy-exploration과 합쳐서 어떻게 통합적으로 업데이트 하는지에 대해서 알아보겠습니다.</p>\n<p>먼저 Variance-reduced advantage function estimator을 계산하기 위해 learned state-value fucntion $V(s)$을 사용합니다. 여기에는 두 가지 방법이 있습니다.</p>\n<ul>\n<li>Generalized advantage estimation</li>\n<li>Finite horizontal estimators</li>\n</ul>\n<p>만약 policy와 value function 간 parameter sharing하는 neural network architecture를 사용한다면, policy surrogate와 value function error term을 combine한 loss function을 사용해야 합니다. 또한 이 loss function에 entropy bonus term을 추가하여, 충분한 exploration이 될 수 있도록 합니다.(exploration하는 부분은 <a href=\"https://arxiv.org/pdf/1602.01783.pdf\" target=\"_blank\" rel=\"noopener\">A3C 논문</a>에 나와있습니다.)</p>\n<p>그래서 앞써 다뤘던 objective function을 $L^{CLIP}$라고 표현한다면 PPO에서 제시하는 통합적으로 최대화해야하는 objective function은 다음과 같이 표현할 수 있습니다.</p>\n<p>$$L_t^{CLIP+VF+S} (\\theta) = \\hat{E}_t [L^{CLIP} (\\theta) - c_1 L_t^{VF} (\\theta) + c_2 S[\\pi_\\theta] (s_t)]$$</p>\n<p>위의 수식에 대한 추가적인 설명은 다음과 같습니다.</p>\n<ul>\n<li>$c_1, c_2$ : coefficients</li>\n<li>$S$ : entropy bonus</li>\n<li>$L_t^{VF}$ : squared-error loss $(V_\\theta (s_t) - V_t^{targ})^2$</li>\n</ul>\n<p>위의 objective function을 최대화하면 Reinforcement Learning을 통한 policy 학습, state-value function의 학습, exploration을 할 수 있습니다.</p>\n<p>추가적으로 A3C와 같은 요즘 인기있는 PG의 style에서는 T time steps(T는 episode length 보다 훨씬 작은 크기) 동안 policy에 따라서 sample들을 얻고, 이 sample들을 업데이트에 사용합니다. 이러한 방식은 time step T 까지만 고려하는 advantage estimator가 필요하며, A3C에서 다음과 같이 사용합니다.</p>\n<p>$$\\hat{A}_t = - V(s_t) + r_t + \\gamma r_{t+1} + \\cdots + \\gamma^{T-t+1} r_{T-1} + \\gamma^{T-t}V(s_T)$$</p>\n<ul>\n<li>여기서 $t$는 주어진 length T trajectory segment 내에서 $[0, T]$에 있는 time index입니다.</li>\n</ul>\n<p>위의 수식에 더하여 generalized version인 GAE의 truncated version(generalized advantage estimation)을 사용합니다.($\\lambda$가 1이면 위 식과 같아집니다.) 수식은 아래와 같습니다. </p>\n<p>$$\\hat{A}_t = \\delta_t + (\\gamma\\lambda)\\delta_{t+1} + \\cdots + (\\gamma\\lambda)^{T-t+1}\\delta_{T-1},$$$$where \\,\\,\\,\\, \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$</p>\n<p>그래서 고정된 length T trajectory segment를 사용한 PPO algorithm은 다음과 같은 pseudo code로 나타낼 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/q00y8x7eryl1ncd/Screen%20Shot%202018-07-26%20at%202.28.03%20PM.png?dl=1\" width=\"800\"> </center>\n\n<ul>\n<li>여기서 actor는 A3C처럼 병렬로 두어도 상관없습니다.</li>\n<li>advantage estimate 계산을 통해서 surrogate loss를 계산합니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"7-Experimnet\"><a href=\"#7-Experimnet\" class=\"headerlink\" title=\"7. Experimnet\"></a>7. Experimnet</h1><p><br></p>\n<h2 id=\"7-1-Surrogate-Objectives의-비교\"><a href=\"#7-1-Surrogate-Objectives의-비교\" class=\"headerlink\" title=\"7.1 Surrogate Objectives의 비교\"></a>7.1 Surrogate Objectives의 비교</h2><p>이번 section에서는 3가지 Surrogate Objectives를 비교분석 하고 있습니다.</p>\n<ul>\n<li>No clipping or penalty: $L_t(\\theta) = r_t(\\theta)\\hat{A}_t$</li>\n<li>Clipping: $L_t(\\theta) = min(r_t(\\theta) \\, \\hat{A}_t, \\, clip(r_t(\\theta), 1-\\epsilon,1+\\epsilon) \\, \\hat{A}_t)$</li>\n<li>KL penalty(fixed or adaptive): <img src=\"https://www.dropbox.com/s/ksnhlxz2riuns1p/Screen%20Shot%202018-07-26%20at%202.42.50%20PM.png?dl=1\" width=\"270\"></li>\n</ul>\n<p>그리고 환경은 MuJoCo를 사용하며 OpenAI Gym 환경에서 실험을 진행하였습니다. 그리고 네트워크들은 2개의 hidden layer를 가지며 각각 64개의 unit들을 가지고 있습니다. 그리고 tanh를 활성화 함수로써 사용하고 있으며 네트워크는 Gaussian distribution을 출력하는 것으로 구성되어 있습니다. 그리고 Policy Network와 Value Network는 파라미터들을 공유하고 있지 않으며, 위에서 설명한 Exploration을 위한 entropy는 사용하지 않습니다.</p>\n<p>7개의 환경에서 실험하였으며 각 환경들은 HalfCheetah, Hopper, InvertedDoublePendulum, InvertedPerdulum, Reacher, Swimmer, Walker2d를 사용하였으며 모든 환경들은 v1 버전을 사용하였습니다.</p>\n<p>아래의 표는 실험 결과입니다. 세 가지 Surrogate Objective에 대해 각 파라미터들을 표에 표기된 방식대로 변화하며 실험하였습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/4xbkxh4m82mu1vo/Screen%20Shot%202018-07-26%20at%202.45.42%20PM.png?dl=1\" width=\"450\"> </center>\n\n<p><br></p>\n<h2 id=\"7-2-Comparison-to-other-Algorithms-in-the-Continuous-Domain\"><a href=\"#7-2-Comparison-to-other-Algorithms-in-the-Continuous-Domain\" class=\"headerlink\" title=\"7.2 Comparison to other Algorithms in the Continuous Domain\"></a>7.2 Comparison to other Algorithms in the Continuous Domain</h2><p>이번 section에서는 Clipping 버전의 PPO와 다른 알고리즘들 간의 비교를 연속적인 Action을 가지는 환경에서 실험을 한 결과를 보여주고 있습니다. 다른 알고리즘은 TRPO, cross-entropy method, vanilla policy gradient with adaptive stepsize, A2C, A2C with trust region를 사용하였습니다. 환경과 사용한 네트워크의 구성은 이전 section과 동일합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/tf7djngioxnxtef/Screen%20Shot%202018-07-26%20at%202.50.23%20PM.png?dl=1\" width=\"900\"> </center>\n\n<p>여타 다른 알고리즘보다 좋은 성능을 보이는 것을 알 수 있습니다.</p>\n<p><br></p>\n<h2 id=\"7-3-Showcase-in-the-Continuous-Domain-Humanoid-Running-and-Steering\"><a href=\"#7-3-Showcase-in-the-Continuous-Domain-Humanoid-Running-and-Steering\" class=\"headerlink\" title=\"7.3 Showcase in the Continuous Domain: Humanoid Running and Steering\"></a>7.3 Showcase in the Continuous Domain: Humanoid Running and Steering</h2><p>이번 section에서는 Humanoid-v0, HumanoidFlagrun-v0, HumanoidFlagrunHarder-v0의 환경에서 Roboschool을 사용하여 실험하였습니다.</p>\n<ul>\n<li>Humanoid-v0는 단순히 앞으로 걸어나가는 환경</li>\n<li>HumanoidFlagrun-v0은 200스텝마다 혹은 목적지에 도착할 때 마다 위치가 바뀌는 목적지에 걸어서 도달하는 환경입니다. </li>\n<li>HumanoidFlagrunHarder-v0은 환경이 초기화될 때 마다 특정 경계안에 Humanoid가 위치하게 되며 특정 영역 밖으로 걸어나가는 것을 수행하는 환경입니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/y112ua7il6l0296/Screen%20Shot%202018-07-26%20at%202.53.01%20PM.png?dl=1\" width=\"900\"> </center>\n\n<ul>\n<li>Roboschool을 사용하여 3D humanoid control task에서 PPO 알고리즘으로 학습시킨 Learning curve입니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/09scyk6zeviyswt/Screen%20Shot%202018-07-26%20at%202.53.10%20PM.png?dl=1\" width=\"900\"> </center>\n\n<ul>\n<li>Roboschool Humanoid Flagrun에서 학습된 policy의 frame들입니다. </li>\n</ul>\n<p><br></p>\n<h2 id=\"7-4-Comparison-to-Other-Algorithms-on-the-Atari-Domain\"><a href=\"#7-4-Comparison-to-Other-Algorithms-on-the-Atari-Domain\" class=\"headerlink\" title=\"7.4 Comparison to Other Algorithms on the Atari Domain\"></a>7.4 Comparison to Other Algorithms on the Atari Domain</h2><p>이번 section에서는 Atari Domain에서 PPO, A2C, ACER(Actor Critic with Experience Replay)의 3가지 알고리즘을 비교합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/jxuqdncxpnoyqgi/Screen%20Shot%202018-07-26%20at%203.00.00%20PM.png?dl=1\" width=\"600\"> </center>\n\n<p>전체 training에 대해서는 PPO가 ACER보다 좋은 성능을 내고 있습니다. 하지만 마지막 100 에피소드만을 비교했을 때는 PPO보다 ACER이 더 좋은 성능을 보이고 있습니다. 이는 PPO가 더 빨리 최종 능력에 도달하지만, ACER이 가진 potential이 더 높다는 것입니다.</p>\n<p><br><br></p>\n<h1 id=\"8-Conclusion\"><a href=\"#8-Conclusion\" class=\"headerlink\" title=\"8. Conclusion\"></a>8. Conclusion</h1><p>이 논문에서는 policy update를 하기 위한 방법으로 stochastic gradient ascent의 multiple epchs를 사용하는 policy optimization method들의 하나의 알고리즘은 Proximal Policy Optimization(PPO)를 소개합니다.</p>\n<p>이 알고리즘은 trust region method의 stability와 reliability를 가집니다. 여기에 더하여 학습하기에 훨씬 더 간단하고, 더 일반적인 setting으로 적용하기에 편한 A3C로서 code를 구성하기에도 편하고, 계산량도 훨씬 덜합니다. 그리고 전반적으로 더 좋은 성능을 가집니다.</p>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"GAE-여행하기\"><a href=\"#GAE-여행하기\" class=\"headerlink\" title=\"GAE 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/\">GAE 여행하기</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"PPO-Code\"><a href=\"#PPO-Code\" class=\"headerlink\" title=\"PPO Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/ppo_gae.py\" target=\"_blank\" rel=\"noopener\">PPO Code</a></h2>"},{"title":"Implicit Quantile Networks for Distributional Reinforcement Learning","date":"2018-10-30T11:38:40.000Z","author":"민규식","subtitle":"Distributional RL 3번째 논문","_content":"\n<center> <img src=\"https://www.dropbox.com/s/mmxtgylh0ntatp4/IQN_paper.png?dl=1\" width=\"800\"> </center>\n\n논문 저자 : [Will Dabney](https://arxiv.org/search/cs?searchtype=author&query=Dabney%2C+W), [Georg Ostrovski](https://arxiv.org/search/cs?searchtype=author&query=Ostrovski%2C+G), [David Silver](https://arxiv.org/search/cs?searchtype=author&query=Silver%2C+D), [Rémi Munos](https://arxiv.org/search/cs?searchtype=author&query=Munos%2C+R)    \n논문 링크 : [ArXiv](https://arxiv.org/abs/1806.06923)\nProceeding : The 36th International Conference on Machine Learning (*ICML 2018*)        \n정리 : 민규식\n\n\n\n## Introduction\n\n본 게시물은 2018년 6월에 발표된 논문 [Implicit Quantile Networks for Distributional Reinforcement Learning](https://arxiv.org/abs/1806.06923) 의 내용에 대해 설명합니다.\n\n<p align= \"center\">\n\n<img src=\"https://www.dropbox.com/s/mmxtgylh0ntatp4/IQN_paper.png?dl=1\" alt=\"paper\" style=\"width: 800px;\"/>\n\n </p>\n\n<br>\n\n## Algorithm \n\nIQN의 경우 QR-DQN과 비교했을 때 크게 다음의 2가지 정도에서 차이를 보입니다. \n\n- 동일한 확률로 나눈 Quantile을 이용하는 대신 확률들을  random sampling하고 해당하는 support를 도출\n- 네트워크 구조\n\n\n\n위의 내용들에 대해 하나하나 살펴보도록 하겠습니다. \n\n<br>\n\n### 1. Sampling\n\n\n\n#### QR-DQN vs IQN\n\nQR-DQN 논문에서는 quantile regression 기법을 이용하여 Wasserstein distance를 줄이는 방향으로 분포를 학습하였고 이에 따라 distributional RL의 수렴성을 증명하였습니다. 이에 따라 IQN 논문에서도 quantile regression 기법을 그대로 이용합니다. 심지어 QR-DQN 논문에서 사용한 Quantile huber loss도 그대로 사용합니다. Target network, experience replay, epsilon-greedy도 QR-DQN과 동일하게 사용합니다. 하지만 IQN에서는 `Cumulative Distribution Function`을 동일한 확률로 나누는 대신 random sampling을 통해 취득한 tau에 해당하는 support를 도출합니다. \n\n예를 들어보겠습니다. Quantile의 수를 4라고 해보겠습니다. 이 경우 QR-DQN의 quantile값은 [0.25, 0.5, 0.75, 1]이지만 QR-DQN은 Wasserstein distance를 최소로 하기 위해 quantile의 중앙값에 해당하는 support를 도출합니다. 즉 [0.125, 0.375, 0.625, 0.875]에 해당하는 support들을 추정합니다. \n\n하지만 IQN 논문에서는 quantile값 tau를 0~1 사이에서 임의로 sampling합니다. Quantile이 4개인 경우 랜덤하게 추출한 0~1사이의 4개의 값이 [0.12, 0.32, 0.78, 0.92] 라고 해보겠습니다. \n\n 위의 예시를 그림으로 표현한 것이 아래와 같습니다. \n\n<p align=\"center\">\n<img src=\"https://www.dropbox.com/s/pffm77vus3k4uex/qr_dqn_iqn.png?dl=1\" alt=\"QRDQN vs IQN\" width=\"800\"/>\n\n</p>\n\n해당 내용에 대해서는 논문에서 나타낸 그림이 굉장히 잘 표현하고 있습니다. 이 그림의 경우 DQN, C51, QR-DQN, IQN의 경우를 아래와 같이 모두 비교하고 있습니다. \n\n\n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/unsy98e3xmz4a5n/compare_paper.png?dl=1\" alt=\"Comparison from paper\" width=\"800\"/>\n\n</p>\n\n다들 network 구조에도 차이가 존재하지만 일단은 output만을 비교해보도록 하겠습니다. \n\n- DQN: 각 action에 대한 value\n- C51: 각 action에 대한 value distribution 중 확률 (support는 고정값으로 사용)\n- QR-DQN: 각 action에 대한 value distribution 중 support (확률은 고정값으로 사용)\n- IQN: 각 action에 대한 value distribution 중 support (확률은 random sampling)\n\n\n\n그럼 이렇게 sampling을 한 tau를 통해 support를 추정하는 것은 어떤 좋은 점이 있을까요? 논문에 따르면 `Risk-Sensitive`하게 Policy를 선택할 수 있습니다. 해당 내용에 대해 한번 살펴보도록 하겠습니다.  \n\n\n\n#### Risk-Sensitive Reinforcement Learning\n\n일단은 여기서 말하는 `Risk`가 무엇을 의미하는지 먼저 알아보도록 하겠습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/vvlartdbmh51n69/risk_sensitive1.png?dl=1\" alt=\"Risk sensitive1\" width=\"600\"/>\n\n</p>\n\n위의 그림을 보시면 2개의 action에 대한 value distribution이 각각 존재합니다. Action 1 (a1)에 대한 distribution은 분산이 작기 때문에 평균값에 가까운 return을 받을 확률이 높습니다. 하지만 value distribution의 기대값이 action2 (a2)보다 작습니다. Distributional RL에서는 distribution의 기대값을 비교하여 action을 선택하기 때문에 이런 경우 a2가 선택될 것입니다. \n\na2의 경우 분산이 매우 큰 distribution입니다. 이런 분포에서는 경우에 따라 매우 작은 return이 도출될 수도 있고, 매우 높은 return이 도출될수도 있습니다. Distributional RL에서는 이렇게 분산이 커서 결과에 대한 확신이 낮은 경우 **\"Risk가 크다\"**고 합니다. 반대로 결과에 대한 확신이 상대적으로 높은 a1의 경우 a2에 비해 **\"Risk가 작다\"**라고 할 수 있는 것이죠. \n\nSampling을 통해 학습을 수행하고 action을 선택하는 경우 이 risk에 따라 action을 선택하는 것이 가능합니다. 이런 risk sensitive policy에는 다음의 2가지가 있습니다. \n\n- Risk-averse policy\n- Risk-seeking policy \n\n<br>\n\n[A Comprehensive Survey on Safe Reinforcement Learning](http://www.jmlr.org/papers/volume16/garcia15a/garcia15a.pdf) 논문을 보시면 다음과 같은 내용이 나옵니다.  \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/xivtpfaufuaf732/risk_sensitive_criterion.png?dl=1\" alt=\"Risk sensitive criterion\" width=\"800\"/>\n\n</p>\n\n위의 definition을 보시면 scalar parameter beta를 통해 risk의 level을 결정합니다. 이 beta를 risk sensitivity parameter라고 합니다. 이 beta가 0보다 큰 경우 risk-averse, 0보다 작은 경우 risk-seeking, 그리고 0인 경우 risk-neutral policy가 됩니다. \n\n어떻게 beta를 통해 risk의 level을 결정할 수 있는지, sampling을 이용하면 어떻게 risk-sensitive policy를 결정할 수 있는지 한번 알아보겠습니다.  \n\n우선 risk-sensitive policy에 따라 action을 선택하는 식은 다음과 같습니다.  \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/cw6uk13z25bzhgu/risk_sensitive_equation.png?dl=1\" alt=\"Risk sensitive criterion\" width=\"600\"/>\n\n</p>\n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/jtvk920xe5ubqs7/risk_averse.png?dl=1\" alt=\"Risk averse\" width=\"800\"/>\n\n</p>\n\n위의 경우는 RIsk-averse 입니다. 기존의 경우 a2의 distribution에 대한 평균값이 a1의 distribution에 대한 평균값보다 크기 때문에 a2를 최종적인 action으로 선택했을 것입니다. 하지만 a1보다 a2의 분산이 훨씬 크기 때문에 beta가 양수인 경우 (distribution의 평균) - beta * (distribution의 분산)의 계산을 수행하면 연산의 결과값은 a2보다 a1이 더 큽니다. 이에 따라 risk가 더 낮은 a1을 선택하게 되는 것이고 위와 같은 과정을 통해 risk-averse policy에 따라 action을 선택하게 됩니다.   \n\nCDF의 낮은 영역에서만 tau를 sampling하는 경우 위와 유사한 결과를 확인할 수 있습니다. Tau를 낮은 범위 내에서 sampling하는 경우 낮은 value들에 대한 tau들이 sampling되지만 a2의 경우 a1보다 분산이 훨씬 크기 때문에 sampling 된 결과들의 기대값은 a1이 a2보다 크게 됩니다. \n\n\n\nRisk-seeking의 경우 위의 risk-averse와 반대의 상황을 확인할 수 있습니다. \n\n \n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/19q9zx7o8b4npj7/risk_seeking.png?dl=1\" alt=\"Risk seeking\" width=\"800\"/>\n\n</p>\n\n위의 경우는 RIsk-seeking 입니다. 기존의 경우 a1의 distribution에 대한 평균값이 a2의 distribution에 대한 평균값보다 크기 때문에 a1를 최종적인 action으로 선택했을 것입니다. 하지만 a1보다 a2의 분산이 훨씬 크기 때문에 beta가 음수인 경우 (distribution의 평균) - beta * (distribution의 분산)의 계산을 수행하면 연산의 결과값은 a1보다 a2가 더 큽니다. 이에 따라 risk가 더 큰 a2를 선택하게 되는 것이고 위와 같은 과정을 통해 risk-seeking policy에 따라 action을 선택하게 됩니다.   \n\nCDF의 높은 영역에서만 tau를 sampling하는 경우 위와 유사한 결과를 확인할 수 있습니다. Tau를 높은 범위 내에서 sampling하는 경우 높은 value들에 대한 tau들이 sampling되지만 a2의 경우 a1보다 분산이 훨씬 크기 때문에 sampling 된 결과들의 기대값은 a2가 a1보다 크게 됩니다. \n\n<br>\n\n논문에서는 다음과 같은 4가지 기법들을 이용해 tau를 위한 sampling distribution을 변경하고 다양한 risk-sensitive policy를 선택하게 됩니다. \n\n- Cumulative Probability Weighting (CPW)\n- Wang\n- Power formula (POW)\n- Conditional Value-at-Risk (CVaR)\n\n\n\n위의 예시 중에 가장 간단한 CVaR에 대해서 살펴보도록 하겠습니다. CVaR의 수식은 다음과 같습니다. \n\n <img src=\"https://www.dropbox.com/s/3bvwmom4ky6pmhp/CVaR.png?dl=1\" alt=\"CVar\" width=\"300\"/>\n\n원래 tau는 [0,1] 중에서 uniform한 확률로 선택합니다. 하지만 위의 경우 eta를 예를 들어 0.25로 하면 0.25tau가 됩니다. 즉 [0, 0.25] 중에서 uniform한 확률로 tau를 sampling하게 됩니다. 결과적으로 risk-averse policy에 따라 action을 선택하게 됩니다.  \n\n<br>\n\n### 2. Network\n\n여기서는 IQN 네트워크의 구조에 대해 살펴보도록 하겠습니다. 일단 위에서 봤던 그림을 다시 한번 살펴보도록 하겠습니다.\n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/unsy98e3xmz4a5n/compare_paper.png?dl=1\" alt=\"Comparison from paper\" width=\"800\"/>\n\n</p>\n\n위의 그림에서 DQN과 IQN만 비교해보도록 하겠습니다. \n\n특정 action a에 대해 DQN은 다음과 같은 함수들로 나타낼 수 있습니다. \n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/br22t2a08dgz03a/DQN_equation.png?dl=1\" alt=\"DQN\" width=\"300\"/>\n\n</p>\n\n위의 식에서 **psi**는 convolution layers에 의한 연산을, **f**는 fully-connected layers에 의한 연산을 나타냅니다. \n\nIQN도 DQN과 동일한 function인 **psi**와 **f**를 이용합니다. 그 대신 거기에 tau를 embedding 해주는 함수 **phi**를 추가적으로 사용합니다. 이에 따라 특정 action a에 대해 IQN을 함수로 나타낸 것이 다음과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/h6z8ppz9z9tkr8y/IQN_equation.png?dl=1\" alt=\"DQN\" width=\"500\"/>\n\n</p>\n\n위의 식을 살펴보자면 convolution function (psi)를 통해 얻은 결과와 tau에 대한 embedding function (phi)를 통해 얻은 결과를 element-wise하게 곱해줍니다. 그리고 그 결과에 fully-connected layer 연산을 수행하여 최종적으로 action a에 대한 value distribution을 얻습니다.  \n\n즉 위의 그림에서도 볼 수 있듯이 tau를 embedding하는 function인 phi를 제외하고는 모두 DQN과 같다고 할 수 있습니다. 이에 따라 embedding function (phi)에 대해서 한번 살펴보도록 하겠습니다. 이 함수의 역할은 하나의 sampling된 tau를 벡터로 embedding 해주는 것입니다. 본 논문에서 embedding에 대한 function은 다음과 같은 **n cosine basis function** 입니다. \n\n\n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/uxnaqayrxoazj93/embedding.png?dl=1\" alt=\"Embedding\" width=\"600\"/>\n\n</p>\n\n위의 식에서 n은 embedding dimension이며 값은 64로 설정하였습니다. tau는 sampling된 값이며 w와 b는 linear layer 연산을 위한 weight와 bias입니다. 위 수식만을 봐서는 embedding을 어떻게 수행할지 감이 오지 않을 수 있습니다! 하나의  quantile에 대해 어떻게 sampling을 하는지 그림과 함께 예를 들어보도록 하겠습니다. \n\n\n\n1. 우선 0~1 사이의 서로 다른 값을 [batch size x 1]의 사이즈로  random sampling 합니다. 이 값들은 각각의 tau값을 나타냅니다. \n\n\n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/y5gzs2vdpn6qlp3/embedding_progress1.png?dl=1\" alt=\"Embedding progress 1\" width=\"500\"/>\n\n</p>\n\n2. [Batch size x 1]로 sampling한 tau들을 복제하여 embedding dimension 만큼 쌓아줍니다. 본 논문의 경우 embedding dimension은 64입니다. 위 과정에 대한 결과의 사이즈는 [batch size x embedding dim]이 됩니다. \n\n   <p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/n86yp26tlcovjch/embedding_progress2.png?dl=1\" width=\"600\"/>\n\n</p>\n\n\n\n3. 모든 [batch size x embedding dim]의 모든 row에 0 ~ (n-1) 을 1씩 증가시킨 값을 곱해줍니다. 그리고 모든 값에 pi를 곱해줍니다. 이렇게 계산한 결과의 dimension은 여전히 [batch size x embedding dim] 입니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/i1qs8zf162eniuu/embedding_progress3.png?dl=1\" alt=\"Embedding\" width=\"1000\"/>\n\n</p>\n\n4. 다음으로는 3의 결과에 cos 연산을 해주고 weight를 곱하고 bias를 더해줍니다. 그리고 해당 결과에 최종적으로 ReLU 함수를 적용해줍니다. 여기서 weight의 사이즈는 [embedding dim x convolution 결과의 크기] 이며 bias의 사이즈는 [convolution 결과의 크기] 입니다. 결국 embedding 연산의 최종 결과는 [batch size x convolution 결과의 크기]가 됩니다. Convolution 연산의 최종 결과 또한 크기가 [batch size x convolution 결과의 크기] 입니다. 이에 따라 둘은 크기가 같아지게 되고 이에 따라 element-wise 하게 곱할 수 있게 됩니다. \n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/bnbxq3aj6c5kmrt/embedding_progress4.png?dl=1\" alt=\"Embedding\" width=\"800\"/>\n\n</p>\n\n여기까지가 Embedding function의 내용입니다! \n\n여기서 약간의 의문이 들 수 있을 것이라 생각합니다. 왜 **n cosine basis function**을 이용할까요? 논문에서는 다양한 함수에 대해서 실험을 수행하였고 그 결과 **n cosine basis function**이 가장 좋은 결과를 보였다고 합니다. 여러 함수에 대해 테스트한 결과는 다음과 같습니다. \n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/09idlykr8xr6fo9/embedding_compare.png?dl=1\" alt=\"Embedding\" width=\"800\"/>\n\n</p>\n\n<br>\n\n### 3. Quantile Huber Loss \n\n사실 본 논문에서는 QR-DQN에서 사용했던 **Quantile Huber Loss**를 그대로 이용합니다. 하지만 본 논문에서는 식을 다음과 같이 표시합니다. \n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/ie5l2z5cwagoj86/loss.png?dl=1\" alt=\"Embedding\" width=\"500\"/>\n\n</p>\n\n본 논문에서 N은 추정을 위해 일반 network 연산에서 sampling한 tau의 수 입니다. N'은 target distribution 도출을 위해 target network 연산에서 sampling한 tau의 수 입니다. 위 식에 대해서는 QR-DQN에서 자세히 설명하였으므로 추가적인 설명은 하지 않도록 하겠습니다. 다만 본 논문에서는 N과 N'를 다양하게 바꿔가면서 테스트한 결과를 다음과 같이 보여줍니다. \n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/t2eeof94yhq57gb/various_n.png?dl=1\" alt=\"Various N\" width=\"700\"/>\n\n</p>\n\n위의 결과는 6개의 atari game에 대한 human-normalized agent performance의 평균을 나타냅니다. 왼쪽의 경우 초반 10M frame을 학습하였을 때 결과이며 오른쪽의 경우 마지막 10M frame의 학습 결과입니다. 다른 알고리즘의 경우 각각 왼쪽과 오른쪽에 대한 결과가 다음과 같습니다. \n\n- DQN: (32, 253)\n- QR-DQN: (144, 1243)\n\n<br>\n\n### 4. Action 선택\n\n 본 논문에서  action을 선택하는 식이 QR-DQN과 비교했을 때 약간의 차이가 있습니다. 해당 식은 다음과 같습니다. \n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/76l4gi57vasj300/action_equation.png?dl=1\" alt=\"Action equation\" width=\"500\"/>\n\n</p>\n\n위 식은 단순히 sampling된 quantile을 통해 구한 모든 support의 값을 평균한 것입니다. \n\n<br>\n\n 본 논문의 알고리즘은 다음과 같습니다. \n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/fbxxlz25l6dunxi/algorithm.png?dl=1\" alt=\"algorithm (IQN)\" width=\"700\"/>\n\n</p>\n\n<br>\n\n## Result\n\n본 알고리즘의 성능은 Atari-57 benchmark를 통해 검증되었습니다. \n\n본 알고리즘에서 설정한 파라미터 값은 다음과 같습니다. \n\n\n\n### Risk Sensitive Policy\n\n우선 Risk의 정도를 다양하게 한 결과가 아래와 같습니다. \n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/8bcsng48ywep3ay/sampling_distribution_result.png?dl=1\" alt=\"Risk compare result\" width=\"1000\"/>\n\n</p>\n\n위의 결과에서 보면 CPW(.71)과 Norm(3)의 경우 distribution의 양 끝부분에 대한 영향을 감소시킵니다. Wang(.75)의 경우 Risk-seeking, Wang(-.75), Pow(-2), CVaR(.25) 그리고 CVaR(.1)의 경우 Risk-averse 한 policy를 선택합니다. \n\n위의 결과를 비교해보면 다음과 같은 순서대로 우수한 성능을 보입니다. \n\nRisk-averse > Neutral > Risk-seeking \n\n이에 대해서는 대부분의 게임이 오래 살아있을 수록 받는 reward가 증가하기 때문에 불확실하면서 높은 reward를 선택하는 risk-seeking policy보다 작더라도 안정적인 reward를 선택하는 risk-averse policy가 좋은 성능을 보이는 것이라 생각할 수 있습니다. \n\n<br>\n\n### Atari-57 Result\n\nIQN 알고리즘을 57개의 Atari game에 테스트한 결과가 다음과 같습니다. \n\n<p align=\"center\">\n<img src=\"https://www.dropbox.com/s/p530cm694ey9v0h/Result1.png?dl=1\" alt=\"Result1\" width=\"600\"/>\n\n</p>\n\n<p align=\"center\">\n<img src=\"https://www.dropbox.com/s/p4v7brznezqo4lk/Result2.png?dl=1\" alt=\"Result2\" width=\"600\"/>\n\n</p>\n\n위의 결과에서 볼 수 있듯이 IQN은 단일 알고리즘임에도 불구하고 여러가지 기법을 조합한 Rainbow에 약간 못미치는 성능을 보입니다. 하지만 다른 단일 알고리즘인 Prioritized experience replay, C51, QR-DQN에 비해서는 모두 우수한 성능을 보이고 있습니다. 특히 QR-DQN에 비해 많은 변화가 없음에도 불구하고 기존의 단일 알고리즘 중 가장 좋은 성능을 보였던 QR-DQN보다 월등히 우수한 성능을 보이고 있습니다. \n\n<br>\n\n### Robustness Test\n\n다음의 결과는 같이 작업을 수행한 `윤승제`님께서 테스트해주신 결과입니다. Cartpole을 기본 파라미터로 학습을 수행한 후 **Pole의 길이**와 **Pole의 질량**을 다양하게 변경해가면서 테스트를 수행하였습니다. 알고리즘은 DQN과 IQN의 결과를 비교하였습니다.\n\n\n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/wbvlmmc8oudw4kb/robustness.png?dl=1\" alt=\"Result_robustness\" width=\"800\"/>\n\n</p>\n\n까맣게 될수록 500점을 내지 못하고 중간에 떨어진 것입니다. 위의 결과에서 볼 수 있듯이 DQN은 파라미터의 변화에 취약하여 값을 변경하는 것에 따라 성능이 크게 저하되는 것을 확인할 수 있었습니다. 하지만 IQN의 경우 파라미터 변화에 매우 robust하여 parameter가 변해도 대부분의 경우 500점을 얻는 것을 확인할 수 있었습니다. \n\n\n\n## Conclusion\n\nIQN은 QR-DQN에 비해 다음의 사항들만 변경해줬음에도 불구하고 훨씬 뛰어난 성능을 보이는 알고리즘입니다.  \n\n- Quantile을 random sampling하고 그때의 support를 도출하여 value distribution 취득\n- Random sampling을 이용하여 risk-sensitive policy에 따른 action 선택 가능\n- Network의 구조 변화 (DQN과 동일하나 convolution 연산의 결과를 embedding function 결과와 element-wise하게 곱해줌) \n\n<br>\n\nIQN의 경우 다양한 deep reinforcement learning 알고리즘들에 비해 좋은 성능을 보였으며 특히 단일 알고리즘임에도 불구하고 여러 알고리즘의 결합체인 Rainbow에 약간 못미치는 성능을 보였습니다. 또한 quantile regression을 이용하여 wasserstein distance를 줄이는 만큼 distributional RL의 수렴성을 증명했다는 QR-DQN의 장점은 그대로 가지고 있는 알고리즘이라 할 수 있습니다. \n\n<br>\n\nIQN을 끝으로 Deepmind에서 현재까지 (2018.11.01) 발표한 distributional RL에 대한 논문들에 대한 리뷰를 마치도록 하겠습니다!! 감사합니다!! :smile:\n\n<br>\n\n## Implementation\n\n본 논문의 코드는 다음의 Github를 참고해주세요. \n\n[Github](https://github.com/reinforcement-learning-kr/distributional_rl)\n\n<br>\n\n## Other Posts\n\n[Distributional RL 개요](https://reinforcement-learning-kr.github.io/2018/09/27/Distributional_intro/)\n\n[C51](https://reinforcement-learning-kr.github.io/2018/10/02/C51/)\n\n[QR-DQN](https://reinforcement-learning-kr.github.io/2018/10/22/QR-DQN/)\n\n<br>\n\n## Reference\n\n- [Implicit Quantile Networks for Distributional Reinforcement Learning](https://arxiv.org/abs/1806.06923)\n\n\n<br>\n\n## Team\n\n민규식: [Github](https://github.com/Kyushik), [Facebook](https://www.facebook.com/kyushik.min)\n\n차금강: [Github](https://github.com/chagmgang), [Facebook](https://www.facebook.com/profile.php?id=100002147815509)\n\n윤승제: [Github](https://github.com/sjYoondeltar), [Facebook](https://www.facebook.com/seungje.yoon)\n\n김하영: [Github](https://github.com/hayoung-kim), [Facebook](https://www.facebook.com/altairyoung)\n\n김정대: [Github](https://github.com/kekmodel), [Facebook](https://www.facebook.com/kekmodel)\n\n\n","source":"_posts/IQN.md","raw":"---\ntitle: Implicit Quantile Networks for Distributional Reinforcement Learning\ndate: 2018-10-30 20:38:40\ntags: [\"논문\", \"Distributional RL\", \"IQN\"]\ncategories: 논문 정리\nauthor: 민규식\nsubtitle: Distributional RL 3번째 논문\n\n\n\n---\n\n<center> <img src=\"https://www.dropbox.com/s/mmxtgylh0ntatp4/IQN_paper.png?dl=1\" width=\"800\"> </center>\n\n논문 저자 : [Will Dabney](https://arxiv.org/search/cs?searchtype=author&query=Dabney%2C+W), [Georg Ostrovski](https://arxiv.org/search/cs?searchtype=author&query=Ostrovski%2C+G), [David Silver](https://arxiv.org/search/cs?searchtype=author&query=Silver%2C+D), [Rémi Munos](https://arxiv.org/search/cs?searchtype=author&query=Munos%2C+R)    \n논문 링크 : [ArXiv](https://arxiv.org/abs/1806.06923)\nProceeding : The 36th International Conference on Machine Learning (*ICML 2018*)        \n정리 : 민규식\n\n\n\n## Introduction\n\n본 게시물은 2018년 6월에 발표된 논문 [Implicit Quantile Networks for Distributional Reinforcement Learning](https://arxiv.org/abs/1806.06923) 의 내용에 대해 설명합니다.\n\n<p align= \"center\">\n\n<img src=\"https://www.dropbox.com/s/mmxtgylh0ntatp4/IQN_paper.png?dl=1\" alt=\"paper\" style=\"width: 800px;\"/>\n\n </p>\n\n<br>\n\n## Algorithm \n\nIQN의 경우 QR-DQN과 비교했을 때 크게 다음의 2가지 정도에서 차이를 보입니다. \n\n- 동일한 확률로 나눈 Quantile을 이용하는 대신 확률들을  random sampling하고 해당하는 support를 도출\n- 네트워크 구조\n\n\n\n위의 내용들에 대해 하나하나 살펴보도록 하겠습니다. \n\n<br>\n\n### 1. Sampling\n\n\n\n#### QR-DQN vs IQN\n\nQR-DQN 논문에서는 quantile regression 기법을 이용하여 Wasserstein distance를 줄이는 방향으로 분포를 학습하였고 이에 따라 distributional RL의 수렴성을 증명하였습니다. 이에 따라 IQN 논문에서도 quantile regression 기법을 그대로 이용합니다. 심지어 QR-DQN 논문에서 사용한 Quantile huber loss도 그대로 사용합니다. Target network, experience replay, epsilon-greedy도 QR-DQN과 동일하게 사용합니다. 하지만 IQN에서는 `Cumulative Distribution Function`을 동일한 확률로 나누는 대신 random sampling을 통해 취득한 tau에 해당하는 support를 도출합니다. \n\n예를 들어보겠습니다. Quantile의 수를 4라고 해보겠습니다. 이 경우 QR-DQN의 quantile값은 [0.25, 0.5, 0.75, 1]이지만 QR-DQN은 Wasserstein distance를 최소로 하기 위해 quantile의 중앙값에 해당하는 support를 도출합니다. 즉 [0.125, 0.375, 0.625, 0.875]에 해당하는 support들을 추정합니다. \n\n하지만 IQN 논문에서는 quantile값 tau를 0~1 사이에서 임의로 sampling합니다. Quantile이 4개인 경우 랜덤하게 추출한 0~1사이의 4개의 값이 [0.12, 0.32, 0.78, 0.92] 라고 해보겠습니다. \n\n 위의 예시를 그림으로 표현한 것이 아래와 같습니다. \n\n<p align=\"center\">\n<img src=\"https://www.dropbox.com/s/pffm77vus3k4uex/qr_dqn_iqn.png?dl=1\" alt=\"QRDQN vs IQN\" width=\"800\"/>\n\n</p>\n\n해당 내용에 대해서는 논문에서 나타낸 그림이 굉장히 잘 표현하고 있습니다. 이 그림의 경우 DQN, C51, QR-DQN, IQN의 경우를 아래와 같이 모두 비교하고 있습니다. \n\n\n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/unsy98e3xmz4a5n/compare_paper.png?dl=1\" alt=\"Comparison from paper\" width=\"800\"/>\n\n</p>\n\n다들 network 구조에도 차이가 존재하지만 일단은 output만을 비교해보도록 하겠습니다. \n\n- DQN: 각 action에 대한 value\n- C51: 각 action에 대한 value distribution 중 확률 (support는 고정값으로 사용)\n- QR-DQN: 각 action에 대한 value distribution 중 support (확률은 고정값으로 사용)\n- IQN: 각 action에 대한 value distribution 중 support (확률은 random sampling)\n\n\n\n그럼 이렇게 sampling을 한 tau를 통해 support를 추정하는 것은 어떤 좋은 점이 있을까요? 논문에 따르면 `Risk-Sensitive`하게 Policy를 선택할 수 있습니다. 해당 내용에 대해 한번 살펴보도록 하겠습니다.  \n\n\n\n#### Risk-Sensitive Reinforcement Learning\n\n일단은 여기서 말하는 `Risk`가 무엇을 의미하는지 먼저 알아보도록 하겠습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/vvlartdbmh51n69/risk_sensitive1.png?dl=1\" alt=\"Risk sensitive1\" width=\"600\"/>\n\n</p>\n\n위의 그림을 보시면 2개의 action에 대한 value distribution이 각각 존재합니다. Action 1 (a1)에 대한 distribution은 분산이 작기 때문에 평균값에 가까운 return을 받을 확률이 높습니다. 하지만 value distribution의 기대값이 action2 (a2)보다 작습니다. Distributional RL에서는 distribution의 기대값을 비교하여 action을 선택하기 때문에 이런 경우 a2가 선택될 것입니다. \n\na2의 경우 분산이 매우 큰 distribution입니다. 이런 분포에서는 경우에 따라 매우 작은 return이 도출될 수도 있고, 매우 높은 return이 도출될수도 있습니다. Distributional RL에서는 이렇게 분산이 커서 결과에 대한 확신이 낮은 경우 **\"Risk가 크다\"**고 합니다. 반대로 결과에 대한 확신이 상대적으로 높은 a1의 경우 a2에 비해 **\"Risk가 작다\"**라고 할 수 있는 것이죠. \n\nSampling을 통해 학습을 수행하고 action을 선택하는 경우 이 risk에 따라 action을 선택하는 것이 가능합니다. 이런 risk sensitive policy에는 다음의 2가지가 있습니다. \n\n- Risk-averse policy\n- Risk-seeking policy \n\n<br>\n\n[A Comprehensive Survey on Safe Reinforcement Learning](http://www.jmlr.org/papers/volume16/garcia15a/garcia15a.pdf) 논문을 보시면 다음과 같은 내용이 나옵니다.  \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/xivtpfaufuaf732/risk_sensitive_criterion.png?dl=1\" alt=\"Risk sensitive criterion\" width=\"800\"/>\n\n</p>\n\n위의 definition을 보시면 scalar parameter beta를 통해 risk의 level을 결정합니다. 이 beta를 risk sensitivity parameter라고 합니다. 이 beta가 0보다 큰 경우 risk-averse, 0보다 작은 경우 risk-seeking, 그리고 0인 경우 risk-neutral policy가 됩니다. \n\n어떻게 beta를 통해 risk의 level을 결정할 수 있는지, sampling을 이용하면 어떻게 risk-sensitive policy를 결정할 수 있는지 한번 알아보겠습니다.  \n\n우선 risk-sensitive policy에 따라 action을 선택하는 식은 다음과 같습니다.  \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/cw6uk13z25bzhgu/risk_sensitive_equation.png?dl=1\" alt=\"Risk sensitive criterion\" width=\"600\"/>\n\n</p>\n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/jtvk920xe5ubqs7/risk_averse.png?dl=1\" alt=\"Risk averse\" width=\"800\"/>\n\n</p>\n\n위의 경우는 RIsk-averse 입니다. 기존의 경우 a2의 distribution에 대한 평균값이 a1의 distribution에 대한 평균값보다 크기 때문에 a2를 최종적인 action으로 선택했을 것입니다. 하지만 a1보다 a2의 분산이 훨씬 크기 때문에 beta가 양수인 경우 (distribution의 평균) - beta * (distribution의 분산)의 계산을 수행하면 연산의 결과값은 a2보다 a1이 더 큽니다. 이에 따라 risk가 더 낮은 a1을 선택하게 되는 것이고 위와 같은 과정을 통해 risk-averse policy에 따라 action을 선택하게 됩니다.   \n\nCDF의 낮은 영역에서만 tau를 sampling하는 경우 위와 유사한 결과를 확인할 수 있습니다. Tau를 낮은 범위 내에서 sampling하는 경우 낮은 value들에 대한 tau들이 sampling되지만 a2의 경우 a1보다 분산이 훨씬 크기 때문에 sampling 된 결과들의 기대값은 a1이 a2보다 크게 됩니다. \n\n\n\nRisk-seeking의 경우 위의 risk-averse와 반대의 상황을 확인할 수 있습니다. \n\n \n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/19q9zx7o8b4npj7/risk_seeking.png?dl=1\" alt=\"Risk seeking\" width=\"800\"/>\n\n</p>\n\n위의 경우는 RIsk-seeking 입니다. 기존의 경우 a1의 distribution에 대한 평균값이 a2의 distribution에 대한 평균값보다 크기 때문에 a1를 최종적인 action으로 선택했을 것입니다. 하지만 a1보다 a2의 분산이 훨씬 크기 때문에 beta가 음수인 경우 (distribution의 평균) - beta * (distribution의 분산)의 계산을 수행하면 연산의 결과값은 a1보다 a2가 더 큽니다. 이에 따라 risk가 더 큰 a2를 선택하게 되는 것이고 위와 같은 과정을 통해 risk-seeking policy에 따라 action을 선택하게 됩니다.   \n\nCDF의 높은 영역에서만 tau를 sampling하는 경우 위와 유사한 결과를 확인할 수 있습니다. Tau를 높은 범위 내에서 sampling하는 경우 높은 value들에 대한 tau들이 sampling되지만 a2의 경우 a1보다 분산이 훨씬 크기 때문에 sampling 된 결과들의 기대값은 a2가 a1보다 크게 됩니다. \n\n<br>\n\n논문에서는 다음과 같은 4가지 기법들을 이용해 tau를 위한 sampling distribution을 변경하고 다양한 risk-sensitive policy를 선택하게 됩니다. \n\n- Cumulative Probability Weighting (CPW)\n- Wang\n- Power formula (POW)\n- Conditional Value-at-Risk (CVaR)\n\n\n\n위의 예시 중에 가장 간단한 CVaR에 대해서 살펴보도록 하겠습니다. CVaR의 수식은 다음과 같습니다. \n\n <img src=\"https://www.dropbox.com/s/3bvwmom4ky6pmhp/CVaR.png?dl=1\" alt=\"CVar\" width=\"300\"/>\n\n원래 tau는 [0,1] 중에서 uniform한 확률로 선택합니다. 하지만 위의 경우 eta를 예를 들어 0.25로 하면 0.25tau가 됩니다. 즉 [0, 0.25] 중에서 uniform한 확률로 tau를 sampling하게 됩니다. 결과적으로 risk-averse policy에 따라 action을 선택하게 됩니다.  \n\n<br>\n\n### 2. Network\n\n여기서는 IQN 네트워크의 구조에 대해 살펴보도록 하겠습니다. 일단 위에서 봤던 그림을 다시 한번 살펴보도록 하겠습니다.\n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/unsy98e3xmz4a5n/compare_paper.png?dl=1\" alt=\"Comparison from paper\" width=\"800\"/>\n\n</p>\n\n위의 그림에서 DQN과 IQN만 비교해보도록 하겠습니다. \n\n특정 action a에 대해 DQN은 다음과 같은 함수들로 나타낼 수 있습니다. \n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/br22t2a08dgz03a/DQN_equation.png?dl=1\" alt=\"DQN\" width=\"300\"/>\n\n</p>\n\n위의 식에서 **psi**는 convolution layers에 의한 연산을, **f**는 fully-connected layers에 의한 연산을 나타냅니다. \n\nIQN도 DQN과 동일한 function인 **psi**와 **f**를 이용합니다. 그 대신 거기에 tau를 embedding 해주는 함수 **phi**를 추가적으로 사용합니다. 이에 따라 특정 action a에 대해 IQN을 함수로 나타낸 것이 다음과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/h6z8ppz9z9tkr8y/IQN_equation.png?dl=1\" alt=\"DQN\" width=\"500\"/>\n\n</p>\n\n위의 식을 살펴보자면 convolution function (psi)를 통해 얻은 결과와 tau에 대한 embedding function (phi)를 통해 얻은 결과를 element-wise하게 곱해줍니다. 그리고 그 결과에 fully-connected layer 연산을 수행하여 최종적으로 action a에 대한 value distribution을 얻습니다.  \n\n즉 위의 그림에서도 볼 수 있듯이 tau를 embedding하는 function인 phi를 제외하고는 모두 DQN과 같다고 할 수 있습니다. 이에 따라 embedding function (phi)에 대해서 한번 살펴보도록 하겠습니다. 이 함수의 역할은 하나의 sampling된 tau를 벡터로 embedding 해주는 것입니다. 본 논문에서 embedding에 대한 function은 다음과 같은 **n cosine basis function** 입니다. \n\n\n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/uxnaqayrxoazj93/embedding.png?dl=1\" alt=\"Embedding\" width=\"600\"/>\n\n</p>\n\n위의 식에서 n은 embedding dimension이며 값은 64로 설정하였습니다. tau는 sampling된 값이며 w와 b는 linear layer 연산을 위한 weight와 bias입니다. 위 수식만을 봐서는 embedding을 어떻게 수행할지 감이 오지 않을 수 있습니다! 하나의  quantile에 대해 어떻게 sampling을 하는지 그림과 함께 예를 들어보도록 하겠습니다. \n\n\n\n1. 우선 0~1 사이의 서로 다른 값을 [batch size x 1]의 사이즈로  random sampling 합니다. 이 값들은 각각의 tau값을 나타냅니다. \n\n\n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/y5gzs2vdpn6qlp3/embedding_progress1.png?dl=1\" alt=\"Embedding progress 1\" width=\"500\"/>\n\n</p>\n\n2. [Batch size x 1]로 sampling한 tau들을 복제하여 embedding dimension 만큼 쌓아줍니다. 본 논문의 경우 embedding dimension은 64입니다. 위 과정에 대한 결과의 사이즈는 [batch size x embedding dim]이 됩니다. \n\n   <p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/n86yp26tlcovjch/embedding_progress2.png?dl=1\" width=\"600\"/>\n\n</p>\n\n\n\n3. 모든 [batch size x embedding dim]의 모든 row에 0 ~ (n-1) 을 1씩 증가시킨 값을 곱해줍니다. 그리고 모든 값에 pi를 곱해줍니다. 이렇게 계산한 결과의 dimension은 여전히 [batch size x embedding dim] 입니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/i1qs8zf162eniuu/embedding_progress3.png?dl=1\" alt=\"Embedding\" width=\"1000\"/>\n\n</p>\n\n4. 다음으로는 3의 결과에 cos 연산을 해주고 weight를 곱하고 bias를 더해줍니다. 그리고 해당 결과에 최종적으로 ReLU 함수를 적용해줍니다. 여기서 weight의 사이즈는 [embedding dim x convolution 결과의 크기] 이며 bias의 사이즈는 [convolution 결과의 크기] 입니다. 결국 embedding 연산의 최종 결과는 [batch size x convolution 결과의 크기]가 됩니다. Convolution 연산의 최종 결과 또한 크기가 [batch size x convolution 결과의 크기] 입니다. 이에 따라 둘은 크기가 같아지게 되고 이에 따라 element-wise 하게 곱할 수 있게 됩니다. \n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/bnbxq3aj6c5kmrt/embedding_progress4.png?dl=1\" alt=\"Embedding\" width=\"800\"/>\n\n</p>\n\n여기까지가 Embedding function의 내용입니다! \n\n여기서 약간의 의문이 들 수 있을 것이라 생각합니다. 왜 **n cosine basis function**을 이용할까요? 논문에서는 다양한 함수에 대해서 실험을 수행하였고 그 결과 **n cosine basis function**이 가장 좋은 결과를 보였다고 합니다. 여러 함수에 대해 테스트한 결과는 다음과 같습니다. \n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/09idlykr8xr6fo9/embedding_compare.png?dl=1\" alt=\"Embedding\" width=\"800\"/>\n\n</p>\n\n<br>\n\n### 3. Quantile Huber Loss \n\n사실 본 논문에서는 QR-DQN에서 사용했던 **Quantile Huber Loss**를 그대로 이용합니다. 하지만 본 논문에서는 식을 다음과 같이 표시합니다. \n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/ie5l2z5cwagoj86/loss.png?dl=1\" alt=\"Embedding\" width=\"500\"/>\n\n</p>\n\n본 논문에서 N은 추정을 위해 일반 network 연산에서 sampling한 tau의 수 입니다. N'은 target distribution 도출을 위해 target network 연산에서 sampling한 tau의 수 입니다. 위 식에 대해서는 QR-DQN에서 자세히 설명하였으므로 추가적인 설명은 하지 않도록 하겠습니다. 다만 본 논문에서는 N과 N'를 다양하게 바꿔가면서 테스트한 결과를 다음과 같이 보여줍니다. \n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/t2eeof94yhq57gb/various_n.png?dl=1\" alt=\"Various N\" width=\"700\"/>\n\n</p>\n\n위의 결과는 6개의 atari game에 대한 human-normalized agent performance의 평균을 나타냅니다. 왼쪽의 경우 초반 10M frame을 학습하였을 때 결과이며 오른쪽의 경우 마지막 10M frame의 학습 결과입니다. 다른 알고리즘의 경우 각각 왼쪽과 오른쪽에 대한 결과가 다음과 같습니다. \n\n- DQN: (32, 253)\n- QR-DQN: (144, 1243)\n\n<br>\n\n### 4. Action 선택\n\n 본 논문에서  action을 선택하는 식이 QR-DQN과 비교했을 때 약간의 차이가 있습니다. 해당 식은 다음과 같습니다. \n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/76l4gi57vasj300/action_equation.png?dl=1\" alt=\"Action equation\" width=\"500\"/>\n\n</p>\n\n위 식은 단순히 sampling된 quantile을 통해 구한 모든 support의 값을 평균한 것입니다. \n\n<br>\n\n 본 논문의 알고리즘은 다음과 같습니다. \n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/fbxxlz25l6dunxi/algorithm.png?dl=1\" alt=\"algorithm (IQN)\" width=\"700\"/>\n\n</p>\n\n<br>\n\n## Result\n\n본 알고리즘의 성능은 Atari-57 benchmark를 통해 검증되었습니다. \n\n본 알고리즘에서 설정한 파라미터 값은 다음과 같습니다. \n\n\n\n### Risk Sensitive Policy\n\n우선 Risk의 정도를 다양하게 한 결과가 아래와 같습니다. \n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/8bcsng48ywep3ay/sampling_distribution_result.png?dl=1\" alt=\"Risk compare result\" width=\"1000\"/>\n\n</p>\n\n위의 결과에서 보면 CPW(.71)과 Norm(3)의 경우 distribution의 양 끝부분에 대한 영향을 감소시킵니다. Wang(.75)의 경우 Risk-seeking, Wang(-.75), Pow(-2), CVaR(.25) 그리고 CVaR(.1)의 경우 Risk-averse 한 policy를 선택합니다. \n\n위의 결과를 비교해보면 다음과 같은 순서대로 우수한 성능을 보입니다. \n\nRisk-averse > Neutral > Risk-seeking \n\n이에 대해서는 대부분의 게임이 오래 살아있을 수록 받는 reward가 증가하기 때문에 불확실하면서 높은 reward를 선택하는 risk-seeking policy보다 작더라도 안정적인 reward를 선택하는 risk-averse policy가 좋은 성능을 보이는 것이라 생각할 수 있습니다. \n\n<br>\n\n### Atari-57 Result\n\nIQN 알고리즘을 57개의 Atari game에 테스트한 결과가 다음과 같습니다. \n\n<p align=\"center\">\n<img src=\"https://www.dropbox.com/s/p530cm694ey9v0h/Result1.png?dl=1\" alt=\"Result1\" width=\"600\"/>\n\n</p>\n\n<p align=\"center\">\n<img src=\"https://www.dropbox.com/s/p4v7brznezqo4lk/Result2.png?dl=1\" alt=\"Result2\" width=\"600\"/>\n\n</p>\n\n위의 결과에서 볼 수 있듯이 IQN은 단일 알고리즘임에도 불구하고 여러가지 기법을 조합한 Rainbow에 약간 못미치는 성능을 보입니다. 하지만 다른 단일 알고리즘인 Prioritized experience replay, C51, QR-DQN에 비해서는 모두 우수한 성능을 보이고 있습니다. 특히 QR-DQN에 비해 많은 변화가 없음에도 불구하고 기존의 단일 알고리즘 중 가장 좋은 성능을 보였던 QR-DQN보다 월등히 우수한 성능을 보이고 있습니다. \n\n<br>\n\n### Robustness Test\n\n다음의 결과는 같이 작업을 수행한 `윤승제`님께서 테스트해주신 결과입니다. Cartpole을 기본 파라미터로 학습을 수행한 후 **Pole의 길이**와 **Pole의 질량**을 다양하게 변경해가면서 테스트를 수행하였습니다. 알고리즘은 DQN과 IQN의 결과를 비교하였습니다.\n\n\n\n<p align=\"center\">\n\n<img src=\"https://www.dropbox.com/s/wbvlmmc8oudw4kb/robustness.png?dl=1\" alt=\"Result_robustness\" width=\"800\"/>\n\n</p>\n\n까맣게 될수록 500점을 내지 못하고 중간에 떨어진 것입니다. 위의 결과에서 볼 수 있듯이 DQN은 파라미터의 변화에 취약하여 값을 변경하는 것에 따라 성능이 크게 저하되는 것을 확인할 수 있었습니다. 하지만 IQN의 경우 파라미터 변화에 매우 robust하여 parameter가 변해도 대부분의 경우 500점을 얻는 것을 확인할 수 있었습니다. \n\n\n\n## Conclusion\n\nIQN은 QR-DQN에 비해 다음의 사항들만 변경해줬음에도 불구하고 훨씬 뛰어난 성능을 보이는 알고리즘입니다.  \n\n- Quantile을 random sampling하고 그때의 support를 도출하여 value distribution 취득\n- Random sampling을 이용하여 risk-sensitive policy에 따른 action 선택 가능\n- Network의 구조 변화 (DQN과 동일하나 convolution 연산의 결과를 embedding function 결과와 element-wise하게 곱해줌) \n\n<br>\n\nIQN의 경우 다양한 deep reinforcement learning 알고리즘들에 비해 좋은 성능을 보였으며 특히 단일 알고리즘임에도 불구하고 여러 알고리즘의 결합체인 Rainbow에 약간 못미치는 성능을 보였습니다. 또한 quantile regression을 이용하여 wasserstein distance를 줄이는 만큼 distributional RL의 수렴성을 증명했다는 QR-DQN의 장점은 그대로 가지고 있는 알고리즘이라 할 수 있습니다. \n\n<br>\n\nIQN을 끝으로 Deepmind에서 현재까지 (2018.11.01) 발표한 distributional RL에 대한 논문들에 대한 리뷰를 마치도록 하겠습니다!! 감사합니다!! :smile:\n\n<br>\n\n## Implementation\n\n본 논문의 코드는 다음의 Github를 참고해주세요. \n\n[Github](https://github.com/reinforcement-learning-kr/distributional_rl)\n\n<br>\n\n## Other Posts\n\n[Distributional RL 개요](https://reinforcement-learning-kr.github.io/2018/09/27/Distributional_intro/)\n\n[C51](https://reinforcement-learning-kr.github.io/2018/10/02/C51/)\n\n[QR-DQN](https://reinforcement-learning-kr.github.io/2018/10/22/QR-DQN/)\n\n<br>\n\n## Reference\n\n- [Implicit Quantile Networks for Distributional Reinforcement Learning](https://arxiv.org/abs/1806.06923)\n\n\n<br>\n\n## Team\n\n민규식: [Github](https://github.com/Kyushik), [Facebook](https://www.facebook.com/kyushik.min)\n\n차금강: [Github](https://github.com/chagmgang), [Facebook](https://www.facebook.com/profile.php?id=100002147815509)\n\n윤승제: [Github](https://github.com/sjYoondeltar), [Facebook](https://www.facebook.com/seungje.yoon)\n\n김하영: [Github](https://github.com/hayoung-kim), [Facebook](https://www.facebook.com/altairyoung)\n\n김정대: [Github](https://github.com/kekmodel), [Facebook](https://www.facebook.com/kekmodel)\n\n\n","slug":"IQN","published":1,"updated":"2019-02-07T11:21:31.993Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjrujlu0800125wfeylhwu3ba","content":"<center> <img src=\"https://www.dropbox.com/s/mmxtgylh0ntatp4/IQN_paper.png?dl=1\" width=\"800\"> </center>\n\n<p>논문 저자 : <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Dabney%2C+W\" target=\"_blank\" rel=\"noopener\">Will Dabney</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Ostrovski%2C+G\" target=\"_blank\" rel=\"noopener\">Georg Ostrovski</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Silver%2C+D\" target=\"_blank\" rel=\"noopener\">David Silver</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Munos%2C+R\" target=\"_blank\" rel=\"noopener\">Rémi Munos</a><br>논문 링크 : <a href=\"https://arxiv.org/abs/1806.06923\" target=\"_blank\" rel=\"noopener\">ArXiv</a><br>Proceeding : The 36th International Conference on Machine Learning (<em>ICML 2018</em>)<br>정리 : 민규식</p>\n<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>본 게시물은 2018년 6월에 발표된 논문 <a href=\"https://arxiv.org/abs/1806.06923\" target=\"_blank\" rel=\"noopener\">Implicit Quantile Networks for Distributional Reinforcement Learning</a> 의 내용에 대해 설명합니다.</p>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/mmxtgylh0ntatp4/IQN_paper.png?dl=1\" alt=\"paper\" style=\"width: 800px;\"><br><br> </p>\n\n<p><br></p>\n<h2 id=\"Algorithm\"><a href=\"#Algorithm\" class=\"headerlink\" title=\"Algorithm\"></a>Algorithm</h2><p>IQN의 경우 QR-DQN과 비교했을 때 크게 다음의 2가지 정도에서 차이를 보입니다. </p>\n<ul>\n<li>동일한 확률로 나눈 Quantile을 이용하는 대신 확률들을  random sampling하고 해당하는 support를 도출</li>\n<li>네트워크 구조</li>\n</ul>\n<p>위의 내용들에 대해 하나하나 살펴보도록 하겠습니다. </p>\n<p><br></p>\n<h3 id=\"1-Sampling\"><a href=\"#1-Sampling\" class=\"headerlink\" title=\"1. Sampling\"></a>1. Sampling</h3><h4 id=\"QR-DQN-vs-IQN\"><a href=\"#QR-DQN-vs-IQN\" class=\"headerlink\" title=\"QR-DQN vs IQN\"></a>QR-DQN vs IQN</h4><p>QR-DQN 논문에서는 quantile regression 기법을 이용하여 Wasserstein distance를 줄이는 방향으로 분포를 학습하였고 이에 따라 distributional RL의 수렴성을 증명하였습니다. 이에 따라 IQN 논문에서도 quantile regression 기법을 그대로 이용합니다. 심지어 QR-DQN 논문에서 사용한 Quantile huber loss도 그대로 사용합니다. Target network, experience replay, epsilon-greedy도 QR-DQN과 동일하게 사용합니다. 하지만 IQN에서는 <code>Cumulative Distribution Function</code>을 동일한 확률로 나누는 대신 random sampling을 통해 취득한 tau에 해당하는 support를 도출합니다. </p>\n<p>예를 들어보겠습니다. Quantile의 수를 4라고 해보겠습니다. 이 경우 QR-DQN의 quantile값은 [0.25, 0.5, 0.75, 1]이지만 QR-DQN은 Wasserstein distance를 최소로 하기 위해 quantile의 중앙값에 해당하는 support를 도출합니다. 즉 [0.125, 0.375, 0.625, 0.875]에 해당하는 support들을 추정합니다. </p>\n<p>하지만 IQN 논문에서는 quantile값 tau를 0~1 사이에서 임의로 sampling합니다. Quantile이 4개인 경우 랜덤하게 추출한 0~1사이의 4개의 값이 [0.12, 0.32, 0.78, 0.92] 라고 해보겠습니다. </p>\n<p> 위의 예시를 그림으로 표현한 것이 아래와 같습니다. </p>\n<p align=\"center\"><br><img src=\"https://www.dropbox.com/s/pffm77vus3k4uex/qr_dqn_iqn.png?dl=1\" alt=\"QRDQN vs IQN\" width=\"800\"><br><br></p>\n\n<p>해당 내용에 대해서는 논문에서 나타낸 그림이 굉장히 잘 표현하고 있습니다. 이 그림의 경우 DQN, C51, QR-DQN, IQN의 경우를 아래와 같이 모두 비교하고 있습니다. </p>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/unsy98e3xmz4a5n/compare_paper.png?dl=1\" alt=\"Comparison from paper\" width=\"800\"><br><br></p>\n\n<p>다들 network 구조에도 차이가 존재하지만 일단은 output만을 비교해보도록 하겠습니다. </p>\n<ul>\n<li>DQN: 각 action에 대한 value</li>\n<li>C51: 각 action에 대한 value distribution 중 확률 (support는 고정값으로 사용)</li>\n<li>QR-DQN: 각 action에 대한 value distribution 중 support (확률은 고정값으로 사용)</li>\n<li>IQN: 각 action에 대한 value distribution 중 support (확률은 random sampling)</li>\n</ul>\n<p>그럼 이렇게 sampling을 한 tau를 통해 support를 추정하는 것은 어떤 좋은 점이 있을까요? 논문에 따르면 <code>Risk-Sensitive</code>하게 Policy를 선택할 수 있습니다. 해당 내용에 대해 한번 살펴보도록 하겠습니다.  </p>\n<h4 id=\"Risk-Sensitive-Reinforcement-Learning\"><a href=\"#Risk-Sensitive-Reinforcement-Learning\" class=\"headerlink\" title=\"Risk-Sensitive Reinforcement Learning\"></a>Risk-Sensitive Reinforcement Learning</h4><p>일단은 여기서 말하는 <code>Risk</code>가 무엇을 의미하는지 먼저 알아보도록 하겠습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/vvlartdbmh51n69/risk_sensitive1.png?dl=1\" alt=\"Risk sensitive1\" width=\"600\"><br><br></p>\n\n<p>위의 그림을 보시면 2개의 action에 대한 value distribution이 각각 존재합니다. Action 1 (a1)에 대한 distribution은 분산이 작기 때문에 평균값에 가까운 return을 받을 확률이 높습니다. 하지만 value distribution의 기대값이 action2 (a2)보다 작습니다. Distributional RL에서는 distribution의 기대값을 비교하여 action을 선택하기 때문에 이런 경우 a2가 선택될 것입니다. </p>\n<p>a2의 경우 분산이 매우 큰 distribution입니다. 이런 분포에서는 경우에 따라 매우 작은 return이 도출될 수도 있고, 매우 높은 return이 도출될수도 있습니다. Distributional RL에서는 이렇게 분산이 커서 결과에 대한 확신이 낮은 경우 <strong>“Risk가 크다”</strong>고 합니다. 반대로 결과에 대한 확신이 상대적으로 높은 a1의 경우 a2에 비해 <strong>“Risk가 작다”</strong>라고 할 수 있는 것이죠. </p>\n<p>Sampling을 통해 학습을 수행하고 action을 선택하는 경우 이 risk에 따라 action을 선택하는 것이 가능합니다. 이런 risk sensitive policy에는 다음의 2가지가 있습니다. </p>\n<ul>\n<li>Risk-averse policy</li>\n<li>Risk-seeking policy </li>\n</ul>\n<p><br></p>\n<p><a href=\"http://www.jmlr.org/papers/volume16/garcia15a/garcia15a.pdf\" target=\"_blank\" rel=\"noopener\">A Comprehensive Survey on Safe Reinforcement Learning</a> 논문을 보시면 다음과 같은 내용이 나옵니다.  </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/xivtpfaufuaf732/risk_sensitive_criterion.png?dl=1\" alt=\"Risk sensitive criterion\" width=\"800\"><br><br></p>\n\n<p>위의 definition을 보시면 scalar parameter beta를 통해 risk의 level을 결정합니다. 이 beta를 risk sensitivity parameter라고 합니다. 이 beta가 0보다 큰 경우 risk-averse, 0보다 작은 경우 risk-seeking, 그리고 0인 경우 risk-neutral policy가 됩니다. </p>\n<p>어떻게 beta를 통해 risk의 level을 결정할 수 있는지, sampling을 이용하면 어떻게 risk-sensitive policy를 결정할 수 있는지 한번 알아보겠습니다.  </p>\n<p>우선 risk-sensitive policy에 따라 action을 선택하는 식은 다음과 같습니다.  </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/cw6uk13z25bzhgu/risk_sensitive_equation.png?dl=1\" alt=\"Risk sensitive criterion\" width=\"600\"><br><br></p>\n\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/jtvk920xe5ubqs7/risk_averse.png?dl=1\" alt=\"Risk averse\" width=\"800\"><br><br></p>\n\n<p>위의 경우는 RIsk-averse 입니다. 기존의 경우 a2의 distribution에 대한 평균값이 a1의 distribution에 대한 평균값보다 크기 때문에 a2를 최종적인 action으로 선택했을 것입니다. 하지만 a1보다 a2의 분산이 훨씬 크기 때문에 beta가 양수인 경우 (distribution의 평균) - beta * (distribution의 분산)의 계산을 수행하면 연산의 결과값은 a2보다 a1이 더 큽니다. 이에 따라 risk가 더 낮은 a1을 선택하게 되는 것이고 위와 같은 과정을 통해 risk-averse policy에 따라 action을 선택하게 됩니다.   </p>\n<p>CDF의 낮은 영역에서만 tau를 sampling하는 경우 위와 유사한 결과를 확인할 수 있습니다. Tau를 낮은 범위 내에서 sampling하는 경우 낮은 value들에 대한 tau들이 sampling되지만 a2의 경우 a1보다 분산이 훨씬 크기 때문에 sampling 된 결과들의 기대값은 a1이 a2보다 크게 됩니다. </p>\n<p>Risk-seeking의 경우 위의 risk-averse와 반대의 상황을 확인할 수 있습니다. </p>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/19q9zx7o8b4npj7/risk_seeking.png?dl=1\" alt=\"Risk seeking\" width=\"800\"><br><br></p>\n\n<p>위의 경우는 RIsk-seeking 입니다. 기존의 경우 a1의 distribution에 대한 평균값이 a2의 distribution에 대한 평균값보다 크기 때문에 a1를 최종적인 action으로 선택했을 것입니다. 하지만 a1보다 a2의 분산이 훨씬 크기 때문에 beta가 음수인 경우 (distribution의 평균) - beta * (distribution의 분산)의 계산을 수행하면 연산의 결과값은 a1보다 a2가 더 큽니다. 이에 따라 risk가 더 큰 a2를 선택하게 되는 것이고 위와 같은 과정을 통해 risk-seeking policy에 따라 action을 선택하게 됩니다.   </p>\n<p>CDF의 높은 영역에서만 tau를 sampling하는 경우 위와 유사한 결과를 확인할 수 있습니다. Tau를 높은 범위 내에서 sampling하는 경우 높은 value들에 대한 tau들이 sampling되지만 a2의 경우 a1보다 분산이 훨씬 크기 때문에 sampling 된 결과들의 기대값은 a2가 a1보다 크게 됩니다. </p>\n<p><br></p>\n<p>논문에서는 다음과 같은 4가지 기법들을 이용해 tau를 위한 sampling distribution을 변경하고 다양한 risk-sensitive policy를 선택하게 됩니다. </p>\n<ul>\n<li>Cumulative Probability Weighting (CPW)</li>\n<li>Wang</li>\n<li>Power formula (POW)</li>\n<li>Conditional Value-at-Risk (CVaR)</li>\n</ul>\n<p>위의 예시 중에 가장 간단한 CVaR에 대해서 살펴보도록 하겠습니다. CVaR의 수식은 다음과 같습니다. </p>\n<p> <img src=\"https://www.dropbox.com/s/3bvwmom4ky6pmhp/CVaR.png?dl=1\" alt=\"CVar\" width=\"300\"></p>\n<p>원래 tau는 [0,1] 중에서 uniform한 확률로 선택합니다. 하지만 위의 경우 eta를 예를 들어 0.25로 하면 0.25tau가 됩니다. 즉 [0, 0.25] 중에서 uniform한 확률로 tau를 sampling하게 됩니다. 결과적으로 risk-averse policy에 따라 action을 선택하게 됩니다.  </p>\n<p><br></p>\n<h3 id=\"2-Network\"><a href=\"#2-Network\" class=\"headerlink\" title=\"2. Network\"></a>2. Network</h3><p>여기서는 IQN 네트워크의 구조에 대해 살펴보도록 하겠습니다. 일단 위에서 봤던 그림을 다시 한번 살펴보도록 하겠습니다.</p>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/unsy98e3xmz4a5n/compare_paper.png?dl=1\" alt=\"Comparison from paper\" width=\"800\"><br><br></p>\n\n<p>위의 그림에서 DQN과 IQN만 비교해보도록 하겠습니다. </p>\n<p>특정 action a에 대해 DQN은 다음과 같은 함수들로 나타낼 수 있습니다. </p>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/br22t2a08dgz03a/DQN_equation.png?dl=1\" alt=\"DQN\" width=\"300\"><br><br></p>\n\n<p>위의 식에서 <strong>psi</strong>는 convolution layers에 의한 연산을, <strong>f</strong>는 fully-connected layers에 의한 연산을 나타냅니다. </p>\n<p>IQN도 DQN과 동일한 function인 <strong>psi</strong>와 <strong>f</strong>를 이용합니다. 그 대신 거기에 tau를 embedding 해주는 함수 <strong>phi</strong>를 추가적으로 사용합니다. 이에 따라 특정 action a에 대해 IQN을 함수로 나타낸 것이 다음과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/h6z8ppz9z9tkr8y/IQN_equation.png?dl=1\" alt=\"DQN\" width=\"500\"><br><br></p>\n\n<p>위의 식을 살펴보자면 convolution function (psi)를 통해 얻은 결과와 tau에 대한 embedding function (phi)를 통해 얻은 결과를 element-wise하게 곱해줍니다. 그리고 그 결과에 fully-connected layer 연산을 수행하여 최종적으로 action a에 대한 value distribution을 얻습니다.  </p>\n<p>즉 위의 그림에서도 볼 수 있듯이 tau를 embedding하는 function인 phi를 제외하고는 모두 DQN과 같다고 할 수 있습니다. 이에 따라 embedding function (phi)에 대해서 한번 살펴보도록 하겠습니다. 이 함수의 역할은 하나의 sampling된 tau를 벡터로 embedding 해주는 것입니다. 본 논문에서 embedding에 대한 function은 다음과 같은 <strong>n cosine basis function</strong> 입니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/uxnaqayrxoazj93/embedding.png?dl=1\" alt=\"Embedding\" width=\"600\"><br><br></p>\n\n<p>위의 식에서 n은 embedding dimension이며 값은 64로 설정하였습니다. tau는 sampling된 값이며 w와 b는 linear layer 연산을 위한 weight와 bias입니다. 위 수식만을 봐서는 embedding을 어떻게 수행할지 감이 오지 않을 수 있습니다! 하나의  quantile에 대해 어떻게 sampling을 하는지 그림과 함께 예를 들어보도록 하겠습니다. </p>\n<ol>\n<li>우선 0~1 사이의 서로 다른 값을 [batch size x 1]의 사이즈로  random sampling 합니다. 이 값들은 각각의 tau값을 나타냅니다. </li>\n</ol>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/y5gzs2vdpn6qlp3/embedding_progress1.png?dl=1\" alt=\"Embedding progress 1\" width=\"500\"><br><br></p>\n\n<ol start=\"2\">\n<li><p>[Batch size x 1]로 sampling한 tau들을 복제하여 embedding dimension 만큼 쌓아줍니다. 본 논문의 경우 embedding dimension은 64입니다. 위 과정에 대한 결과의 사이즈는 [batch size x embedding dim]이 됩니다. </p>\n<p align=\"center\">\n\n</p></li>\n</ol>\n<p><img src=\"https://www.dropbox.com/s/n86yp26tlcovjch/embedding_progress2.png?dl=1\" width=\"600\"></p>\n<p></p><p></p>\n<ol start=\"3\">\n<li>모든 [batch size x embedding dim]의 모든 row에 0 ~ (n-1) 을 1씩 증가시킨 값을 곱해줍니다. 그리고 모든 값에 pi를 곱해줍니다. 이렇게 계산한 결과의 dimension은 여전히 [batch size x embedding dim] 입니다. </li>\n</ol>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/i1qs8zf162eniuu/embedding_progress3.png?dl=1\" alt=\"Embedding\" width=\"1000\"><br><br></p>\n\n<ol start=\"4\">\n<li>다음으로는 3의 결과에 cos 연산을 해주고 weight를 곱하고 bias를 더해줍니다. 그리고 해당 결과에 최종적으로 ReLU 함수를 적용해줍니다. 여기서 weight의 사이즈는 [embedding dim x convolution 결과의 크기] 이며 bias의 사이즈는 [convolution 결과의 크기] 입니다. 결국 embedding 연산의 최종 결과는 [batch size x convolution 결과의 크기]가 됩니다. Convolution 연산의 최종 결과 또한 크기가 [batch size x convolution 결과의 크기] 입니다. 이에 따라 둘은 크기가 같아지게 되고 이에 따라 element-wise 하게 곱할 수 있게 됩니다. </li>\n</ol>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/bnbxq3aj6c5kmrt/embedding_progress4.png?dl=1\" alt=\"Embedding\" width=\"800\"><br><br></p>\n\n<p>여기까지가 Embedding function의 내용입니다! </p>\n<p>여기서 약간의 의문이 들 수 있을 것이라 생각합니다. 왜 <strong>n cosine basis function</strong>을 이용할까요? 논문에서는 다양한 함수에 대해서 실험을 수행하였고 그 결과 <strong>n cosine basis function</strong>이 가장 좋은 결과를 보였다고 합니다. 여러 함수에 대해 테스트한 결과는 다음과 같습니다. </p>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/09idlykr8xr6fo9/embedding_compare.png?dl=1\" alt=\"Embedding\" width=\"800\"><br><br></p>\n\n<p><br></p>\n<h3 id=\"3-Quantile-Huber-Loss\"><a href=\"#3-Quantile-Huber-Loss\" class=\"headerlink\" title=\"3. Quantile Huber Loss\"></a>3. Quantile Huber Loss</h3><p>사실 본 논문에서는 QR-DQN에서 사용했던 <strong>Quantile Huber Loss</strong>를 그대로 이용합니다. 하지만 본 논문에서는 식을 다음과 같이 표시합니다. </p>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/ie5l2z5cwagoj86/loss.png?dl=1\" alt=\"Embedding\" width=\"500\"><br><br></p>\n\n<p>본 논문에서 N은 추정을 위해 일반 network 연산에서 sampling한 tau의 수 입니다. N’은 target distribution 도출을 위해 target network 연산에서 sampling한 tau의 수 입니다. 위 식에 대해서는 QR-DQN에서 자세히 설명하였으므로 추가적인 설명은 하지 않도록 하겠습니다. 다만 본 논문에서는 N과 N’를 다양하게 바꿔가면서 테스트한 결과를 다음과 같이 보여줍니다. </p>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/t2eeof94yhq57gb/various_n.png?dl=1\" alt=\"Various N\" width=\"700\"><br><br></p>\n\n<p>위의 결과는 6개의 atari game에 대한 human-normalized agent performance의 평균을 나타냅니다. 왼쪽의 경우 초반 10M frame을 학습하였을 때 결과이며 오른쪽의 경우 마지막 10M frame의 학습 결과입니다. 다른 알고리즘의 경우 각각 왼쪽과 오른쪽에 대한 결과가 다음과 같습니다. </p>\n<ul>\n<li>DQN: (32, 253)</li>\n<li>QR-DQN: (144, 1243)</li>\n</ul>\n<p><br></p>\n<h3 id=\"4-Action-선택\"><a href=\"#4-Action-선택\" class=\"headerlink\" title=\"4. Action 선택\"></a>4. Action 선택</h3><p> 본 논문에서  action을 선택하는 식이 QR-DQN과 비교했을 때 약간의 차이가 있습니다. 해당 식은 다음과 같습니다. </p>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/76l4gi57vasj300/action_equation.png?dl=1\" alt=\"Action equation\" width=\"500\"><br><br></p>\n\n<p>위 식은 단순히 sampling된 quantile을 통해 구한 모든 support의 값을 평균한 것입니다. </p>\n<p><br></p>\n<p> 본 논문의 알고리즘은 다음과 같습니다. </p>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/fbxxlz25l6dunxi/algorithm.png?dl=1\" alt=\"algorithm (IQN)\" width=\"700\"><br><br></p>\n\n<p><br></p>\n<h2 id=\"Result\"><a href=\"#Result\" class=\"headerlink\" title=\"Result\"></a>Result</h2><p>본 알고리즘의 성능은 Atari-57 benchmark를 통해 검증되었습니다. </p>\n<p>본 알고리즘에서 설정한 파라미터 값은 다음과 같습니다. </p>\n<h3 id=\"Risk-Sensitive-Policy\"><a href=\"#Risk-Sensitive-Policy\" class=\"headerlink\" title=\"Risk Sensitive Policy\"></a>Risk Sensitive Policy</h3><p>우선 Risk의 정도를 다양하게 한 결과가 아래와 같습니다. </p>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/8bcsng48ywep3ay/sampling_distribution_result.png?dl=1\" alt=\"Risk compare result\" width=\"1000\"><br><br></p>\n\n<p>위의 결과에서 보면 CPW(.71)과 Norm(3)의 경우 distribution의 양 끝부분에 대한 영향을 감소시킵니다. Wang(.75)의 경우 Risk-seeking, Wang(-.75), Pow(-2), CVaR(.25) 그리고 CVaR(.1)의 경우 Risk-averse 한 policy를 선택합니다. </p>\n<p>위의 결과를 비교해보면 다음과 같은 순서대로 우수한 성능을 보입니다. </p>\n<p>Risk-averse &gt; Neutral &gt; Risk-seeking </p>\n<p>이에 대해서는 대부분의 게임이 오래 살아있을 수록 받는 reward가 증가하기 때문에 불확실하면서 높은 reward를 선택하는 risk-seeking policy보다 작더라도 안정적인 reward를 선택하는 risk-averse policy가 좋은 성능을 보이는 것이라 생각할 수 있습니다. </p>\n<p><br></p>\n<h3 id=\"Atari-57-Result\"><a href=\"#Atari-57-Result\" class=\"headerlink\" title=\"Atari-57 Result\"></a>Atari-57 Result</h3><p>IQN 알고리즘을 57개의 Atari game에 테스트한 결과가 다음과 같습니다. </p>\n<p align=\"center\"><br><img src=\"https://www.dropbox.com/s/p530cm694ey9v0h/Result1.png?dl=1\" alt=\"Result1\" width=\"600\"><br><br></p>\n\n<p align=\"center\"><br><img src=\"https://www.dropbox.com/s/p4v7brznezqo4lk/Result2.png?dl=1\" alt=\"Result2\" width=\"600\"><br><br></p>\n\n<p>위의 결과에서 볼 수 있듯이 IQN은 단일 알고리즘임에도 불구하고 여러가지 기법을 조합한 Rainbow에 약간 못미치는 성능을 보입니다. 하지만 다른 단일 알고리즘인 Prioritized experience replay, C51, QR-DQN에 비해서는 모두 우수한 성능을 보이고 있습니다. 특히 QR-DQN에 비해 많은 변화가 없음에도 불구하고 기존의 단일 알고리즘 중 가장 좋은 성능을 보였던 QR-DQN보다 월등히 우수한 성능을 보이고 있습니다. </p>\n<p><br></p>\n<h3 id=\"Robustness-Test\"><a href=\"#Robustness-Test\" class=\"headerlink\" title=\"Robustness Test\"></a>Robustness Test</h3><p>다음의 결과는 같이 작업을 수행한 <code>윤승제</code>님께서 테스트해주신 결과입니다. Cartpole을 기본 파라미터로 학습을 수행한 후 <strong>Pole의 길이</strong>와 <strong>Pole의 질량</strong>을 다양하게 변경해가면서 테스트를 수행하였습니다. 알고리즘은 DQN과 IQN의 결과를 비교하였습니다.</p>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/wbvlmmc8oudw4kb/robustness.png?dl=1\" alt=\"Result_robustness\" width=\"800\"><br><br></p>\n\n<p>까맣게 될수록 500점을 내지 못하고 중간에 떨어진 것입니다. 위의 결과에서 볼 수 있듯이 DQN은 파라미터의 변화에 취약하여 값을 변경하는 것에 따라 성능이 크게 저하되는 것을 확인할 수 있었습니다. 하지만 IQN의 경우 파라미터 변화에 매우 robust하여 parameter가 변해도 대부분의 경우 500점을 얻는 것을 확인할 수 있었습니다. </p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>IQN은 QR-DQN에 비해 다음의 사항들만 변경해줬음에도 불구하고 훨씬 뛰어난 성능을 보이는 알고리즘입니다.  </p>\n<ul>\n<li>Quantile을 random sampling하고 그때의 support를 도출하여 value distribution 취득</li>\n<li>Random sampling을 이용하여 risk-sensitive policy에 따른 action 선택 가능</li>\n<li>Network의 구조 변화 (DQN과 동일하나 convolution 연산의 결과를 embedding function 결과와 element-wise하게 곱해줌) </li>\n</ul>\n<p><br></p>\n<p>IQN의 경우 다양한 deep reinforcement learning 알고리즘들에 비해 좋은 성능을 보였으며 특히 단일 알고리즘임에도 불구하고 여러 알고리즘의 결합체인 Rainbow에 약간 못미치는 성능을 보였습니다. 또한 quantile regression을 이용하여 wasserstein distance를 줄이는 만큼 distributional RL의 수렴성을 증명했다는 QR-DQN의 장점은 그대로 가지고 있는 알고리즘이라 할 수 있습니다. </p>\n<p><br></p>\n<p>IQN을 끝으로 Deepmind에서 현재까지 (2018.11.01) 발표한 distributional RL에 대한 논문들에 대한 리뷰를 마치도록 하겠습니다!! 감사합니다!! :smile:</p>\n<p><br></p>\n<h2 id=\"Implementation\"><a href=\"#Implementation\" class=\"headerlink\" title=\"Implementation\"></a>Implementation</h2><p>본 논문의 코드는 다음의 Github를 참고해주세요. </p>\n<p><a href=\"https://github.com/reinforcement-learning-kr/distributional_rl\" target=\"_blank\" rel=\"noopener\">Github</a></p>\n<p><br></p>\n<h2 id=\"Other-Posts\"><a href=\"#Other-Posts\" class=\"headerlink\" title=\"Other Posts\"></a>Other Posts</h2><p><a href=\"https://reinforcement-learning-kr.github.io/2018/09/27/Distributional_intro/\">Distributional RL 개요</a></p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/10/02/C51/\">C51</a></p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/10/22/QR-DQN/\">QR-DQN</a></p>\n<p><br></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://arxiv.org/abs/1806.06923\" target=\"_blank\" rel=\"noopener\">Implicit Quantile Networks for Distributional Reinforcement Learning</a></li>\n</ul>\n<p><br></p>\n<h2 id=\"Team\"><a href=\"#Team\" class=\"headerlink\" title=\"Team\"></a>Team</h2><p>민규식: <a href=\"https://github.com/Kyushik\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/kyushik.min\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>차금강: <a href=\"https://github.com/chagmgang\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/profile.php?id=100002147815509\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>윤승제: <a href=\"https://github.com/sjYoondeltar\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/seungje.yoon\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>김하영: <a href=\"https://github.com/hayoung-kim\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/altairyoung\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>김정대: <a href=\"https://github.com/kekmodel\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/kekmodel\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"https://www.dropbox.com/s/mmxtgylh0ntatp4/IQN_paper.png?dl=1\" width=\"800\"> </center>\n\n<p>논문 저자 : <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Dabney%2C+W\" target=\"_blank\" rel=\"noopener\">Will Dabney</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Ostrovski%2C+G\" target=\"_blank\" rel=\"noopener\">Georg Ostrovski</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Silver%2C+D\" target=\"_blank\" rel=\"noopener\">David Silver</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Munos%2C+R\" target=\"_blank\" rel=\"noopener\">Rémi Munos</a><br>논문 링크 : <a href=\"https://arxiv.org/abs/1806.06923\" target=\"_blank\" rel=\"noopener\">ArXiv</a><br>Proceeding : The 36th International Conference on Machine Learning (<em>ICML 2018</em>)<br>정리 : 민규식</p>\n<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>본 게시물은 2018년 6월에 발표된 논문 <a href=\"https://arxiv.org/abs/1806.06923\" target=\"_blank\" rel=\"noopener\">Implicit Quantile Networks for Distributional Reinforcement Learning</a> 의 내용에 대해 설명합니다.</p>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/mmxtgylh0ntatp4/IQN_paper.png?dl=1\" alt=\"paper\" style=\"width: 800px;\"><br><br> </p>\n\n<p><br></p>\n<h2 id=\"Algorithm\"><a href=\"#Algorithm\" class=\"headerlink\" title=\"Algorithm\"></a>Algorithm</h2><p>IQN의 경우 QR-DQN과 비교했을 때 크게 다음의 2가지 정도에서 차이를 보입니다. </p>\n<ul>\n<li>동일한 확률로 나눈 Quantile을 이용하는 대신 확률들을  random sampling하고 해당하는 support를 도출</li>\n<li>네트워크 구조</li>\n</ul>\n<p>위의 내용들에 대해 하나하나 살펴보도록 하겠습니다. </p>\n<p><br></p>\n<h3 id=\"1-Sampling\"><a href=\"#1-Sampling\" class=\"headerlink\" title=\"1. Sampling\"></a>1. Sampling</h3><h4 id=\"QR-DQN-vs-IQN\"><a href=\"#QR-DQN-vs-IQN\" class=\"headerlink\" title=\"QR-DQN vs IQN\"></a>QR-DQN vs IQN</h4><p>QR-DQN 논문에서는 quantile regression 기법을 이용하여 Wasserstein distance를 줄이는 방향으로 분포를 학습하였고 이에 따라 distributional RL의 수렴성을 증명하였습니다. 이에 따라 IQN 논문에서도 quantile regression 기법을 그대로 이용합니다. 심지어 QR-DQN 논문에서 사용한 Quantile huber loss도 그대로 사용합니다. Target network, experience replay, epsilon-greedy도 QR-DQN과 동일하게 사용합니다. 하지만 IQN에서는 <code>Cumulative Distribution Function</code>을 동일한 확률로 나누는 대신 random sampling을 통해 취득한 tau에 해당하는 support를 도출합니다. </p>\n<p>예를 들어보겠습니다. Quantile의 수를 4라고 해보겠습니다. 이 경우 QR-DQN의 quantile값은 [0.25, 0.5, 0.75, 1]이지만 QR-DQN은 Wasserstein distance를 최소로 하기 위해 quantile의 중앙값에 해당하는 support를 도출합니다. 즉 [0.125, 0.375, 0.625, 0.875]에 해당하는 support들을 추정합니다. </p>\n<p>하지만 IQN 논문에서는 quantile값 tau를 0~1 사이에서 임의로 sampling합니다. Quantile이 4개인 경우 랜덤하게 추출한 0~1사이의 4개의 값이 [0.12, 0.32, 0.78, 0.92] 라고 해보겠습니다. </p>\n<p> 위의 예시를 그림으로 표현한 것이 아래와 같습니다. </p>\n<p align=\"center\"><br><img src=\"https://www.dropbox.com/s/pffm77vus3k4uex/qr_dqn_iqn.png?dl=1\" alt=\"QRDQN vs IQN\" width=\"800\"><br><br></p>\n\n<p>해당 내용에 대해서는 논문에서 나타낸 그림이 굉장히 잘 표현하고 있습니다. 이 그림의 경우 DQN, C51, QR-DQN, IQN의 경우를 아래와 같이 모두 비교하고 있습니다. </p>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/unsy98e3xmz4a5n/compare_paper.png?dl=1\" alt=\"Comparison from paper\" width=\"800\"><br><br></p>\n\n<p>다들 network 구조에도 차이가 존재하지만 일단은 output만을 비교해보도록 하겠습니다. </p>\n<ul>\n<li>DQN: 각 action에 대한 value</li>\n<li>C51: 각 action에 대한 value distribution 중 확률 (support는 고정값으로 사용)</li>\n<li>QR-DQN: 각 action에 대한 value distribution 중 support (확률은 고정값으로 사용)</li>\n<li>IQN: 각 action에 대한 value distribution 중 support (확률은 random sampling)</li>\n</ul>\n<p>그럼 이렇게 sampling을 한 tau를 통해 support를 추정하는 것은 어떤 좋은 점이 있을까요? 논문에 따르면 <code>Risk-Sensitive</code>하게 Policy를 선택할 수 있습니다. 해당 내용에 대해 한번 살펴보도록 하겠습니다.  </p>\n<h4 id=\"Risk-Sensitive-Reinforcement-Learning\"><a href=\"#Risk-Sensitive-Reinforcement-Learning\" class=\"headerlink\" title=\"Risk-Sensitive Reinforcement Learning\"></a>Risk-Sensitive Reinforcement Learning</h4><p>일단은 여기서 말하는 <code>Risk</code>가 무엇을 의미하는지 먼저 알아보도록 하겠습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/vvlartdbmh51n69/risk_sensitive1.png?dl=1\" alt=\"Risk sensitive1\" width=\"600\"><br><br></p>\n\n<p>위의 그림을 보시면 2개의 action에 대한 value distribution이 각각 존재합니다. Action 1 (a1)에 대한 distribution은 분산이 작기 때문에 평균값에 가까운 return을 받을 확률이 높습니다. 하지만 value distribution의 기대값이 action2 (a2)보다 작습니다. Distributional RL에서는 distribution의 기대값을 비교하여 action을 선택하기 때문에 이런 경우 a2가 선택될 것입니다. </p>\n<p>a2의 경우 분산이 매우 큰 distribution입니다. 이런 분포에서는 경우에 따라 매우 작은 return이 도출될 수도 있고, 매우 높은 return이 도출될수도 있습니다. Distributional RL에서는 이렇게 분산이 커서 결과에 대한 확신이 낮은 경우 <strong>“Risk가 크다”</strong>고 합니다. 반대로 결과에 대한 확신이 상대적으로 높은 a1의 경우 a2에 비해 <strong>“Risk가 작다”</strong>라고 할 수 있는 것이죠. </p>\n<p>Sampling을 통해 학습을 수행하고 action을 선택하는 경우 이 risk에 따라 action을 선택하는 것이 가능합니다. 이런 risk sensitive policy에는 다음의 2가지가 있습니다. </p>\n<ul>\n<li>Risk-averse policy</li>\n<li>Risk-seeking policy </li>\n</ul>\n<p><br></p>\n<p><a href=\"http://www.jmlr.org/papers/volume16/garcia15a/garcia15a.pdf\" target=\"_blank\" rel=\"noopener\">A Comprehensive Survey on Safe Reinforcement Learning</a> 논문을 보시면 다음과 같은 내용이 나옵니다.  </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/xivtpfaufuaf732/risk_sensitive_criterion.png?dl=1\" alt=\"Risk sensitive criterion\" width=\"800\"><br><br></p>\n\n<p>위의 definition을 보시면 scalar parameter beta를 통해 risk의 level을 결정합니다. 이 beta를 risk sensitivity parameter라고 합니다. 이 beta가 0보다 큰 경우 risk-averse, 0보다 작은 경우 risk-seeking, 그리고 0인 경우 risk-neutral policy가 됩니다. </p>\n<p>어떻게 beta를 통해 risk의 level을 결정할 수 있는지, sampling을 이용하면 어떻게 risk-sensitive policy를 결정할 수 있는지 한번 알아보겠습니다.  </p>\n<p>우선 risk-sensitive policy에 따라 action을 선택하는 식은 다음과 같습니다.  </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/cw6uk13z25bzhgu/risk_sensitive_equation.png?dl=1\" alt=\"Risk sensitive criterion\" width=\"600\"><br><br></p>\n\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/jtvk920xe5ubqs7/risk_averse.png?dl=1\" alt=\"Risk averse\" width=\"800\"><br><br></p>\n\n<p>위의 경우는 RIsk-averse 입니다. 기존의 경우 a2의 distribution에 대한 평균값이 a1의 distribution에 대한 평균값보다 크기 때문에 a2를 최종적인 action으로 선택했을 것입니다. 하지만 a1보다 a2의 분산이 훨씬 크기 때문에 beta가 양수인 경우 (distribution의 평균) - beta * (distribution의 분산)의 계산을 수행하면 연산의 결과값은 a2보다 a1이 더 큽니다. 이에 따라 risk가 더 낮은 a1을 선택하게 되는 것이고 위와 같은 과정을 통해 risk-averse policy에 따라 action을 선택하게 됩니다.   </p>\n<p>CDF의 낮은 영역에서만 tau를 sampling하는 경우 위와 유사한 결과를 확인할 수 있습니다. Tau를 낮은 범위 내에서 sampling하는 경우 낮은 value들에 대한 tau들이 sampling되지만 a2의 경우 a1보다 분산이 훨씬 크기 때문에 sampling 된 결과들의 기대값은 a1이 a2보다 크게 됩니다. </p>\n<p>Risk-seeking의 경우 위의 risk-averse와 반대의 상황을 확인할 수 있습니다. </p>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/19q9zx7o8b4npj7/risk_seeking.png?dl=1\" alt=\"Risk seeking\" width=\"800\"><br><br></p>\n\n<p>위의 경우는 RIsk-seeking 입니다. 기존의 경우 a1의 distribution에 대한 평균값이 a2의 distribution에 대한 평균값보다 크기 때문에 a1를 최종적인 action으로 선택했을 것입니다. 하지만 a1보다 a2의 분산이 훨씬 크기 때문에 beta가 음수인 경우 (distribution의 평균) - beta * (distribution의 분산)의 계산을 수행하면 연산의 결과값은 a1보다 a2가 더 큽니다. 이에 따라 risk가 더 큰 a2를 선택하게 되는 것이고 위와 같은 과정을 통해 risk-seeking policy에 따라 action을 선택하게 됩니다.   </p>\n<p>CDF의 높은 영역에서만 tau를 sampling하는 경우 위와 유사한 결과를 확인할 수 있습니다. Tau를 높은 범위 내에서 sampling하는 경우 높은 value들에 대한 tau들이 sampling되지만 a2의 경우 a1보다 분산이 훨씬 크기 때문에 sampling 된 결과들의 기대값은 a2가 a1보다 크게 됩니다. </p>\n<p><br></p>\n<p>논문에서는 다음과 같은 4가지 기법들을 이용해 tau를 위한 sampling distribution을 변경하고 다양한 risk-sensitive policy를 선택하게 됩니다. </p>\n<ul>\n<li>Cumulative Probability Weighting (CPW)</li>\n<li>Wang</li>\n<li>Power formula (POW)</li>\n<li>Conditional Value-at-Risk (CVaR)</li>\n</ul>\n<p>위의 예시 중에 가장 간단한 CVaR에 대해서 살펴보도록 하겠습니다. CVaR의 수식은 다음과 같습니다. </p>\n<p> <img src=\"https://www.dropbox.com/s/3bvwmom4ky6pmhp/CVaR.png?dl=1\" alt=\"CVar\" width=\"300\"></p>\n<p>원래 tau는 [0,1] 중에서 uniform한 확률로 선택합니다. 하지만 위의 경우 eta를 예를 들어 0.25로 하면 0.25tau가 됩니다. 즉 [0, 0.25] 중에서 uniform한 확률로 tau를 sampling하게 됩니다. 결과적으로 risk-averse policy에 따라 action을 선택하게 됩니다.  </p>\n<p><br></p>\n<h3 id=\"2-Network\"><a href=\"#2-Network\" class=\"headerlink\" title=\"2. Network\"></a>2. Network</h3><p>여기서는 IQN 네트워크의 구조에 대해 살펴보도록 하겠습니다. 일단 위에서 봤던 그림을 다시 한번 살펴보도록 하겠습니다.</p>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/unsy98e3xmz4a5n/compare_paper.png?dl=1\" alt=\"Comparison from paper\" width=\"800\"><br><br></p>\n\n<p>위의 그림에서 DQN과 IQN만 비교해보도록 하겠습니다. </p>\n<p>특정 action a에 대해 DQN은 다음과 같은 함수들로 나타낼 수 있습니다. </p>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/br22t2a08dgz03a/DQN_equation.png?dl=1\" alt=\"DQN\" width=\"300\"><br><br></p>\n\n<p>위의 식에서 <strong>psi</strong>는 convolution layers에 의한 연산을, <strong>f</strong>는 fully-connected layers에 의한 연산을 나타냅니다. </p>\n<p>IQN도 DQN과 동일한 function인 <strong>psi</strong>와 <strong>f</strong>를 이용합니다. 그 대신 거기에 tau를 embedding 해주는 함수 <strong>phi</strong>를 추가적으로 사용합니다. 이에 따라 특정 action a에 대해 IQN을 함수로 나타낸 것이 다음과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/h6z8ppz9z9tkr8y/IQN_equation.png?dl=1\" alt=\"DQN\" width=\"500\"><br><br></p>\n\n<p>위의 식을 살펴보자면 convolution function (psi)를 통해 얻은 결과와 tau에 대한 embedding function (phi)를 통해 얻은 결과를 element-wise하게 곱해줍니다. 그리고 그 결과에 fully-connected layer 연산을 수행하여 최종적으로 action a에 대한 value distribution을 얻습니다.  </p>\n<p>즉 위의 그림에서도 볼 수 있듯이 tau를 embedding하는 function인 phi를 제외하고는 모두 DQN과 같다고 할 수 있습니다. 이에 따라 embedding function (phi)에 대해서 한번 살펴보도록 하겠습니다. 이 함수의 역할은 하나의 sampling된 tau를 벡터로 embedding 해주는 것입니다. 본 논문에서 embedding에 대한 function은 다음과 같은 <strong>n cosine basis function</strong> 입니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/uxnaqayrxoazj93/embedding.png?dl=1\" alt=\"Embedding\" width=\"600\"><br><br></p>\n\n<p>위의 식에서 n은 embedding dimension이며 값은 64로 설정하였습니다. tau는 sampling된 값이며 w와 b는 linear layer 연산을 위한 weight와 bias입니다. 위 수식만을 봐서는 embedding을 어떻게 수행할지 감이 오지 않을 수 있습니다! 하나의  quantile에 대해 어떻게 sampling을 하는지 그림과 함께 예를 들어보도록 하겠습니다. </p>\n<ol>\n<li>우선 0~1 사이의 서로 다른 값을 [batch size x 1]의 사이즈로  random sampling 합니다. 이 값들은 각각의 tau값을 나타냅니다. </li>\n</ol>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/y5gzs2vdpn6qlp3/embedding_progress1.png?dl=1\" alt=\"Embedding progress 1\" width=\"500\"><br><br></p>\n\n<ol start=\"2\">\n<li><p>[Batch size x 1]로 sampling한 tau들을 복제하여 embedding dimension 만큼 쌓아줍니다. 본 논문의 경우 embedding dimension은 64입니다. 위 과정에 대한 결과의 사이즈는 [batch size x embedding dim]이 됩니다. </p>\n<p align=\"center\">\n\n</p></li>\n</ol>\n<p><img src=\"https://www.dropbox.com/s/n86yp26tlcovjch/embedding_progress2.png?dl=1\" width=\"600\"></p>\n<p></p><p></p>\n<ol start=\"3\">\n<li>모든 [batch size x embedding dim]의 모든 row에 0 ~ (n-1) 을 1씩 증가시킨 값을 곱해줍니다. 그리고 모든 값에 pi를 곱해줍니다. 이렇게 계산한 결과의 dimension은 여전히 [batch size x embedding dim] 입니다. </li>\n</ol>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/i1qs8zf162eniuu/embedding_progress3.png?dl=1\" alt=\"Embedding\" width=\"1000\"><br><br></p>\n\n<ol start=\"4\">\n<li>다음으로는 3의 결과에 cos 연산을 해주고 weight를 곱하고 bias를 더해줍니다. 그리고 해당 결과에 최종적으로 ReLU 함수를 적용해줍니다. 여기서 weight의 사이즈는 [embedding dim x convolution 결과의 크기] 이며 bias의 사이즈는 [convolution 결과의 크기] 입니다. 결국 embedding 연산의 최종 결과는 [batch size x convolution 결과의 크기]가 됩니다. Convolution 연산의 최종 결과 또한 크기가 [batch size x convolution 결과의 크기] 입니다. 이에 따라 둘은 크기가 같아지게 되고 이에 따라 element-wise 하게 곱할 수 있게 됩니다. </li>\n</ol>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/bnbxq3aj6c5kmrt/embedding_progress4.png?dl=1\" alt=\"Embedding\" width=\"800\"><br><br></p>\n\n<p>여기까지가 Embedding function의 내용입니다! </p>\n<p>여기서 약간의 의문이 들 수 있을 것이라 생각합니다. 왜 <strong>n cosine basis function</strong>을 이용할까요? 논문에서는 다양한 함수에 대해서 실험을 수행하였고 그 결과 <strong>n cosine basis function</strong>이 가장 좋은 결과를 보였다고 합니다. 여러 함수에 대해 테스트한 결과는 다음과 같습니다. </p>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/09idlykr8xr6fo9/embedding_compare.png?dl=1\" alt=\"Embedding\" width=\"800\"><br><br></p>\n\n<p><br></p>\n<h3 id=\"3-Quantile-Huber-Loss\"><a href=\"#3-Quantile-Huber-Loss\" class=\"headerlink\" title=\"3. Quantile Huber Loss\"></a>3. Quantile Huber Loss</h3><p>사실 본 논문에서는 QR-DQN에서 사용했던 <strong>Quantile Huber Loss</strong>를 그대로 이용합니다. 하지만 본 논문에서는 식을 다음과 같이 표시합니다. </p>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/ie5l2z5cwagoj86/loss.png?dl=1\" alt=\"Embedding\" width=\"500\"><br><br></p>\n\n<p>본 논문에서 N은 추정을 위해 일반 network 연산에서 sampling한 tau의 수 입니다. N’은 target distribution 도출을 위해 target network 연산에서 sampling한 tau의 수 입니다. 위 식에 대해서는 QR-DQN에서 자세히 설명하였으므로 추가적인 설명은 하지 않도록 하겠습니다. 다만 본 논문에서는 N과 N’를 다양하게 바꿔가면서 테스트한 결과를 다음과 같이 보여줍니다. </p>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/t2eeof94yhq57gb/various_n.png?dl=1\" alt=\"Various N\" width=\"700\"><br><br></p>\n\n<p>위의 결과는 6개의 atari game에 대한 human-normalized agent performance의 평균을 나타냅니다. 왼쪽의 경우 초반 10M frame을 학습하였을 때 결과이며 오른쪽의 경우 마지막 10M frame의 학습 결과입니다. 다른 알고리즘의 경우 각각 왼쪽과 오른쪽에 대한 결과가 다음과 같습니다. </p>\n<ul>\n<li>DQN: (32, 253)</li>\n<li>QR-DQN: (144, 1243)</li>\n</ul>\n<p><br></p>\n<h3 id=\"4-Action-선택\"><a href=\"#4-Action-선택\" class=\"headerlink\" title=\"4. Action 선택\"></a>4. Action 선택</h3><p> 본 논문에서  action을 선택하는 식이 QR-DQN과 비교했을 때 약간의 차이가 있습니다. 해당 식은 다음과 같습니다. </p>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/76l4gi57vasj300/action_equation.png?dl=1\" alt=\"Action equation\" width=\"500\"><br><br></p>\n\n<p>위 식은 단순히 sampling된 quantile을 통해 구한 모든 support의 값을 평균한 것입니다. </p>\n<p><br></p>\n<p> 본 논문의 알고리즘은 다음과 같습니다. </p>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/fbxxlz25l6dunxi/algorithm.png?dl=1\" alt=\"algorithm (IQN)\" width=\"700\"><br><br></p>\n\n<p><br></p>\n<h2 id=\"Result\"><a href=\"#Result\" class=\"headerlink\" title=\"Result\"></a>Result</h2><p>본 알고리즘의 성능은 Atari-57 benchmark를 통해 검증되었습니다. </p>\n<p>본 알고리즘에서 설정한 파라미터 값은 다음과 같습니다. </p>\n<h3 id=\"Risk-Sensitive-Policy\"><a href=\"#Risk-Sensitive-Policy\" class=\"headerlink\" title=\"Risk Sensitive Policy\"></a>Risk Sensitive Policy</h3><p>우선 Risk의 정도를 다양하게 한 결과가 아래와 같습니다. </p>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/8bcsng48ywep3ay/sampling_distribution_result.png?dl=1\" alt=\"Risk compare result\" width=\"1000\"><br><br></p>\n\n<p>위의 결과에서 보면 CPW(.71)과 Norm(3)의 경우 distribution의 양 끝부분에 대한 영향을 감소시킵니다. Wang(.75)의 경우 Risk-seeking, Wang(-.75), Pow(-2), CVaR(.25) 그리고 CVaR(.1)의 경우 Risk-averse 한 policy를 선택합니다. </p>\n<p>위의 결과를 비교해보면 다음과 같은 순서대로 우수한 성능을 보입니다. </p>\n<p>Risk-averse &gt; Neutral &gt; Risk-seeking </p>\n<p>이에 대해서는 대부분의 게임이 오래 살아있을 수록 받는 reward가 증가하기 때문에 불확실하면서 높은 reward를 선택하는 risk-seeking policy보다 작더라도 안정적인 reward를 선택하는 risk-averse policy가 좋은 성능을 보이는 것이라 생각할 수 있습니다. </p>\n<p><br></p>\n<h3 id=\"Atari-57-Result\"><a href=\"#Atari-57-Result\" class=\"headerlink\" title=\"Atari-57 Result\"></a>Atari-57 Result</h3><p>IQN 알고리즘을 57개의 Atari game에 테스트한 결과가 다음과 같습니다. </p>\n<p align=\"center\"><br><img src=\"https://www.dropbox.com/s/p530cm694ey9v0h/Result1.png?dl=1\" alt=\"Result1\" width=\"600\"><br><br></p>\n\n<p align=\"center\"><br><img src=\"https://www.dropbox.com/s/p4v7brznezqo4lk/Result2.png?dl=1\" alt=\"Result2\" width=\"600\"><br><br></p>\n\n<p>위의 결과에서 볼 수 있듯이 IQN은 단일 알고리즘임에도 불구하고 여러가지 기법을 조합한 Rainbow에 약간 못미치는 성능을 보입니다. 하지만 다른 단일 알고리즘인 Prioritized experience replay, C51, QR-DQN에 비해서는 모두 우수한 성능을 보이고 있습니다. 특히 QR-DQN에 비해 많은 변화가 없음에도 불구하고 기존의 단일 알고리즘 중 가장 좋은 성능을 보였던 QR-DQN보다 월등히 우수한 성능을 보이고 있습니다. </p>\n<p><br></p>\n<h3 id=\"Robustness-Test\"><a href=\"#Robustness-Test\" class=\"headerlink\" title=\"Robustness Test\"></a>Robustness Test</h3><p>다음의 결과는 같이 작업을 수행한 <code>윤승제</code>님께서 테스트해주신 결과입니다. Cartpole을 기본 파라미터로 학습을 수행한 후 <strong>Pole의 길이</strong>와 <strong>Pole의 질량</strong>을 다양하게 변경해가면서 테스트를 수행하였습니다. 알고리즘은 DQN과 IQN의 결과를 비교하였습니다.</p>\n<p align=\"center\"><br><br><img src=\"https://www.dropbox.com/s/wbvlmmc8oudw4kb/robustness.png?dl=1\" alt=\"Result_robustness\" width=\"800\"><br><br></p>\n\n<p>까맣게 될수록 500점을 내지 못하고 중간에 떨어진 것입니다. 위의 결과에서 볼 수 있듯이 DQN은 파라미터의 변화에 취약하여 값을 변경하는 것에 따라 성능이 크게 저하되는 것을 확인할 수 있었습니다. 하지만 IQN의 경우 파라미터 변화에 매우 robust하여 parameter가 변해도 대부분의 경우 500점을 얻는 것을 확인할 수 있었습니다. </p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>IQN은 QR-DQN에 비해 다음의 사항들만 변경해줬음에도 불구하고 훨씬 뛰어난 성능을 보이는 알고리즘입니다.  </p>\n<ul>\n<li>Quantile을 random sampling하고 그때의 support를 도출하여 value distribution 취득</li>\n<li>Random sampling을 이용하여 risk-sensitive policy에 따른 action 선택 가능</li>\n<li>Network의 구조 변화 (DQN과 동일하나 convolution 연산의 결과를 embedding function 결과와 element-wise하게 곱해줌) </li>\n</ul>\n<p><br></p>\n<p>IQN의 경우 다양한 deep reinforcement learning 알고리즘들에 비해 좋은 성능을 보였으며 특히 단일 알고리즘임에도 불구하고 여러 알고리즘의 결합체인 Rainbow에 약간 못미치는 성능을 보였습니다. 또한 quantile regression을 이용하여 wasserstein distance를 줄이는 만큼 distributional RL의 수렴성을 증명했다는 QR-DQN의 장점은 그대로 가지고 있는 알고리즘이라 할 수 있습니다. </p>\n<p><br></p>\n<p>IQN을 끝으로 Deepmind에서 현재까지 (2018.11.01) 발표한 distributional RL에 대한 논문들에 대한 리뷰를 마치도록 하겠습니다!! 감사합니다!! :smile:</p>\n<p><br></p>\n<h2 id=\"Implementation\"><a href=\"#Implementation\" class=\"headerlink\" title=\"Implementation\"></a>Implementation</h2><p>본 논문의 코드는 다음의 Github를 참고해주세요. </p>\n<p><a href=\"https://github.com/reinforcement-learning-kr/distributional_rl\" target=\"_blank\" rel=\"noopener\">Github</a></p>\n<p><br></p>\n<h2 id=\"Other-Posts\"><a href=\"#Other-Posts\" class=\"headerlink\" title=\"Other Posts\"></a>Other Posts</h2><p><a href=\"https://reinforcement-learning-kr.github.io/2018/09/27/Distributional_intro/\">Distributional RL 개요</a></p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/10/02/C51/\">C51</a></p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/10/22/QR-DQN/\">QR-DQN</a></p>\n<p><br></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://arxiv.org/abs/1806.06923\" target=\"_blank\" rel=\"noopener\">Implicit Quantile Networks for Distributional Reinforcement Learning</a></li>\n</ul>\n<p><br></p>\n<h2 id=\"Team\"><a href=\"#Team\" class=\"headerlink\" title=\"Team\"></a>Team</h2><p>민규식: <a href=\"https://github.com/Kyushik\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/kyushik.min\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>차금강: <a href=\"https://github.com/chagmgang\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/profile.php?id=100002147815509\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>윤승제: <a href=\"https://github.com/sjYoondeltar\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/seungje.yoon\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>김하영: <a href=\"https://github.com/hayoung-kim\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/altairyoung\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>김정대: <a href=\"https://github.com/kekmodel\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/kekmodel\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n"},{"title":"Apprenticeship Learning via Inverse Reinforcement Learning","date":"2019-01-31T15:00:00.000Z","author":"이승현","subtitle":"Inverse RL 2번째 논문","_content":"\n<center> <img src=\"../../../../img/irl/app_1.png\" width=\"850\"> </center>\n\n논문 저자 : David Silver, Guy Lever, Nicloas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller\n논문 링크 : http://people.eecs.berkeley.edu/~russell/classes/cs294/s11/readings/Abbeel+Ng:2004.pdf\nProceeding : International Conference on Machine Learning (ICML) 2014\n\n---\n\n# 1. 들어가며...\n\n- Stochastic Policy Gradient (DPG) Theorem을 제안합니다.\n    - 중요한 중요한 점은 DPG는 Expected gradient of the action-value function의 형태라는 것입니다.\n- Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient (SPG)와 동일해집니다. (Theorem 2)\n    - Theorem 2로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG에 적용할 수 있게 됩니다.\n        - 예. Sutton PG, natural gradients, actor-critic, episodic/batch methods\n- 적절한 exploration 을 위해 model-free, off-policy actor-critic algorithm 을 제안합니다\n    - action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility condition을 제공합니다. (Theorem 3)\n- DPG 는 SPG 보다 성능이 좋습니다.\n    - 특히 high dimensional action spaces를 가지는 tasks에서의 성능 향상이 큽니다.\n        - SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는 state spaces에 대해서만 평균을 취합니다.\n        - 결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됩니다.\n        - 무한정 학습을 시키면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정합니다.\n    - 기존 기법들에 비해 computation 양이 많지 않습니다.\n        - Computation 은 action dimensionality 와 policy parameters 수에 비례합니다.\n\n<br><br>\n\n# 2. Background\n\n<br>\n## 2.1 Performance objective function\n\n$$\n\\begin{align}\nJ(\\pi_{\\theta}) &= \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\pi_{\\theta}(s,a)r(s,a)da ds = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[r(s,a)]\n\\end{align}\n$$\n\n<br>\n## 2.2 SPG Theorem\n- State distribution $ \\rho^{\\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없습니다.\n$$\\begin{eqnarray}\\nabla_{\\theta}J(\\pi_{\\theta}) &=& \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\\\ &=& E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]\n\\end{eqnarray}$$\n\n<br>\n## 2.3 Stochastic Actor-Critic Algorithms\n- Actor와 Critic이 번갈아가면서 동작하며 stochastic policy를 최적화하는 기법입니다.\n- Actor: $ Q^{\\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $를 이용해 stochastic policy gradient를 ascent하는 방향으로 policy parameter $ \\theta $를 업데이트함으로써 stochastic policy를 발전시킵니다.\n    - $ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $\n- Critic: SARSA나 Q-learning같은 Temporal-difference (TD) learning을 이용해 action-value function의 parameter, $ w $를 업데이트함으로써 $ Q^w(s,a) $가 $ Q^{\\pi}(s,a) $과 유사해지도록 합니다.\n- 실제 값인 $ Q^{\\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $를 사용하게 되면, 일반적으로 bias가 발생합니다. 하지만, compatible condition에 부합하는 $ Q^w(s,a) $를 사용하게 되면, bias가 발생하지 않습니다.\n\n<br>\n## 2.4 Off-policy Actor-Critic\n- Distinct behavior policy $ \\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic\n- Performance objective function\n    - $\\begin{eqnarray}\n        J_{\\beta}(\\pi_{\\theta}) \n        &=& \\int_{S}\\rho^{\\beta}(s)V^{\\pi}(s)ds \\nonumber \\\\\\\\\n        &=& \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads\n        \\end{eqnarray} $\n- off-policy policy gradient\n    - $ \\begin{eqnarray}\n        \\nabla_{\\theta}J_{\\beta}(\\pi_{\\theta}) &\\approx& \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\end{eqnarray} $\n        $=E_{s \\sim \\rho^{\\beta}, a \\sim \\beta}[\\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)}\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]$\n    - off-policy policy gradient 식에서의 물결 표시는 [Degris, 2012b](https://arxiv.org/abs/1205.4839) 논문에 근거합니다.\n        - Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같습니다. (빨간색 상자에 있는 항목을 삭제함으로써 근사합니다.)\n            - <img src=\"https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1\" width=500px>\n        - [Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\\nabla_{u}𝑄^{\\pi,\\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮습니다.\n            - <img src=\"https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1\" width=500px>\n    - off-policy policy gradient 식에서 $ \\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)} $는 importance sampling ratio 입니다.\n        - off-policy actor-critic에서는 $ \\beta $에 의해 샘플링된 trajectory를 이용해서 stochastic policy $ \\pi $를 예측하는 것이기 때문에 importance sampling이 필요합니다.\n\n\n<br><br>\n\n# 3. Gradient of Deterministic Policies\n\n<br>\n## 3.1 Regularity Conditions\n- 어떠한 이론이 성립하기 위한 전제 조건\n- Regularity conditions A.1\n    - $ p(s'|s,a), \\nabla_{a}p(s'|s,a), \\mu_{\\theta}(s), \\nabla_{\\theta}\\mu_{\\theta}(s), r(s,a), \\nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s' $ and $ x $.\n- regularity conditions A.2\n    - There exists a $ b $ and $ L $ such that $ \\sup_{s}p_{1}(s) < b $, $ \\sup_{a,s,s'}p(s'|s,a) < b $, $ \\sup_{a,s}r(s,a) < b $, $ \\sup_{a,s,s'}\\|\\nabla_{a}p(s'|s,a)\\| < L $, and $ \\sup_{a,s}\\|\\nabla_{a}r(s,a)\\| < L $.\n\n<br>\n## 3.2 Deterministic Policy Gradient Theorem\n- Deterministic policy\n    - $ \\mu_{\\theta} : S \\to A $ with parameter vector $ \\theta \\in \\mathbb{R}^n $\n- Probability distribution\n    - $ p(s \\to s', t, \\mu) $\n- Discounted state distribution\n    - $ \\rho^{\\mu}(s) $\n- Performance objective\n\n$$\nJ(\\mu_{\\theta}) = E[r^{\\gamma}_{1} | \\mu] \n$$\n\n$$\n= \\int_{S}\\rho^{\\mu}(s)r(s,\\mu_{\\theta}(s))ds \n= E_{s \\sim \\rho^{\\mu}}[r(s,\\mu_{\\theta}(s))]\n$$\n\n- DPG Theorem\n    - MDP 가 A.1 만족한다면, 아래 식이 성립합니다.\n    $\\nabla_{\\theta}J(\\mu_{\\theta}) = \\int_{S}\\rho^{\\mu}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}ds \\nonumber$\n    $= E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]   \\nonumber $ \n    \n\t- DPG는 state space에 대해서만 평균을 취하면 되기에, state와 action space 모두에 대해 평균을 취해야 하는 SPG에 비해 data efficiency가 좋습니다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 됩니다.\n\n<br>    \n## 3.3 DPG 형태에 대한 informal intuition\n- Generalized policy iteration\n    - 정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration\n        - 위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴합니다.\n- 정책 평가\n    - action-value function $ Q^{\\pi}(s,a) $ or $ Q^{\\mu}(s,a) $을 estimate 하는 것 입니다.\n- 정책 발전\n    - 위 estimated action-value function에 따라 정책을 update하는 것 입니다.\n    - 주로 action-value function에 대한 greedy maximisation을 사용합니다.\n        - $ \\mu^{k+1}(s) = \\arg\\max\\limits_{a}Q^{\\mu^{k}}(s,a) $\n        - greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이로 인해 continuous action spaces에서 계산량이 급격히 늘어납니다.\n    - 그래서 policy gradient 방법이 나옵니다.\n        - policy 를 $ \\theta $에 대해서 parameterize 합니다.\n        - 매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $마다 policy parameter를 action-value function $ Q $의 $ \\theta $에 대한 gradient $ \\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s)) $ 방향으로 proportional하게 update 합니다.\n        - 하지만 각 state는 다른 방향을 제시할 수 있기에, state distribution $ \\rho^{\\mu}(s) $에 대한 기대값을 취해 policy parameter를 update 할 수도 있습니다.\n            - $ \\theta^{k+1} = \\theta^{k} + \\alpha E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s))] $\n        - 이는 chain-rule에 따라 아래와 같이 분리될 수 있습니다.\n            - $ \\theta^{k+1} = \\theta^{k} + \\alpha E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu^{k}}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $ (7)\n            - chain rule: $ \\frac{\\partial Q}{\\partial \\theta} = \\frac{\\partial a}{\\partial \\theta} \\frac{\\partial Q}{\\partial a} $\n        - 하지만 state distribution $ \\rho^{\\mu} $은 정책에 dependent 합니다.\n            - 정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state가 변하기 때문에 state distribution이 변하게 됩니다.\n        - 그렇기에 정책 update 시 state distribution에 대한 gradient를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있습니다.\n        - deterministic policy gradient theorem은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update해도 performance objective의 gradient를 정확하게 따름을 의미합니다.\n\n<br>\n## 3.4 DPG는 SPG의 limiting case\n- stochastic policy parameterization\n    - $ \\pi_{\\mu_{\\theta},\\sigma} $ by a deterministic policy $ \\mu_{\\theta} : S \\to A $ and a variance parameter $ \\sigma $\n    - $ \\sigma = 0 $ 이면, $ \\pi_{\\mu_{\\theta},\\sigma} \\equiv \\mu_{\\theta} $\n- Theorem 2. Policy의 variance가 0에 수렴하면, 즉, $ \\sigma \\to 0 $, stochastic policy gradient와 deterministic policy gradient는 동일해집니다.\n    - 조건: stochastic policy $ \\pi_{\\mu_{\\theta},\\sigma} = \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $\n        - $ \\sigma $는 variance입니다.\n        - $ \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $는 condition B.1을 만족합니다.\n        - MDP는 conditions A.1과 A.2를 만족합니다.\n    - 결과:\n        - $ \\lim\\limits_{\\sigma\\downarrow0}\\nabla_{\\theta}J(\\pi_{\\mu_{\\theta},\\sigma}) = \\nabla_{\\theta}J(\\mu_{\\theta})  $\n            - 좌변은 standard stochastic gradient이며, 우변은 deterministic gradient입니다.\n    - 의미:\n        - deterministic policy gradient는 stochastic policy gradient의 특수 case 입니다.\n        - 기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있습니다.\n            - 기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)\n\n<br><br>\n\n# 4. Deterministic Actor-Critic Algorithms\n1. SARSA critic를 이용한 on-policy actor-critic\n    - 단점\n        - deterministic policy에 의해 행동하면 exploration이 잘 되지 않기에, sub-optimal에 빠지기 쉽습니다.\n    - 목적\n        - 교훈/정보제공\n        - 환경에서 충분한 noise를 제공하여 exploration을 시킬 수 있다면, deterministic policy를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있습니다.\n            - 예. 바람이 agent의 행동에 영향(noise)을 줌\n    - Remind: 살사(SARSA) update rule\n        - $ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $\n    - Algorithm\n        - Critic은 MSE를 $ \\bf minimize $하는 방향, 즉, action-value function을 stochastic gradient $ \\bf descent $ 방법으로 update합니다.\n            - $ MSE = [Q^{\\mu}(s,a) - Q^{w}(s,a)]^2 $\n                - critic은 실제 $ Q^{\\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $로 대체하여 action-value function을 estimate하며, 이 둘 간 mean square error를 최소화하는 것이 목표입니다.\n            - $ \\nabla_{w}MSE \\approx -2 * [r + \\gamma Q^{w}(s',a') - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $\n                - $ \\nabla_{w}MSE = -2 * [Q^{\\mu}(s,a) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $\n                - $ Q^{\\mu}(s,a) $ 를 $ r + \\gamma Q^{w}(s',a') $로 대체\n                    - $ Q^{\\mu}(s,a) = r + \\gamma Q^{\\mu}(s',a') $\n            - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $\n                - $w_{t+1} = w_{t} - \\alpha_{w}\\nabla_{w}MSE  \\nonumber$\n                $ \\approx w_{t} - \\alpha_{w} * (-2 * [r + \\gamma Q^{w}(s',a') - Q^{w}(s,a)] \\nabla_{w}Q^{w}(s,a)$\n                - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $\n        - Actor는 식(9)에 따라 보상이 $ \\bf maximize $되는 방향, 즉, deterministic policy를 stochastic gradient $ \\bf ascent $ 방법으로 update합니다.\n            - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $\n2. Q-learning 을 이용한 off-policy actor-critic\n    - stochastic behavior policy $ \\beta(a|s) $에 의해 생성된 trajectories로부터 deterministic target policy $ \\mu_{\\theta}(s) $를 학습하는 off-policy actor-critic입니다\n    - performance objective\n        - $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)V^{\\mu}(s)ds \\nonumber \\\\\\\\$\n          $= \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds \\nonumber \\\\\\\\$\n          $= E_{s \\sim \\rho^{\\beta}}[Q^{\\mu}(s,\\mu_{\\theta}(s))]$\n    - off-policy deterministic policy gradient\n        - $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n            - 논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됩니다.\n            - $ \\begin{eqnarray}\n                \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) &\\approx& \\int_{S}\\rho^{\\beta}(s)\\nabla_{\\theta}\\mu_{\\theta}(a|s)Q^{\\mu}(s,a)ds \\nonumber \\\\\n                &=& E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]\n                \\end{eqnarray} $\n            - 근거: Action이 deterministic하기에 stochastic 경우와는 다르게 performance objective에서 action에 대해 평균을 구할 필요가 없습니다. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b]에서 처럼 곱의 미분을 통해 생기는 action-value function에 대한 gradient term을 생략할 필요가 사라집니다.\n    - Remind: 큐러닝(Q-learning) update rule\n        - $ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $\n    - algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)\n        - 살사를 이용한 on-policy deterministic actor-critic과 아래 부분을 제외하고는 같습니다.\n            - target policy는 $ \\beta(a|s) $에 의해 생성된 trajectories를 통해 학습합니다.\n            - 업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $이 아니라 정책으로부터 나온 행동 값 $ \\mu_{\\theta}(s_{t+1}) $을 사용합니다.\n                - $ \\mu_{\\theta}(s_{t+1}) $ 은 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $\n    - Stochastic off-policy actor-critic은 대개 actor와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient에선 importance sampling이 필요없습니다.\n        - Actor 는 deterministic 이기에 sampling 자체가 필요없습니다.\n            - Stochastic policy인 경우, Actor에서 importance sampling이 필요한 이유는 상태 $ s $에서의 가치 함수 값 $ V^{\\pi}(s) $을 estimate하기 위해 $ \\pi $가 아니라 $ \\beta $에 따라 sampling을 한 후, 평균을 내기 때문입니다.\n            - 하지만 deterministic policy인 경우, 상태 $ s $에서의 가치 함수 값 $ V^{\\pi}(s) = Q^{\\pi}(s,\\mu_{\\theta}) $ 즉, action이 상태 s에 대해 deterministic이기에 sampling을 통해 estimate할 필요가 없고, 따라서 importance sampling도 필요없어집니다.\n            - stochastic vs. deterministic performance objective\n                - stochastic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads $\n                - deterministic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds $\n        - Critic이 사용하는 Q-learning은 importance sampling이 필요없는 off policy 알고리즘입니다.\n            - Q-learning도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있습니다.\n3. compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic\n    - 위 살사/Q-learning 기반 on/off-policy는 아래와 같은 문제가 존재합니다.\n        - function approximator에 의한 bias\n            - 일반적으로 $ Q^{\\mu}(s,a) $ 를 $ Q^{w}(s,a) $로 대체하여 deterministic policy gradient를 구하면, 그 gradient는 ascent하는 방향이 아닐 수도 있습니다.\n        - off-policy learning에 의한 instabilities\n    - 그래서 stochastic처럼 $ \\nabla_{a}Q^{\\mu}(s,a) $를 $ \\nabla_{a}Q^{w}(s,a) $로 대체해도 deterministic policy gradient에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $를 찾아야 합니다.\n    - Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $는 deterministic policy $ \\mu_{\\theta}(s) $와 compatible 합니다. 즉, $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n        - $ \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $입니다.\n        - $ w $는 $ MSE(\\theta, w) = E[\\epsilon(s;\\theta,w)^{\\top}\\epsilon(s;\\theta,w)] $를 최소화합니다.\n            - $ \\epsilon(s;\\theta,w) = \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} - \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}  $\n    - Theorem 3은 on-policy 뿐만 아니라 off-policy에도 적용 가능합니다.\n    - $ Q^{w}(s,a) = (a-\\mu_{\\theta}(s))^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top} w + V^{v}(s) $\n        - 어떠한 deterministic policy에 대해서도 위 형태와 같은 compatible function approximator가 존재합니다.\n        - 앞의 term은 advantage를, 뒤의 term은 value로 볼 수 있습니다.\n    - $ Q^{w}(s,a) = \\phi(s,a)^{\\top} w + v^{\\top}\\phi(s) $\n        - 정의 : $ \\phi(s,a) \\overset{\\underset{\\mathrm{def}}{}}{=} \\nabla_{\\theta}\\mu_{\\theta}(s)(a-\\mu_{\\theta}(s)) $\n        - 일례 : $ V^{v}(s) = v^{\\top}\\phi(s) $\n        - Theorem 3 만족 여부\n            - 첫 번째 조건 만족합니다.\n            - 두 번째 조건은 대강 만족합니다.\n                - $ \\nabla_{a}Q^{\\mu}(s,a) $에 대한 unbiased sample을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $를 학습합니다.\n                - 이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \\approx Q^{\\mu}(s,a) $인 reasonable solution을 찾을 수 있기에 대강 $ \\nabla_{a}Q^{w}(s,a) \\approx \\nabla_{a}Q^{\\mu}(s,a) $이 될 것입니다.\n        - action-value function에 대한 linear function approximator는 큰 값을 가지는 actions에 대해선 diverge할 수 있어 global하게 action-values 예측하기에는 좋지 않지만, local critic에 사용할 때는 매우 유용합니다.\n            - 즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\\mu_{\\theta}(s)+\\delta) = \\delta^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $로, diverge하지 않고, 값을 얻을 수 있습니다.\n    - COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)\n        - Critic: 실제 action-value function에 대한 linear function approximator인 $ Q^{w}(s,a) = \\phi(s,a)^{\\top}w $를 estimate합니다.\n            - $ \\phi(s,a) = a^{\\top}\\nabla_{\\theta}\\mu_{\\theta} $\n            - Behavior policy $ \\beta(a|s) $로부터 얻은 samples를 이용하여 Q-learning이나 gradient Q-learning과 같은 off-policy algorithm으로 학습 가능합니다.\n        - Actor: estimated action-value function에 대한 gradient를 $ \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $로 치환 후, 정책을 업데이트합니다.\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $\n    - off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있습니다.\n        - $ \\mu_{\\theta}(s_{t+1}) $이 diverge할 수도 있기 때문으로 판단됩니다.\n        - 그렇기에 simple Q-learning 대신 다른 기법이 필요합니다.\n    - 그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm을 제안합니다.\n        - gradient temporal-difference learning에 기반한 기법들은 true gradient descent algorithm이며, converge가 보장됩니다. (Sutton, 2009)\n            - 기본 아이디어는 stochastic gradient descent로 mean-squared projected Bellman error (MSPBE)를 최소화하는 것입니다.\n            - critic이 actor보다 빠른 time-scale로 update되도록 step size들을 잘 조절하면, critic은 MSPBE를 최소화하는 parameters로 converge하게 됩니다.\n            - critic에 gradient temporal-difference learning의 일종인 gradient Q-learning을 사용한 논문입니다. (Maei, 2010)\n    - COPDAC-GQ algorithm\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) - \\alpha_{w}\\gamma\\phi(s_{t+1}, \\mu_{\\theta}(s_{t+1}))(\\phi(s_{t},a_{t})^{\\top} u_{t}) $\n        - $ v_{t+1} = v_{t} + \\alpha_{v}\\delta_{t}\\phi(s_{t}) - \\alpha_{v}\\gamma\\phi(s_{t+1})(\\phi(s_{t},a_{t})^{\\top} u_{t}) $\n        - $ u_{t+1} = u_{t} + \\alpha_{u}(\\delta_{t}-\\phi(s_{t}, a_{t})^{\\top} u_{t})\\phi(s_{t}, a_{t}) $\n    - stochastic actor-critic과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $입니다.\n        - m은 action dimensions, n은 number of policy parameters\n    - Natural policy gradient를 이용해 deterministic policies를 찾을 수 있습니다.\n        - $ M(\\theta)^{-1}\\nabla_{\\theta}J(\\mu_{\\theta}) $은 any metric $ M(\\theta) $에 대한 performance objective (식(14))의 steepest ascent direction 입니다. (Toussaint, 2012)\n        - Natural gradient는 Fisher information metric $ M_{\\pi}(\\theta) $에 대한 steepest ascent direction 입니다.\n            -  $ M_{\\pi}(\\theta) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}] $\n            - Fisher information metric은 policy reparameterization에 대해 불변입니다. (Bagnell, 2003)\n        - deterministic policies에 대한 metric으로 $ M_{\\mu}(\\theta) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}] $을 사용합니다.\n        \t- 이는 variance가 0인 policy에 대한 Fisher information metric으로 볼 수 있습니다.\n        \t- $ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\theta}(a\\vert s)}$에서 policy variance가 0이면, 특정 s에 대한 $ \\pi_{\\theta}(a|s)$만 1이 되고, 나머지는 0입니다.\n        - deterministic policy gradient theorem과 compatible function approximation을 결합하면 $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] $이 됩니다.\n            - $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n            - $ \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)} \\approx \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $\n        - 그렇기에 steepest ascent direction은 $ M_{\\mu}(\\theta)^{-1}\\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = w $이 됩니다.\n            - $ E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}]^{-1}E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] = w $\n        - 이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ에서 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $ 식을 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}w_{t} $로 바꿔주기만 하면 됩니다.\n\n<br><br>\n\n# 5. Experiments\n\n<br>\n## 5.1. Continuous Bandit\n- Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행합니다.\n    - Action dimension이 커질수록 성능 차이가 심합니다.\n    - 빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있습니다.\n    - <img src=\"https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1\">\n\n<br>\n## 5.2. Continuous Reinforcement Learning\n- COPDAC-Q, SAC, off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행합니다.\n    - COPDAC-Q의 성능이 약간 더 좋습니다.\n    - COPDAC-Q의 학습이 더 빨리 이뤄집니다.\n    - <img src=\"https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1\">\n\n<br>\n## 5.3. Octopus Arm\n- 목표: 6 segments octopus arm (20 action dimensions & 50 state dimensions)을 control하여 target을 맞추는 것입니다.\n    - COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤입니다.\n    - <img src=\"https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1\" width=600px>\n    - 기존 기법들은 action spaces 혹은 action과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않습니다.\n        - 기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 보여줬으면 하지만 실험을 하지 않았습니다.\n    - 8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보입니다.\n- [참고] Octopus Arm 이란?\n    - [OctopusArm Youtube Link](https://www.youtube.com/watch?v=AxeeHif0euY)\n    - <img src=\"https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1\">\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [Sutton PG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/)\n\n<br>\n\n# 다음으로\n\n## [DDPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/)","source":"_posts/2_app.md","raw":"---\ntitle: Apprenticeship Learning via Inverse Reinforcement Learning\ndate: 2019-02-01\ntags: [\"프로젝트\", \"GAIL하자!\"]\ncategories: 프로젝트\nauthor: 이승현\nsubtitle: Inverse RL 2번째 논문\n---\n\n<center> <img src=\"../../../../img/irl/app_1.png\" width=\"850\"> </center>\n\n논문 저자 : David Silver, Guy Lever, Nicloas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller\n논문 링크 : http://people.eecs.berkeley.edu/~russell/classes/cs294/s11/readings/Abbeel+Ng:2004.pdf\nProceeding : International Conference on Machine Learning (ICML) 2014\n\n---\n\n# 1. 들어가며...\n\n- Stochastic Policy Gradient (DPG) Theorem을 제안합니다.\n    - 중요한 중요한 점은 DPG는 Expected gradient of the action-value function의 형태라는 것입니다.\n- Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient (SPG)와 동일해집니다. (Theorem 2)\n    - Theorem 2로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG에 적용할 수 있게 됩니다.\n        - 예. Sutton PG, natural gradients, actor-critic, episodic/batch methods\n- 적절한 exploration 을 위해 model-free, off-policy actor-critic algorithm 을 제안합니다\n    - action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility condition을 제공합니다. (Theorem 3)\n- DPG 는 SPG 보다 성능이 좋습니다.\n    - 특히 high dimensional action spaces를 가지는 tasks에서의 성능 향상이 큽니다.\n        - SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는 state spaces에 대해서만 평균을 취합니다.\n        - 결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됩니다.\n        - 무한정 학습을 시키면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정합니다.\n    - 기존 기법들에 비해 computation 양이 많지 않습니다.\n        - Computation 은 action dimensionality 와 policy parameters 수에 비례합니다.\n\n<br><br>\n\n# 2. Background\n\n<br>\n## 2.1 Performance objective function\n\n$$\n\\begin{align}\nJ(\\pi_{\\theta}) &= \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\pi_{\\theta}(s,a)r(s,a)da ds = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[r(s,a)]\n\\end{align}\n$$\n\n<br>\n## 2.2 SPG Theorem\n- State distribution $ \\rho^{\\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없습니다.\n$$\\begin{eqnarray}\\nabla_{\\theta}J(\\pi_{\\theta}) &=& \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\\\ &=& E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]\n\\end{eqnarray}$$\n\n<br>\n## 2.3 Stochastic Actor-Critic Algorithms\n- Actor와 Critic이 번갈아가면서 동작하며 stochastic policy를 최적화하는 기법입니다.\n- Actor: $ Q^{\\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $를 이용해 stochastic policy gradient를 ascent하는 방향으로 policy parameter $ \\theta $를 업데이트함으로써 stochastic policy를 발전시킵니다.\n    - $ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $\n- Critic: SARSA나 Q-learning같은 Temporal-difference (TD) learning을 이용해 action-value function의 parameter, $ w $를 업데이트함으로써 $ Q^w(s,a) $가 $ Q^{\\pi}(s,a) $과 유사해지도록 합니다.\n- 실제 값인 $ Q^{\\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $를 사용하게 되면, 일반적으로 bias가 발생합니다. 하지만, compatible condition에 부합하는 $ Q^w(s,a) $를 사용하게 되면, bias가 발생하지 않습니다.\n\n<br>\n## 2.4 Off-policy Actor-Critic\n- Distinct behavior policy $ \\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic\n- Performance objective function\n    - $\\begin{eqnarray}\n        J_{\\beta}(\\pi_{\\theta}) \n        &=& \\int_{S}\\rho^{\\beta}(s)V^{\\pi}(s)ds \\nonumber \\\\\\\\\n        &=& \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads\n        \\end{eqnarray} $\n- off-policy policy gradient\n    - $ \\begin{eqnarray}\n        \\nabla_{\\theta}J_{\\beta}(\\pi_{\\theta}) &\\approx& \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\end{eqnarray} $\n        $=E_{s \\sim \\rho^{\\beta}, a \\sim \\beta}[\\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)}\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]$\n    - off-policy policy gradient 식에서의 물결 표시는 [Degris, 2012b](https://arxiv.org/abs/1205.4839) 논문에 근거합니다.\n        - Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같습니다. (빨간색 상자에 있는 항목을 삭제함으로써 근사합니다.)\n            - <img src=\"https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1\" width=500px>\n        - [Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\\nabla_{u}𝑄^{\\pi,\\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮습니다.\n            - <img src=\"https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1\" width=500px>\n    - off-policy policy gradient 식에서 $ \\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)} $는 importance sampling ratio 입니다.\n        - off-policy actor-critic에서는 $ \\beta $에 의해 샘플링된 trajectory를 이용해서 stochastic policy $ \\pi $를 예측하는 것이기 때문에 importance sampling이 필요합니다.\n\n\n<br><br>\n\n# 3. Gradient of Deterministic Policies\n\n<br>\n## 3.1 Regularity Conditions\n- 어떠한 이론이 성립하기 위한 전제 조건\n- Regularity conditions A.1\n    - $ p(s'|s,a), \\nabla_{a}p(s'|s,a), \\mu_{\\theta}(s), \\nabla_{\\theta}\\mu_{\\theta}(s), r(s,a), \\nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s' $ and $ x $.\n- regularity conditions A.2\n    - There exists a $ b $ and $ L $ such that $ \\sup_{s}p_{1}(s) < b $, $ \\sup_{a,s,s'}p(s'|s,a) < b $, $ \\sup_{a,s}r(s,a) < b $, $ \\sup_{a,s,s'}\\|\\nabla_{a}p(s'|s,a)\\| < L $, and $ \\sup_{a,s}\\|\\nabla_{a}r(s,a)\\| < L $.\n\n<br>\n## 3.2 Deterministic Policy Gradient Theorem\n- Deterministic policy\n    - $ \\mu_{\\theta} : S \\to A $ with parameter vector $ \\theta \\in \\mathbb{R}^n $\n- Probability distribution\n    - $ p(s \\to s', t, \\mu) $\n- Discounted state distribution\n    - $ \\rho^{\\mu}(s) $\n- Performance objective\n\n$$\nJ(\\mu_{\\theta}) = E[r^{\\gamma}_{1} | \\mu] \n$$\n\n$$\n= \\int_{S}\\rho^{\\mu}(s)r(s,\\mu_{\\theta}(s))ds \n= E_{s \\sim \\rho^{\\mu}}[r(s,\\mu_{\\theta}(s))]\n$$\n\n- DPG Theorem\n    - MDP 가 A.1 만족한다면, 아래 식이 성립합니다.\n    $\\nabla_{\\theta}J(\\mu_{\\theta}) = \\int_{S}\\rho^{\\mu}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}ds \\nonumber$\n    $= E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]   \\nonumber $ \n    \n\t- DPG는 state space에 대해서만 평균을 취하면 되기에, state와 action space 모두에 대해 평균을 취해야 하는 SPG에 비해 data efficiency가 좋습니다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 됩니다.\n\n<br>    \n## 3.3 DPG 형태에 대한 informal intuition\n- Generalized policy iteration\n    - 정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration\n        - 위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴합니다.\n- 정책 평가\n    - action-value function $ Q^{\\pi}(s,a) $ or $ Q^{\\mu}(s,a) $을 estimate 하는 것 입니다.\n- 정책 발전\n    - 위 estimated action-value function에 따라 정책을 update하는 것 입니다.\n    - 주로 action-value function에 대한 greedy maximisation을 사용합니다.\n        - $ \\mu^{k+1}(s) = \\arg\\max\\limits_{a}Q^{\\mu^{k}}(s,a) $\n        - greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이로 인해 continuous action spaces에서 계산량이 급격히 늘어납니다.\n    - 그래서 policy gradient 방법이 나옵니다.\n        - policy 를 $ \\theta $에 대해서 parameterize 합니다.\n        - 매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $마다 policy parameter를 action-value function $ Q $의 $ \\theta $에 대한 gradient $ \\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s)) $ 방향으로 proportional하게 update 합니다.\n        - 하지만 각 state는 다른 방향을 제시할 수 있기에, state distribution $ \\rho^{\\mu}(s) $에 대한 기대값을 취해 policy parameter를 update 할 수도 있습니다.\n            - $ \\theta^{k+1} = \\theta^{k} + \\alpha E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s))] $\n        - 이는 chain-rule에 따라 아래와 같이 분리될 수 있습니다.\n            - $ \\theta^{k+1} = \\theta^{k} + \\alpha E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu^{k}}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $ (7)\n            - chain rule: $ \\frac{\\partial Q}{\\partial \\theta} = \\frac{\\partial a}{\\partial \\theta} \\frac{\\partial Q}{\\partial a} $\n        - 하지만 state distribution $ \\rho^{\\mu} $은 정책에 dependent 합니다.\n            - 정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state가 변하기 때문에 state distribution이 변하게 됩니다.\n        - 그렇기에 정책 update 시 state distribution에 대한 gradient를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있습니다.\n        - deterministic policy gradient theorem은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update해도 performance objective의 gradient를 정확하게 따름을 의미합니다.\n\n<br>\n## 3.4 DPG는 SPG의 limiting case\n- stochastic policy parameterization\n    - $ \\pi_{\\mu_{\\theta},\\sigma} $ by a deterministic policy $ \\mu_{\\theta} : S \\to A $ and a variance parameter $ \\sigma $\n    - $ \\sigma = 0 $ 이면, $ \\pi_{\\mu_{\\theta},\\sigma} \\equiv \\mu_{\\theta} $\n- Theorem 2. Policy의 variance가 0에 수렴하면, 즉, $ \\sigma \\to 0 $, stochastic policy gradient와 deterministic policy gradient는 동일해집니다.\n    - 조건: stochastic policy $ \\pi_{\\mu_{\\theta},\\sigma} = \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $\n        - $ \\sigma $는 variance입니다.\n        - $ \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $는 condition B.1을 만족합니다.\n        - MDP는 conditions A.1과 A.2를 만족합니다.\n    - 결과:\n        - $ \\lim\\limits_{\\sigma\\downarrow0}\\nabla_{\\theta}J(\\pi_{\\mu_{\\theta},\\sigma}) = \\nabla_{\\theta}J(\\mu_{\\theta})  $\n            - 좌변은 standard stochastic gradient이며, 우변은 deterministic gradient입니다.\n    - 의미:\n        - deterministic policy gradient는 stochastic policy gradient의 특수 case 입니다.\n        - 기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있습니다.\n            - 기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)\n\n<br><br>\n\n# 4. Deterministic Actor-Critic Algorithms\n1. SARSA critic를 이용한 on-policy actor-critic\n    - 단점\n        - deterministic policy에 의해 행동하면 exploration이 잘 되지 않기에, sub-optimal에 빠지기 쉽습니다.\n    - 목적\n        - 교훈/정보제공\n        - 환경에서 충분한 noise를 제공하여 exploration을 시킬 수 있다면, deterministic policy를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있습니다.\n            - 예. 바람이 agent의 행동에 영향(noise)을 줌\n    - Remind: 살사(SARSA) update rule\n        - $ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $\n    - Algorithm\n        - Critic은 MSE를 $ \\bf minimize $하는 방향, 즉, action-value function을 stochastic gradient $ \\bf descent $ 방법으로 update합니다.\n            - $ MSE = [Q^{\\mu}(s,a) - Q^{w}(s,a)]^2 $\n                - critic은 실제 $ Q^{\\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $로 대체하여 action-value function을 estimate하며, 이 둘 간 mean square error를 최소화하는 것이 목표입니다.\n            - $ \\nabla_{w}MSE \\approx -2 * [r + \\gamma Q^{w}(s',a') - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $\n                - $ \\nabla_{w}MSE = -2 * [Q^{\\mu}(s,a) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $\n                - $ Q^{\\mu}(s,a) $ 를 $ r + \\gamma Q^{w}(s',a') $로 대체\n                    - $ Q^{\\mu}(s,a) = r + \\gamma Q^{\\mu}(s',a') $\n            - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $\n                - $w_{t+1} = w_{t} - \\alpha_{w}\\nabla_{w}MSE  \\nonumber$\n                $ \\approx w_{t} - \\alpha_{w} * (-2 * [r + \\gamma Q^{w}(s',a') - Q^{w}(s,a)] \\nabla_{w}Q^{w}(s,a)$\n                - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $\n        - Actor는 식(9)에 따라 보상이 $ \\bf maximize $되는 방향, 즉, deterministic policy를 stochastic gradient $ \\bf ascent $ 방법으로 update합니다.\n            - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $\n2. Q-learning 을 이용한 off-policy actor-critic\n    - stochastic behavior policy $ \\beta(a|s) $에 의해 생성된 trajectories로부터 deterministic target policy $ \\mu_{\\theta}(s) $를 학습하는 off-policy actor-critic입니다\n    - performance objective\n        - $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)V^{\\mu}(s)ds \\nonumber \\\\\\\\$\n          $= \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds \\nonumber \\\\\\\\$\n          $= E_{s \\sim \\rho^{\\beta}}[Q^{\\mu}(s,\\mu_{\\theta}(s))]$\n    - off-policy deterministic policy gradient\n        - $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n            - 논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됩니다.\n            - $ \\begin{eqnarray}\n                \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) &\\approx& \\int_{S}\\rho^{\\beta}(s)\\nabla_{\\theta}\\mu_{\\theta}(a|s)Q^{\\mu}(s,a)ds \\nonumber \\\\\n                &=& E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]\n                \\end{eqnarray} $\n            - 근거: Action이 deterministic하기에 stochastic 경우와는 다르게 performance objective에서 action에 대해 평균을 구할 필요가 없습니다. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b]에서 처럼 곱의 미분을 통해 생기는 action-value function에 대한 gradient term을 생략할 필요가 사라집니다.\n    - Remind: 큐러닝(Q-learning) update rule\n        - $ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $\n    - algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)\n        - 살사를 이용한 on-policy deterministic actor-critic과 아래 부분을 제외하고는 같습니다.\n            - target policy는 $ \\beta(a|s) $에 의해 생성된 trajectories를 통해 학습합니다.\n            - 업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $이 아니라 정책으로부터 나온 행동 값 $ \\mu_{\\theta}(s_{t+1}) $을 사용합니다.\n                - $ \\mu_{\\theta}(s_{t+1}) $ 은 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $\n    - Stochastic off-policy actor-critic은 대개 actor와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient에선 importance sampling이 필요없습니다.\n        - Actor 는 deterministic 이기에 sampling 자체가 필요없습니다.\n            - Stochastic policy인 경우, Actor에서 importance sampling이 필요한 이유는 상태 $ s $에서의 가치 함수 값 $ V^{\\pi}(s) $을 estimate하기 위해 $ \\pi $가 아니라 $ \\beta $에 따라 sampling을 한 후, 평균을 내기 때문입니다.\n            - 하지만 deterministic policy인 경우, 상태 $ s $에서의 가치 함수 값 $ V^{\\pi}(s) = Q^{\\pi}(s,\\mu_{\\theta}) $ 즉, action이 상태 s에 대해 deterministic이기에 sampling을 통해 estimate할 필요가 없고, 따라서 importance sampling도 필요없어집니다.\n            - stochastic vs. deterministic performance objective\n                - stochastic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads $\n                - deterministic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds $\n        - Critic이 사용하는 Q-learning은 importance sampling이 필요없는 off policy 알고리즘입니다.\n            - Q-learning도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있습니다.\n3. compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic\n    - 위 살사/Q-learning 기반 on/off-policy는 아래와 같은 문제가 존재합니다.\n        - function approximator에 의한 bias\n            - 일반적으로 $ Q^{\\mu}(s,a) $ 를 $ Q^{w}(s,a) $로 대체하여 deterministic policy gradient를 구하면, 그 gradient는 ascent하는 방향이 아닐 수도 있습니다.\n        - off-policy learning에 의한 instabilities\n    - 그래서 stochastic처럼 $ \\nabla_{a}Q^{\\mu}(s,a) $를 $ \\nabla_{a}Q^{w}(s,a) $로 대체해도 deterministic policy gradient에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $를 찾아야 합니다.\n    - Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $는 deterministic policy $ \\mu_{\\theta}(s) $와 compatible 합니다. 즉, $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n        - $ \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $입니다.\n        - $ w $는 $ MSE(\\theta, w) = E[\\epsilon(s;\\theta,w)^{\\top}\\epsilon(s;\\theta,w)] $를 최소화합니다.\n            - $ \\epsilon(s;\\theta,w) = \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} - \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}  $\n    - Theorem 3은 on-policy 뿐만 아니라 off-policy에도 적용 가능합니다.\n    - $ Q^{w}(s,a) = (a-\\mu_{\\theta}(s))^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top} w + V^{v}(s) $\n        - 어떠한 deterministic policy에 대해서도 위 형태와 같은 compatible function approximator가 존재합니다.\n        - 앞의 term은 advantage를, 뒤의 term은 value로 볼 수 있습니다.\n    - $ Q^{w}(s,a) = \\phi(s,a)^{\\top} w + v^{\\top}\\phi(s) $\n        - 정의 : $ \\phi(s,a) \\overset{\\underset{\\mathrm{def}}{}}{=} \\nabla_{\\theta}\\mu_{\\theta}(s)(a-\\mu_{\\theta}(s)) $\n        - 일례 : $ V^{v}(s) = v^{\\top}\\phi(s) $\n        - Theorem 3 만족 여부\n            - 첫 번째 조건 만족합니다.\n            - 두 번째 조건은 대강 만족합니다.\n                - $ \\nabla_{a}Q^{\\mu}(s,a) $에 대한 unbiased sample을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $를 학습합니다.\n                - 이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \\approx Q^{\\mu}(s,a) $인 reasonable solution을 찾을 수 있기에 대강 $ \\nabla_{a}Q^{w}(s,a) \\approx \\nabla_{a}Q^{\\mu}(s,a) $이 될 것입니다.\n        - action-value function에 대한 linear function approximator는 큰 값을 가지는 actions에 대해선 diverge할 수 있어 global하게 action-values 예측하기에는 좋지 않지만, local critic에 사용할 때는 매우 유용합니다.\n            - 즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\\mu_{\\theta}(s)+\\delta) = \\delta^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $로, diverge하지 않고, 값을 얻을 수 있습니다.\n    - COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)\n        - Critic: 실제 action-value function에 대한 linear function approximator인 $ Q^{w}(s,a) = \\phi(s,a)^{\\top}w $를 estimate합니다.\n            - $ \\phi(s,a) = a^{\\top}\\nabla_{\\theta}\\mu_{\\theta} $\n            - Behavior policy $ \\beta(a|s) $로부터 얻은 samples를 이용하여 Q-learning이나 gradient Q-learning과 같은 off-policy algorithm으로 학습 가능합니다.\n        - Actor: estimated action-value function에 대한 gradient를 $ \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $로 치환 후, 정책을 업데이트합니다.\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $\n    - off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있습니다.\n        - $ \\mu_{\\theta}(s_{t+1}) $이 diverge할 수도 있기 때문으로 판단됩니다.\n        - 그렇기에 simple Q-learning 대신 다른 기법이 필요합니다.\n    - 그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm을 제안합니다.\n        - gradient temporal-difference learning에 기반한 기법들은 true gradient descent algorithm이며, converge가 보장됩니다. (Sutton, 2009)\n            - 기본 아이디어는 stochastic gradient descent로 mean-squared projected Bellman error (MSPBE)를 최소화하는 것입니다.\n            - critic이 actor보다 빠른 time-scale로 update되도록 step size들을 잘 조절하면, critic은 MSPBE를 최소화하는 parameters로 converge하게 됩니다.\n            - critic에 gradient temporal-difference learning의 일종인 gradient Q-learning을 사용한 논문입니다. (Maei, 2010)\n    - COPDAC-GQ algorithm\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) - \\alpha_{w}\\gamma\\phi(s_{t+1}, \\mu_{\\theta}(s_{t+1}))(\\phi(s_{t},a_{t})^{\\top} u_{t}) $\n        - $ v_{t+1} = v_{t} + \\alpha_{v}\\delta_{t}\\phi(s_{t}) - \\alpha_{v}\\gamma\\phi(s_{t+1})(\\phi(s_{t},a_{t})^{\\top} u_{t}) $\n        - $ u_{t+1} = u_{t} + \\alpha_{u}(\\delta_{t}-\\phi(s_{t}, a_{t})^{\\top} u_{t})\\phi(s_{t}, a_{t}) $\n    - stochastic actor-critic과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $입니다.\n        - m은 action dimensions, n은 number of policy parameters\n    - Natural policy gradient를 이용해 deterministic policies를 찾을 수 있습니다.\n        - $ M(\\theta)^{-1}\\nabla_{\\theta}J(\\mu_{\\theta}) $은 any metric $ M(\\theta) $에 대한 performance objective (식(14))의 steepest ascent direction 입니다. (Toussaint, 2012)\n        - Natural gradient는 Fisher information metric $ M_{\\pi}(\\theta) $에 대한 steepest ascent direction 입니다.\n            -  $ M_{\\pi}(\\theta) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}] $\n            - Fisher information metric은 policy reparameterization에 대해 불변입니다. (Bagnell, 2003)\n        - deterministic policies에 대한 metric으로 $ M_{\\mu}(\\theta) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}] $을 사용합니다.\n        \t- 이는 variance가 0인 policy에 대한 Fisher information metric으로 볼 수 있습니다.\n        \t- $ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\theta}(a\\vert s)}$에서 policy variance가 0이면, 특정 s에 대한 $ \\pi_{\\theta}(a|s)$만 1이 되고, 나머지는 0입니다.\n        - deterministic policy gradient theorem과 compatible function approximation을 결합하면 $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] $이 됩니다.\n            - $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n            - $ \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)} \\approx \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $\n        - 그렇기에 steepest ascent direction은 $ M_{\\mu}(\\theta)^{-1}\\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = w $이 됩니다.\n            - $ E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}]^{-1}E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] = w $\n        - 이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ에서 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $ 식을 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}w_{t} $로 바꿔주기만 하면 됩니다.\n\n<br><br>\n\n# 5. Experiments\n\n<br>\n## 5.1. Continuous Bandit\n- Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행합니다.\n    - Action dimension이 커질수록 성능 차이가 심합니다.\n    - 빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있습니다.\n    - <img src=\"https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1\">\n\n<br>\n## 5.2. Continuous Reinforcement Learning\n- COPDAC-Q, SAC, off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행합니다.\n    - COPDAC-Q의 성능이 약간 더 좋습니다.\n    - COPDAC-Q의 학습이 더 빨리 이뤄집니다.\n    - <img src=\"https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1\">\n\n<br>\n## 5.3. Octopus Arm\n- 목표: 6 segments octopus arm (20 action dimensions & 50 state dimensions)을 control하여 target을 맞추는 것입니다.\n    - COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤입니다.\n    - <img src=\"https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1\" width=600px>\n    - 기존 기법들은 action spaces 혹은 action과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않습니다.\n        - 기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 보여줬으면 하지만 실험을 하지 않았습니다.\n    - 8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보입니다.\n- [참고] Octopus Arm 이란?\n    - [OctopusArm Youtube Link](https://www.youtube.com/watch?v=AxeeHif0euY)\n    - <img src=\"https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1\">\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [Sutton PG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/)\n\n<br>\n\n# 다음으로\n\n## [DDPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/)","slug":"2_app","published":1,"updated":"2019-02-07T11:33:53.810Z","_id":"cjrujlu0u001d5wfehdudraw1","comments":1,"layout":"post","photos":[],"link":"","content":"<center> <img src=\"../../../../img/irl/app_1.png\" width=\"850\"> </center>\n\n<p>논문 저자 : David Silver, Guy Lever, Nicloas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller<br>논문 링크 : <a href=\"http://people.eecs.berkeley.edu/~russell/classes/cs294/s11/readings/Abbeel+Ng:2004.pdf\" target=\"_blank\" rel=\"noopener\">http://people.eecs.berkeley.edu/~russell/classes/cs294/s11/readings/Abbeel+Ng:2004.pdf</a><br>Proceeding : International Conference on Machine Learning (ICML) 2014</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><ul>\n<li>Stochastic Policy Gradient (DPG) Theorem을 제안합니다.<ul>\n<li>중요한 중요한 점은 DPG는 Expected gradient of the action-value function의 형태라는 것입니다.</li>\n</ul>\n</li>\n<li>Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient (SPG)와 동일해집니다. (Theorem 2)<ul>\n<li>Theorem 2로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG에 적용할 수 있게 됩니다.<ul>\n<li>예. Sutton PG, natural gradients, actor-critic, episodic/batch methods</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>적절한 exploration 을 위해 model-free, off-policy actor-critic algorithm 을 제안합니다<ul>\n<li>action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility condition을 제공합니다. (Theorem 3)</li>\n</ul>\n</li>\n<li>DPG 는 SPG 보다 성능이 좋습니다.<ul>\n<li>특히 high dimensional action spaces를 가지는 tasks에서의 성능 향상이 큽니다.<ul>\n<li>SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는 state spaces에 대해서만 평균을 취합니다.</li>\n<li>결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됩니다.</li>\n<li>무한정 학습을 시키면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정합니다.</li>\n</ul>\n</li>\n<li>기존 기법들에 비해 computation 양이 많지 않습니다.<ul>\n<li>Computation 은 action dimensionality 와 policy parameters 수에 비례합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"2-Background\"><a href=\"#2-Background\" class=\"headerlink\" title=\"2. Background\"></a>2. Background</h1><p><br></p>\n<h2 id=\"2-1-Performance-objective-function\"><a href=\"#2-1-Performance-objective-function\" class=\"headerlink\" title=\"2.1 Performance objective function\"></a>2.1 Performance objective function</h2><p>$$<br>\\begin{align}<br>J(\\pi_{\\theta}) &amp;= \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\pi_{\\theta}(s,a)r(s,a)da ds = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[r(s,a)]<br>\\end{align}<br>$$</p>\n<p><br></p>\n<h2 id=\"2-2-SPG-Theorem\"><a href=\"#2-2-SPG-Theorem\" class=\"headerlink\" title=\"2.2 SPG Theorem\"></a>2.2 SPG Theorem</h2><ul>\n<li>State distribution $ \\rho^{\\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없습니다.<br>$$\\begin{eqnarray}\\nabla_{\\theta}J(\\pi_{\\theta}) &amp;=&amp; \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\ &amp;=&amp; E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]<br>\\end{eqnarray}$$</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-3-Stochastic-Actor-Critic-Algorithms\"><a href=\"#2-3-Stochastic-Actor-Critic-Algorithms\" class=\"headerlink\" title=\"2.3 Stochastic Actor-Critic Algorithms\"></a>2.3 Stochastic Actor-Critic Algorithms</h2><ul>\n<li>Actor와 Critic이 번갈아가면서 동작하며 stochastic policy를 최적화하는 기법입니다.</li>\n<li>Actor: $ Q^{\\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $를 이용해 stochastic policy gradient를 ascent하는 방향으로 policy parameter $ \\theta $를 업데이트함으로써 stochastic policy를 발전시킵니다.<ul>\n<li>$ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $</li>\n</ul>\n</li>\n<li>Critic: SARSA나 Q-learning같은 Temporal-difference (TD) learning을 이용해 action-value function의 parameter, $ w $를 업데이트함으로써 $ Q^w(s,a) $가 $ Q^{\\pi}(s,a) $과 유사해지도록 합니다.</li>\n<li>실제 값인 $ Q^{\\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $를 사용하게 되면, 일반적으로 bias가 발생합니다. 하지만, compatible condition에 부합하는 $ Q^w(s,a) $를 사용하게 되면, bias가 발생하지 않습니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-4-Off-policy-Actor-Critic\"><a href=\"#2-4-Off-policy-Actor-Critic\" class=\"headerlink\" title=\"2.4 Off-policy Actor-Critic\"></a>2.4 Off-policy Actor-Critic</h2><ul>\n<li>Distinct behavior policy $ \\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic</li>\n<li>Performance objective function<ul>\n<li>$\\begin{eqnarray}<br>  J_{\\beta}(\\pi_{\\theta})<br>  &amp;=&amp; \\int_{S}\\rho^{\\beta}(s)V^{\\pi}(s)ds \\nonumber \\\\<br>  &amp;=&amp; \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads<br>  \\end{eqnarray} $</li>\n</ul>\n</li>\n<li>off-policy policy gradient<ul>\n<li>$ \\begin{eqnarray}<br>  \\nabla_{\\theta}J_{\\beta}(\\pi_{\\theta}) &amp;\\approx&amp; \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\end{eqnarray} $<br>  $=E_{s \\sim \\rho^{\\beta}, a \\sim \\beta}[\\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)}\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]$</li>\n<li>off-policy policy gradient 식에서의 물결 표시는 <a href=\"https://arxiv.org/abs/1205.4839\" target=\"_blank\" rel=\"noopener\">Degris, 2012b</a> 논문에 근거합니다.<ul>\n<li>Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같습니다. (빨간색 상자에 있는 항목을 삭제함으로써 근사합니다.)<ul>\n<li><img src=\"https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1\" width=\"500px\"></li>\n</ul>\n</li>\n<li>[Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\\nabla_{u}𝑄^{\\pi,\\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮습니다.<ul>\n<li><img src=\"https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1\" width=\"500px\"></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>off-policy policy gradient 식에서 $ \\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)} $는 importance sampling ratio 입니다.<ul>\n<li>off-policy actor-critic에서는 $ \\beta $에 의해 샘플링된 trajectory를 이용해서 stochastic policy $ \\pi $를 예측하는 것이기 때문에 importance sampling이 필요합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"3-Gradient-of-Deterministic-Policies\"><a href=\"#3-Gradient-of-Deterministic-Policies\" class=\"headerlink\" title=\"3. Gradient of Deterministic Policies\"></a>3. Gradient of Deterministic Policies</h1><p><br></p>\n<h2 id=\"3-1-Regularity-Conditions\"><a href=\"#3-1-Regularity-Conditions\" class=\"headerlink\" title=\"3.1 Regularity Conditions\"></a>3.1 Regularity Conditions</h2><ul>\n<li>어떠한 이론이 성립하기 위한 전제 조건</li>\n<li>Regularity conditions A.1<ul>\n<li>$ p(s’|s,a), \\nabla_{a}p(s’|s,a), \\mu_{\\theta}(s), \\nabla_{\\theta}\\mu_{\\theta}(s), r(s,a), \\nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s’ $ and $ x $.</li>\n</ul>\n</li>\n<li>regularity conditions A.2<ul>\n<li>There exists a $ b $ and $ L $ such that $ \\sup_{s}p_{1}(s) &lt; b $, $ \\sup_{a,s,s’}p(s’|s,a) &lt; b $, $ \\sup_{a,s}r(s,a) &lt; b $, $ \\sup_{a,s,s’}|\\nabla_{a}p(s’|s,a)| &lt; L $, and $ \\sup_{a,s}|\\nabla_{a}r(s,a)| &lt; L $.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-2-Deterministic-Policy-Gradient-Theorem\"><a href=\"#3-2-Deterministic-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"3.2 Deterministic Policy Gradient Theorem\"></a>3.2 Deterministic Policy Gradient Theorem</h2><ul>\n<li>Deterministic policy<ul>\n<li>$ \\mu_{\\theta} : S \\to A $ with parameter vector $ \\theta \\in \\mathbb{R}^n $</li>\n</ul>\n</li>\n<li>Probability distribution<ul>\n<li>$ p(s \\to s’, t, \\mu) $</li>\n</ul>\n</li>\n<li>Discounted state distribution<ul>\n<li>$ \\rho^{\\mu}(s) $</li>\n</ul>\n</li>\n<li>Performance objective</li>\n</ul>\n<p>$$<br>J(\\mu_{\\theta}) = E[r^{\\gamma}_{1} | \\mu]<br>$$</p>\n<p>$$<br>= \\int_{S}\\rho^{\\mu}(s)r(s,\\mu_{\\theta}(s))ds<br>= E_{s \\sim \\rho^{\\mu}}[r(s,\\mu_{\\theta}(s))]<br>$$</p>\n<ul>\n<li><p>DPG Theorem</p>\n<ul>\n<li><p>MDP 가 A.1 만족한다면, 아래 식이 성립합니다.<br>$\\nabla_{\\theta}J(\\mu_{\\theta}) = \\int_{S}\\rho^{\\mu}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}ds \\nonumber$<br>$= E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]   \\nonumber $ </p>\n</li>\n<li><p>DPG는 state space에 대해서만 평균을 취하면 되기에, state와 action space 모두에 대해 평균을 취해야 하는 SPG에 비해 data efficiency가 좋습니다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 됩니다.</p>\n</li>\n</ul>\n</li>\n</ul>\n<p><br>    </p>\n<h2 id=\"3-3-DPG-형태에-대한-informal-intuition\"><a href=\"#3-3-DPG-형태에-대한-informal-intuition\" class=\"headerlink\" title=\"3.3 DPG 형태에 대한 informal intuition\"></a>3.3 DPG 형태에 대한 informal intuition</h2><ul>\n<li>Generalized policy iteration<ul>\n<li>정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration<ul>\n<li>위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>정책 평가<ul>\n<li>action-value function $ Q^{\\pi}(s,a) $ or $ Q^{\\mu}(s,a) $을 estimate 하는 것 입니다.</li>\n</ul>\n</li>\n<li>정책 발전<ul>\n<li>위 estimated action-value function에 따라 정책을 update하는 것 입니다.</li>\n<li>주로 action-value function에 대한 greedy maximisation을 사용합니다.<ul>\n<li>$ \\mu^{k+1}(s) = \\arg\\max\\limits_{a}Q^{\\mu^{k}}(s,a) $</li>\n<li>greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이로 인해 continuous action spaces에서 계산량이 급격히 늘어납니다.</li>\n</ul>\n</li>\n<li>그래서 policy gradient 방법이 나옵니다.<ul>\n<li>policy 를 $ \\theta $에 대해서 parameterize 합니다.</li>\n<li>매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $마다 policy parameter를 action-value function $ Q $의 $ \\theta $에 대한 gradient $ \\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s)) $ 방향으로 proportional하게 update 합니다.</li>\n<li>하지만 각 state는 다른 방향을 제시할 수 있기에, state distribution $ \\rho^{\\mu}(s) $에 대한 기대값을 취해 policy parameter를 update 할 수도 있습니다.<ul>\n<li>$ \\theta^{k+1} = \\theta^{k} + \\alpha E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s))] $</li>\n</ul>\n</li>\n<li>이는 chain-rule에 따라 아래와 같이 분리될 수 있습니다.<ul>\n<li>$ \\theta^{k+1} = \\theta^{k} + \\alpha E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu^{k}}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $ (7)</li>\n<li>chain rule: $ \\frac{\\partial Q}{\\partial \\theta} = \\frac{\\partial a}{\\partial \\theta} \\frac{\\partial Q}{\\partial a} $</li>\n</ul>\n</li>\n<li>하지만 state distribution $ \\rho^{\\mu} $은 정책에 dependent 합니다.<ul>\n<li>정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state가 변하기 때문에 state distribution이 변하게 됩니다.</li>\n</ul>\n</li>\n<li>그렇기에 정책 update 시 state distribution에 대한 gradient를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있습니다.</li>\n<li>deterministic policy gradient theorem은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update해도 performance objective의 gradient를 정확하게 따름을 의미합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-4-DPG는-SPG의-limiting-case\"><a href=\"#3-4-DPG는-SPG의-limiting-case\" class=\"headerlink\" title=\"3.4 DPG는 SPG의 limiting case\"></a>3.4 DPG는 SPG의 limiting case</h2><ul>\n<li>stochastic policy parameterization<ul>\n<li>$ \\pi_{\\mu_{\\theta},\\sigma} $ by a deterministic policy $ \\mu_{\\theta} : S \\to A $ and a variance parameter $ \\sigma $</li>\n<li>$ \\sigma = 0 $ 이면, $ \\pi_{\\mu_{\\theta},\\sigma} \\equiv \\mu_{\\theta} $</li>\n</ul>\n</li>\n<li>Theorem 2. Policy의 variance가 0에 수렴하면, 즉, $ \\sigma \\to 0 $, stochastic policy gradient와 deterministic policy gradient는 동일해집니다.<ul>\n<li>조건: stochastic policy $ \\pi_{\\mu_{\\theta},\\sigma} = \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $<ul>\n<li>$ \\sigma $는 variance입니다.</li>\n<li>$ \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $는 condition B.1을 만족합니다.</li>\n<li>MDP는 conditions A.1과 A.2를 만족합니다.</li>\n</ul>\n</li>\n<li>결과:<ul>\n<li>$ \\lim\\limits_{\\sigma\\downarrow0}\\nabla_{\\theta}J(\\pi_{\\mu_{\\theta},\\sigma}) = \\nabla_{\\theta}J(\\mu_{\\theta})  $<ul>\n<li>좌변은 standard stochastic gradient이며, 우변은 deterministic gradient입니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>의미:<ul>\n<li>deterministic policy gradient는 stochastic policy gradient의 특수 case 입니다.</li>\n<li>기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있습니다.<ul>\n<li>기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"4-Deterministic-Actor-Critic-Algorithms\"><a href=\"#4-Deterministic-Actor-Critic-Algorithms\" class=\"headerlink\" title=\"4. Deterministic Actor-Critic Algorithms\"></a>4. Deterministic Actor-Critic Algorithms</h1><ol>\n<li>SARSA critic를 이용한 on-policy actor-critic<ul>\n<li>단점<ul>\n<li>deterministic policy에 의해 행동하면 exploration이 잘 되지 않기에, sub-optimal에 빠지기 쉽습니다.</li>\n</ul>\n</li>\n<li>목적<ul>\n<li>교훈/정보제공</li>\n<li>환경에서 충분한 noise를 제공하여 exploration을 시킬 수 있다면, deterministic policy를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있습니다.<ul>\n<li>예. 바람이 agent의 행동에 영향(noise)을 줌</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Remind: 살사(SARSA) update rule<ul>\n<li>$ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $</li>\n</ul>\n</li>\n<li>Algorithm<ul>\n<li>Critic은 MSE를 $ \\bf minimize $하는 방향, 즉, action-value function을 stochastic gradient $ \\bf descent $ 방법으로 update합니다.<ul>\n<li>$ MSE = [Q^{\\mu}(s,a) - Q^{w}(s,a)]^2 $<ul>\n<li>critic은 실제 $ Q^{\\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $로 대체하여 action-value function을 estimate하며, 이 둘 간 mean square error를 최소화하는 것이 목표입니다.</li>\n</ul>\n</li>\n<li>$ \\nabla_{w}MSE \\approx -2 * [r + \\gamma Q^{w}(s’,a’) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $<ul>\n<li>$ \\nabla_{w}MSE = -2 * [Q^{\\mu}(s,a) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $</li>\n<li>$ Q^{\\mu}(s,a) $ 를 $ r + \\gamma Q^{w}(s’,a’) $로 대체<ul>\n<li>$ Q^{\\mu}(s,a) = r + \\gamma Q^{\\mu}(s’,a’) $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $<ul>\n<li>$w_{t+1} = w_{t} - \\alpha_{w}\\nabla_{w}MSE  \\nonumber$<br>$ \\approx w_{t} - \\alpha_{w} <em> (-2 </em> [r + \\gamma Q^{w}(s’,a’) - Q^{w}(s,a)] \\nabla_{w}Q^{w}(s,a)$</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Actor는 식(9)에 따라 보상이 $ \\bf maximize $되는 방향, 즉, deterministic policy를 stochastic gradient $ \\bf ascent $ 방법으로 update합니다.<ul>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Q-learning 을 이용한 off-policy actor-critic<ul>\n<li>stochastic behavior policy $ \\beta(a|s) $에 의해 생성된 trajectories로부터 deterministic target policy $ \\mu_{\\theta}(s) $를 학습하는 off-policy actor-critic입니다</li>\n<li>performance objective<ul>\n<li>$ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)V^{\\mu}(s)ds \\nonumber \\\\$<br>$= \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds \\nonumber \\\\$<br>$= E_{s \\sim \\rho^{\\beta}}[Q^{\\mu}(s,\\mu_{\\theta}(s))]$</li>\n</ul>\n</li>\n<li>off-policy deterministic policy gradient<ul>\n<li>$ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $<ul>\n<li>논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됩니다.</li>\n<li>$ \\begin{eqnarray}<br>  \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) &amp;\\approx&amp; \\int_{S}\\rho^{\\beta}(s)\\nabla_{\\theta}\\mu_{\\theta}(a|s)Q^{\\mu}(s,a)ds \\nonumber \\<br>  &amp;=&amp; E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]<br>  \\end{eqnarray} $</li>\n<li>근거: Action이 deterministic하기에 stochastic 경우와는 다르게 performance objective에서 action에 대해 평균을 구할 필요가 없습니다. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b]에서 처럼 곱의 미분을 통해 생기는 action-value function에 대한 gradient term을 생략할 필요가 사라집니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Remind: 큐러닝(Q-learning) update rule<ul>\n<li>$ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $</li>\n</ul>\n</li>\n<li>algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)<ul>\n<li>살사를 이용한 on-policy deterministic actor-critic과 아래 부분을 제외하고는 같습니다.<ul>\n<li>target policy는 $ \\beta(a|s) $에 의해 생성된 trajectories를 통해 학습합니다.</li>\n<li>업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $이 아니라 정책으로부터 나온 행동 값 $ \\mu_{\\theta}(s_{t+1}) $을 사용합니다.<ul>\n<li>$ \\mu_{\\theta}(s_{t+1}) $ 은 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $</li>\n</ul>\n</li>\n<li>Stochastic off-policy actor-critic은 대개 actor와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient에선 importance sampling이 필요없습니다.<ul>\n<li>Actor 는 deterministic 이기에 sampling 자체가 필요없습니다.<ul>\n<li>Stochastic policy인 경우, Actor에서 importance sampling이 필요한 이유는 상태 $ s $에서의 가치 함수 값 $ V^{\\pi}(s) $을 estimate하기 위해 $ \\pi $가 아니라 $ \\beta $에 따라 sampling을 한 후, 평균을 내기 때문입니다.</li>\n<li>하지만 deterministic policy인 경우, 상태 $ s $에서의 가치 함수 값 $ V^{\\pi}(s) = Q^{\\pi}(s,\\mu_{\\theta}) $ 즉, action이 상태 s에 대해 deterministic이기에 sampling을 통해 estimate할 필요가 없고, 따라서 importance sampling도 필요없어집니다.</li>\n<li>stochastic vs. deterministic performance objective<ul>\n<li>stochastic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads $</li>\n<li>deterministic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Critic이 사용하는 Q-learning은 importance sampling이 필요없는 off policy 알고리즘입니다.<ul>\n<li>Q-learning도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic<ul>\n<li>위 살사/Q-learning 기반 on/off-policy는 아래와 같은 문제가 존재합니다.<ul>\n<li>function approximator에 의한 bias<ul>\n<li>일반적으로 $ Q^{\\mu}(s,a) $ 를 $ Q^{w}(s,a) $로 대체하여 deterministic policy gradient를 구하면, 그 gradient는 ascent하는 방향이 아닐 수도 있습니다.</li>\n</ul>\n</li>\n<li>off-policy learning에 의한 instabilities</li>\n</ul>\n</li>\n<li>그래서 stochastic처럼 $ \\nabla_{a}Q^{\\mu}(s,a) $를 $ \\nabla_{a}Q^{w}(s,a) $로 대체해도 deterministic policy gradient에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $를 찾아야 합니다.</li>\n<li>Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $는 deterministic policy $ \\mu_{\\theta}(s) $와 compatible 합니다. 즉, $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $<ul>\n<li>$ \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $입니다.</li>\n<li>$ w $는 $ MSE(\\theta, w) = E[\\epsilon(s;\\theta,w)^{\\top}\\epsilon(s;\\theta,w)] $를 최소화합니다.<ul>\n<li>$ \\epsilon(s;\\theta,w) = \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} - \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}  $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Theorem 3은 on-policy 뿐만 아니라 off-policy에도 적용 가능합니다.</li>\n<li>$ Q^{w}(s,a) = (a-\\mu_{\\theta}(s))^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top} w + V^{v}(s) $<ul>\n<li>어떠한 deterministic policy에 대해서도 위 형태와 같은 compatible function approximator가 존재합니다.</li>\n<li>앞의 term은 advantage를, 뒤의 term은 value로 볼 수 있습니다.</li>\n</ul>\n</li>\n<li>$ Q^{w}(s,a) = \\phi(s,a)^{\\top} w + v^{\\top}\\phi(s) $<ul>\n<li>정의 : $ \\phi(s,a) \\overset{\\underset{\\mathrm{def}}{}}{=} \\nabla_{\\theta}\\mu_{\\theta}(s)(a-\\mu_{\\theta}(s)) $</li>\n<li>일례 : $ V^{v}(s) = v^{\\top}\\phi(s) $</li>\n<li>Theorem 3 만족 여부<ul>\n<li>첫 번째 조건 만족합니다.</li>\n<li>두 번째 조건은 대강 만족합니다.<ul>\n<li>$ \\nabla_{a}Q^{\\mu}(s,a) $에 대한 unbiased sample을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $를 학습합니다.</li>\n<li>이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \\approx Q^{\\mu}(s,a) $인 reasonable solution을 찾을 수 있기에 대강 $ \\nabla_{a}Q^{w}(s,a) \\approx \\nabla_{a}Q^{\\mu}(s,a) $이 될 것입니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>action-value function에 대한 linear function approximator는 큰 값을 가지는 actions에 대해선 diverge할 수 있어 global하게 action-values 예측하기에는 좋지 않지만, local critic에 사용할 때는 매우 유용합니다.<ul>\n<li>즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\\mu_{\\theta}(s)+\\delta) = \\delta^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $로, diverge하지 않고, 값을 얻을 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)<ul>\n<li>Critic: 실제 action-value function에 대한 linear function approximator인 $ Q^{w}(s,a) = \\phi(s,a)^{\\top}w $를 estimate합니다.<ul>\n<li>$ \\phi(s,a) = a^{\\top}\\nabla_{\\theta}\\mu_{\\theta} $</li>\n<li>Behavior policy $ \\beta(a|s) $로부터 얻은 samples를 이용하여 Q-learning이나 gradient Q-learning과 같은 off-policy algorithm으로 학습 가능합니다.</li>\n</ul>\n</li>\n<li>Actor: estimated action-value function에 대한 gradient를 $ \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $로 치환 후, 정책을 업데이트합니다.</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $</li>\n</ul>\n</li>\n<li>off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있습니다.<ul>\n<li>$ \\mu_{\\theta}(s_{t+1}) $이 diverge할 수도 있기 때문으로 판단됩니다.</li>\n<li>그렇기에 simple Q-learning 대신 다른 기법이 필요합니다.</li>\n</ul>\n</li>\n<li>그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm을 제안합니다.<ul>\n<li>gradient temporal-difference learning에 기반한 기법들은 true gradient descent algorithm이며, converge가 보장됩니다. (Sutton, 2009)<ul>\n<li>기본 아이디어는 stochastic gradient descent로 mean-squared projected Bellman error (MSPBE)를 최소화하는 것입니다.</li>\n<li>critic이 actor보다 빠른 time-scale로 update되도록 step size들을 잘 조절하면, critic은 MSPBE를 최소화하는 parameters로 converge하게 됩니다.</li>\n<li>critic에 gradient temporal-difference learning의 일종인 gradient Q-learning을 사용한 논문입니다. (Maei, 2010)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>COPDAC-GQ algorithm<ul>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) - \\alpha_{w}\\gamma\\phi(s_{t+1}, \\mu_{\\theta}(s_{t+1}))(\\phi(s_{t},a_{t})^{\\top} u_{t}) $</li>\n<li>$ v_{t+1} = v_{t} + \\alpha_{v}\\delta_{t}\\phi(s_{t}) - \\alpha_{v}\\gamma\\phi(s_{t+1})(\\phi(s_{t},a_{t})^{\\top} u_{t}) $</li>\n<li>$ u_{t+1} = u_{t} + \\alpha_{u}(\\delta_{t}-\\phi(s_{t}, a_{t})^{\\top} u_{t})\\phi(s_{t}, a_{t}) $</li>\n</ul>\n</li>\n<li>stochastic actor-critic과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $입니다.<ul>\n<li>m은 action dimensions, n은 number of policy parameters</li>\n</ul>\n</li>\n<li>Natural policy gradient를 이용해 deterministic policies를 찾을 수 있습니다.<ul>\n<li>$ M(\\theta)^{-1}\\nabla_{\\theta}J(\\mu_{\\theta}) $은 any metric $ M(\\theta) $에 대한 performance objective (식(14))의 steepest ascent direction 입니다. (Toussaint, 2012)</li>\n<li>Natural gradient는 Fisher information metric $ M_{\\pi}(\\theta) $에 대한 steepest ascent direction 입니다.<ul>\n<li>$ M_{\\pi}(\\theta) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}] $</li>\n<li>Fisher information metric은 policy reparameterization에 대해 불변입니다. (Bagnell, 2003)</li>\n</ul>\n</li>\n<li>deterministic policies에 대한 metric으로 $ M_{\\mu}(\\theta) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}] $을 사용합니다.<ul>\n<li>이는 variance가 0인 policy에 대한 Fisher information metric으로 볼 수 있습니다.</li>\n<li>$ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\theta}(a\\vert s)}$에서 policy variance가 0이면, 특정 s에 대한 $ \\pi_{\\theta}(a|s)$만 1이 되고, 나머지는 0입니다.</li>\n</ul>\n</li>\n<li>deterministic policy gradient theorem과 compatible function approximation을 결합하면 $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] $이 됩니다.<ul>\n<li>$ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $</li>\n<li>$ \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)} \\approx \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $</li>\n</ul>\n</li>\n<li>그렇기에 steepest ascent direction은 $ M_{\\mu}(\\theta)^{-1}\\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = w $이 됩니다.<ul>\n<li>$ E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}]^{-1}E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] = w $</li>\n</ul>\n</li>\n<li>이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ에서 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $ 식을 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}w_{t} $로 바꿔주기만 하면 됩니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p><br><br></p>\n<h1 id=\"5-Experiments\"><a href=\"#5-Experiments\" class=\"headerlink\" title=\"5. Experiments\"></a>5. Experiments</h1><p><br></p>\n<h2 id=\"5-1-Continuous-Bandit\"><a href=\"#5-1-Continuous-Bandit\" class=\"headerlink\" title=\"5.1. Continuous Bandit\"></a>5.1. Continuous Bandit</h2><ul>\n<li>Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행합니다.<ul>\n<li>Action dimension이 커질수록 성능 차이가 심합니다.</li>\n<li>빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있습니다.</li>\n<li><img src=\"https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"5-2-Continuous-Reinforcement-Learning\"><a href=\"#5-2-Continuous-Reinforcement-Learning\" class=\"headerlink\" title=\"5.2. Continuous Reinforcement Learning\"></a>5.2. Continuous Reinforcement Learning</h2><ul>\n<li>COPDAC-Q, SAC, off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행합니다.<ul>\n<li>COPDAC-Q의 성능이 약간 더 좋습니다.</li>\n<li>COPDAC-Q의 학습이 더 빨리 이뤄집니다.</li>\n<li><img src=\"https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"5-3-Octopus-Arm\"><a href=\"#5-3-Octopus-Arm\" class=\"headerlink\" title=\"5.3. Octopus Arm\"></a>5.3. Octopus Arm</h2><ul>\n<li>목표: 6 segments octopus arm (20 action dimensions &amp; 50 state dimensions)을 control하여 target을 맞추는 것입니다.<ul>\n<li>COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤입니다.</li>\n<li><img src=\"https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1\" width=\"600px\"></li>\n<li>기존 기법들은 action spaces 혹은 action과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않습니다.<ul>\n<li>기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 보여줬으면 하지만 실험을 하지 않았습니다.</li>\n</ul>\n</li>\n<li>8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보입니다.</li>\n</ul>\n</li>\n<li>[참고] Octopus Arm 이란?<ul>\n<li><a href=\"https://www.youtube.com/watch?v=AxeeHif0euY\" target=\"_blank\" rel=\"noopener\">OctopusArm Youtube Link</a></li>\n<li><img src=\"https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"Sutton-PG-여행하기\"><a href=\"#Sutton-PG-여행하기\" class=\"headerlink\" title=\"Sutton PG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/\">Sutton PG 여행하기</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"DDPG-여행하기\"><a href=\"#DDPG-여행하기\" class=\"headerlink\" title=\"DDPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/\">DDPG 여행하기</a></h2>","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"../../../../img/irl/app_1.png\" width=\"850\"> </center>\n\n<p>논문 저자 : David Silver, Guy Lever, Nicloas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller<br>논문 링크 : <a href=\"http://people.eecs.berkeley.edu/~russell/classes/cs294/s11/readings/Abbeel+Ng:2004.pdf\" target=\"_blank\" rel=\"noopener\">http://people.eecs.berkeley.edu/~russell/classes/cs294/s11/readings/Abbeel+Ng:2004.pdf</a><br>Proceeding : International Conference on Machine Learning (ICML) 2014</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><ul>\n<li>Stochastic Policy Gradient (DPG) Theorem을 제안합니다.<ul>\n<li>중요한 중요한 점은 DPG는 Expected gradient of the action-value function의 형태라는 것입니다.</li>\n</ul>\n</li>\n<li>Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient (SPG)와 동일해집니다. (Theorem 2)<ul>\n<li>Theorem 2로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG에 적용할 수 있게 됩니다.<ul>\n<li>예. Sutton PG, natural gradients, actor-critic, episodic/batch methods</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>적절한 exploration 을 위해 model-free, off-policy actor-critic algorithm 을 제안합니다<ul>\n<li>action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility condition을 제공합니다. (Theorem 3)</li>\n</ul>\n</li>\n<li>DPG 는 SPG 보다 성능이 좋습니다.<ul>\n<li>특히 high dimensional action spaces를 가지는 tasks에서의 성능 향상이 큽니다.<ul>\n<li>SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는 state spaces에 대해서만 평균을 취합니다.</li>\n<li>결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됩니다.</li>\n<li>무한정 학습을 시키면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정합니다.</li>\n</ul>\n</li>\n<li>기존 기법들에 비해 computation 양이 많지 않습니다.<ul>\n<li>Computation 은 action dimensionality 와 policy parameters 수에 비례합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"2-Background\"><a href=\"#2-Background\" class=\"headerlink\" title=\"2. Background\"></a>2. Background</h1><p><br></p>\n<h2 id=\"2-1-Performance-objective-function\"><a href=\"#2-1-Performance-objective-function\" class=\"headerlink\" title=\"2.1 Performance objective function\"></a>2.1 Performance objective function</h2><p>$$<br>\\begin{align}<br>J(\\pi_{\\theta}) &amp;= \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\pi_{\\theta}(s,a)r(s,a)da ds = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[r(s,a)]<br>\\end{align}<br>$$</p>\n<p><br></p>\n<h2 id=\"2-2-SPG-Theorem\"><a href=\"#2-2-SPG-Theorem\" class=\"headerlink\" title=\"2.2 SPG Theorem\"></a>2.2 SPG Theorem</h2><ul>\n<li>State distribution $ \\rho^{\\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없습니다.<br>$$\\begin{eqnarray}\\nabla_{\\theta}J(\\pi_{\\theta}) &amp;=&amp; \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\ &amp;=&amp; E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]<br>\\end{eqnarray}$$</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-3-Stochastic-Actor-Critic-Algorithms\"><a href=\"#2-3-Stochastic-Actor-Critic-Algorithms\" class=\"headerlink\" title=\"2.3 Stochastic Actor-Critic Algorithms\"></a>2.3 Stochastic Actor-Critic Algorithms</h2><ul>\n<li>Actor와 Critic이 번갈아가면서 동작하며 stochastic policy를 최적화하는 기법입니다.</li>\n<li>Actor: $ Q^{\\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $를 이용해 stochastic policy gradient를 ascent하는 방향으로 policy parameter $ \\theta $를 업데이트함으로써 stochastic policy를 발전시킵니다.<ul>\n<li>$ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $</li>\n</ul>\n</li>\n<li>Critic: SARSA나 Q-learning같은 Temporal-difference (TD) learning을 이용해 action-value function의 parameter, $ w $를 업데이트함으로써 $ Q^w(s,a) $가 $ Q^{\\pi}(s,a) $과 유사해지도록 합니다.</li>\n<li>실제 값인 $ Q^{\\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $를 사용하게 되면, 일반적으로 bias가 발생합니다. 하지만, compatible condition에 부합하는 $ Q^w(s,a) $를 사용하게 되면, bias가 발생하지 않습니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-4-Off-policy-Actor-Critic\"><a href=\"#2-4-Off-policy-Actor-Critic\" class=\"headerlink\" title=\"2.4 Off-policy Actor-Critic\"></a>2.4 Off-policy Actor-Critic</h2><ul>\n<li>Distinct behavior policy $ \\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic</li>\n<li>Performance objective function<ul>\n<li>$\\begin{eqnarray}<br>  J_{\\beta}(\\pi_{\\theta})<br>  &amp;=&amp; \\int_{S}\\rho^{\\beta}(s)V^{\\pi}(s)ds \\nonumber \\\\<br>  &amp;=&amp; \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads<br>  \\end{eqnarray} $</li>\n</ul>\n</li>\n<li>off-policy policy gradient<ul>\n<li>$ \\begin{eqnarray}<br>  \\nabla_{\\theta}J_{\\beta}(\\pi_{\\theta}) &amp;\\approx&amp; \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\end{eqnarray} $<br>  $=E_{s \\sim \\rho^{\\beta}, a \\sim \\beta}[\\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)}\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]$</li>\n<li>off-policy policy gradient 식에서의 물결 표시는 <a href=\"https://arxiv.org/abs/1205.4839\" target=\"_blank\" rel=\"noopener\">Degris, 2012b</a> 논문에 근거합니다.<ul>\n<li>Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같습니다. (빨간색 상자에 있는 항목을 삭제함으로써 근사합니다.)<ul>\n<li><img src=\"https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1\" width=\"500px\"></li>\n</ul>\n</li>\n<li>[Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\\nabla_{u}𝑄^{\\pi,\\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮습니다.<ul>\n<li><img src=\"https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1\" width=\"500px\"></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>off-policy policy gradient 식에서 $ \\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)} $는 importance sampling ratio 입니다.<ul>\n<li>off-policy actor-critic에서는 $ \\beta $에 의해 샘플링된 trajectory를 이용해서 stochastic policy $ \\pi $를 예측하는 것이기 때문에 importance sampling이 필요합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"3-Gradient-of-Deterministic-Policies\"><a href=\"#3-Gradient-of-Deterministic-Policies\" class=\"headerlink\" title=\"3. Gradient of Deterministic Policies\"></a>3. Gradient of Deterministic Policies</h1><p><br></p>\n<h2 id=\"3-1-Regularity-Conditions\"><a href=\"#3-1-Regularity-Conditions\" class=\"headerlink\" title=\"3.1 Regularity Conditions\"></a>3.1 Regularity Conditions</h2><ul>\n<li>어떠한 이론이 성립하기 위한 전제 조건</li>\n<li>Regularity conditions A.1<ul>\n<li>$ p(s’|s,a), \\nabla_{a}p(s’|s,a), \\mu_{\\theta}(s), \\nabla_{\\theta}\\mu_{\\theta}(s), r(s,a), \\nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s’ $ and $ x $.</li>\n</ul>\n</li>\n<li>regularity conditions A.2<ul>\n<li>There exists a $ b $ and $ L $ such that $ \\sup_{s}p_{1}(s) &lt; b $, $ \\sup_{a,s,s’}p(s’|s,a) &lt; b $, $ \\sup_{a,s}r(s,a) &lt; b $, $ \\sup_{a,s,s’}|\\nabla_{a}p(s’|s,a)| &lt; L $, and $ \\sup_{a,s}|\\nabla_{a}r(s,a)| &lt; L $.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-2-Deterministic-Policy-Gradient-Theorem\"><a href=\"#3-2-Deterministic-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"3.2 Deterministic Policy Gradient Theorem\"></a>3.2 Deterministic Policy Gradient Theorem</h2><ul>\n<li>Deterministic policy<ul>\n<li>$ \\mu_{\\theta} : S \\to A $ with parameter vector $ \\theta \\in \\mathbb{R}^n $</li>\n</ul>\n</li>\n<li>Probability distribution<ul>\n<li>$ p(s \\to s’, t, \\mu) $</li>\n</ul>\n</li>\n<li>Discounted state distribution<ul>\n<li>$ \\rho^{\\mu}(s) $</li>\n</ul>\n</li>\n<li>Performance objective</li>\n</ul>\n<p>$$<br>J(\\mu_{\\theta}) = E[r^{\\gamma}_{1} | \\mu]<br>$$</p>\n<p>$$<br>= \\int_{S}\\rho^{\\mu}(s)r(s,\\mu_{\\theta}(s))ds<br>= E_{s \\sim \\rho^{\\mu}}[r(s,\\mu_{\\theta}(s))]<br>$$</p>\n<ul>\n<li><p>DPG Theorem</p>\n<ul>\n<li><p>MDP 가 A.1 만족한다면, 아래 식이 성립합니다.<br>$\\nabla_{\\theta}J(\\mu_{\\theta}) = \\int_{S}\\rho^{\\mu}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}ds \\nonumber$<br>$= E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]   \\nonumber $ </p>\n</li>\n<li><p>DPG는 state space에 대해서만 평균을 취하면 되기에, state와 action space 모두에 대해 평균을 취해야 하는 SPG에 비해 data efficiency가 좋습니다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 됩니다.</p>\n</li>\n</ul>\n</li>\n</ul>\n<p><br>    </p>\n<h2 id=\"3-3-DPG-형태에-대한-informal-intuition\"><a href=\"#3-3-DPG-형태에-대한-informal-intuition\" class=\"headerlink\" title=\"3.3 DPG 형태에 대한 informal intuition\"></a>3.3 DPG 형태에 대한 informal intuition</h2><ul>\n<li>Generalized policy iteration<ul>\n<li>정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration<ul>\n<li>위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>정책 평가<ul>\n<li>action-value function $ Q^{\\pi}(s,a) $ or $ Q^{\\mu}(s,a) $을 estimate 하는 것 입니다.</li>\n</ul>\n</li>\n<li>정책 발전<ul>\n<li>위 estimated action-value function에 따라 정책을 update하는 것 입니다.</li>\n<li>주로 action-value function에 대한 greedy maximisation을 사용합니다.<ul>\n<li>$ \\mu^{k+1}(s) = \\arg\\max\\limits_{a}Q^{\\mu^{k}}(s,a) $</li>\n<li>greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이로 인해 continuous action spaces에서 계산량이 급격히 늘어납니다.</li>\n</ul>\n</li>\n<li>그래서 policy gradient 방법이 나옵니다.<ul>\n<li>policy 를 $ \\theta $에 대해서 parameterize 합니다.</li>\n<li>매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $마다 policy parameter를 action-value function $ Q $의 $ \\theta $에 대한 gradient $ \\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s)) $ 방향으로 proportional하게 update 합니다.</li>\n<li>하지만 각 state는 다른 방향을 제시할 수 있기에, state distribution $ \\rho^{\\mu}(s) $에 대한 기대값을 취해 policy parameter를 update 할 수도 있습니다.<ul>\n<li>$ \\theta^{k+1} = \\theta^{k} + \\alpha E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s))] $</li>\n</ul>\n</li>\n<li>이는 chain-rule에 따라 아래와 같이 분리될 수 있습니다.<ul>\n<li>$ \\theta^{k+1} = \\theta^{k} + \\alpha E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu^{k}}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $ (7)</li>\n<li>chain rule: $ \\frac{\\partial Q}{\\partial \\theta} = \\frac{\\partial a}{\\partial \\theta} \\frac{\\partial Q}{\\partial a} $</li>\n</ul>\n</li>\n<li>하지만 state distribution $ \\rho^{\\mu} $은 정책에 dependent 합니다.<ul>\n<li>정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state가 변하기 때문에 state distribution이 변하게 됩니다.</li>\n</ul>\n</li>\n<li>그렇기에 정책 update 시 state distribution에 대한 gradient를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있습니다.</li>\n<li>deterministic policy gradient theorem은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update해도 performance objective의 gradient를 정확하게 따름을 의미합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-4-DPG는-SPG의-limiting-case\"><a href=\"#3-4-DPG는-SPG의-limiting-case\" class=\"headerlink\" title=\"3.4 DPG는 SPG의 limiting case\"></a>3.4 DPG는 SPG의 limiting case</h2><ul>\n<li>stochastic policy parameterization<ul>\n<li>$ \\pi_{\\mu_{\\theta},\\sigma} $ by a deterministic policy $ \\mu_{\\theta} : S \\to A $ and a variance parameter $ \\sigma $</li>\n<li>$ \\sigma = 0 $ 이면, $ \\pi_{\\mu_{\\theta},\\sigma} \\equiv \\mu_{\\theta} $</li>\n</ul>\n</li>\n<li>Theorem 2. Policy의 variance가 0에 수렴하면, 즉, $ \\sigma \\to 0 $, stochastic policy gradient와 deterministic policy gradient는 동일해집니다.<ul>\n<li>조건: stochastic policy $ \\pi_{\\mu_{\\theta},\\sigma} = \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $<ul>\n<li>$ \\sigma $는 variance입니다.</li>\n<li>$ \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $는 condition B.1을 만족합니다.</li>\n<li>MDP는 conditions A.1과 A.2를 만족합니다.</li>\n</ul>\n</li>\n<li>결과:<ul>\n<li>$ \\lim\\limits_{\\sigma\\downarrow0}\\nabla_{\\theta}J(\\pi_{\\mu_{\\theta},\\sigma}) = \\nabla_{\\theta}J(\\mu_{\\theta})  $<ul>\n<li>좌변은 standard stochastic gradient이며, 우변은 deterministic gradient입니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>의미:<ul>\n<li>deterministic policy gradient는 stochastic policy gradient의 특수 case 입니다.</li>\n<li>기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있습니다.<ul>\n<li>기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"4-Deterministic-Actor-Critic-Algorithms\"><a href=\"#4-Deterministic-Actor-Critic-Algorithms\" class=\"headerlink\" title=\"4. Deterministic Actor-Critic Algorithms\"></a>4. Deterministic Actor-Critic Algorithms</h1><ol>\n<li>SARSA critic를 이용한 on-policy actor-critic<ul>\n<li>단점<ul>\n<li>deterministic policy에 의해 행동하면 exploration이 잘 되지 않기에, sub-optimal에 빠지기 쉽습니다.</li>\n</ul>\n</li>\n<li>목적<ul>\n<li>교훈/정보제공</li>\n<li>환경에서 충분한 noise를 제공하여 exploration을 시킬 수 있다면, deterministic policy를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있습니다.<ul>\n<li>예. 바람이 agent의 행동에 영향(noise)을 줌</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Remind: 살사(SARSA) update rule<ul>\n<li>$ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $</li>\n</ul>\n</li>\n<li>Algorithm<ul>\n<li>Critic은 MSE를 $ \\bf minimize $하는 방향, 즉, action-value function을 stochastic gradient $ \\bf descent $ 방법으로 update합니다.<ul>\n<li>$ MSE = [Q^{\\mu}(s,a) - Q^{w}(s,a)]^2 $<ul>\n<li>critic은 실제 $ Q^{\\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $로 대체하여 action-value function을 estimate하며, 이 둘 간 mean square error를 최소화하는 것이 목표입니다.</li>\n</ul>\n</li>\n<li>$ \\nabla_{w}MSE \\approx -2 * [r + \\gamma Q^{w}(s’,a’) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $<ul>\n<li>$ \\nabla_{w}MSE = -2 * [Q^{\\mu}(s,a) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $</li>\n<li>$ Q^{\\mu}(s,a) $ 를 $ r + \\gamma Q^{w}(s’,a’) $로 대체<ul>\n<li>$ Q^{\\mu}(s,a) = r + \\gamma Q^{\\mu}(s’,a’) $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $<ul>\n<li>$w_{t+1} = w_{t} - \\alpha_{w}\\nabla_{w}MSE  \\nonumber$<br>$ \\approx w_{t} - \\alpha_{w} <em> (-2 </em> [r + \\gamma Q^{w}(s’,a’) - Q^{w}(s,a)] \\nabla_{w}Q^{w}(s,a)$</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Actor는 식(9)에 따라 보상이 $ \\bf maximize $되는 방향, 즉, deterministic policy를 stochastic gradient $ \\bf ascent $ 방법으로 update합니다.<ul>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Q-learning 을 이용한 off-policy actor-critic<ul>\n<li>stochastic behavior policy $ \\beta(a|s) $에 의해 생성된 trajectories로부터 deterministic target policy $ \\mu_{\\theta}(s) $를 학습하는 off-policy actor-critic입니다</li>\n<li>performance objective<ul>\n<li>$ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)V^{\\mu}(s)ds \\nonumber \\\\$<br>$= \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds \\nonumber \\\\$<br>$= E_{s \\sim \\rho^{\\beta}}[Q^{\\mu}(s,\\mu_{\\theta}(s))]$</li>\n</ul>\n</li>\n<li>off-policy deterministic policy gradient<ul>\n<li>$ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $<ul>\n<li>논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됩니다.</li>\n<li>$ \\begin{eqnarray}<br>  \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) &amp;\\approx&amp; \\int_{S}\\rho^{\\beta}(s)\\nabla_{\\theta}\\mu_{\\theta}(a|s)Q^{\\mu}(s,a)ds \\nonumber \\<br>  &amp;=&amp; E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]<br>  \\end{eqnarray} $</li>\n<li>근거: Action이 deterministic하기에 stochastic 경우와는 다르게 performance objective에서 action에 대해 평균을 구할 필요가 없습니다. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b]에서 처럼 곱의 미분을 통해 생기는 action-value function에 대한 gradient term을 생략할 필요가 사라집니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Remind: 큐러닝(Q-learning) update rule<ul>\n<li>$ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $</li>\n</ul>\n</li>\n<li>algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)<ul>\n<li>살사를 이용한 on-policy deterministic actor-critic과 아래 부분을 제외하고는 같습니다.<ul>\n<li>target policy는 $ \\beta(a|s) $에 의해 생성된 trajectories를 통해 학습합니다.</li>\n<li>업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $이 아니라 정책으로부터 나온 행동 값 $ \\mu_{\\theta}(s_{t+1}) $을 사용합니다.<ul>\n<li>$ \\mu_{\\theta}(s_{t+1}) $ 은 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $</li>\n</ul>\n</li>\n<li>Stochastic off-policy actor-critic은 대개 actor와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient에선 importance sampling이 필요없습니다.<ul>\n<li>Actor 는 deterministic 이기에 sampling 자체가 필요없습니다.<ul>\n<li>Stochastic policy인 경우, Actor에서 importance sampling이 필요한 이유는 상태 $ s $에서의 가치 함수 값 $ V^{\\pi}(s) $을 estimate하기 위해 $ \\pi $가 아니라 $ \\beta $에 따라 sampling을 한 후, 평균을 내기 때문입니다.</li>\n<li>하지만 deterministic policy인 경우, 상태 $ s $에서의 가치 함수 값 $ V^{\\pi}(s) = Q^{\\pi}(s,\\mu_{\\theta}) $ 즉, action이 상태 s에 대해 deterministic이기에 sampling을 통해 estimate할 필요가 없고, 따라서 importance sampling도 필요없어집니다.</li>\n<li>stochastic vs. deterministic performance objective<ul>\n<li>stochastic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads $</li>\n<li>deterministic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Critic이 사용하는 Q-learning은 importance sampling이 필요없는 off policy 알고리즘입니다.<ul>\n<li>Q-learning도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic<ul>\n<li>위 살사/Q-learning 기반 on/off-policy는 아래와 같은 문제가 존재합니다.<ul>\n<li>function approximator에 의한 bias<ul>\n<li>일반적으로 $ Q^{\\mu}(s,a) $ 를 $ Q^{w}(s,a) $로 대체하여 deterministic policy gradient를 구하면, 그 gradient는 ascent하는 방향이 아닐 수도 있습니다.</li>\n</ul>\n</li>\n<li>off-policy learning에 의한 instabilities</li>\n</ul>\n</li>\n<li>그래서 stochastic처럼 $ \\nabla_{a}Q^{\\mu}(s,a) $를 $ \\nabla_{a}Q^{w}(s,a) $로 대체해도 deterministic policy gradient에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $를 찾아야 합니다.</li>\n<li>Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $는 deterministic policy $ \\mu_{\\theta}(s) $와 compatible 합니다. 즉, $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $<ul>\n<li>$ \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $입니다.</li>\n<li>$ w $는 $ MSE(\\theta, w) = E[\\epsilon(s;\\theta,w)^{\\top}\\epsilon(s;\\theta,w)] $를 최소화합니다.<ul>\n<li>$ \\epsilon(s;\\theta,w) = \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} - \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}  $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Theorem 3은 on-policy 뿐만 아니라 off-policy에도 적용 가능합니다.</li>\n<li>$ Q^{w}(s,a) = (a-\\mu_{\\theta}(s))^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top} w + V^{v}(s) $<ul>\n<li>어떠한 deterministic policy에 대해서도 위 형태와 같은 compatible function approximator가 존재합니다.</li>\n<li>앞의 term은 advantage를, 뒤의 term은 value로 볼 수 있습니다.</li>\n</ul>\n</li>\n<li>$ Q^{w}(s,a) = \\phi(s,a)^{\\top} w + v^{\\top}\\phi(s) $<ul>\n<li>정의 : $ \\phi(s,a) \\overset{\\underset{\\mathrm{def}}{}}{=} \\nabla_{\\theta}\\mu_{\\theta}(s)(a-\\mu_{\\theta}(s)) $</li>\n<li>일례 : $ V^{v}(s) = v^{\\top}\\phi(s) $</li>\n<li>Theorem 3 만족 여부<ul>\n<li>첫 번째 조건 만족합니다.</li>\n<li>두 번째 조건은 대강 만족합니다.<ul>\n<li>$ \\nabla_{a}Q^{\\mu}(s,a) $에 대한 unbiased sample을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $를 학습합니다.</li>\n<li>이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \\approx Q^{\\mu}(s,a) $인 reasonable solution을 찾을 수 있기에 대강 $ \\nabla_{a}Q^{w}(s,a) \\approx \\nabla_{a}Q^{\\mu}(s,a) $이 될 것입니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>action-value function에 대한 linear function approximator는 큰 값을 가지는 actions에 대해선 diverge할 수 있어 global하게 action-values 예측하기에는 좋지 않지만, local critic에 사용할 때는 매우 유용합니다.<ul>\n<li>즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\\mu_{\\theta}(s)+\\delta) = \\delta^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $로, diverge하지 않고, 값을 얻을 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)<ul>\n<li>Critic: 실제 action-value function에 대한 linear function approximator인 $ Q^{w}(s,a) = \\phi(s,a)^{\\top}w $를 estimate합니다.<ul>\n<li>$ \\phi(s,a) = a^{\\top}\\nabla_{\\theta}\\mu_{\\theta} $</li>\n<li>Behavior policy $ \\beta(a|s) $로부터 얻은 samples를 이용하여 Q-learning이나 gradient Q-learning과 같은 off-policy algorithm으로 학습 가능합니다.</li>\n</ul>\n</li>\n<li>Actor: estimated action-value function에 대한 gradient를 $ \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $로 치환 후, 정책을 업데이트합니다.</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $</li>\n</ul>\n</li>\n<li>off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있습니다.<ul>\n<li>$ \\mu_{\\theta}(s_{t+1}) $이 diverge할 수도 있기 때문으로 판단됩니다.</li>\n<li>그렇기에 simple Q-learning 대신 다른 기법이 필요합니다.</li>\n</ul>\n</li>\n<li>그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm을 제안합니다.<ul>\n<li>gradient temporal-difference learning에 기반한 기법들은 true gradient descent algorithm이며, converge가 보장됩니다. (Sutton, 2009)<ul>\n<li>기본 아이디어는 stochastic gradient descent로 mean-squared projected Bellman error (MSPBE)를 최소화하는 것입니다.</li>\n<li>critic이 actor보다 빠른 time-scale로 update되도록 step size들을 잘 조절하면, critic은 MSPBE를 최소화하는 parameters로 converge하게 됩니다.</li>\n<li>critic에 gradient temporal-difference learning의 일종인 gradient Q-learning을 사용한 논문입니다. (Maei, 2010)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>COPDAC-GQ algorithm<ul>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) - \\alpha_{w}\\gamma\\phi(s_{t+1}, \\mu_{\\theta}(s_{t+1}))(\\phi(s_{t},a_{t})^{\\top} u_{t}) $</li>\n<li>$ v_{t+1} = v_{t} + \\alpha_{v}\\delta_{t}\\phi(s_{t}) - \\alpha_{v}\\gamma\\phi(s_{t+1})(\\phi(s_{t},a_{t})^{\\top} u_{t}) $</li>\n<li>$ u_{t+1} = u_{t} + \\alpha_{u}(\\delta_{t}-\\phi(s_{t}, a_{t})^{\\top} u_{t})\\phi(s_{t}, a_{t}) $</li>\n</ul>\n</li>\n<li>stochastic actor-critic과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $입니다.<ul>\n<li>m은 action dimensions, n은 number of policy parameters</li>\n</ul>\n</li>\n<li>Natural policy gradient를 이용해 deterministic policies를 찾을 수 있습니다.<ul>\n<li>$ M(\\theta)^{-1}\\nabla_{\\theta}J(\\mu_{\\theta}) $은 any metric $ M(\\theta) $에 대한 performance objective (식(14))의 steepest ascent direction 입니다. (Toussaint, 2012)</li>\n<li>Natural gradient는 Fisher information metric $ M_{\\pi}(\\theta) $에 대한 steepest ascent direction 입니다.<ul>\n<li>$ M_{\\pi}(\\theta) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}] $</li>\n<li>Fisher information metric은 policy reparameterization에 대해 불변입니다. (Bagnell, 2003)</li>\n</ul>\n</li>\n<li>deterministic policies에 대한 metric으로 $ M_{\\mu}(\\theta) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}] $을 사용합니다.<ul>\n<li>이는 variance가 0인 policy에 대한 Fisher information metric으로 볼 수 있습니다.</li>\n<li>$ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\theta}(a\\vert s)}$에서 policy variance가 0이면, 특정 s에 대한 $ \\pi_{\\theta}(a|s)$만 1이 되고, 나머지는 0입니다.</li>\n</ul>\n</li>\n<li>deterministic policy gradient theorem과 compatible function approximation을 결합하면 $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] $이 됩니다.<ul>\n<li>$ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $</li>\n<li>$ \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)} \\approx \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $</li>\n</ul>\n</li>\n<li>그렇기에 steepest ascent direction은 $ M_{\\mu}(\\theta)^{-1}\\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = w $이 됩니다.<ul>\n<li>$ E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}]^{-1}E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] = w $</li>\n</ul>\n</li>\n<li>이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ에서 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $ 식을 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}w_{t} $로 바꿔주기만 하면 됩니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p><br><br></p>\n<h1 id=\"5-Experiments\"><a href=\"#5-Experiments\" class=\"headerlink\" title=\"5. Experiments\"></a>5. Experiments</h1><p><br></p>\n<h2 id=\"5-1-Continuous-Bandit\"><a href=\"#5-1-Continuous-Bandit\" class=\"headerlink\" title=\"5.1. Continuous Bandit\"></a>5.1. Continuous Bandit</h2><ul>\n<li>Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행합니다.<ul>\n<li>Action dimension이 커질수록 성능 차이가 심합니다.</li>\n<li>빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있습니다.</li>\n<li><img src=\"https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"5-2-Continuous-Reinforcement-Learning\"><a href=\"#5-2-Continuous-Reinforcement-Learning\" class=\"headerlink\" title=\"5.2. Continuous Reinforcement Learning\"></a>5.2. Continuous Reinforcement Learning</h2><ul>\n<li>COPDAC-Q, SAC, off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행합니다.<ul>\n<li>COPDAC-Q의 성능이 약간 더 좋습니다.</li>\n<li>COPDAC-Q의 학습이 더 빨리 이뤄집니다.</li>\n<li><img src=\"https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"5-3-Octopus-Arm\"><a href=\"#5-3-Octopus-Arm\" class=\"headerlink\" title=\"5.3. Octopus Arm\"></a>5.3. Octopus Arm</h2><ul>\n<li>목표: 6 segments octopus arm (20 action dimensions &amp; 50 state dimensions)을 control하여 target을 맞추는 것입니다.<ul>\n<li>COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤입니다.</li>\n<li><img src=\"https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1\" width=\"600px\"></li>\n<li>기존 기법들은 action spaces 혹은 action과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않습니다.<ul>\n<li>기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 보여줬으면 하지만 실험을 하지 않았습니다.</li>\n</ul>\n</li>\n<li>8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보입니다.</li>\n</ul>\n</li>\n<li>[참고] Octopus Arm 이란?<ul>\n<li><a href=\"https://www.youtube.com/watch?v=AxeeHif0euY\" target=\"_blank\" rel=\"noopener\">OctopusArm Youtube Link</a></li>\n<li><img src=\"https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"Sutton-PG-여행하기\"><a href=\"#Sutton-PG-여행하기\" class=\"headerlink\" title=\"Sutton PG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/\">Sutton PG 여행하기</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"DDPG-여행하기\"><a href=\"#DDPG-여행하기\" class=\"headerlink\" title=\"DDPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/\">DDPG 여행하기</a></h2>"},{"title":"Policy Gradient Methods for Reinforcement Learning with Function Approximation","date":"2018-06-28T05:18:32.000Z","author":"김동민, 이동민","subtitle":"피지여행 1번째 논문","_content":"\n<center> <img src=\"https://www.dropbox.com/s/83i4atz733ali0w/Screen%20Shot%202018-07-18%20at%2012.10.52%20AM.png?dl=1\" width=\"600\"> </center>\n\n논문 저자 : Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour    \n논문 링크 : [NIPS](http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)\nProceeding : Advances in Neural Information Processing Systems (NIPS) 2000        \n정리 : 김동민, 이동민\n\n---\n\n# 1. Intro to Policy Gradient\n\n이 논문은 policy gradient (PG) 기법의 효시와도 같으며 향후 많은 파생연구를 낳은 중요한 논문입니다. 7페이지의 짧은 논문이지만 읽기에 만만한 논문은 아닙니다. 이 논문을 이해하기 위해 필요한 배경지식을 먼저 설명하고 논문을 차근차근 살펴보도록 하겠습니다.\n\n<br>\n## 1.1 Value Function Approach\n\n전통적으로 강화학습 기법은 value function을 기반으로 동작하였습니다. 특정 state에서 vaue function 또는 value function을 근사하는 함수(function approximation)를 최대화하는 action을 찾는 greedy action-selection policy가  대표적입니다.\n\n논문에서는 이러한 방법은 deterministic한 policy를 찾는 쪽으로 나아가게 되지만 종종 최적의 policy는 stochastic한 성질을 가지기 때문에 이 방법으로는 최적의 policy를 찾을 수 없다고 언급하고 있습니다. (그러나 이 논문이 나오고 나서 한참 후 David Silver를 필두로한 DeepMind의 연구진들은 high-dimensional action space를 가지는 application에서 보다 빠르게 동작하는 [deterministic policy gradient](../../../06/27/2_dpg/)을 개발하였습니다.) 아마도 이러한 부분은 $\\epsilon$-greedy action-selection method로 개선될 수 있을 것입니다. 또 하나의 문제는 value function의 작은 변화로 인해서 action이 크게 변할 수도 있다는 것입니다. 이것은 알고리듬의 수렴성에 문제를 야기할 수 있습니다. 이러한 문제점을 해결하기 위하여 policy search라는 새로운 기법이 고안됩니다.\n\n<br>\n## 1.2 Policy Search\n\npolicy search는 최적의 policy $\\pi^*$를 reward로부터 직접 찾습니다. policy search 방법은 크게 두 가지로 분류할 수 있습니다. 첫 번째는 gradient-based optimization으로 policy gradient method가 이에 속합니다. 두 번째는 gradient-free optimization으로 진화(evolutionary) 연산을 이용하는 것이 이에 속합니다. 이 논문에서는 제목에서 알 수 있듯이 gradient-based optimization을 다룹니다. gradient는 변화량을 의미합니다. 특정 policy x와 또 다른 policy y가 있을 때 policy가 얼마나 많이 변화했는지 어떻게 모델링할 수 있을까요? policy의 변화를 제어하는 어떤 파라미터가 있다면 이 파라미터를 조정하여 policy를 변화시킬 수 있습니다. policy x에 해당하는 파라미터값과 policy y에 해당하는 파라미터값의 차이가 policy의 변화량이라고 할 수 있습니다. 이와 같은 방식으로 policy의 변화를 모델링하기 위하여 파라미터 $\\theta$를 이용하여 policy를 $\\pi_{\\theta}$로 표현할 수 있습니다. 최적의 policy를 찾기 위하여 expected return $E\\left[R|\\theta\\right]$이 최대화되도록 parameter를 조정합니다. 이를 간단한 수식으로 정리하면 다음과 같습니다.\n\n$$\n\\pi^* = \\arg \\max\\limits_\\pi  E\\left[ {\\left. R \\right|\\pi } \\right] \\to {\\text{original problem} } \n\\\\\\\\  {\\Downarrow} \\\\\\\\\n{\\text{policy parameterization by } } \\pi_{\\theta}: \\Theta  \\to \\Pi \\\\\\\\ {\\Downarrow} $$\n\n$$\\pi^* = \\arg \\max\\limits_{\\theta}  E\\left[ {\\left. R \\right|\\theta } \\right] \\to {\\text{policy search problem} }\n$$\n\n<br>\n## 1.3 How to Obtain the Expected Return\nexpected return을 최대화하는 방향으로 policy를 update한다고 하였습니다. 그렇다면 expected return은 어떻게 구할 수 있을까요? 여기에도 크게 두 가지 방법이 있습니다. 첫 번째는 deterministic approximation으로 Markov decision process의 dynamics를 모델링한 후 수식을 통해 구하는 것입니다. 두 번째 방법은 monte carlo estimation으로 dynamics에 대한 모델을 하지 않고 많은 sample들을 얻은 후 empirical하게 expected return을 계산하는 방법입니다. 어느 방법이 더 좋을지는 풀고자 하는 문제의 특성에 따라 다를 것입니다. dynamics에 대한 모델이 어렵거나 변화가 큰 경우에는 두 번째 방법이 좀 더 현실적이지만 gradient를 구하는 것은 더 어렵습니다. 결국 gradient를 esimate하는방법을 고안해야 합니다. 가장 유명한 gradient estimation 방법이 1992년 R. J. Williams에 의해서 제안된 [REINFORCE](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf) 기법입니다. REINFORCE는 Monte Carlo estimate 또는 likelihood-ratio estimate라고 부르는 방법을 이용합니다. 이 방법에 대해서 좀 더 알아볼까요?\n\n### 1.3.1 Monte Carlo Gradient Estimation\n다음과 같은 parameter $\\theta$를 가지는 random variable $X$가 있습니다: $X:\\Omega\\mapsto\\mathcal{X}$. 그리고 이 $x$에 대한 함수 $f$가 있습니다: $f:\\mathcal{X}\\mapsto\\mathbb{R}$. expected return처럼 $E[f(x)]$를 최대화하고자 합니다. 이를 위해서는 $\\nabla_{\\theta}E[f(x)]$를 구해야 합니다. 이 때 log derivate trick을 이용하여 다음과 같이 수식을 변형시킬 수 있습니다.\n\n$$\n\\begin{align}\n\\nabla_{\\theta} E_{p(x;\\theta)}\\left[ {f(x)} \\right] \n&= \\nabla_\\theta\\int{f\\left( x \\right)p\\left( {x;\\theta} \\right) dx}\n\\\\\\\\\n&= \\int { {\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= \\int {\\frac{ {p\\left( {x;\\theta } \\right)} }{ {p\\left( {x;\\theta } \\right)} }{\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= \\int {p\\left( {x;\\theta } \\right){\\nabla_\\theta }\\log p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= E_{p(x;\\theta)}[f(x)\\nabla_\\theta \\log p({x;\\theta})]\n\\end{align}\n$$\n\n이제 expectation은 많은 샘플들을 모아서 평균을 취하는 Monte Carlo 기법으로 근사화할 수 있습니다.\n\n$${\\nabla_{\\theta} }{E_{p\\left( {x;\\theta } \\right)} }\\left[ {f\\left( x \\right)} \\right] = \\frac{1}{N}\\sum_{n=1}^N f\\left(X_n\\right)\\nabla_\\theta\\log p\\left(X_n;\\theta\\right)$$\n\n이 방법을 사용하기 위해서 필요한 조건은 $\\log p\\left(X_n;\\theta\\right)$가 미분가능해야 한다는 것 뿐입니다. 그러나 이 방법은 얻은 샘플들에 의존하기 때문에 경우에 따라서 큰 variance를 가질 수 있습니다.\n\n<br><br>\n\n# 2. Policy Gradient Methods\n\n이제 본격적으로 policy gradient 기법에 대해서 알아보겠습니다.\n\n<br>\n## 2.1 System Model\n\n논문에서 사용하는 수학기호(notation)와 가정들을 설명하는 시스템모델은 다음과 같습니다.\n\n* Markov decision process (MDP) at each time $t\\in{0,1,2,...}$\n* $S_t\\in\\mathcal{S}$: state\n* $A_t\\in\\mathcal{A}$: action\n* $r_t\\in \\mathcal{R}$: reward\n* $\\theta\\in\\mathbb{R}^l$: vector of policy parameters, for $l\\ll\\left|\\mathcal{S}\\right|$\n* $\\mathcal{P} _{s s'}^a = \\Pr[S _{t+1}=s' \\vert S_t=s,A_t=a]$: state transition probabilities\n* $\\mathcal{R} _{s s'}^a = E[R _{t+1}\\vert S_t=s,A_t=a]$: expected reward\n* $\\pi(s,a,\\theta)=\\Pr[A_t=a|S_t=s,\\theta]$: policy, shortened as $\\pi(s,a)$ or $\\pi(a|s)$\n* $\\theta$: policy parameter들의 vector  \n* $\\rho$: 해당 policy들의 성능을 나타내는 척도 (예. average reward per step)  \n* $\\alpha$: positive-definite한 step size \n\n<br>\n## 2.2 Policy Gradent Approach\npolicy gradient는 stochastic policy를 자체적인 파리미터를 가진 function approximator를 이용해서 근사화시킵니다.\n\n\npolicy parameter는 gradient에 비례하여 업데이트됩니다.\n\n$$\\Delta\\theta \\approx \\alpha\\frac{\\partial\\rho}{\\partial\\theta}$$\n\n$\\rho$는 $\\theta$에 대하여 미분가능해야겠죠? 위와 같이 업데이트하면 local optimal policy로 수렴합니다. 논문의 가장 중요한 contribution은 특정 조건을 만족하는 function approximator를 이용하여 경험(experience, sample)을 축적하고 이것들을 이용하여 위의 gradient를 unbiased estimate할 수 있음을 증명한 것입니다.\n\n먼저 $\\rho$, 즉, reward를 표현하는 두 가지 방법에 대해서 알아보겠습니다.\n\n<br>\n## 2.3 Average Reward Formulation\nAverage reward formulation은 시간의 흐름에 따른 reward를 표현한다기보다는 모든 시간의 reward를 평균을 내서 표현하는 방법입니다.\n\n* Long‐term expected reward per step\n$$\\rho(\\pi)=\\lim_{n\\to\\infty}\\frac{1}{n}E[r_1+r_2+\\cdots+r_n|\\pi] = \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\mathcal{R}_s^a$$\n\n    - $d^\\pi(s): \\lim_{t\\to\\infty}\\Pr[S_t=s|s_0,\\pi]$: stationary distribution of states under $\\pi$\n    - $d^\\pi(s)$가 존재한다고 가정합니다. 그리고 모든 policy에 대해서 $d^\\pi(s)$는 $s_0$와 independent합니다.\n    - 위의 (limit수식과 summation수식 사이의) 등식은 왜 성립할까요? Time average와 Ensemble average가 같다는 뜻으로 [ergodic](https://en.wikipedia.org/wiki/Ergodicity)한 system에서 성립합니다.\n\n* Value of a state-action pair given a policy\n$$Q^{\\pi} (s,a) = \\sum_{t=1}^\\infty E[r_t - \\rho(\\pi)|S_0=s, A_0=a, \\pi]$$\n    - 위의 Q-function의 표현식이 정의인지 유도된 것인지는 이 포스트를 작성하는 저희도 아직 확실히 모르겠습니다. 다만, $r_t$들을 더하면 무한대로 발산할 가능성이 매우 높은데 $\\rho$를 빼줌으로해서 어찌보면 평균 reward로부터의 차이를 더하는 것이라고 할 수 있고 이것은 bound된 값일 가능성이 더 높기 때문에 Q-function을 이렇게 표현하는 것이 더 안전하다고 할 수 있습니다. 이 부분에 대해서 좋은 의견이 있으시면 피드백을 주시면 감사하겠습니다!\n\n* State-value function\n$$V^\\pi(s) = \\sum_{a} \\pi(s,a)Q^\\pi(s,a)$$\n\n<br>\n## 2.4 Start-State Formulation\nstart-state formulation은 시간의 흐름에 따라 감소하는 reward들을 표현합니다.\n\n* Long-term expected reward per step with a designated start state $S_0$\n$$\\rho(\\pi) = E\\left[\\left.\\sum_{t=1}^\\infty \\gamma^{t-1}r_t\\right|S_0,\\pi\\right]$$\n    - $\\gamma\\in\\left[0,1\\right]$: discount rate\n\n* Value of a state‐action pair given a policy\n$$Q^\\pi(s,a) = E\\left[\\left.\\sum_{k=1}^\\infty \\gamma^{k-1} r_{t+k}\\right|S_t=s, A_t=a, \\pi\\right]$$\n\n* Alternative form of $Q^\\pi(s,a)$\n$$\nQ^\\pi(s,a) = R_s^a + \\gamma\\sum _{s'} \\mathcal{P} _{s s'}^a V^{\\pi}(s')\n$$\n\n<br>\n## 2.5 Policy Gradient Theorem\n논문의 중요한 결과인 Theorem 1은 다음과 같습니다.\n\n**Theorem 1 (Policy Gradient)** *For any MDP, in either the average‐reward or start‐state formulations,*\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$\n\n놀라운 부분은 expected return의 gradient를 취할 때 $\\frac{\\partial d^\\pi(s)}{\\partial\\theta}$를 구하지 않아도 된다는 점입니다. 즉, policy의 변화가 state distribution에 영향을 주지 않는다는 것입니다. 이것은 다시 말하면 아래 수식과 같이 우리는 $\\frac{\\partial\\rho}{\\partial\\theta}$에 대한 unbiased estimator를 구할 수 있다는 뜻입니다.\n\n$$E_s\\left[\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\\right] = \\frac{\\partial\\rho}{\\partial\\theta}$$\n\n이로인해서 우리는 gradient를 sampling을 통해서 추정할 수 있습니다. 그렇지만 이것은 sample이 아주 많을 때만 성립합니다!\n\n또 한 가지 문제는 우리는 $Q^\\pi(s,a)$의 정확한 값을 알 수 없다는 것 입니다. 이것을 estimate하기 위해서 현재의 return값 $R_t$를 이용할 수 있습니다.\n\n<br>\n## 2.6 Proof of Policy Gradient Theorem\n증명을 살펴보겠습니다.\n\n* average-reward formulation을 이용할 때의 증명은 아래와 같습니다.\n\n$$\n\\begin{align}\n\\frac{\\partial V^\\pi(s)}{\\partial\\theta} &\\overset{\\underset{\\mathrm{def} }{} }{=}\\frac{\\partial}{\\partial\\theta}\\sum_a \\pi(s,a)Q^\\pi(s,a)\n\\\\\\\\\n&=\\sum_a \\frac{\\partial}{\\partial\\theta}\\left[\\pi(s,a)Q^\\pi(s,a)\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial Q^\\pi(s,a)}{\\partial\\theta}\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial}{\\partial\\theta}\\left[R_s^a -\\rho(\\pi) + \\sum_{s'}\\mathcal{P}_{s s'}^a V^{\\pi}(s')\\right]\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\left[-\\frac{\\partial\\rho}{\\partial\\theta} + \\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta}\\right]\\right]\\\\\\\\\n\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta}\\right] - \\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n양변에 stationary distribution에 대한 평균을 취합니다.\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta} \n\\\\\\\\\n&- \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n모든 state $s$에서 모든 state $s'$으로 이동하는 것은 state $s'$에 존재할 확률이므로 다음과 같이 표현할 수 있습니다.\n\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s') \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta} - \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n뒤의 두 항은 같은 식이므로 0이 됩니다. 따라서 다음과 같이 표현됩니다.\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\n\\end{align}\n$$\n\n$\\frac{\\partial\\rho}{\\partial\\theta}$는 reward의 파라미터에 대한 미분값입니다. reward 자체가 이미 모든 state에 대한 평균값이므로 $s$의 함수가 아닙니다.  $s$에 대한 함수가 아니므로 $\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}=\\frac{\\partial\\rho}{\\partial\\theta}\\sum_s d^\\pi(s)=\\frac{\\partial\\rho}{\\partial\\theta}$입니다. 여기서 $d^\\pi(s)$는 state에 대한 확률값이죠. 모든 state에 대해서 이 값을 다 더하면 1이 됩니다.\n이를 이용해서 위의 식을 다시 표현하면 아래와 같고 이를 통해 증명이 완성됩니다.\n\n$$\n\\begin{align}\n\\therefore\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\n\\end{align}\n$$\n\n* 다음으로 start-state formulation에 대한 증명입니다.\n\nstart-state formulation은 이 논문에서도 나오고 서튼책에서도 나오는데, 서튼책의 증명이 더 설명이 자세합니다. 그래서 논문에 있는 증명이 아닌 서튼책에 있는 증명으로 설명드리겠습니다. 다만, notation이 약간 다른데, 큰 문제 없이 이해하실 수 있으므로 별도의 설명은 하지 않겠습니다. (서튼책 [Link](https://drive.google.com/file/d/1xeUDVGWGUUv1-ccUMAZHJLej2C7aAFWY/view))\n\n$$\\nabla{v_\\pi}(s)=\\nabla\\big[\\sum_a\\pi(a|s)q_\\pi(s,a)\\big]$$ \n\n다음으로 $(f\\cdot g)'=f'\\cdot g+f\\cdot g'$ (product rule of calculus)에 의해 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\nabla q_\\pi(s,a)\\big]$$  \n\n이어서 $p(s',r|s,a) := Pr[S_t=s', R_t=r|S_{t-1}=s,A_{t-1}=a ]$ (서튼책 3.2 공식) 에 의해 다음과 같이 나타낼 수 있습니다.\n\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_{\\pi}(s,a)+\\pi(a|s)\\nabla\\big[\\sum_{s', r}p(s',r|s,a)(r+v_{\\pi}(s'))\\big]\\big]$$  \n\n계속해서 $p(s'|s,a)=\\sum_{r\\in R}p(s',r|s,a)$ (서튼책 3.4 공식) 에 의해 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s'}p(s'|s,a)\\nabla v_\\pi(s') \\big]$$ \n\n여기서 $\\nabla v_\\pi(s')$ 부분을 unrolling을 하면 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s'}p(s'|s,a)\\sum_{a'}\\big[\\nabla\\pi(a'|s')q_\\pi(s',a')\\\\\\\\+\\pi(a'|s')\\sum_{s''}p(s''|s',a')\\nabla v_\\pi(s''))\\big] \\big]$$ \n\n$\\nabla v_\\pi(s'')$에 대해서 unrolling을 하고 또 나오는 다른 항에 대해서도 계속 반복하다보면 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_{x\\in S}\\sum_{k=0}^{\\infty}\\Pr(s\\rightarrow x,k,\\pi)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n여기에서 $\\Pr(s\\rightarrow x,k,\\pi)$ 는 policy $\\pi$ 에 대해 state $s$에서 state $x$까지 $k$ step만큼 움직일 때의 변환 확률입니다. 따라서 위의 수식을 아래처럼 다시 나타낼 수 있습니다.\n <!-- \\begin{align}  -->\n$$\\nabla J(\\theta)=\\nabla v_\\pi (s_0)$$ \n$$=\\sum_s(\\sum_{k=0}^{\\infty}\\Pr(s_0\\rightarrow s,k,\\pi))\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n$$=\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ \n<!-- \\end{align} -->\n- 서튼책에서 $\\eta(s)$는 다음과 같이 표현할 수 있습니다.\n    - $\\eta(s)$ = 평균적으로 하나의 episode마다 state에서 머무른 time step의 수 (page 199)\n- 논문에서는 $\\eta(s)$를 다음과 같이 표현합니다.\n    - $d^\\pi(s)=\\sum_{t=0}^{\\infty}\\gamma^t Pr(s_t=s|s_0,\\pi)$ \n    ($\\gamma=1$ is allowed only in episodic tasks = discounted weighting of states)\n\n여기서부터가 논문에는 없는 내용입니다. 위의 최종 수식을 다시 한 번 적으면 아래와 같습니다.\n$$\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n위의 수식을 아래와 같이 바꿀 수 있습니다.\n$$\\sum_{s'} \\eta(s') \\cdot\\sum_s \\frac{\\eta(s)}{\\sum_{s'}\\eta(s')} \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n그러면 이것을 새로운 기호로 다시 나타낼 수 있습니다.\n$$\\sum_{s'} \\eta(s') \\cdot\\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ \n- 여기서 $\\mu(s)$ 는 state distribution이라고 하며, weight를 state distribution으로 바꿔주는 과정이라고 생각할 수 있습니다. 쉽게 말해 어떠한 state에 agent가 머무르는 확률입니다.\n\n위의 수식은 아래의 수식과 비례합니다. 따라서 최종 형태는 다음과 같습니다.\n$$\\propto \\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n<br><br>\n\n# 3. Policy Gradient with Approximation\n이 장에서는 앞서 다뤘던 Theorem 1 중에 $Q^\\pi$에 대해서 중점적으로 다룹니다. 어떠한 $Q^\\pi$가 학습된 function approximator로 근사된다고 합시다. \n\n$f_w$ ($f_w$는 $S × A \\rightarrow \\Re$)는 parameter $w$를 가지는 어떠한 $Q^\\pi$ 에 대한 근사치입니다. 학습된 function approximator에 대해서 생각하기 때문에, 학습된 $f_w$는 error를 최소화하는 방향으로 parameter $w$를 업데이트합니다. 그러면 다음과 같은 관계식을 생각할 수 있습니다.\n$$\\Delta w_t \\propto \\frac{\\partial}{\\partial w}\\big[ {\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]^2 \\propto \\big[{\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s_t,a_t)}{\\partial w}$$ \n(여기서 ${\\hat Q}^\\pi(s_t,a_t)$ 는 $Q^\\pi(s_t,a_t)$의 unbiased estimator $R_t$입니다.)\n\n그러면 위와 같은 수식이 local optimum에 수렴을 했을 때, 다음과 같이 나타낼 수 있습니다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n- 위의 수식에 대한 추가 설명\n    - local optimum으로 수렴을 하기 때문에 당연히 위의 수식은 0이 됩니다.\n    - 또한 stochastic policy이기 때문에 local optimum으로 수렴하려면 모든 state, action에 대해 expectation을 해야합니다. 따라서 $\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)$이 붙게 됩니다.\n\n<br>\n## 3.1 Theorem 2: Policy Gradient with Function Approximation\n\n만약 $f_w$가 아래의 등식을 만족한다고 합시다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n또한 $f_w$가 policy parameterization과 양립할 수 있다고 합시다. policy parameterization에 대한 여러가지 의미가 있는데 이 논문에서는 다음을 의미합니다.\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$\n- 위의 수식에 대한 추가 설명\n    - 이 수식은 Monte-Calro Estimation을 이용하기 위한 조건입니다.\n    - Compatibility Condition이라고 부릅니다.\n    - 우변(= $\\nabla\\log\\pi(s,a)$)으로 유도함으로써 아래의 수식과 같이 sampling을 이용한 추정이 가능해집니다.\n\n따라서 위의 두 수식을 이용하여 아래와 같이 나타낼 수 있습니다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s,a)$$\n\n<br>\n## 3.2 Proof of Theorem 2\n\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$\n\n위의 두 수식을 합치면 다음과 같이 나타낼 수 있습니다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}=0$$\n\n분자와 분모에 있는 $\\pi(s,a)$를 지우면 아래와 같습니다.\n$$\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]=0$$\n\n이어서 위의 수식이 0이기 때문에 policy gradient theorem(앞서 다뤘던 Theorem 1)에서 위의 수식을 뺄 수 있습니다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)-\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]$$\n\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-Q^\\pi(s_t,a_t)+f_w(s_t,a_t) \\big]$$\n\n$$\\therefore\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s_t,a_t)$$\n\n<br><br>\n\n# 4. Application to Deriving Algorithms and Advantages\n\n<br>\n## 4.1 Application to Deriving Algorithms\nfeature의 linear combination에서 Gibbs distribution (softmax)를 하나의 policy로 생각해볼 수 있습니다.\n$$\\pi(s,a)=\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_b e^{\\theta^{T}\\phi_{sb} }}$$\n- $\\phi_{sa}$: state-action pair $s$, $a$를 나타내는 $l$-dimensional feature vector\n\ncompatible condition을 적용하면 다음과 같습니다. (4.2 Proof of application of compatible condition 참고)\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}=\\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}$$ \n\n이어서 $f_w$을 적분한 natural parameterization은 다음과 같습니다.\n$$f_w(s,a)=w^T\\big[ \\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}\\big]$$\n\n- 위의 수식에 대한 추가 설명\n    - $f_w$는 policy로서 같은 feature들에 대해 linear합니다.\n    - $f_w$는 각각의 state에 대해서 평균이 0입니다. ($\\sum_a\\pi(s,a)f_w(s,a)=0$)\n    - advantage function $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$의 하나의 근사치로서 $f_w$를 생각해도 좋습니다.\n\n<br>\n## 4.2 Proof of application of compatible condition\n증명을 하기 전에 다음을 가정합니다.\n1. $(\\frac{f(x)}{g(x)})'=f'(x)\\cdot \\frac{1}{g(x)}-f(x)\\cdot \\frac{g'(x)}{g(x)^{2} }=\\frac{f'(x)\\cdot g(x)-f(x)\\cdot g'(x)}{g(x)^{2} }$\n2. $f(\\theta)=e^{\\theta^{T}\\phi_{sa} }$, $g(\\theta)=\\sum_be^{\\theta^{T}\\phi_{sb} }$\n\n$$\n\\begin{align}\n\\frac{\\partial f_w(s,a)}{\\partial w}&=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\big(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big)'\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\big[ \\phi_{sa}(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})-(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})\\frac{\\sum_b\\phi_{sb}e^{ {\\theta^T}\\phi_{sb} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big]\\frac{1}{\\pi(s,a)}\\\\\\\\\n&=\\big[ \\phi_{sa}\\pi(s,a)-\\pi(s,a)\\sum_b\\phi_{sb}\\pi(s,b)\\big]\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\pi(s,a)\\big[ \\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\\big]\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&\\therefore\\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\n\\end{align}\n$$\n\n<br>\n## 4.3 Application to Advantages\nPolicy Gradient with Function Approximation Theorem (Theorem 2)는 advantage function으로 확장될 수 있습니다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_sd^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ f_w(s,a)+v(s) \\big]$$\n- 위의 수식에 대한 추가설명\n    - $v$ ($v$는 $S\\rightarrow\\Re$) 는 arbitrary function입니다.\n    - 이 수식은 $\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}=0$이기 때문에 가능해집니다.\n    - 즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없습니다.\n    - $v$의 선택은 Theorem들에 영향을 미치지 못하지만, 실질적으로 gradient estimator의 variance에 영향을 미칩니다.\n    - 이러한 문제는 전체적으로 이전의 연구에 reinforcement baseline의 사용에 있어서 유사합니다.\n    - (comment) 위의 수식에서 $f_w(s,a)+v(s)$와 Application to Deriving Algorithms의 $f_w(s,a)$와는 다른 것입니다. Application to Deriving Algorithms의 $f_w(s,a)$은 softmax에 의해 스스로 advantage function의 역할을 할 수 있습니다. 하지만 보통의 경우에는 그러지 못할 수도 있기 때문에 위의 수식처럼 $f_w(s,a)+v(s)$을 추가하여 zero mean만들어서 variance를 줄일 수 있습니다.\n\n<br><br>\n\n# 5. Convergence of Policy Iteration with Function Approximation\n\n<br>\n## 5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation\n\npolicy iteration with function approximation은 locally optimal policy에 수렴합니다. $\\pi$와 $f_w$를 \n\n1. compatibility condition을 만족하는 policy와 value function에 대한\n2. 그리고 $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|<B<\\infty$에 대한\n\n어떠한 미분가능한 function approximator라고 합시다.\n\n- (comment) $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|<B<\\infty$에서 $\\pi(s,a)$라는 function은 이계미분값이 존재하고, 임의의 상수인 (Bound) B에 bound되어 있기 때문에 function의 그래프는 smooth하다고 볼 수 있습니다. (아래의 그림 중 빨간색 그래프 참고)\n\n<center> <img src=\"https://www.dropbox.com/s/xx02ejfg5ao19ps/Screen%20Shot%202018-07-10%20at%203.41.06%20PM.png?dl=1\" width=\"200\"> </center>\n\n이어서 ${\\alpha_k}$는 $\\lim_{k\\rightarrow\\infty}\\alpha_k=0$이며 $\\sum_k\\alpha_k=\\infty$를 만족하는 step-size sequence라고 합시다.\n\n그 때, bounded reward를 가진 MDP에 대해\n1. 어떠한 $\\theta_0, \\pi_k=\\pi(\\cdot,\\cdot,\\theta_k)$\n2. 그리고 $\\sum_sd^{\\pi_{k} }(s)\\sum_a\\pi_k(s,a)\\big[ Q^{\\pi_{k} }(s,a)-f_w(s,a)\\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$로 인하여 $w_k=w$, $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$\n\n으로 정의된 sequence $\\rho(\\pi_k)$는 $\\lim_{k\\rightarrow\\infty}\\frac{\\partial\\rho(\\pi_k)}{\\partial\\theta}=0$이기 때문에 수렴합니다.\n- sequence $\\rho(\\pi_k)_{k=0}^\\infty$에 대한 추가 설명\n    - 2번의 식을 통해 자연스럽게 actor-critic으로 연결됩니다.\n    - $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$ 에 따라 $\\theta$가 1, 2, ..., $\\infty$로 갈텐데, 거기에 따른 objective function or performance의 sequence입니다.\n    - (comment) 굳이 sequence라는 표현이 없어도 될 것 같습니다. 어짜피 k가 $\\infty$로 가면 $\\rho(\\pi_k)$가 수렴한다는 의미이기 때문에 불필요해보입니다.\n\n<br>\n## 5.2 Proof of Theorem 3\n- Theorem 2는 $\\theta_k$ update가 gradient의 error를 최소화한다는 것을 증명했습니다.\n- $\\frac{\\partial^{2}\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}$와 MDP의 reward에서의 bound는, $\\frac{\\partial^{2}\\rho}{\\partial\\theta_i\\partial\\theta_j}$ 또한 bound된다는 것을 증명합니다.\n- step-size 필요조건 때문에 이러한 bound된 것들은 Proposition 3.5 from page 96 of Bertsekas and Tsitsiklis (1996)에 적용하기 위해 필요한 조건입니다.\n- Proposition 3.5 from page 96 of Bertsekas and Tsitsiklis (1996)은 local optimum으로 수렴한다는 것을 증명했습니다.\n- Proposition 3.5를 자세하게 설명하지는 않겠습니다. 일반적인 gradient method의 수렴성을 증명한 것이며, 이 논문의 policy gradient method도 gradient method의 일종이므로 특정 조건을 만족할 때 수렴한다는 것입니다. 수렴하는 원리는 소위 말하는 Lipschitz continuity condition을 만족하기 때문인데 다음과 같습니다. $L>0$인 임의의 상수입니다.\n$$\n\\parallel \\nabla f(r) - \\nabla f(\\bar{r}) \\parallel \\leq L \\parallel r - \\bar{r} \\parallel\n$$\n즉, 업데이트가 진행되어 나감에 따라서 gradient의 차이가 점점 줄어들게 되므로 언젠가는 0으로 수렴하게 됩니다.\n\n<br><br>\n\n# 6. Summary \n\n논문에서 설명한 policy gradient 기법을 요약하면 다음과 같습니다.\n\n* Original maximization problem: $\\max\\rho_{\\theta}=E\\left[R\\right]$\n* gradient ascent method를 이용한 parameter update: $\\theta_{t+1} = \\theta_{t} + \\alpha\\left.\\frac{\\partial E[R]}{\\partial\\theta}\\right|_{\\theta_t}$\n    - 그러나 우리는 gradient를 모르고, 추정하기도 어렵습니다. 왜냐하면 expectation이 안에 있기 때문입니다.\n    - 그렇다면 이것을 추정 가능한 형태로 바꿔야합니다. 다시 말해 expectation이 밖에 있는 형태로 바꾸는 것입니다.\n    - Expectation이 밖에 있으면 왜 추정이 유리할까요? 바로 Sample mean을 취하면 되기 때문입니다.\n* Approximate gradient\n    - 방법1: 논문의 Theorem 1을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다.\n    $$\\frac{\\partial E[R]}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$\n    - 방법2: Log derivative trick을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다. (Theorem 2)\n    $$\\theta_{t+1} = \\theta_{t} + \\alpha E\\left[ \\left. R\\frac{\\partial}{\\partial\\theta}\\log p_{\\theta} \\right\\vert \\theta_t\\right]$$\n    - 그리고 특정 trajectory를 따라가면서 return값을 구하고 이것을 여러 번 수행하여 sample mean을 취하면 gradient를 추정하는 것이 가능합니다.\n    - 이러한 기법을 제시한 기존의 연구가 REINFORCE입니다.\n    - 여러 Trajectory를 이용하므로 variance가 높을 수 밖에 없습니다.\n    - Advantage를 활용하거나 하는 방식으로 이후 여러 연구가 진행되었습니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 다음으로\n\n## [DPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/)","source":"_posts/1_sutton-pg.md","raw":"---\ntitle: Policy Gradient Methods for Reinforcement Learning with Function Approximation\ndate: 2018-06-28 14:18:32\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 김동민, 이동민\nsubtitle: 피지여행 1번째 논문\n---\n\n<center> <img src=\"https://www.dropbox.com/s/83i4atz733ali0w/Screen%20Shot%202018-07-18%20at%2012.10.52%20AM.png?dl=1\" width=\"600\"> </center>\n\n논문 저자 : Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour    \n논문 링크 : [NIPS](http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)\nProceeding : Advances in Neural Information Processing Systems (NIPS) 2000        \n정리 : 김동민, 이동민\n\n---\n\n# 1. Intro to Policy Gradient\n\n이 논문은 policy gradient (PG) 기법의 효시와도 같으며 향후 많은 파생연구를 낳은 중요한 논문입니다. 7페이지의 짧은 논문이지만 읽기에 만만한 논문은 아닙니다. 이 논문을 이해하기 위해 필요한 배경지식을 먼저 설명하고 논문을 차근차근 살펴보도록 하겠습니다.\n\n<br>\n## 1.1 Value Function Approach\n\n전통적으로 강화학습 기법은 value function을 기반으로 동작하였습니다. 특정 state에서 vaue function 또는 value function을 근사하는 함수(function approximation)를 최대화하는 action을 찾는 greedy action-selection policy가  대표적입니다.\n\n논문에서는 이러한 방법은 deterministic한 policy를 찾는 쪽으로 나아가게 되지만 종종 최적의 policy는 stochastic한 성질을 가지기 때문에 이 방법으로는 최적의 policy를 찾을 수 없다고 언급하고 있습니다. (그러나 이 논문이 나오고 나서 한참 후 David Silver를 필두로한 DeepMind의 연구진들은 high-dimensional action space를 가지는 application에서 보다 빠르게 동작하는 [deterministic policy gradient](../../../06/27/2_dpg/)을 개발하였습니다.) 아마도 이러한 부분은 $\\epsilon$-greedy action-selection method로 개선될 수 있을 것입니다. 또 하나의 문제는 value function의 작은 변화로 인해서 action이 크게 변할 수도 있다는 것입니다. 이것은 알고리듬의 수렴성에 문제를 야기할 수 있습니다. 이러한 문제점을 해결하기 위하여 policy search라는 새로운 기법이 고안됩니다.\n\n<br>\n## 1.2 Policy Search\n\npolicy search는 최적의 policy $\\pi^*$를 reward로부터 직접 찾습니다. policy search 방법은 크게 두 가지로 분류할 수 있습니다. 첫 번째는 gradient-based optimization으로 policy gradient method가 이에 속합니다. 두 번째는 gradient-free optimization으로 진화(evolutionary) 연산을 이용하는 것이 이에 속합니다. 이 논문에서는 제목에서 알 수 있듯이 gradient-based optimization을 다룹니다. gradient는 변화량을 의미합니다. 특정 policy x와 또 다른 policy y가 있을 때 policy가 얼마나 많이 변화했는지 어떻게 모델링할 수 있을까요? policy의 변화를 제어하는 어떤 파라미터가 있다면 이 파라미터를 조정하여 policy를 변화시킬 수 있습니다. policy x에 해당하는 파라미터값과 policy y에 해당하는 파라미터값의 차이가 policy의 변화량이라고 할 수 있습니다. 이와 같은 방식으로 policy의 변화를 모델링하기 위하여 파라미터 $\\theta$를 이용하여 policy를 $\\pi_{\\theta}$로 표현할 수 있습니다. 최적의 policy를 찾기 위하여 expected return $E\\left[R|\\theta\\right]$이 최대화되도록 parameter를 조정합니다. 이를 간단한 수식으로 정리하면 다음과 같습니다.\n\n$$\n\\pi^* = \\arg \\max\\limits_\\pi  E\\left[ {\\left. R \\right|\\pi } \\right] \\to {\\text{original problem} } \n\\\\\\\\  {\\Downarrow} \\\\\\\\\n{\\text{policy parameterization by } } \\pi_{\\theta}: \\Theta  \\to \\Pi \\\\\\\\ {\\Downarrow} $$\n\n$$\\pi^* = \\arg \\max\\limits_{\\theta}  E\\left[ {\\left. R \\right|\\theta } \\right] \\to {\\text{policy search problem} }\n$$\n\n<br>\n## 1.3 How to Obtain the Expected Return\nexpected return을 최대화하는 방향으로 policy를 update한다고 하였습니다. 그렇다면 expected return은 어떻게 구할 수 있을까요? 여기에도 크게 두 가지 방법이 있습니다. 첫 번째는 deterministic approximation으로 Markov decision process의 dynamics를 모델링한 후 수식을 통해 구하는 것입니다. 두 번째 방법은 monte carlo estimation으로 dynamics에 대한 모델을 하지 않고 많은 sample들을 얻은 후 empirical하게 expected return을 계산하는 방법입니다. 어느 방법이 더 좋을지는 풀고자 하는 문제의 특성에 따라 다를 것입니다. dynamics에 대한 모델이 어렵거나 변화가 큰 경우에는 두 번째 방법이 좀 더 현실적이지만 gradient를 구하는 것은 더 어렵습니다. 결국 gradient를 esimate하는방법을 고안해야 합니다. 가장 유명한 gradient estimation 방법이 1992년 R. J. Williams에 의해서 제안된 [REINFORCE](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf) 기법입니다. REINFORCE는 Monte Carlo estimate 또는 likelihood-ratio estimate라고 부르는 방법을 이용합니다. 이 방법에 대해서 좀 더 알아볼까요?\n\n### 1.3.1 Monte Carlo Gradient Estimation\n다음과 같은 parameter $\\theta$를 가지는 random variable $X$가 있습니다: $X:\\Omega\\mapsto\\mathcal{X}$. 그리고 이 $x$에 대한 함수 $f$가 있습니다: $f:\\mathcal{X}\\mapsto\\mathbb{R}$. expected return처럼 $E[f(x)]$를 최대화하고자 합니다. 이를 위해서는 $\\nabla_{\\theta}E[f(x)]$를 구해야 합니다. 이 때 log derivate trick을 이용하여 다음과 같이 수식을 변형시킬 수 있습니다.\n\n$$\n\\begin{align}\n\\nabla_{\\theta} E_{p(x;\\theta)}\\left[ {f(x)} \\right] \n&= \\nabla_\\theta\\int{f\\left( x \\right)p\\left( {x;\\theta} \\right) dx}\n\\\\\\\\\n&= \\int { {\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= \\int {\\frac{ {p\\left( {x;\\theta } \\right)} }{ {p\\left( {x;\\theta } \\right)} }{\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= \\int {p\\left( {x;\\theta } \\right){\\nabla_\\theta }\\log p\\left( {x;\\theta } \\right)f\\left( x \\right)dx} \n\\\\\\\\\n&= E_{p(x;\\theta)}[f(x)\\nabla_\\theta \\log p({x;\\theta})]\n\\end{align}\n$$\n\n이제 expectation은 많은 샘플들을 모아서 평균을 취하는 Monte Carlo 기법으로 근사화할 수 있습니다.\n\n$${\\nabla_{\\theta} }{E_{p\\left( {x;\\theta } \\right)} }\\left[ {f\\left( x \\right)} \\right] = \\frac{1}{N}\\sum_{n=1}^N f\\left(X_n\\right)\\nabla_\\theta\\log p\\left(X_n;\\theta\\right)$$\n\n이 방법을 사용하기 위해서 필요한 조건은 $\\log p\\left(X_n;\\theta\\right)$가 미분가능해야 한다는 것 뿐입니다. 그러나 이 방법은 얻은 샘플들에 의존하기 때문에 경우에 따라서 큰 variance를 가질 수 있습니다.\n\n<br><br>\n\n# 2. Policy Gradient Methods\n\n이제 본격적으로 policy gradient 기법에 대해서 알아보겠습니다.\n\n<br>\n## 2.1 System Model\n\n논문에서 사용하는 수학기호(notation)와 가정들을 설명하는 시스템모델은 다음과 같습니다.\n\n* Markov decision process (MDP) at each time $t\\in{0,1,2,...}$\n* $S_t\\in\\mathcal{S}$: state\n* $A_t\\in\\mathcal{A}$: action\n* $r_t\\in \\mathcal{R}$: reward\n* $\\theta\\in\\mathbb{R}^l$: vector of policy parameters, for $l\\ll\\left|\\mathcal{S}\\right|$\n* $\\mathcal{P} _{s s'}^a = \\Pr[S _{t+1}=s' \\vert S_t=s,A_t=a]$: state transition probabilities\n* $\\mathcal{R} _{s s'}^a = E[R _{t+1}\\vert S_t=s,A_t=a]$: expected reward\n* $\\pi(s,a,\\theta)=\\Pr[A_t=a|S_t=s,\\theta]$: policy, shortened as $\\pi(s,a)$ or $\\pi(a|s)$\n* $\\theta$: policy parameter들의 vector  \n* $\\rho$: 해당 policy들의 성능을 나타내는 척도 (예. average reward per step)  \n* $\\alpha$: positive-definite한 step size \n\n<br>\n## 2.2 Policy Gradent Approach\npolicy gradient는 stochastic policy를 자체적인 파리미터를 가진 function approximator를 이용해서 근사화시킵니다.\n\n\npolicy parameter는 gradient에 비례하여 업데이트됩니다.\n\n$$\\Delta\\theta \\approx \\alpha\\frac{\\partial\\rho}{\\partial\\theta}$$\n\n$\\rho$는 $\\theta$에 대하여 미분가능해야겠죠? 위와 같이 업데이트하면 local optimal policy로 수렴합니다. 논문의 가장 중요한 contribution은 특정 조건을 만족하는 function approximator를 이용하여 경험(experience, sample)을 축적하고 이것들을 이용하여 위의 gradient를 unbiased estimate할 수 있음을 증명한 것입니다.\n\n먼저 $\\rho$, 즉, reward를 표현하는 두 가지 방법에 대해서 알아보겠습니다.\n\n<br>\n## 2.3 Average Reward Formulation\nAverage reward formulation은 시간의 흐름에 따른 reward를 표현한다기보다는 모든 시간의 reward를 평균을 내서 표현하는 방법입니다.\n\n* Long‐term expected reward per step\n$$\\rho(\\pi)=\\lim_{n\\to\\infty}\\frac{1}{n}E[r_1+r_2+\\cdots+r_n|\\pi] = \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\mathcal{R}_s^a$$\n\n    - $d^\\pi(s): \\lim_{t\\to\\infty}\\Pr[S_t=s|s_0,\\pi]$: stationary distribution of states under $\\pi$\n    - $d^\\pi(s)$가 존재한다고 가정합니다. 그리고 모든 policy에 대해서 $d^\\pi(s)$는 $s_0$와 independent합니다.\n    - 위의 (limit수식과 summation수식 사이의) 등식은 왜 성립할까요? Time average와 Ensemble average가 같다는 뜻으로 [ergodic](https://en.wikipedia.org/wiki/Ergodicity)한 system에서 성립합니다.\n\n* Value of a state-action pair given a policy\n$$Q^{\\pi} (s,a) = \\sum_{t=1}^\\infty E[r_t - \\rho(\\pi)|S_0=s, A_0=a, \\pi]$$\n    - 위의 Q-function의 표현식이 정의인지 유도된 것인지는 이 포스트를 작성하는 저희도 아직 확실히 모르겠습니다. 다만, $r_t$들을 더하면 무한대로 발산할 가능성이 매우 높은데 $\\rho$를 빼줌으로해서 어찌보면 평균 reward로부터의 차이를 더하는 것이라고 할 수 있고 이것은 bound된 값일 가능성이 더 높기 때문에 Q-function을 이렇게 표현하는 것이 더 안전하다고 할 수 있습니다. 이 부분에 대해서 좋은 의견이 있으시면 피드백을 주시면 감사하겠습니다!\n\n* State-value function\n$$V^\\pi(s) = \\sum_{a} \\pi(s,a)Q^\\pi(s,a)$$\n\n<br>\n## 2.4 Start-State Formulation\nstart-state formulation은 시간의 흐름에 따라 감소하는 reward들을 표현합니다.\n\n* Long-term expected reward per step with a designated start state $S_0$\n$$\\rho(\\pi) = E\\left[\\left.\\sum_{t=1}^\\infty \\gamma^{t-1}r_t\\right|S_0,\\pi\\right]$$\n    - $\\gamma\\in\\left[0,1\\right]$: discount rate\n\n* Value of a state‐action pair given a policy\n$$Q^\\pi(s,a) = E\\left[\\left.\\sum_{k=1}^\\infty \\gamma^{k-1} r_{t+k}\\right|S_t=s, A_t=a, \\pi\\right]$$\n\n* Alternative form of $Q^\\pi(s,a)$\n$$\nQ^\\pi(s,a) = R_s^a + \\gamma\\sum _{s'} \\mathcal{P} _{s s'}^a V^{\\pi}(s')\n$$\n\n<br>\n## 2.5 Policy Gradient Theorem\n논문의 중요한 결과인 Theorem 1은 다음과 같습니다.\n\n**Theorem 1 (Policy Gradient)** *For any MDP, in either the average‐reward or start‐state formulations,*\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$\n\n놀라운 부분은 expected return의 gradient를 취할 때 $\\frac{\\partial d^\\pi(s)}{\\partial\\theta}$를 구하지 않아도 된다는 점입니다. 즉, policy의 변화가 state distribution에 영향을 주지 않는다는 것입니다. 이것은 다시 말하면 아래 수식과 같이 우리는 $\\frac{\\partial\\rho}{\\partial\\theta}$에 대한 unbiased estimator를 구할 수 있다는 뜻입니다.\n\n$$E_s\\left[\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\\right] = \\frac{\\partial\\rho}{\\partial\\theta}$$\n\n이로인해서 우리는 gradient를 sampling을 통해서 추정할 수 있습니다. 그렇지만 이것은 sample이 아주 많을 때만 성립합니다!\n\n또 한 가지 문제는 우리는 $Q^\\pi(s,a)$의 정확한 값을 알 수 없다는 것 입니다. 이것을 estimate하기 위해서 현재의 return값 $R_t$를 이용할 수 있습니다.\n\n<br>\n## 2.6 Proof of Policy Gradient Theorem\n증명을 살펴보겠습니다.\n\n* average-reward formulation을 이용할 때의 증명은 아래와 같습니다.\n\n$$\n\\begin{align}\n\\frac{\\partial V^\\pi(s)}{\\partial\\theta} &\\overset{\\underset{\\mathrm{def} }{} }{=}\\frac{\\partial}{\\partial\\theta}\\sum_a \\pi(s,a)Q^\\pi(s,a)\n\\\\\\\\\n&=\\sum_a \\frac{\\partial}{\\partial\\theta}\\left[\\pi(s,a)Q^\\pi(s,a)\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial Q^\\pi(s,a)}{\\partial\\theta}\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial}{\\partial\\theta}\\left[R_s^a -\\rho(\\pi) + \\sum_{s'}\\mathcal{P}_{s s'}^a V^{\\pi}(s')\\right]\\right]\\\\\\\\\n&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\left[-\\frac{\\partial\\rho}{\\partial\\theta} + \\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta}\\right]\\right]\\\\\\\\\n\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta}\\right] - \\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n양변에 stationary distribution에 대한 평균을 취합니다.\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\sum_{s'}\\mathcal{P}_{s s'}^a \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta} \n\\\\\\\\\n&- \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n모든 state $s$에서 모든 state $s'$으로 이동하는 것은 state $s'$에 존재할 확률이므로 다음과 같이 표현할 수 있습니다.\n\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s') \\frac{\\partial V^{\\pi}(s')}{\\partial\\theta} - \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}\n\\end{align}\n$$\n\n뒤의 두 항은 같은 식이므로 0이 됩니다. 따라서 다음과 같이 표현됩니다.\n$$\n\\begin{align}\n\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\n\\end{align}\n$$\n\n$\\frac{\\partial\\rho}{\\partial\\theta}$는 reward의 파라미터에 대한 미분값입니다. reward 자체가 이미 모든 state에 대한 평균값이므로 $s$의 함수가 아닙니다.  $s$에 대한 함수가 아니므로 $\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}=\\frac{\\partial\\rho}{\\partial\\theta}\\sum_s d^\\pi(s)=\\frac{\\partial\\rho}{\\partial\\theta}$입니다. 여기서 $d^\\pi(s)$는 state에 대한 확률값이죠. 모든 state에 대해서 이 값을 다 더하면 1이 됩니다.\n이를 이용해서 위의 식을 다시 표현하면 아래와 같고 이를 통해 증명이 완성됩니다.\n\n$$\n\\begin{align}\n\\therefore\\frac{\\partial\\rho}{\\partial\\theta}&=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\n\\end{align}\n$$\n\n* 다음으로 start-state formulation에 대한 증명입니다.\n\nstart-state formulation은 이 논문에서도 나오고 서튼책에서도 나오는데, 서튼책의 증명이 더 설명이 자세합니다. 그래서 논문에 있는 증명이 아닌 서튼책에 있는 증명으로 설명드리겠습니다. 다만, notation이 약간 다른데, 큰 문제 없이 이해하실 수 있으므로 별도의 설명은 하지 않겠습니다. (서튼책 [Link](https://drive.google.com/file/d/1xeUDVGWGUUv1-ccUMAZHJLej2C7aAFWY/view))\n\n$$\\nabla{v_\\pi}(s)=\\nabla\\big[\\sum_a\\pi(a|s)q_\\pi(s,a)\\big]$$ \n\n다음으로 $(f\\cdot g)'=f'\\cdot g+f\\cdot g'$ (product rule of calculus)에 의해 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\nabla q_\\pi(s,a)\\big]$$  \n\n이어서 $p(s',r|s,a) := Pr[S_t=s', R_t=r|S_{t-1}=s,A_{t-1}=a ]$ (서튼책 3.2 공식) 에 의해 다음과 같이 나타낼 수 있습니다.\n\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_{\\pi}(s,a)+\\pi(a|s)\\nabla\\big[\\sum_{s', r}p(s',r|s,a)(r+v_{\\pi}(s'))\\big]\\big]$$  \n\n계속해서 $p(s'|s,a)=\\sum_{r\\in R}p(s',r|s,a)$ (서튼책 3.4 공식) 에 의해 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s'}p(s'|s,a)\\nabla v_\\pi(s') \\big]$$ \n\n여기서 $\\nabla v_\\pi(s')$ 부분을 unrolling을 하면 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s'}p(s'|s,a)\\sum_{a'}\\big[\\nabla\\pi(a'|s')q_\\pi(s',a')\\\\\\\\+\\pi(a'|s')\\sum_{s''}p(s''|s',a')\\nabla v_\\pi(s''))\\big] \\big]$$ \n\n$\\nabla v_\\pi(s'')$에 대해서 unrolling을 하고 또 나오는 다른 항에 대해서도 계속 반복하다보면 다음과 같이 나타낼 수 있습니다.\n$$=\\sum_{x\\in S}\\sum_{k=0}^{\\infty}\\Pr(s\\rightarrow x,k,\\pi)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n여기에서 $\\Pr(s\\rightarrow x,k,\\pi)$ 는 policy $\\pi$ 에 대해 state $s$에서 state $x$까지 $k$ step만큼 움직일 때의 변환 확률입니다. 따라서 위의 수식을 아래처럼 다시 나타낼 수 있습니다.\n <!-- \\begin{align}  -->\n$$\\nabla J(\\theta)=\\nabla v_\\pi (s_0)$$ \n$$=\\sum_s(\\sum_{k=0}^{\\infty}\\Pr(s_0\\rightarrow s,k,\\pi))\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n$$=\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ \n<!-- \\end{align} -->\n- 서튼책에서 $\\eta(s)$는 다음과 같이 표현할 수 있습니다.\n    - $\\eta(s)$ = 평균적으로 하나의 episode마다 state에서 머무른 time step의 수 (page 199)\n- 논문에서는 $\\eta(s)$를 다음과 같이 표현합니다.\n    - $d^\\pi(s)=\\sum_{t=0}^{\\infty}\\gamma^t Pr(s_t=s|s_0,\\pi)$ \n    ($\\gamma=1$ is allowed only in episodic tasks = discounted weighting of states)\n\n여기서부터가 논문에는 없는 내용입니다. 위의 최종 수식을 다시 한 번 적으면 아래와 같습니다.\n$$\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n위의 수식을 아래와 같이 바꿀 수 있습니다.\n$$\\sum_{s'} \\eta(s') \\cdot\\sum_s \\frac{\\eta(s)}{\\sum_{s'}\\eta(s')} \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n그러면 이것을 새로운 기호로 다시 나타낼 수 있습니다.\n$$\\sum_{s'} \\eta(s') \\cdot\\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ \n- 여기서 $\\mu(s)$ 는 state distribution이라고 하며, weight를 state distribution으로 바꿔주는 과정이라고 생각할 수 있습니다. 쉽게 말해 어떠한 state에 agent가 머무르는 확률입니다.\n\n위의 수식은 아래의 수식과 비례합니다. 따라서 최종 형태는 다음과 같습니다.\n$$\\propto \\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$\n\n<br><br>\n\n# 3. Policy Gradient with Approximation\n이 장에서는 앞서 다뤘던 Theorem 1 중에 $Q^\\pi$에 대해서 중점적으로 다룹니다. 어떠한 $Q^\\pi$가 학습된 function approximator로 근사된다고 합시다. \n\n$f_w$ ($f_w$는 $S × A \\rightarrow \\Re$)는 parameter $w$를 가지는 어떠한 $Q^\\pi$ 에 대한 근사치입니다. 학습된 function approximator에 대해서 생각하기 때문에, 학습된 $f_w$는 error를 최소화하는 방향으로 parameter $w$를 업데이트합니다. 그러면 다음과 같은 관계식을 생각할 수 있습니다.\n$$\\Delta w_t \\propto \\frac{\\partial}{\\partial w}\\big[ {\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]^2 \\propto \\big[{\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s_t,a_t)}{\\partial w}$$ \n(여기서 ${\\hat Q}^\\pi(s_t,a_t)$ 는 $Q^\\pi(s_t,a_t)$의 unbiased estimator $R_t$입니다.)\n\n그러면 위와 같은 수식이 local optimum에 수렴을 했을 때, 다음과 같이 나타낼 수 있습니다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n- 위의 수식에 대한 추가 설명\n    - local optimum으로 수렴을 하기 때문에 당연히 위의 수식은 0이 됩니다.\n    - 또한 stochastic policy이기 때문에 local optimum으로 수렴하려면 모든 state, action에 대해 expectation을 해야합니다. 따라서 $\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)$이 붙게 됩니다.\n\n<br>\n## 3.1 Theorem 2: Policy Gradient with Function Approximation\n\n만약 $f_w$가 아래의 등식을 만족한다고 합시다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n또한 $f_w$가 policy parameterization과 양립할 수 있다고 합시다. policy parameterization에 대한 여러가지 의미가 있는데 이 논문에서는 다음을 의미합니다.\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$\n- 위의 수식에 대한 추가 설명\n    - 이 수식은 Monte-Calro Estimation을 이용하기 위한 조건입니다.\n    - Compatibility Condition이라고 부릅니다.\n    - 우변(= $\\nabla\\log\\pi(s,a)$)으로 유도함으로써 아래의 수식과 같이 sampling을 이용한 추정이 가능해집니다.\n\n따라서 위의 두 수식을 이용하여 아래와 같이 나타낼 수 있습니다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s,a)$$\n\n<br>\n## 3.2 Proof of Theorem 2\n\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$\n\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$\n\n위의 두 수식을 합치면 다음과 같이 나타낼 수 있습니다.\n$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}=0$$\n\n분자와 분모에 있는 $\\pi(s,a)$를 지우면 아래와 같습니다.\n$$\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]=0$$\n\n이어서 위의 수식이 0이기 때문에 policy gradient theorem(앞서 다뤘던 Theorem 1)에서 위의 수식을 뺄 수 있습니다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)-\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]$$\n\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-Q^\\pi(s_t,a_t)+f_w(s_t,a_t) \\big]$$\n\n$$\\therefore\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s_t,a_t)$$\n\n<br><br>\n\n# 4. Application to Deriving Algorithms and Advantages\n\n<br>\n## 4.1 Application to Deriving Algorithms\nfeature의 linear combination에서 Gibbs distribution (softmax)를 하나의 policy로 생각해볼 수 있습니다.\n$$\\pi(s,a)=\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_b e^{\\theta^{T}\\phi_{sb} }}$$\n- $\\phi_{sa}$: state-action pair $s$, $a$를 나타내는 $l$-dimensional feature vector\n\ncompatible condition을 적용하면 다음과 같습니다. (4.2 Proof of application of compatible condition 참고)\n$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}=\\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}$$ \n\n이어서 $f_w$을 적분한 natural parameterization은 다음과 같습니다.\n$$f_w(s,a)=w^T\\big[ \\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}\\big]$$\n\n- 위의 수식에 대한 추가 설명\n    - $f_w$는 policy로서 같은 feature들에 대해 linear합니다.\n    - $f_w$는 각각의 state에 대해서 평균이 0입니다. ($\\sum_a\\pi(s,a)f_w(s,a)=0$)\n    - advantage function $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$의 하나의 근사치로서 $f_w$를 생각해도 좋습니다.\n\n<br>\n## 4.2 Proof of application of compatible condition\n증명을 하기 전에 다음을 가정합니다.\n1. $(\\frac{f(x)}{g(x)})'=f'(x)\\cdot \\frac{1}{g(x)}-f(x)\\cdot \\frac{g'(x)}{g(x)^{2} }=\\frac{f'(x)\\cdot g(x)-f(x)\\cdot g'(x)}{g(x)^{2} }$\n2. $f(\\theta)=e^{\\theta^{T}\\phi_{sa} }$, $g(\\theta)=\\sum_be^{\\theta^{T}\\phi_{sb} }$\n\n$$\n\\begin{align}\n\\frac{\\partial f_w(s,a)}{\\partial w}&=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\big(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big)'\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\big[ \\phi_{sa}(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})-(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})\\frac{\\sum_b\\phi_{sb}e^{ {\\theta^T}\\phi_{sb} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big]\\frac{1}{\\pi(s,a)}\\\\\\\\\n&=\\big[ \\phi_{sa}\\pi(s,a)-\\pi(s,a)\\sum_b\\phi_{sb}\\pi(s,b)\\big]\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&=\\pi(s,a)\\big[ \\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\\big]\\frac{1}{\\pi(s,a)}\n\\\\\\\\\n&\\therefore\\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\n\\end{align}\n$$\n\n<br>\n## 4.3 Application to Advantages\nPolicy Gradient with Function Approximation Theorem (Theorem 2)는 advantage function으로 확장될 수 있습니다.\n$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_sd^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ f_w(s,a)+v(s) \\big]$$\n- 위의 수식에 대한 추가설명\n    - $v$ ($v$는 $S\\rightarrow\\Re$) 는 arbitrary function입니다.\n    - 이 수식은 $\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}=0$이기 때문에 가능해집니다.\n    - 즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없습니다.\n    - $v$의 선택은 Theorem들에 영향을 미치지 못하지만, 실질적으로 gradient estimator의 variance에 영향을 미칩니다.\n    - 이러한 문제는 전체적으로 이전의 연구에 reinforcement baseline의 사용에 있어서 유사합니다.\n    - (comment) 위의 수식에서 $f_w(s,a)+v(s)$와 Application to Deriving Algorithms의 $f_w(s,a)$와는 다른 것입니다. Application to Deriving Algorithms의 $f_w(s,a)$은 softmax에 의해 스스로 advantage function의 역할을 할 수 있습니다. 하지만 보통의 경우에는 그러지 못할 수도 있기 때문에 위의 수식처럼 $f_w(s,a)+v(s)$을 추가하여 zero mean만들어서 variance를 줄일 수 있습니다.\n\n<br><br>\n\n# 5. Convergence of Policy Iteration with Function Approximation\n\n<br>\n## 5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation\n\npolicy iteration with function approximation은 locally optimal policy에 수렴합니다. $\\pi$와 $f_w$를 \n\n1. compatibility condition을 만족하는 policy와 value function에 대한\n2. 그리고 $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|<B<\\infty$에 대한\n\n어떠한 미분가능한 function approximator라고 합시다.\n\n- (comment) $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|<B<\\infty$에서 $\\pi(s,a)$라는 function은 이계미분값이 존재하고, 임의의 상수인 (Bound) B에 bound되어 있기 때문에 function의 그래프는 smooth하다고 볼 수 있습니다. (아래의 그림 중 빨간색 그래프 참고)\n\n<center> <img src=\"https://www.dropbox.com/s/xx02ejfg5ao19ps/Screen%20Shot%202018-07-10%20at%203.41.06%20PM.png?dl=1\" width=\"200\"> </center>\n\n이어서 ${\\alpha_k}$는 $\\lim_{k\\rightarrow\\infty}\\alpha_k=0$이며 $\\sum_k\\alpha_k=\\infty$를 만족하는 step-size sequence라고 합시다.\n\n그 때, bounded reward를 가진 MDP에 대해\n1. 어떠한 $\\theta_0, \\pi_k=\\pi(\\cdot,\\cdot,\\theta_k)$\n2. 그리고 $\\sum_sd^{\\pi_{k} }(s)\\sum_a\\pi_k(s,a)\\big[ Q^{\\pi_{k} }(s,a)-f_w(s,a)\\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$로 인하여 $w_k=w$, $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$\n\n으로 정의된 sequence $\\rho(\\pi_k)$는 $\\lim_{k\\rightarrow\\infty}\\frac{\\partial\\rho(\\pi_k)}{\\partial\\theta}=0$이기 때문에 수렴합니다.\n- sequence $\\rho(\\pi_k)_{k=0}^\\infty$에 대한 추가 설명\n    - 2번의 식을 통해 자연스럽게 actor-critic으로 연결됩니다.\n    - $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$ 에 따라 $\\theta$가 1, 2, ..., $\\infty$로 갈텐데, 거기에 따른 objective function or performance의 sequence입니다.\n    - (comment) 굳이 sequence라는 표현이 없어도 될 것 같습니다. 어짜피 k가 $\\infty$로 가면 $\\rho(\\pi_k)$가 수렴한다는 의미이기 때문에 불필요해보입니다.\n\n<br>\n## 5.2 Proof of Theorem 3\n- Theorem 2는 $\\theta_k$ update가 gradient의 error를 최소화한다는 것을 증명했습니다.\n- $\\frac{\\partial^{2}\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}$와 MDP의 reward에서의 bound는, $\\frac{\\partial^{2}\\rho}{\\partial\\theta_i\\partial\\theta_j}$ 또한 bound된다는 것을 증명합니다.\n- step-size 필요조건 때문에 이러한 bound된 것들은 Proposition 3.5 from page 96 of Bertsekas and Tsitsiklis (1996)에 적용하기 위해 필요한 조건입니다.\n- Proposition 3.5 from page 96 of Bertsekas and Tsitsiklis (1996)은 local optimum으로 수렴한다는 것을 증명했습니다.\n- Proposition 3.5를 자세하게 설명하지는 않겠습니다. 일반적인 gradient method의 수렴성을 증명한 것이며, 이 논문의 policy gradient method도 gradient method의 일종이므로 특정 조건을 만족할 때 수렴한다는 것입니다. 수렴하는 원리는 소위 말하는 Lipschitz continuity condition을 만족하기 때문인데 다음과 같습니다. $L>0$인 임의의 상수입니다.\n$$\n\\parallel \\nabla f(r) - \\nabla f(\\bar{r}) \\parallel \\leq L \\parallel r - \\bar{r} \\parallel\n$$\n즉, 업데이트가 진행되어 나감에 따라서 gradient의 차이가 점점 줄어들게 되므로 언젠가는 0으로 수렴하게 됩니다.\n\n<br><br>\n\n# 6. Summary \n\n논문에서 설명한 policy gradient 기법을 요약하면 다음과 같습니다.\n\n* Original maximization problem: $\\max\\rho_{\\theta}=E\\left[R\\right]$\n* gradient ascent method를 이용한 parameter update: $\\theta_{t+1} = \\theta_{t} + \\alpha\\left.\\frac{\\partial E[R]}{\\partial\\theta}\\right|_{\\theta_t}$\n    - 그러나 우리는 gradient를 모르고, 추정하기도 어렵습니다. 왜냐하면 expectation이 안에 있기 때문입니다.\n    - 그렇다면 이것을 추정 가능한 형태로 바꿔야합니다. 다시 말해 expectation이 밖에 있는 형태로 바꾸는 것입니다.\n    - Expectation이 밖에 있으면 왜 추정이 유리할까요? 바로 Sample mean을 취하면 되기 때문입니다.\n* Approximate gradient\n    - 방법1: 논문의 Theorem 1을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다.\n    $$\\frac{\\partial E[R]}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$\n    - 방법2: Log derivative trick을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다. (Theorem 2)\n    $$\\theta_{t+1} = \\theta_{t} + \\alpha E\\left[ \\left. R\\frac{\\partial}{\\partial\\theta}\\log p_{\\theta} \\right\\vert \\theta_t\\right]$$\n    - 그리고 특정 trajectory를 따라가면서 return값을 구하고 이것을 여러 번 수행하여 sample mean을 취하면 gradient를 추정하는 것이 가능합니다.\n    - 이러한 기법을 제시한 기존의 연구가 REINFORCE입니다.\n    - 여러 Trajectory를 이용하므로 variance가 높을 수 밖에 없습니다.\n    - Advantage를 활용하거나 하는 방식으로 이후 여러 연구가 진행되었습니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 다음으로\n\n## [DPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/)","slug":"1_sutton-pg","published":1,"updated":"2019-02-07T11:21:31.920Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjrujlu0v001e5wfe04dwv4em","content":"<center> <img src=\"https://www.dropbox.com/s/83i4atz733ali0w/Screen%20Shot%202018-07-18%20at%2012.10.52%20AM.png?dl=1\" width=\"600\"> </center>\n\n<p>논문 저자 : Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour<br>논문 링크 : <a href=\"http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf\" target=\"_blank\" rel=\"noopener\">NIPS</a><br>Proceeding : Advances in Neural Information Processing Systems (NIPS) 2000<br>정리 : 김동민, 이동민</p>\n<hr>\n<h1 id=\"1-Intro-to-Policy-Gradient\"><a href=\"#1-Intro-to-Policy-Gradient\" class=\"headerlink\" title=\"1. Intro to Policy Gradient\"></a>1. Intro to Policy Gradient</h1><p>이 논문은 policy gradient (PG) 기법의 효시와도 같으며 향후 많은 파생연구를 낳은 중요한 논문입니다. 7페이지의 짧은 논문이지만 읽기에 만만한 논문은 아닙니다. 이 논문을 이해하기 위해 필요한 배경지식을 먼저 설명하고 논문을 차근차근 살펴보도록 하겠습니다.</p>\n<p><br></p>\n<h2 id=\"1-1-Value-Function-Approach\"><a href=\"#1-1-Value-Function-Approach\" class=\"headerlink\" title=\"1.1 Value Function Approach\"></a>1.1 Value Function Approach</h2><p>전통적으로 강화학습 기법은 value function을 기반으로 동작하였습니다. 특정 state에서 vaue function 또는 value function을 근사하는 함수(function approximation)를 최대화하는 action을 찾는 greedy action-selection policy가  대표적입니다.</p>\n<p>논문에서는 이러한 방법은 deterministic한 policy를 찾는 쪽으로 나아가게 되지만 종종 최적의 policy는 stochastic한 성질을 가지기 때문에 이 방법으로는 최적의 policy를 찾을 수 없다고 언급하고 있습니다. (그러나 이 논문이 나오고 나서 한참 후 David Silver를 필두로한 DeepMind의 연구진들은 high-dimensional action space를 가지는 application에서 보다 빠르게 동작하는 <a href=\"../../../06/27/2_dpg/\">deterministic policy gradient</a>을 개발하였습니다.) 아마도 이러한 부분은 $\\epsilon$-greedy action-selection method로 개선될 수 있을 것입니다. 또 하나의 문제는 value function의 작은 변화로 인해서 action이 크게 변할 수도 있다는 것입니다. 이것은 알고리듬의 수렴성에 문제를 야기할 수 있습니다. 이러한 문제점을 해결하기 위하여 policy search라는 새로운 기법이 고안됩니다.</p>\n<p><br></p>\n<h2 id=\"1-2-Policy-Search\"><a href=\"#1-2-Policy-Search\" class=\"headerlink\" title=\"1.2 Policy Search\"></a>1.2 Policy Search</h2><p>policy search는 최적의 policy $\\pi^*$를 reward로부터 직접 찾습니다. policy search 방법은 크게 두 가지로 분류할 수 있습니다. 첫 번째는 gradient-based optimization으로 policy gradient method가 이에 속합니다. 두 번째는 gradient-free optimization으로 진화(evolutionary) 연산을 이용하는 것이 이에 속합니다. 이 논문에서는 제목에서 알 수 있듯이 gradient-based optimization을 다룹니다. gradient는 변화량을 의미합니다. 특정 policy x와 또 다른 policy y가 있을 때 policy가 얼마나 많이 변화했는지 어떻게 모델링할 수 있을까요? policy의 변화를 제어하는 어떤 파라미터가 있다면 이 파라미터를 조정하여 policy를 변화시킬 수 있습니다. policy x에 해당하는 파라미터값과 policy y에 해당하는 파라미터값의 차이가 policy의 변화량이라고 할 수 있습니다. 이와 같은 방식으로 policy의 변화를 모델링하기 위하여 파라미터 $\\theta$를 이용하여 policy를 $\\pi_{\\theta}$로 표현할 수 있습니다. 최적의 policy를 찾기 위하여 expected return $E\\left[R|\\theta\\right]$이 최대화되도록 parameter를 조정합니다. 이를 간단한 수식으로 정리하면 다음과 같습니다.</p>\n<p>$$<br>\\pi^* = \\arg \\max\\limits_\\pi  E\\left[ {\\left. R \\right|\\pi } \\right] \\to {\\text{original problem} }<br>\\\\  {\\Downarrow} \\\\<br>{\\text{policy parameterization by } } \\pi_{\\theta}: \\Theta  \\to \\Pi \\\\ {\\Downarrow} $$</p>\n<p>$$\\pi^* = \\arg \\max\\limits_{\\theta}  E\\left[ {\\left. R \\right|\\theta } \\right] \\to {\\text{policy search problem} }<br>$$</p>\n<p><br></p>\n<h2 id=\"1-3-How-to-Obtain-the-Expected-Return\"><a href=\"#1-3-How-to-Obtain-the-Expected-Return\" class=\"headerlink\" title=\"1.3 How to Obtain the Expected Return\"></a>1.3 How to Obtain the Expected Return</h2><p>expected return을 최대화하는 방향으로 policy를 update한다고 하였습니다. 그렇다면 expected return은 어떻게 구할 수 있을까요? 여기에도 크게 두 가지 방법이 있습니다. 첫 번째는 deterministic approximation으로 Markov decision process의 dynamics를 모델링한 후 수식을 통해 구하는 것입니다. 두 번째 방법은 monte carlo estimation으로 dynamics에 대한 모델을 하지 않고 많은 sample들을 얻은 후 empirical하게 expected return을 계산하는 방법입니다. 어느 방법이 더 좋을지는 풀고자 하는 문제의 특성에 따라 다를 것입니다. dynamics에 대한 모델이 어렵거나 변화가 큰 경우에는 두 번째 방법이 좀 더 현실적이지만 gradient를 구하는 것은 더 어렵습니다. 결국 gradient를 esimate하는방법을 고안해야 합니다. 가장 유명한 gradient estimation 방법이 1992년 R. J. Williams에 의해서 제안된 <a href=\"https://link.springer.com/content/pdf/10.1007/BF00992696.pdf\" target=\"_blank\" rel=\"noopener\">REINFORCE</a> 기법입니다. REINFORCE는 Monte Carlo estimate 또는 likelihood-ratio estimate라고 부르는 방법을 이용합니다. 이 방법에 대해서 좀 더 알아볼까요?</p>\n<h3 id=\"1-3-1-Monte-Carlo-Gradient-Estimation\"><a href=\"#1-3-1-Monte-Carlo-Gradient-Estimation\" class=\"headerlink\" title=\"1.3.1 Monte Carlo Gradient Estimation\"></a>1.3.1 Monte Carlo Gradient Estimation</h3><p>다음과 같은 parameter $\\theta$를 가지는 random variable $X$가 있습니다: $X:\\Omega\\mapsto\\mathcal{X}$. 그리고 이 $x$에 대한 함수 $f$가 있습니다: $f:\\mathcal{X}\\mapsto\\mathbb{R}$. expected return처럼 $E[f(x)]$를 최대화하고자 합니다. 이를 위해서는 $\\nabla_{\\theta}E[f(x)]$를 구해야 합니다. 이 때 log derivate trick을 이용하여 다음과 같이 수식을 변형시킬 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\nabla_{\\theta} E_{p(x;\\theta)}\\left[ {f(x)} \\right]<br>&amp;= \\nabla_\\theta\\int{f\\left( x \\right)p\\left( {x;\\theta} \\right) dx}<br>\\\\<br>&amp;= \\int { {\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= \\int {\\frac{ {p\\left( {x;\\theta } \\right)} }{ {p\\left( {x;\\theta } \\right)} }{\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= \\int {p\\left( {x;\\theta } \\right){\\nabla_\\theta }\\log p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= E_{p(x;\\theta)}[f(x)\\nabla_\\theta \\log p({x;\\theta})]<br>\\end{align}<br>$$</p>\n<p>이제 expectation은 많은 샘플들을 모아서 평균을 취하는 Monte Carlo 기법으로 근사화할 수 있습니다.</p>\n<p>$${\\nabla_{\\theta} }{E_{p\\left( {x;\\theta } \\right)} }\\left[ {f\\left( x \\right)} \\right] = \\frac{1}{N}\\sum_{n=1}^N f\\left(X_n\\right)\\nabla_\\theta\\log p\\left(X_n;\\theta\\right)$$</p>\n<p>이 방법을 사용하기 위해서 필요한 조건은 $\\log p\\left(X_n;\\theta\\right)$가 미분가능해야 한다는 것 뿐입니다. 그러나 이 방법은 얻은 샘플들에 의존하기 때문에 경우에 따라서 큰 variance를 가질 수 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"2-Policy-Gradient-Methods\"><a href=\"#2-Policy-Gradient-Methods\" class=\"headerlink\" title=\"2. Policy Gradient Methods\"></a>2. Policy Gradient Methods</h1><p>이제 본격적으로 policy gradient 기법에 대해서 알아보겠습니다.</p>\n<p><br></p>\n<h2 id=\"2-1-System-Model\"><a href=\"#2-1-System-Model\" class=\"headerlink\" title=\"2.1 System Model\"></a>2.1 System Model</h2><p>논문에서 사용하는 수학기호(notation)와 가정들을 설명하는 시스템모델은 다음과 같습니다.</p>\n<ul>\n<li>Markov decision process (MDP) at each time $t\\in{0,1,2,…}$</li>\n<li>$S_t\\in\\mathcal{S}$: state</li>\n<li>$A_t\\in\\mathcal{A}$: action</li>\n<li>$r_t\\in \\mathcal{R}$: reward</li>\n<li>$\\theta\\in\\mathbb{R}^l$: vector of policy parameters, for $l\\ll\\left|\\mathcal{S}\\right|$</li>\n<li>$\\mathcal{P} _{s s’}^a = \\Pr[S _{t+1}=s’ \\vert S_t=s,A_t=a]$: state transition probabilities</li>\n<li>$\\mathcal{R} _{s s’}^a = E[R _{t+1}\\vert S_t=s,A_t=a]$: expected reward</li>\n<li>$\\pi(s,a,\\theta)=\\Pr[A_t=a|S_t=s,\\theta]$: policy, shortened as $\\pi(s,a)$ or $\\pi(a|s)$</li>\n<li>$\\theta$: policy parameter들의 vector  </li>\n<li>$\\rho$: 해당 policy들의 성능을 나타내는 척도 (예. average reward per step)  </li>\n<li>$\\alpha$: positive-definite한 step size </li>\n</ul>\n<p><br></p>\n<h2 id=\"2-2-Policy-Gradent-Approach\"><a href=\"#2-2-Policy-Gradent-Approach\" class=\"headerlink\" title=\"2.2 Policy Gradent Approach\"></a>2.2 Policy Gradent Approach</h2><p>policy gradient는 stochastic policy를 자체적인 파리미터를 가진 function approximator를 이용해서 근사화시킵니다.</p>\n<p>policy parameter는 gradient에 비례하여 업데이트됩니다.</p>\n<p>$$\\Delta\\theta \\approx \\alpha\\frac{\\partial\\rho}{\\partial\\theta}$$</p>\n<p>$\\rho$는 $\\theta$에 대하여 미분가능해야겠죠? 위와 같이 업데이트하면 local optimal policy로 수렴합니다. 논문의 가장 중요한 contribution은 특정 조건을 만족하는 function approximator를 이용하여 경험(experience, sample)을 축적하고 이것들을 이용하여 위의 gradient를 unbiased estimate할 수 있음을 증명한 것입니다.</p>\n<p>먼저 $\\rho$, 즉, reward를 표현하는 두 가지 방법에 대해서 알아보겠습니다.</p>\n<p><br></p>\n<h2 id=\"2-3-Average-Reward-Formulation\"><a href=\"#2-3-Average-Reward-Formulation\" class=\"headerlink\" title=\"2.3 Average Reward Formulation\"></a>2.3 Average Reward Formulation</h2><p>Average reward formulation은 시간의 흐름에 따른 reward를 표현한다기보다는 모든 시간의 reward를 평균을 내서 표현하는 방법입니다.</p>\n<ul>\n<li><p>Long‐term expected reward per step<br>$$\\rho(\\pi)=\\lim_{n\\to\\infty}\\frac{1}{n}E[r_1+r_2+\\cdots+r_n|\\pi] = \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\mathcal{R}_s^a$$</p>\n<ul>\n<li>$d^\\pi(s): \\lim_{t\\to\\infty}\\Pr[S_t=s|s_0,\\pi]$: stationary distribution of states under $\\pi$</li>\n<li>$d^\\pi(s)$가 존재한다고 가정합니다. 그리고 모든 policy에 대해서 $d^\\pi(s)$는 $s_0$와 independent합니다.</li>\n<li>위의 (limit수식과 summation수식 사이의) 등식은 왜 성립할까요? Time average와 Ensemble average가 같다는 뜻으로 <a href=\"https://en.wikipedia.org/wiki/Ergodicity\" target=\"_blank\" rel=\"noopener\">ergodic</a>한 system에서 성립합니다.</li>\n</ul>\n</li>\n<li><p>Value of a state-action pair given a policy<br>$$Q^{\\pi} (s,a) = \\sum_{t=1}^\\infty E[r_t - \\rho(\\pi)|S_0=s, A_0=a, \\pi]$$</p>\n<ul>\n<li>위의 Q-function의 표현식이 정의인지 유도된 것인지는 이 포스트를 작성하는 저희도 아직 확실히 모르겠습니다. 다만, $r_t$들을 더하면 무한대로 발산할 가능성이 매우 높은데 $\\rho$를 빼줌으로해서 어찌보면 평균 reward로부터의 차이를 더하는 것이라고 할 수 있고 이것은 bound된 값일 가능성이 더 높기 때문에 Q-function을 이렇게 표현하는 것이 더 안전하다고 할 수 있습니다. 이 부분에 대해서 좋은 의견이 있으시면 피드백을 주시면 감사하겠습니다!</li>\n</ul>\n</li>\n<li><p>State-value function<br>$$V^\\pi(s) = \\sum_{a} \\pi(s,a)Q^\\pi(s,a)$$</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-4-Start-State-Formulation\"><a href=\"#2-4-Start-State-Formulation\" class=\"headerlink\" title=\"2.4 Start-State Formulation\"></a>2.4 Start-State Formulation</h2><p>start-state formulation은 시간의 흐름에 따라 감소하는 reward들을 표현합니다.</p>\n<ul>\n<li><p>Long-term expected reward per step with a designated start state $S_0$<br>$$\\rho(\\pi) = E\\left[\\left.\\sum_{t=1}^\\infty \\gamma^{t-1}r_t\\right|S_0,\\pi\\right]$$</p>\n<ul>\n<li>$\\gamma\\in\\left[0,1\\right]$: discount rate</li>\n</ul>\n</li>\n<li><p>Value of a state‐action pair given a policy<br>$$Q^\\pi(s,a) = E\\left[\\left.\\sum_{k=1}^\\infty \\gamma^{k-1} r_{t+k}\\right|S_t=s, A_t=a, \\pi\\right]$$</p>\n</li>\n<li><p>Alternative form of $Q^\\pi(s,a)$<br>$$<br>Q^\\pi(s,a) = R_s^a + \\gamma\\sum _{s’} \\mathcal{P} _{s s’}^a V^{\\pi}(s’)<br>$$</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-5-Policy-Gradient-Theorem\"><a href=\"#2-5-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"2.5 Policy Gradient Theorem\"></a>2.5 Policy Gradient Theorem</h2><p>논문의 중요한 결과인 Theorem 1은 다음과 같습니다.</p>\n<p><strong>Theorem 1 (Policy Gradient)</strong> <em>For any MDP, in either the average‐reward or start‐state formulations,</em><br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$</p>\n<p>놀라운 부분은 expected return의 gradient를 취할 때 $\\frac{\\partial d^\\pi(s)}{\\partial\\theta}$를 구하지 않아도 된다는 점입니다. 즉, policy의 변화가 state distribution에 영향을 주지 않는다는 것입니다. 이것은 다시 말하면 아래 수식과 같이 우리는 $\\frac{\\partial\\rho}{\\partial\\theta}$에 대한 unbiased estimator를 구할 수 있다는 뜻입니다.</p>\n<p>$$E_s\\left[\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\\right] = \\frac{\\partial\\rho}{\\partial\\theta}$$</p>\n<p>이로인해서 우리는 gradient를 sampling을 통해서 추정할 수 있습니다. 그렇지만 이것은 sample이 아주 많을 때만 성립합니다!</p>\n<p>또 한 가지 문제는 우리는 $Q^\\pi(s,a)$의 정확한 값을 알 수 없다는 것 입니다. 이것을 estimate하기 위해서 현재의 return값 $R_t$를 이용할 수 있습니다.</p>\n<p><br></p>\n<h2 id=\"2-6-Proof-of-Policy-Gradient-Theorem\"><a href=\"#2-6-Proof-of-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"2.6 Proof of Policy Gradient Theorem\"></a>2.6 Proof of Policy Gradient Theorem</h2><p>증명을 살펴보겠습니다.</p>\n<ul>\n<li>average-reward formulation을 이용할 때의 증명은 아래와 같습니다.</li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\frac{\\partial V^\\pi(s)}{\\partial\\theta} &amp;\\overset{\\underset{\\mathrm{def} }{} }{=}\\frac{\\partial}{\\partial\\theta}\\sum_a \\pi(s,a)Q^\\pi(s,a)<br>\\\\<br>&amp;=\\sum_a \\frac{\\partial}{\\partial\\theta}\\left[\\pi(s,a)Q^\\pi(s,a)\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial Q^\\pi(s,a)}{\\partial\\theta}\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial}{\\partial\\theta}\\left[R_s^a -\\rho(\\pi) + \\sum_{s’}\\mathcal{P}_{s s’}^a V^{\\pi}(s’)\\right]\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\left[-\\frac{\\partial\\rho}{\\partial\\theta} + \\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}\\right]\\right]\\\\<br>\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}\\right] - \\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>양변에 stationary distribution에 대한 평균을 취합니다.<br>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}<br>\\\\<br>&amp;- \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>모든 state $s$에서 모든 state $s’$으로 이동하는 것은 state $s’$에 존재할 확률이므로 다음과 같이 표현할 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s’) \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta} - \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>뒤의 두 항은 같은 식이므로 0이 됩니다. 따라서 다음과 같이 표현됩니다.<br>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)<br>\\end{align}<br>$$</p>\n<p>$\\frac{\\partial\\rho}{\\partial\\theta}$는 reward의 파라미터에 대한 미분값입니다. reward 자체가 이미 모든 state에 대한 평균값이므로 $s$의 함수가 아닙니다.  $s$에 대한 함수가 아니므로 $\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}=\\frac{\\partial\\rho}{\\partial\\theta}\\sum_s d^\\pi(s)=\\frac{\\partial\\rho}{\\partial\\theta}$입니다. 여기서 $d^\\pi(s)$는 state에 대한 확률값이죠. 모든 state에 대해서 이 값을 다 더하면 1이 됩니다.<br>이를 이용해서 위의 식을 다시 표현하면 아래와 같고 이를 통해 증명이 완성됩니다.</p>\n<p>$$<br>\\begin{align}<br>\\therefore\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)<br>\\end{align}<br>$$</p>\n<ul>\n<li>다음으로 start-state formulation에 대한 증명입니다.</li>\n</ul>\n<p>start-state formulation은 이 논문에서도 나오고 서튼책에서도 나오는데, 서튼책의 증명이 더 설명이 자세합니다. 그래서 논문에 있는 증명이 아닌 서튼책에 있는 증명으로 설명드리겠습니다. 다만, notation이 약간 다른데, 큰 문제 없이 이해하실 수 있으므로 별도의 설명은 하지 않겠습니다. (서튼책 <a href=\"https://drive.google.com/file/d/1xeUDVGWGUUv1-ccUMAZHJLej2C7aAFWY/view\" target=\"_blank\" rel=\"noopener\">Link</a>)</p>\n<p>$$\\nabla{v_\\pi}(s)=\\nabla\\big[\\sum_a\\pi(a|s)q_\\pi(s,a)\\big]$$ </p>\n<p>다음으로 $(f\\cdot g)’=f’\\cdot g+f\\cdot g’$ (product rule of calculus)에 의해 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\nabla q_\\pi(s,a)\\big]$$  </p>\n<p>이어서 $p(s’,r|s,a) := Pr[S_t=s’, R_t=r|S_{t-1}=s,A_{t-1}=a ]$ (서튼책 3.2 공식) 에 의해 다음과 같이 나타낼 수 있습니다.</p>\n<p>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_{\\pi}(s,a)+\\pi(a|s)\\nabla\\big[\\sum_{s’, r}p(s’,r|s,a)(r+v_{\\pi}(s’))\\big]\\big]$$  </p>\n<p>계속해서 $p(s’|s,a)=\\sum_{r\\in R}p(s’,r|s,a)$ (서튼책 3.4 공식) 에 의해 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s’}p(s’|s,a)\\nabla v_\\pi(s’) \\big]$$ </p>\n<p>여기서 $\\nabla v_\\pi(s’)$ 부분을 unrolling을 하면 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s’}p(s’|s,a)\\sum_{a’}\\big[\\nabla\\pi(a’|s’)q_\\pi(s’,a’)\\\\+\\pi(a’|s’)\\sum_{s’’}p(s’’|s’,a’)\\nabla v_\\pi(s’’))\\big] \\big]$$ </p>\n<p>$\\nabla v_\\pi(s’’)$에 대해서 unrolling을 하고 또 나오는 다른 항에 대해서도 계속 반복하다보면 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_{x\\in S}\\sum_{k=0}^{\\infty}\\Pr(s\\rightarrow x,k,\\pi)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p>여기에서 $\\Pr(s\\rightarrow x,k,\\pi)$ 는 policy $\\pi$ 에 대해 state $s$에서 state $x$까지 $k$ step만큼 움직일 때의 변환 확률입니다. 따라서 위의 수식을 아래처럼 다시 나타낼 수 있습니다.<br> <!-- \\begin{align}  --><br>$$\\nabla J(\\theta)=\\nabla v_\\pi (s_0)$$<br>$$=\\sum_s(\\sum_{k=0}^{\\infty}\\Pr(s_0\\rightarrow s,k,\\pi))\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$<br>$$=\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$<br><!-- \\end{align} --></p>\n<ul>\n<li>서튼책에서 $\\eta(s)$는 다음과 같이 표현할 수 있습니다.<ul>\n<li>$\\eta(s)$ = 평균적으로 하나의 episode마다 state에서 머무른 time step의 수 (page 199)</li>\n</ul>\n</li>\n<li>논문에서는 $\\eta(s)$를 다음과 같이 표현합니다.<ul>\n<li>$d^\\pi(s)=\\sum_{t=0}^{\\infty}\\gamma^t Pr(s_t=s|s_0,\\pi)$<br>($\\gamma=1$ is allowed only in episodic tasks = discounted weighting of states)</li>\n</ul>\n</li>\n</ul>\n<p>여기서부터가 논문에는 없는 내용입니다. 위의 최종 수식을 다시 한 번 적으면 아래와 같습니다.<br>$$\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p>위의 수식을 아래와 같이 바꿀 수 있습니다.<br>$$\\sum_{s’} \\eta(s’) \\cdot\\sum_s \\frac{\\eta(s)}{\\sum_{s’}\\eta(s’)} \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p>그러면 이것을 새로운 기호로 다시 나타낼 수 있습니다.<br>$$\\sum_{s’} \\eta(s’) \\cdot\\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ </p>\n<ul>\n<li>여기서 $\\mu(s)$ 는 state distribution이라고 하며, weight를 state distribution으로 바꿔주는 과정이라고 생각할 수 있습니다. 쉽게 말해 어떠한 state에 agent가 머무르는 확률입니다.</li>\n</ul>\n<p>위의 수식은 아래의 수식과 비례합니다. 따라서 최종 형태는 다음과 같습니다.<br>$$\\propto \\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p><br><br></p>\n<h1 id=\"3-Policy-Gradient-with-Approximation\"><a href=\"#3-Policy-Gradient-with-Approximation\" class=\"headerlink\" title=\"3. Policy Gradient with Approximation\"></a>3. Policy Gradient with Approximation</h1><p>이 장에서는 앞서 다뤘던 Theorem 1 중에 $Q^\\pi$에 대해서 중점적으로 다룹니다. 어떠한 $Q^\\pi$가 학습된 function approximator로 근사된다고 합시다. </p>\n<p>$f_w$ ($f_w$는 $S × A \\rightarrow \\Re$)는 parameter $w$를 가지는 어떠한 $Q^\\pi$ 에 대한 근사치입니다. 학습된 function approximator에 대해서 생각하기 때문에, 학습된 $f_w$는 error를 최소화하는 방향으로 parameter $w$를 업데이트합니다. 그러면 다음과 같은 관계식을 생각할 수 있습니다.<br>$$\\Delta w_t \\propto \\frac{\\partial}{\\partial w}\\big[ {\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]^2 \\propto \\big[{\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s_t,a_t)}{\\partial w}$$<br>(여기서 ${\\hat Q}^\\pi(s_t,a_t)$ 는 $Q^\\pi(s_t,a_t)$의 unbiased estimator $R_t$입니다.)</p>\n<p>그러면 위와 같은 수식이 local optimum에 수렴을 했을 때, 다음과 같이 나타낼 수 있습니다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<ul>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>local optimum으로 수렴을 하기 때문에 당연히 위의 수식은 0이 됩니다.</li>\n<li>또한 stochastic policy이기 때문에 local optimum으로 수렴하려면 모든 state, action에 대해 expectation을 해야합니다. 따라서 $\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)$이 붙게 됩니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-1-Theorem-2-Policy-Gradient-with-Function-Approximation\"><a href=\"#3-1-Theorem-2-Policy-Gradient-with-Function-Approximation\" class=\"headerlink\" title=\"3.1 Theorem 2: Policy Gradient with Function Approximation\"></a>3.1 Theorem 2: Policy Gradient with Function Approximation</h2><p>만약 $f_w$가 아래의 등식을 만족한다고 합시다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<p>또한 $f_w$가 policy parameterization과 양립할 수 있다고 합시다. policy parameterization에 대한 여러가지 의미가 있는데 이 논문에서는 다음을 의미합니다.<br>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$</p>\n<ul>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>이 수식은 Monte-Calro Estimation을 이용하기 위한 조건입니다.</li>\n<li>Compatibility Condition이라고 부릅니다.</li>\n<li>우변(= $\\nabla\\log\\pi(s,a)$)으로 유도함으로써 아래의 수식과 같이 sampling을 이용한 추정이 가능해집니다.</li>\n</ul>\n</li>\n</ul>\n<p>따라서 위의 두 수식을 이용하여 아래와 같이 나타낼 수 있습니다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s,a)$$</p>\n<p><br></p>\n<h2 id=\"3-2-Proof-of-Theorem-2\"><a href=\"#3-2-Proof-of-Theorem-2\" class=\"headerlink\" title=\"3.2 Proof of Theorem 2\"></a>3.2 Proof of Theorem 2</h2><p>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<p>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$</p>\n<p>위의 두 수식을 합치면 다음과 같이 나타낼 수 있습니다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}=0$$</p>\n<p>분자와 분모에 있는 $\\pi(s,a)$를 지우면 아래와 같습니다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]=0$$</p>\n<p>이어서 위의 수식이 0이기 때문에 policy gradient theorem(앞서 다뤘던 Theorem 1)에서 위의 수식을 뺄 수 있습니다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)-\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]$$</p>\n<p>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-Q^\\pi(s_t,a_t)+f_w(s_t,a_t) \\big]$$</p>\n<p>$$\\therefore\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s_t,a_t)$$</p>\n<p><br><br></p>\n<h1 id=\"4-Application-to-Deriving-Algorithms-and-Advantages\"><a href=\"#4-Application-to-Deriving-Algorithms-and-Advantages\" class=\"headerlink\" title=\"4. Application to Deriving Algorithms and Advantages\"></a>4. Application to Deriving Algorithms and Advantages</h1><p><br></p>\n<h2 id=\"4-1-Application-to-Deriving-Algorithms\"><a href=\"#4-1-Application-to-Deriving-Algorithms\" class=\"headerlink\" title=\"4.1 Application to Deriving Algorithms\"></a>4.1 Application to Deriving Algorithms</h2><p>feature의 linear combination에서 Gibbs distribution (softmax)를 하나의 policy로 생각해볼 수 있습니다.<br>$$\\pi(s,a)=\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_b e^{\\theta^{T}\\phi_{sb} }}$$</p>\n<ul>\n<li>$\\phi_{sa}$: state-action pair $s$, $a$를 나타내는 $l$-dimensional feature vector</li>\n</ul>\n<p>compatible condition을 적용하면 다음과 같습니다. (4.2 Proof of application of compatible condition 참고)<br>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}=\\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}$$ </p>\n<p>이어서 $f_w$을 적분한 natural parameterization은 다음과 같습니다.<br>$$f_w(s,a)=w^T\\big[ \\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}\\big]$$</p>\n<ul>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>$f_w$는 policy로서 같은 feature들에 대해 linear합니다.</li>\n<li>$f_w$는 각각의 state에 대해서 평균이 0입니다. ($\\sum_a\\pi(s,a)f_w(s,a)=0$)</li>\n<li>advantage function $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$의 하나의 근사치로서 $f_w$를 생각해도 좋습니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"4-2-Proof-of-application-of-compatible-condition\"><a href=\"#4-2-Proof-of-application-of-compatible-condition\" class=\"headerlink\" title=\"4.2 Proof of application of compatible condition\"></a>4.2 Proof of application of compatible condition</h2><p>증명을 하기 전에 다음을 가정합니다.</p>\n<ol>\n<li>$(\\frac{f(x)}{g(x)})’=f’(x)\\cdot \\frac{1}{g(x)}-f(x)\\cdot \\frac{g’(x)}{g(x)^{2} }=\\frac{f’(x)\\cdot g(x)-f(x)\\cdot g’(x)}{g(x)^{2} }$</li>\n<li>$f(\\theta)=e^{\\theta^{T}\\phi_{sa} }$, $g(\\theta)=\\sum_be^{\\theta^{T}\\phi_{sb} }$</li>\n</ol>\n<p>$$<br>\\begin{align}<br>\\frac{\\partial f_w(s,a)}{\\partial w}&amp;=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\big(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big)’\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\big[ \\phi_{sa}(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})-(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})\\frac{\\sum_b\\phi_{sb}e^{ {\\theta^T}\\phi_{sb} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big]\\frac{1}{\\pi(s,a)}\\\\<br>&amp;=\\big[ \\phi_{sa}\\pi(s,a)-\\pi(s,a)\\sum_b\\phi_{sb}\\pi(s,b)\\big]\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\pi(s,a)\\big[ \\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\\big]\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;\\therefore\\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}<br>\\end{align}<br>$$</p>\n<p><br></p>\n<h2 id=\"4-3-Application-to-Advantages\"><a href=\"#4-3-Application-to-Advantages\" class=\"headerlink\" title=\"4.3 Application to Advantages\"></a>4.3 Application to Advantages</h2><p>Policy Gradient with Function Approximation Theorem (Theorem 2)는 advantage function으로 확장될 수 있습니다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_sd^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ f_w(s,a)+v(s) \\big]$$</p>\n<ul>\n<li>위의 수식에 대한 추가설명<ul>\n<li>$v$ ($v$는 $S\\rightarrow\\Re$) 는 arbitrary function입니다.</li>\n<li>이 수식은 $\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}=0$이기 때문에 가능해집니다.</li>\n<li>즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없습니다.</li>\n<li>$v$의 선택은 Theorem들에 영향을 미치지 못하지만, 실질적으로 gradient estimator의 variance에 영향을 미칩니다.</li>\n<li>이러한 문제는 전체적으로 이전의 연구에 reinforcement baseline의 사용에 있어서 유사합니다.</li>\n<li>(comment) 위의 수식에서 $f_w(s,a)+v(s)$와 Application to Deriving Algorithms의 $f_w(s,a)$와는 다른 것입니다. Application to Deriving Algorithms의 $f_w(s,a)$은 softmax에 의해 스스로 advantage function의 역할을 할 수 있습니다. 하지만 보통의 경우에는 그러지 못할 수도 있기 때문에 위의 수식처럼 $f_w(s,a)+v(s)$을 추가하여 zero mean만들어서 variance를 줄일 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"5-Convergence-of-Policy-Iteration-with-Function-Approximation\"><a href=\"#5-Convergence-of-Policy-Iteration-with-Function-Approximation\" class=\"headerlink\" title=\"5. Convergence of Policy Iteration with Function Approximation\"></a>5. Convergence of Policy Iteration with Function Approximation</h1><p><br></p>\n<h2 id=\"5-1-Theorem-3-Convergence-of-Policy-Iteration-with-Function-Approximation\"><a href=\"#5-1-Theorem-3-Convergence-of-Policy-Iteration-with-Function-Approximation\" class=\"headerlink\" title=\"5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation\"></a>5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation</h2><p>policy iteration with function approximation은 locally optimal policy에 수렴합니다. $\\pi$와 $f_w$를 </p>\n<ol>\n<li>compatibility condition을 만족하는 policy와 value function에 대한</li>\n<li>그리고 $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|&lt;B&lt;\\infty$에 대한</li>\n</ol>\n<p>어떠한 미분가능한 function approximator라고 합시다.</p>\n<ul>\n<li>(comment) $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|&lt;B&lt;\\infty$에서 $\\pi(s,a)$라는 function은 이계미분값이 존재하고, 임의의 상수인 (Bound) B에 bound되어 있기 때문에 function의 그래프는 smooth하다고 볼 수 있습니다. (아래의 그림 중 빨간색 그래프 참고)</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/xx02ejfg5ao19ps/Screen%20Shot%202018-07-10%20at%203.41.06%20PM.png?dl=1\" width=\"200\"> </center>\n\n<p>이어서 ${\\alpha_k}$는 $\\lim_{k\\rightarrow\\infty}\\alpha_k=0$이며 $\\sum_k\\alpha_k=\\infty$를 만족하는 step-size sequence라고 합시다.</p>\n<p>그 때, bounded reward를 가진 MDP에 대해</p>\n<ol>\n<li>어떠한 $\\theta_0, \\pi_k=\\pi(\\cdot,\\cdot,\\theta_k)$</li>\n<li>그리고 $\\sum_sd^{\\pi_{k} }(s)\\sum_a\\pi_k(s,a)\\big[ Q^{\\pi_{k} }(s,a)-f_w(s,a)\\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$로 인하여 $w_k=w$, $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$</li>\n</ol>\n<p>으로 정의된 sequence $\\rho(\\pi_k)$는 $\\lim_{k\\rightarrow\\infty}\\frac{\\partial\\rho(\\pi_k)}{\\partial\\theta}=0$이기 때문에 수렴합니다.</p>\n<ul>\n<li>sequence $\\rho(\\pi_k)_{k=0}^\\infty$에 대한 추가 설명<ul>\n<li>2번의 식을 통해 자연스럽게 actor-critic으로 연결됩니다.</li>\n<li>$\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$ 에 따라 $\\theta$가 1, 2, …, $\\infty$로 갈텐데, 거기에 따른 objective function or performance의 sequence입니다.</li>\n<li>(comment) 굳이 sequence라는 표현이 없어도 될 것 같습니다. 어짜피 k가 $\\infty$로 가면 $\\rho(\\pi_k)$가 수렴한다는 의미이기 때문에 불필요해보입니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"5-2-Proof-of-Theorem-3\"><a href=\"#5-2-Proof-of-Theorem-3\" class=\"headerlink\" title=\"5.2 Proof of Theorem 3\"></a>5.2 Proof of Theorem 3</h2><ul>\n<li>Theorem 2는 $\\theta_k$ update가 gradient의 error를 최소화한다는 것을 증명했습니다.</li>\n<li>$\\frac{\\partial^{2}\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}$와 MDP의 reward에서의 bound는, $\\frac{\\partial^{2}\\rho}{\\partial\\theta_i\\partial\\theta_j}$ 또한 bound된다는 것을 증명합니다.</li>\n<li>step-size 필요조건 때문에 이러한 bound된 것들은 Proposition 3.5 from page 96 of Bertsekas and Tsitsiklis (1996)에 적용하기 위해 필요한 조건입니다.</li>\n<li>Proposition 3.5 from page 96 of Bertsekas and Tsitsiklis (1996)은 local optimum으로 수렴한다는 것을 증명했습니다.</li>\n<li>Proposition 3.5를 자세하게 설명하지는 않겠습니다. 일반적인 gradient method의 수렴성을 증명한 것이며, 이 논문의 policy gradient method도 gradient method의 일종이므로 특정 조건을 만족할 때 수렴한다는 것입니다. 수렴하는 원리는 소위 말하는 Lipschitz continuity condition을 만족하기 때문인데 다음과 같습니다. $L&gt;0$인 임의의 상수입니다.<br>$$<br>\\parallel \\nabla f(r) - \\nabla f(\\bar{r}) \\parallel \\leq L \\parallel r - \\bar{r} \\parallel<br>$$<br>즉, 업데이트가 진행되어 나감에 따라서 gradient의 차이가 점점 줄어들게 되므로 언젠가는 0으로 수렴하게 됩니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"6-Summary\"><a href=\"#6-Summary\" class=\"headerlink\" title=\"6. Summary\"></a>6. Summary</h1><p>논문에서 설명한 policy gradient 기법을 요약하면 다음과 같습니다.</p>\n<ul>\n<li>Original maximization problem: $\\max\\rho_{\\theta}=E\\left[R\\right]$</li>\n<li>gradient ascent method를 이용한 parameter update: $\\theta_{t+1} = \\theta_{t} + \\alpha\\left.\\frac{\\partial E[R]}{\\partial\\theta}\\right|_{\\theta_t}$<ul>\n<li>그러나 우리는 gradient를 모르고, 추정하기도 어렵습니다. 왜냐하면 expectation이 안에 있기 때문입니다.</li>\n<li>그렇다면 이것을 추정 가능한 형태로 바꿔야합니다. 다시 말해 expectation이 밖에 있는 형태로 바꾸는 것입니다.</li>\n<li>Expectation이 밖에 있으면 왜 추정이 유리할까요? 바로 Sample mean을 취하면 되기 때문입니다.</li>\n</ul>\n</li>\n<li>Approximate gradient<ul>\n<li>방법1: 논문의 Theorem 1을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다.<br>$$\\frac{\\partial E[R]}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$</li>\n<li>방법2: Log derivative trick을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다. (Theorem 2)<br>$$\\theta_{t+1} = \\theta_{t} + \\alpha E\\left[ \\left. R\\frac{\\partial}{\\partial\\theta}\\log p_{\\theta} \\right\\vert \\theta_t\\right]$$</li>\n<li>그리고 특정 trajectory를 따라가면서 return값을 구하고 이것을 여러 번 수행하여 sample mean을 취하면 gradient를 추정하는 것이 가능합니다.</li>\n<li>이러한 기법을 제시한 기존의 연구가 REINFORCE입니다.</li>\n<li>여러 Trajectory를 이용하므로 variance가 높을 수 밖에 없습니다.</li>\n<li>Advantage를 활용하거나 하는 방식으로 이후 여러 연구가 진행되었습니다.</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"DPG-여행하기\"><a href=\"#DPG-여행하기\" class=\"headerlink\" title=\"DPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/\">DPG 여행하기</a></h2>","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"https://www.dropbox.com/s/83i4atz733ali0w/Screen%20Shot%202018-07-18%20at%2012.10.52%20AM.png?dl=1\" width=\"600\"> </center>\n\n<p>논문 저자 : Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour<br>논문 링크 : <a href=\"http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf\" target=\"_blank\" rel=\"noopener\">NIPS</a><br>Proceeding : Advances in Neural Information Processing Systems (NIPS) 2000<br>정리 : 김동민, 이동민</p>\n<hr>\n<h1 id=\"1-Intro-to-Policy-Gradient\"><a href=\"#1-Intro-to-Policy-Gradient\" class=\"headerlink\" title=\"1. Intro to Policy Gradient\"></a>1. Intro to Policy Gradient</h1><p>이 논문은 policy gradient (PG) 기법의 효시와도 같으며 향후 많은 파생연구를 낳은 중요한 논문입니다. 7페이지의 짧은 논문이지만 읽기에 만만한 논문은 아닙니다. 이 논문을 이해하기 위해 필요한 배경지식을 먼저 설명하고 논문을 차근차근 살펴보도록 하겠습니다.</p>\n<p><br></p>\n<h2 id=\"1-1-Value-Function-Approach\"><a href=\"#1-1-Value-Function-Approach\" class=\"headerlink\" title=\"1.1 Value Function Approach\"></a>1.1 Value Function Approach</h2><p>전통적으로 강화학습 기법은 value function을 기반으로 동작하였습니다. 특정 state에서 vaue function 또는 value function을 근사하는 함수(function approximation)를 최대화하는 action을 찾는 greedy action-selection policy가  대표적입니다.</p>\n<p>논문에서는 이러한 방법은 deterministic한 policy를 찾는 쪽으로 나아가게 되지만 종종 최적의 policy는 stochastic한 성질을 가지기 때문에 이 방법으로는 최적의 policy를 찾을 수 없다고 언급하고 있습니다. (그러나 이 논문이 나오고 나서 한참 후 David Silver를 필두로한 DeepMind의 연구진들은 high-dimensional action space를 가지는 application에서 보다 빠르게 동작하는 <a href=\"../../../06/27/2_dpg/\">deterministic policy gradient</a>을 개발하였습니다.) 아마도 이러한 부분은 $\\epsilon$-greedy action-selection method로 개선될 수 있을 것입니다. 또 하나의 문제는 value function의 작은 변화로 인해서 action이 크게 변할 수도 있다는 것입니다. 이것은 알고리듬의 수렴성에 문제를 야기할 수 있습니다. 이러한 문제점을 해결하기 위하여 policy search라는 새로운 기법이 고안됩니다.</p>\n<p><br></p>\n<h2 id=\"1-2-Policy-Search\"><a href=\"#1-2-Policy-Search\" class=\"headerlink\" title=\"1.2 Policy Search\"></a>1.2 Policy Search</h2><p>policy search는 최적의 policy $\\pi^*$를 reward로부터 직접 찾습니다. policy search 방법은 크게 두 가지로 분류할 수 있습니다. 첫 번째는 gradient-based optimization으로 policy gradient method가 이에 속합니다. 두 번째는 gradient-free optimization으로 진화(evolutionary) 연산을 이용하는 것이 이에 속합니다. 이 논문에서는 제목에서 알 수 있듯이 gradient-based optimization을 다룹니다. gradient는 변화량을 의미합니다. 특정 policy x와 또 다른 policy y가 있을 때 policy가 얼마나 많이 변화했는지 어떻게 모델링할 수 있을까요? policy의 변화를 제어하는 어떤 파라미터가 있다면 이 파라미터를 조정하여 policy를 변화시킬 수 있습니다. policy x에 해당하는 파라미터값과 policy y에 해당하는 파라미터값의 차이가 policy의 변화량이라고 할 수 있습니다. 이와 같은 방식으로 policy의 변화를 모델링하기 위하여 파라미터 $\\theta$를 이용하여 policy를 $\\pi_{\\theta}$로 표현할 수 있습니다. 최적의 policy를 찾기 위하여 expected return $E\\left[R|\\theta\\right]$이 최대화되도록 parameter를 조정합니다. 이를 간단한 수식으로 정리하면 다음과 같습니다.</p>\n<p>$$<br>\\pi^* = \\arg \\max\\limits_\\pi  E\\left[ {\\left. R \\right|\\pi } \\right] \\to {\\text{original problem} }<br>\\\\  {\\Downarrow} \\\\<br>{\\text{policy parameterization by } } \\pi_{\\theta}: \\Theta  \\to \\Pi \\\\ {\\Downarrow} $$</p>\n<p>$$\\pi^* = \\arg \\max\\limits_{\\theta}  E\\left[ {\\left. R \\right|\\theta } \\right] \\to {\\text{policy search problem} }<br>$$</p>\n<p><br></p>\n<h2 id=\"1-3-How-to-Obtain-the-Expected-Return\"><a href=\"#1-3-How-to-Obtain-the-Expected-Return\" class=\"headerlink\" title=\"1.3 How to Obtain the Expected Return\"></a>1.3 How to Obtain the Expected Return</h2><p>expected return을 최대화하는 방향으로 policy를 update한다고 하였습니다. 그렇다면 expected return은 어떻게 구할 수 있을까요? 여기에도 크게 두 가지 방법이 있습니다. 첫 번째는 deterministic approximation으로 Markov decision process의 dynamics를 모델링한 후 수식을 통해 구하는 것입니다. 두 번째 방법은 monte carlo estimation으로 dynamics에 대한 모델을 하지 않고 많은 sample들을 얻은 후 empirical하게 expected return을 계산하는 방법입니다. 어느 방법이 더 좋을지는 풀고자 하는 문제의 특성에 따라 다를 것입니다. dynamics에 대한 모델이 어렵거나 변화가 큰 경우에는 두 번째 방법이 좀 더 현실적이지만 gradient를 구하는 것은 더 어렵습니다. 결국 gradient를 esimate하는방법을 고안해야 합니다. 가장 유명한 gradient estimation 방법이 1992년 R. J. Williams에 의해서 제안된 <a href=\"https://link.springer.com/content/pdf/10.1007/BF00992696.pdf\" target=\"_blank\" rel=\"noopener\">REINFORCE</a> 기법입니다. REINFORCE는 Monte Carlo estimate 또는 likelihood-ratio estimate라고 부르는 방법을 이용합니다. 이 방법에 대해서 좀 더 알아볼까요?</p>\n<h3 id=\"1-3-1-Monte-Carlo-Gradient-Estimation\"><a href=\"#1-3-1-Monte-Carlo-Gradient-Estimation\" class=\"headerlink\" title=\"1.3.1 Monte Carlo Gradient Estimation\"></a>1.3.1 Monte Carlo Gradient Estimation</h3><p>다음과 같은 parameter $\\theta$를 가지는 random variable $X$가 있습니다: $X:\\Omega\\mapsto\\mathcal{X}$. 그리고 이 $x$에 대한 함수 $f$가 있습니다: $f:\\mathcal{X}\\mapsto\\mathbb{R}$. expected return처럼 $E[f(x)]$를 최대화하고자 합니다. 이를 위해서는 $\\nabla_{\\theta}E[f(x)]$를 구해야 합니다. 이 때 log derivate trick을 이용하여 다음과 같이 수식을 변형시킬 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\nabla_{\\theta} E_{p(x;\\theta)}\\left[ {f(x)} \\right]<br>&amp;= \\nabla_\\theta\\int{f\\left( x \\right)p\\left( {x;\\theta} \\right) dx}<br>\\\\<br>&amp;= \\int { {\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= \\int {\\frac{ {p\\left( {x;\\theta } \\right)} }{ {p\\left( {x;\\theta } \\right)} }{\\nabla_\\theta }p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= \\int {p\\left( {x;\\theta } \\right){\\nabla_\\theta }\\log p\\left( {x;\\theta } \\right)f\\left( x \\right)dx}<br>\\\\<br>&amp;= E_{p(x;\\theta)}[f(x)\\nabla_\\theta \\log p({x;\\theta})]<br>\\end{align}<br>$$</p>\n<p>이제 expectation은 많은 샘플들을 모아서 평균을 취하는 Monte Carlo 기법으로 근사화할 수 있습니다.</p>\n<p>$${\\nabla_{\\theta} }{E_{p\\left( {x;\\theta } \\right)} }\\left[ {f\\left( x \\right)} \\right] = \\frac{1}{N}\\sum_{n=1}^N f\\left(X_n\\right)\\nabla_\\theta\\log p\\left(X_n;\\theta\\right)$$</p>\n<p>이 방법을 사용하기 위해서 필요한 조건은 $\\log p\\left(X_n;\\theta\\right)$가 미분가능해야 한다는 것 뿐입니다. 그러나 이 방법은 얻은 샘플들에 의존하기 때문에 경우에 따라서 큰 variance를 가질 수 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"2-Policy-Gradient-Methods\"><a href=\"#2-Policy-Gradient-Methods\" class=\"headerlink\" title=\"2. Policy Gradient Methods\"></a>2. Policy Gradient Methods</h1><p>이제 본격적으로 policy gradient 기법에 대해서 알아보겠습니다.</p>\n<p><br></p>\n<h2 id=\"2-1-System-Model\"><a href=\"#2-1-System-Model\" class=\"headerlink\" title=\"2.1 System Model\"></a>2.1 System Model</h2><p>논문에서 사용하는 수학기호(notation)와 가정들을 설명하는 시스템모델은 다음과 같습니다.</p>\n<ul>\n<li>Markov decision process (MDP) at each time $t\\in{0,1,2,…}$</li>\n<li>$S_t\\in\\mathcal{S}$: state</li>\n<li>$A_t\\in\\mathcal{A}$: action</li>\n<li>$r_t\\in \\mathcal{R}$: reward</li>\n<li>$\\theta\\in\\mathbb{R}^l$: vector of policy parameters, for $l\\ll\\left|\\mathcal{S}\\right|$</li>\n<li>$\\mathcal{P} _{s s’}^a = \\Pr[S _{t+1}=s’ \\vert S_t=s,A_t=a]$: state transition probabilities</li>\n<li>$\\mathcal{R} _{s s’}^a = E[R _{t+1}\\vert S_t=s,A_t=a]$: expected reward</li>\n<li>$\\pi(s,a,\\theta)=\\Pr[A_t=a|S_t=s,\\theta]$: policy, shortened as $\\pi(s,a)$ or $\\pi(a|s)$</li>\n<li>$\\theta$: policy parameter들의 vector  </li>\n<li>$\\rho$: 해당 policy들의 성능을 나타내는 척도 (예. average reward per step)  </li>\n<li>$\\alpha$: positive-definite한 step size </li>\n</ul>\n<p><br></p>\n<h2 id=\"2-2-Policy-Gradent-Approach\"><a href=\"#2-2-Policy-Gradent-Approach\" class=\"headerlink\" title=\"2.2 Policy Gradent Approach\"></a>2.2 Policy Gradent Approach</h2><p>policy gradient는 stochastic policy를 자체적인 파리미터를 가진 function approximator를 이용해서 근사화시킵니다.</p>\n<p>policy parameter는 gradient에 비례하여 업데이트됩니다.</p>\n<p>$$\\Delta\\theta \\approx \\alpha\\frac{\\partial\\rho}{\\partial\\theta}$$</p>\n<p>$\\rho$는 $\\theta$에 대하여 미분가능해야겠죠? 위와 같이 업데이트하면 local optimal policy로 수렴합니다. 논문의 가장 중요한 contribution은 특정 조건을 만족하는 function approximator를 이용하여 경험(experience, sample)을 축적하고 이것들을 이용하여 위의 gradient를 unbiased estimate할 수 있음을 증명한 것입니다.</p>\n<p>먼저 $\\rho$, 즉, reward를 표현하는 두 가지 방법에 대해서 알아보겠습니다.</p>\n<p><br></p>\n<h2 id=\"2-3-Average-Reward-Formulation\"><a href=\"#2-3-Average-Reward-Formulation\" class=\"headerlink\" title=\"2.3 Average Reward Formulation\"></a>2.3 Average Reward Formulation</h2><p>Average reward formulation은 시간의 흐름에 따른 reward를 표현한다기보다는 모든 시간의 reward를 평균을 내서 표현하는 방법입니다.</p>\n<ul>\n<li><p>Long‐term expected reward per step<br>$$\\rho(\\pi)=\\lim_{n\\to\\infty}\\frac{1}{n}E[r_1+r_2+\\cdots+r_n|\\pi] = \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\mathcal{R}_s^a$$</p>\n<ul>\n<li>$d^\\pi(s): \\lim_{t\\to\\infty}\\Pr[S_t=s|s_0,\\pi]$: stationary distribution of states under $\\pi$</li>\n<li>$d^\\pi(s)$가 존재한다고 가정합니다. 그리고 모든 policy에 대해서 $d^\\pi(s)$는 $s_0$와 independent합니다.</li>\n<li>위의 (limit수식과 summation수식 사이의) 등식은 왜 성립할까요? Time average와 Ensemble average가 같다는 뜻으로 <a href=\"https://en.wikipedia.org/wiki/Ergodicity\" target=\"_blank\" rel=\"noopener\">ergodic</a>한 system에서 성립합니다.</li>\n</ul>\n</li>\n<li><p>Value of a state-action pair given a policy<br>$$Q^{\\pi} (s,a) = \\sum_{t=1}^\\infty E[r_t - \\rho(\\pi)|S_0=s, A_0=a, \\pi]$$</p>\n<ul>\n<li>위의 Q-function의 표현식이 정의인지 유도된 것인지는 이 포스트를 작성하는 저희도 아직 확실히 모르겠습니다. 다만, $r_t$들을 더하면 무한대로 발산할 가능성이 매우 높은데 $\\rho$를 빼줌으로해서 어찌보면 평균 reward로부터의 차이를 더하는 것이라고 할 수 있고 이것은 bound된 값일 가능성이 더 높기 때문에 Q-function을 이렇게 표현하는 것이 더 안전하다고 할 수 있습니다. 이 부분에 대해서 좋은 의견이 있으시면 피드백을 주시면 감사하겠습니다!</li>\n</ul>\n</li>\n<li><p>State-value function<br>$$V^\\pi(s) = \\sum_{a} \\pi(s,a)Q^\\pi(s,a)$$</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-4-Start-State-Formulation\"><a href=\"#2-4-Start-State-Formulation\" class=\"headerlink\" title=\"2.4 Start-State Formulation\"></a>2.4 Start-State Formulation</h2><p>start-state formulation은 시간의 흐름에 따라 감소하는 reward들을 표현합니다.</p>\n<ul>\n<li><p>Long-term expected reward per step with a designated start state $S_0$<br>$$\\rho(\\pi) = E\\left[\\left.\\sum_{t=1}^\\infty \\gamma^{t-1}r_t\\right|S_0,\\pi\\right]$$</p>\n<ul>\n<li>$\\gamma\\in\\left[0,1\\right]$: discount rate</li>\n</ul>\n</li>\n<li><p>Value of a state‐action pair given a policy<br>$$Q^\\pi(s,a) = E\\left[\\left.\\sum_{k=1}^\\infty \\gamma^{k-1} r_{t+k}\\right|S_t=s, A_t=a, \\pi\\right]$$</p>\n</li>\n<li><p>Alternative form of $Q^\\pi(s,a)$<br>$$<br>Q^\\pi(s,a) = R_s^a + \\gamma\\sum _{s’} \\mathcal{P} _{s s’}^a V^{\\pi}(s’)<br>$$</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-5-Policy-Gradient-Theorem\"><a href=\"#2-5-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"2.5 Policy Gradient Theorem\"></a>2.5 Policy Gradient Theorem</h2><p>논문의 중요한 결과인 Theorem 1은 다음과 같습니다.</p>\n<p><strong>Theorem 1 (Policy Gradient)</strong> <em>For any MDP, in either the average‐reward or start‐state formulations,</em><br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$</p>\n<p>놀라운 부분은 expected return의 gradient를 취할 때 $\\frac{\\partial d^\\pi(s)}{\\partial\\theta}$를 구하지 않아도 된다는 점입니다. 즉, policy의 변화가 state distribution에 영향을 주지 않는다는 것입니다. 이것은 다시 말하면 아래 수식과 같이 우리는 $\\frac{\\partial\\rho}{\\partial\\theta}$에 대한 unbiased estimator를 구할 수 있다는 뜻입니다.</p>\n<p>$$E_s\\left[\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)\\right] = \\frac{\\partial\\rho}{\\partial\\theta}$$</p>\n<p>이로인해서 우리는 gradient를 sampling을 통해서 추정할 수 있습니다. 그렇지만 이것은 sample이 아주 많을 때만 성립합니다!</p>\n<p>또 한 가지 문제는 우리는 $Q^\\pi(s,a)$의 정확한 값을 알 수 없다는 것 입니다. 이것을 estimate하기 위해서 현재의 return값 $R_t$를 이용할 수 있습니다.</p>\n<p><br></p>\n<h2 id=\"2-6-Proof-of-Policy-Gradient-Theorem\"><a href=\"#2-6-Proof-of-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"2.6 Proof of Policy Gradient Theorem\"></a>2.6 Proof of Policy Gradient Theorem</h2><p>증명을 살펴보겠습니다.</p>\n<ul>\n<li>average-reward formulation을 이용할 때의 증명은 아래와 같습니다.</li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\frac{\\partial V^\\pi(s)}{\\partial\\theta} &amp;\\overset{\\underset{\\mathrm{def} }{} }{=}\\frac{\\partial}{\\partial\\theta}\\sum_a \\pi(s,a)Q^\\pi(s,a)<br>\\\\<br>&amp;=\\sum_a \\frac{\\partial}{\\partial\\theta}\\left[\\pi(s,a)Q^\\pi(s,a)\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial Q^\\pi(s,a)}{\\partial\\theta}\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\frac{\\partial}{\\partial\\theta}\\left[R_s^a -\\rho(\\pi) + \\sum_{s’}\\mathcal{P}_{s s’}^a V^{\\pi}(s’)\\right]\\right]\\\\<br>&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\left[-\\frac{\\partial\\rho}{\\partial\\theta} + \\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}\\right]\\right]\\\\<br>\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_a \\left[\\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+\\pi(s,a)\\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}\\right] - \\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>양변에 stationary distribution에 대한 평균을 취합니다.<br>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s)\\sum_a\\pi(s,a)\\sum_{s’}\\mathcal{P}_{s s’}^a \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta}<br>\\\\<br>&amp;- \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>모든 state $s$에서 모든 state $s’$으로 이동하는 것은 state $s’$에 존재할 확률이므로 다음과 같이 표현할 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)+ \\sum_s d^\\pi(s’) \\frac{\\partial V^{\\pi}(s’)}{\\partial\\theta} - \\sum_s d^\\pi(s)\\frac{\\partial V^\\pi(s)}{\\partial\\theta}<br>\\end{align}<br>$$</p>\n<p>뒤의 두 항은 같은 식이므로 0이 됩니다. 따라서 다음과 같이 표현됩니다.<br>$$<br>\\begin{align}<br>\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)<br>\\end{align}<br>$$</p>\n<p>$\\frac{\\partial\\rho}{\\partial\\theta}$는 reward의 파라미터에 대한 미분값입니다. reward 자체가 이미 모든 state에 대한 평균값이므로 $s$의 함수가 아닙니다.  $s$에 대한 함수가 아니므로 $\\sum_s d^\\pi(s)\\frac{\\partial\\rho}{\\partial\\theta}=\\frac{\\partial\\rho}{\\partial\\theta}\\sum_s d^\\pi(s)=\\frac{\\partial\\rho}{\\partial\\theta}$입니다. 여기서 $d^\\pi(s)$는 state에 대한 확률값이죠. 모든 state에 대해서 이 값을 다 더하면 1이 됩니다.<br>이를 이용해서 위의 식을 다시 표현하면 아래와 같고 이를 통해 증명이 완성됩니다.</p>\n<p>$$<br>\\begin{align}<br>\\therefore\\frac{\\partial\\rho}{\\partial\\theta}&amp;=\\sum_s d^\\pi(s) \\sum_a \\frac{\\partial \\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)<br>\\end{align}<br>$$</p>\n<ul>\n<li>다음으로 start-state formulation에 대한 증명입니다.</li>\n</ul>\n<p>start-state formulation은 이 논문에서도 나오고 서튼책에서도 나오는데, 서튼책의 증명이 더 설명이 자세합니다. 그래서 논문에 있는 증명이 아닌 서튼책에 있는 증명으로 설명드리겠습니다. 다만, notation이 약간 다른데, 큰 문제 없이 이해하실 수 있으므로 별도의 설명은 하지 않겠습니다. (서튼책 <a href=\"https://drive.google.com/file/d/1xeUDVGWGUUv1-ccUMAZHJLej2C7aAFWY/view\" target=\"_blank\" rel=\"noopener\">Link</a>)</p>\n<p>$$\\nabla{v_\\pi}(s)=\\nabla\\big[\\sum_a\\pi(a|s)q_\\pi(s,a)\\big]$$ </p>\n<p>다음으로 $(f\\cdot g)’=f’\\cdot g+f\\cdot g’$ (product rule of calculus)에 의해 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\nabla q_\\pi(s,a)\\big]$$  </p>\n<p>이어서 $p(s’,r|s,a) := Pr[S_t=s’, R_t=r|S_{t-1}=s,A_{t-1}=a ]$ (서튼책 3.2 공식) 에 의해 다음과 같이 나타낼 수 있습니다.</p>\n<p>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_{\\pi}(s,a)+\\pi(a|s)\\nabla\\big[\\sum_{s’, r}p(s’,r|s,a)(r+v_{\\pi}(s’))\\big]\\big]$$  </p>\n<p>계속해서 $p(s’|s,a)=\\sum_{r\\in R}p(s’,r|s,a)$ (서튼책 3.4 공식) 에 의해 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s’}p(s’|s,a)\\nabla v_\\pi(s’) \\big]$$ </p>\n<p>여기서 $\\nabla v_\\pi(s’)$ 부분을 unrolling을 하면 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_a\\big[\\nabla\\pi(a|s)q_\\pi(s,a)+\\pi(a|s)\\sum_{s’}p(s’|s,a)\\sum_{a’}\\big[\\nabla\\pi(a’|s’)q_\\pi(s’,a’)\\\\+\\pi(a’|s’)\\sum_{s’’}p(s’’|s’,a’)\\nabla v_\\pi(s’’))\\big] \\big]$$ </p>\n<p>$\\nabla v_\\pi(s’’)$에 대해서 unrolling을 하고 또 나오는 다른 항에 대해서도 계속 반복하다보면 다음과 같이 나타낼 수 있습니다.<br>$$=\\sum_{x\\in S}\\sum_{k=0}^{\\infty}\\Pr(s\\rightarrow x,k,\\pi)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p>여기에서 $\\Pr(s\\rightarrow x,k,\\pi)$ 는 policy $\\pi$ 에 대해 state $s$에서 state $x$까지 $k$ step만큼 움직일 때의 변환 확률입니다. 따라서 위의 수식을 아래처럼 다시 나타낼 수 있습니다.<br> <!-- \\begin{align}  --><br>$$\\nabla J(\\theta)=\\nabla v_\\pi (s_0)$$<br>$$=\\sum_s(\\sum_{k=0}^{\\infty}\\Pr(s_0\\rightarrow s,k,\\pi))\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$<br>$$=\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$<br><!-- \\end{align} --></p>\n<ul>\n<li>서튼책에서 $\\eta(s)$는 다음과 같이 표현할 수 있습니다.<ul>\n<li>$\\eta(s)$ = 평균적으로 하나의 episode마다 state에서 머무른 time step의 수 (page 199)</li>\n</ul>\n</li>\n<li>논문에서는 $\\eta(s)$를 다음과 같이 표현합니다.<ul>\n<li>$d^\\pi(s)=\\sum_{t=0}^{\\infty}\\gamma^t Pr(s_t=s|s_0,\\pi)$<br>($\\gamma=1$ is allowed only in episodic tasks = discounted weighting of states)</li>\n</ul>\n</li>\n</ul>\n<p>여기서부터가 논문에는 없는 내용입니다. 위의 최종 수식을 다시 한 번 적으면 아래와 같습니다.<br>$$\\sum_s \\eta(s) \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p>위의 수식을 아래와 같이 바꿀 수 있습니다.<br>$$\\sum_{s’} \\eta(s’) \\cdot\\sum_s \\frac{\\eta(s)}{\\sum_{s’}\\eta(s’)} \\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p>그러면 이것을 새로운 기호로 다시 나타낼 수 있습니다.<br>$$\\sum_{s’} \\eta(s’) \\cdot\\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$ </p>\n<ul>\n<li>여기서 $\\mu(s)$ 는 state distribution이라고 하며, weight를 state distribution으로 바꿔주는 과정이라고 생각할 수 있습니다. 쉽게 말해 어떠한 state에 agent가 머무르는 확률입니다.</li>\n</ul>\n<p>위의 수식은 아래의 수식과 비례합니다. 따라서 최종 형태는 다음과 같습니다.<br>$$\\propto \\sum_s \\mu(s)\\sum_a \\nabla \\pi(a|s)q_\\pi(s,a)$$</p>\n<p><br><br></p>\n<h1 id=\"3-Policy-Gradient-with-Approximation\"><a href=\"#3-Policy-Gradient-with-Approximation\" class=\"headerlink\" title=\"3. Policy Gradient with Approximation\"></a>3. Policy Gradient with Approximation</h1><p>이 장에서는 앞서 다뤘던 Theorem 1 중에 $Q^\\pi$에 대해서 중점적으로 다룹니다. 어떠한 $Q^\\pi$가 학습된 function approximator로 근사된다고 합시다. </p>\n<p>$f_w$ ($f_w$는 $S × A \\rightarrow \\Re$)는 parameter $w$를 가지는 어떠한 $Q^\\pi$ 에 대한 근사치입니다. 학습된 function approximator에 대해서 생각하기 때문에, 학습된 $f_w$는 error를 최소화하는 방향으로 parameter $w$를 업데이트합니다. 그러면 다음과 같은 관계식을 생각할 수 있습니다.<br>$$\\Delta w_t \\propto \\frac{\\partial}{\\partial w}\\big[ {\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]^2 \\propto \\big[{\\hat Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s_t,a_t)}{\\partial w}$$<br>(여기서 ${\\hat Q}^\\pi(s_t,a_t)$ 는 $Q^\\pi(s_t,a_t)$의 unbiased estimator $R_t$입니다.)</p>\n<p>그러면 위와 같은 수식이 local optimum에 수렴을 했을 때, 다음과 같이 나타낼 수 있습니다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<ul>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>local optimum으로 수렴을 하기 때문에 당연히 위의 수식은 0이 됩니다.</li>\n<li>또한 stochastic policy이기 때문에 local optimum으로 수렴하려면 모든 state, action에 대해 expectation을 해야합니다. 따라서 $\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)$이 붙게 됩니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-1-Theorem-2-Policy-Gradient-with-Function-Approximation\"><a href=\"#3-1-Theorem-2-Policy-Gradient-with-Function-Approximation\" class=\"headerlink\" title=\"3.1 Theorem 2: Policy Gradient with Function Approximation\"></a>3.1 Theorem 2: Policy Gradient with Function Approximation</h2><p>만약 $f_w$가 아래의 등식을 만족한다고 합시다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<p>또한 $f_w$가 policy parameterization과 양립할 수 있다고 합시다. policy parameterization에 대한 여러가지 의미가 있는데 이 논문에서는 다음을 의미합니다.<br>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$</p>\n<ul>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>이 수식은 Monte-Calro Estimation을 이용하기 위한 조건입니다.</li>\n<li>Compatibility Condition이라고 부릅니다.</li>\n<li>우변(= $\\nabla\\log\\pi(s,a)$)으로 유도함으로써 아래의 수식과 같이 sampling을 이용한 추정이 가능해집니다.</li>\n</ul>\n</li>\n</ul>\n<p>따라서 위의 두 수식을 이용하여 아래와 같이 나타낼 수 있습니다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s,a)$$</p>\n<p><br></p>\n<h2 id=\"3-2-Proof-of-Theorem-2\"><a href=\"#3-2-Proof-of-Theorem-2\" class=\"headerlink\" title=\"3.2 Proof of Theorem 2\"></a>3.2 Proof of Theorem 2</h2><p>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$$</p>\n<p>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}$$</p>\n<p>위의 두 수식을 합치면 다음과 같이 나타낼 수 있습니다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\pi(s,a)\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]\\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\frac{1}{\\pi(s,a)}=0$$</p>\n<p>분자와 분모에 있는 $\\pi(s,a)$를 지우면 아래와 같습니다.<br>$$\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial \\pi(s,a)}{\\partial \\theta}\\big[ {Q}^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]=0$$</p>\n<p>이어서 위의 수식이 0이기 때문에 policy gradient theorem(앞서 다뤘던 Theorem 1)에서 위의 수식을 뺄 수 있습니다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)-\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-f_w(s_t,a_t) \\big]$$</p>\n<p>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ Q^\\pi(s_t,a_t)-Q^\\pi(s_t,a_t)+f_w(s_t,a_t) \\big]$$</p>\n<p>$$\\therefore\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a \\frac{\\partial\\pi(s,a)}{\\partial\\theta}f_w(s_t,a_t)$$</p>\n<p><br><br></p>\n<h1 id=\"4-Application-to-Deriving-Algorithms-and-Advantages\"><a href=\"#4-Application-to-Deriving-Algorithms-and-Advantages\" class=\"headerlink\" title=\"4. Application to Deriving Algorithms and Advantages\"></a>4. Application to Deriving Algorithms and Advantages</h1><p><br></p>\n<h2 id=\"4-1-Application-to-Deriving-Algorithms\"><a href=\"#4-1-Application-to-Deriving-Algorithms\" class=\"headerlink\" title=\"4.1 Application to Deriving Algorithms\"></a>4.1 Application to Deriving Algorithms</h2><p>feature의 linear combination에서 Gibbs distribution (softmax)를 하나의 policy로 생각해볼 수 있습니다.<br>$$\\pi(s,a)=\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_b e^{\\theta^{T}\\phi_{sb} }}$$</p>\n<ul>\n<li>$\\phi_{sa}$: state-action pair $s$, $a$를 나타내는 $l$-dimensional feature vector</li>\n</ul>\n<p>compatible condition을 적용하면 다음과 같습니다. (4.2 Proof of application of compatible condition 참고)<br>$$\\frac{\\partial f_w(s,a)}{\\partial w}=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}=\\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}$$ </p>\n<p>이어서 $f_w$을 적분한 natural parameterization은 다음과 같습니다.<br>$$f_w(s,a)=w^T\\big[ \\phi_{sa}-\\sum_b \\pi(s,b)\\phi_{sb}\\big]$$</p>\n<ul>\n<li>위의 수식에 대한 추가 설명<ul>\n<li>$f_w$는 policy로서 같은 feature들에 대해 linear합니다.</li>\n<li>$f_w$는 각각의 state에 대해서 평균이 0입니다. ($\\sum_a\\pi(s,a)f_w(s,a)=0$)</li>\n<li>advantage function $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$의 하나의 근사치로서 $f_w$를 생각해도 좋습니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"4-2-Proof-of-application-of-compatible-condition\"><a href=\"#4-2-Proof-of-application-of-compatible-condition\" class=\"headerlink\" title=\"4.2 Proof of application of compatible condition\"></a>4.2 Proof of application of compatible condition</h2><p>증명을 하기 전에 다음을 가정합니다.</p>\n<ol>\n<li>$(\\frac{f(x)}{g(x)})’=f’(x)\\cdot \\frac{1}{g(x)}-f(x)\\cdot \\frac{g’(x)}{g(x)^{2} }=\\frac{f’(x)\\cdot g(x)-f(x)\\cdot g’(x)}{g(x)^{2} }$</li>\n<li>$f(\\theta)=e^{\\theta^{T}\\phi_{sa} }$, $g(\\theta)=\\sum_be^{\\theta^{T}\\phi_{sb} }$</li>\n</ol>\n<p>$$<br>\\begin{align}<br>\\frac{\\partial f_w(s,a)}{\\partial w}&amp;=\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\big(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big)’\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\big[ \\phi_{sa}(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})-(\\frac{e^{\\theta^{T}\\phi_{sa} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }})\\frac{\\sum_b\\phi_{sb}e^{ {\\theta^T}\\phi_{sb} }}{\\sum_be^{\\theta^{T}\\phi_{sb} }}\\big]\\frac{1}{\\pi(s,a)}\\\\<br>&amp;=\\big[ \\phi_{sa}\\pi(s,a)-\\pi(s,a)\\sum_b\\phi_{sb}\\pi(s,b)\\big]\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;=\\pi(s,a)\\big[ \\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}\\big]\\frac{1}{\\pi(s,a)}<br>\\\\<br>&amp;\\therefore\\phi_{sa}-\\sum_b\\pi(s,b)\\phi_{sb}<br>\\end{align}<br>$$</p>\n<p><br></p>\n<h2 id=\"4-3-Application-to-Advantages\"><a href=\"#4-3-Application-to-Advantages\" class=\"headerlink\" title=\"4.3 Application to Advantages\"></a>4.3 Application to Advantages</h2><p>Policy Gradient with Function Approximation Theorem (Theorem 2)는 advantage function으로 확장될 수 있습니다.<br>$$\\frac{\\partial\\rho}{\\partial\\theta}=\\sum_sd^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}\\big[ f_w(s,a)+v(s) \\big]$$</p>\n<ul>\n<li>위의 수식에 대한 추가설명<ul>\n<li>$v$ ($v$는 $S\\rightarrow\\Re$) 는 arbitrary function입니다.</li>\n<li>이 수식은 $\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}=0$이기 때문에 가능해집니다.</li>\n<li>즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없습니다.</li>\n<li>$v$의 선택은 Theorem들에 영향을 미치지 못하지만, 실질적으로 gradient estimator의 variance에 영향을 미칩니다.</li>\n<li>이러한 문제는 전체적으로 이전의 연구에 reinforcement baseline의 사용에 있어서 유사합니다.</li>\n<li>(comment) 위의 수식에서 $f_w(s,a)+v(s)$와 Application to Deriving Algorithms의 $f_w(s,a)$와는 다른 것입니다. Application to Deriving Algorithms의 $f_w(s,a)$은 softmax에 의해 스스로 advantage function의 역할을 할 수 있습니다. 하지만 보통의 경우에는 그러지 못할 수도 있기 때문에 위의 수식처럼 $f_w(s,a)+v(s)$을 추가하여 zero mean만들어서 variance를 줄일 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"5-Convergence-of-Policy-Iteration-with-Function-Approximation\"><a href=\"#5-Convergence-of-Policy-Iteration-with-Function-Approximation\" class=\"headerlink\" title=\"5. Convergence of Policy Iteration with Function Approximation\"></a>5. Convergence of Policy Iteration with Function Approximation</h1><p><br></p>\n<h2 id=\"5-1-Theorem-3-Convergence-of-Policy-Iteration-with-Function-Approximation\"><a href=\"#5-1-Theorem-3-Convergence-of-Policy-Iteration-with-Function-Approximation\" class=\"headerlink\" title=\"5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation\"></a>5.1 Theorem 3: Convergence of Policy Iteration with Function Approximation</h2><p>policy iteration with function approximation은 locally optimal policy에 수렴합니다. $\\pi$와 $f_w$를 </p>\n<ol>\n<li>compatibility condition을 만족하는 policy와 value function에 대한</li>\n<li>그리고 $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|&lt;B&lt;\\infty$에 대한</li>\n</ol>\n<p>어떠한 미분가능한 function approximator라고 합시다.</p>\n<ul>\n<li>(comment) $\\max_{\\theta, s, a, i, j}\\big| \\frac{\\partial^2\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}\\big|&lt;B&lt;\\infty$에서 $\\pi(s,a)$라는 function은 이계미분값이 존재하고, 임의의 상수인 (Bound) B에 bound되어 있기 때문에 function의 그래프는 smooth하다고 볼 수 있습니다. (아래의 그림 중 빨간색 그래프 참고)</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/xx02ejfg5ao19ps/Screen%20Shot%202018-07-10%20at%203.41.06%20PM.png?dl=1\" width=\"200\"> </center>\n\n<p>이어서 ${\\alpha_k}$는 $\\lim_{k\\rightarrow\\infty}\\alpha_k=0$이며 $\\sum_k\\alpha_k=\\infty$를 만족하는 step-size sequence라고 합시다.</p>\n<p>그 때, bounded reward를 가진 MDP에 대해</p>\n<ol>\n<li>어떠한 $\\theta_0, \\pi_k=\\pi(\\cdot,\\cdot,\\theta_k)$</li>\n<li>그리고 $\\sum_sd^{\\pi_{k} }(s)\\sum_a\\pi_k(s,a)\\big[ Q^{\\pi_{k} }(s,a)-f_w(s,a)\\big]\\frac{\\partial f_w(s,a)}{\\partial w}=0$로 인하여 $w_k=w$, $\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$</li>\n</ol>\n<p>으로 정의된 sequence $\\rho(\\pi_k)$는 $\\lim_{k\\rightarrow\\infty}\\frac{\\partial\\rho(\\pi_k)}{\\partial\\theta}=0$이기 때문에 수렴합니다.</p>\n<ul>\n<li>sequence $\\rho(\\pi_k)_{k=0}^\\infty$에 대한 추가 설명<ul>\n<li>2번의 식을 통해 자연스럽게 actor-critic으로 연결됩니다.</li>\n<li>$\\theta_{k+1}=\\theta_k+\\alpha_k\\sum_sd^{\\pi_{k} }(s)\\sum_a\\frac{\\partial\\pi_k(s,a)}{\\partial\\theta}f_{w_{k} }(s,a)$ 에 따라 $\\theta$가 1, 2, …, $\\infty$로 갈텐데, 거기에 따른 objective function or performance의 sequence입니다.</li>\n<li>(comment) 굳이 sequence라는 표현이 없어도 될 것 같습니다. 어짜피 k가 $\\infty$로 가면 $\\rho(\\pi_k)$가 수렴한다는 의미이기 때문에 불필요해보입니다.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"5-2-Proof-of-Theorem-3\"><a href=\"#5-2-Proof-of-Theorem-3\" class=\"headerlink\" title=\"5.2 Proof of Theorem 3\"></a>5.2 Proof of Theorem 3</h2><ul>\n<li>Theorem 2는 $\\theta_k$ update가 gradient의 error를 최소화한다는 것을 증명했습니다.</li>\n<li>$\\frac{\\partial^{2}\\pi(s,a)}{\\partial\\theta_i\\partial\\theta_j}$와 MDP의 reward에서의 bound는, $\\frac{\\partial^{2}\\rho}{\\partial\\theta_i\\partial\\theta_j}$ 또한 bound된다는 것을 증명합니다.</li>\n<li>step-size 필요조건 때문에 이러한 bound된 것들은 Proposition 3.5 from page 96 of Bertsekas and Tsitsiklis (1996)에 적용하기 위해 필요한 조건입니다.</li>\n<li>Proposition 3.5 from page 96 of Bertsekas and Tsitsiklis (1996)은 local optimum으로 수렴한다는 것을 증명했습니다.</li>\n<li>Proposition 3.5를 자세하게 설명하지는 않겠습니다. 일반적인 gradient method의 수렴성을 증명한 것이며, 이 논문의 policy gradient method도 gradient method의 일종이므로 특정 조건을 만족할 때 수렴한다는 것입니다. 수렴하는 원리는 소위 말하는 Lipschitz continuity condition을 만족하기 때문인데 다음과 같습니다. $L&gt;0$인 임의의 상수입니다.<br>$$<br>\\parallel \\nabla f(r) - \\nabla f(\\bar{r}) \\parallel \\leq L \\parallel r - \\bar{r} \\parallel<br>$$<br>즉, 업데이트가 진행되어 나감에 따라서 gradient의 차이가 점점 줄어들게 되므로 언젠가는 0으로 수렴하게 됩니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"6-Summary\"><a href=\"#6-Summary\" class=\"headerlink\" title=\"6. Summary\"></a>6. Summary</h1><p>논문에서 설명한 policy gradient 기법을 요약하면 다음과 같습니다.</p>\n<ul>\n<li>Original maximization problem: $\\max\\rho_{\\theta}=E\\left[R\\right]$</li>\n<li>gradient ascent method를 이용한 parameter update: $\\theta_{t+1} = \\theta_{t} + \\alpha\\left.\\frac{\\partial E[R]}{\\partial\\theta}\\right|_{\\theta_t}$<ul>\n<li>그러나 우리는 gradient를 모르고, 추정하기도 어렵습니다. 왜냐하면 expectation이 안에 있기 때문입니다.</li>\n<li>그렇다면 이것을 추정 가능한 형태로 바꿔야합니다. 다시 말해 expectation이 밖에 있는 형태로 바꾸는 것입니다.</li>\n<li>Expectation이 밖에 있으면 왜 추정이 유리할까요? 바로 Sample mean을 취하면 되기 때문입니다.</li>\n</ul>\n</li>\n<li>Approximate gradient<ul>\n<li>방법1: 논문의 Theorem 1을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다.<br>$$\\frac{\\partial E[R]}{\\partial\\theta}=\\sum_s d^\\pi(s)\\sum_a\\frac{\\partial\\pi(s,a)}{\\partial\\theta}Q^\\pi(s,a)$$</li>\n<li>방법2: Log derivative trick을 이용하면 gradient와 expectation의 위치를 바꿀 수 있습니다. (Theorem 2)<br>$$\\theta_{t+1} = \\theta_{t} + \\alpha E\\left[ \\left. R\\frac{\\partial}{\\partial\\theta}\\log p_{\\theta} \\right\\vert \\theta_t\\right]$$</li>\n<li>그리고 특정 trajectory를 따라가면서 return값을 구하고 이것을 여러 번 수행하여 sample mean을 취하면 gradient를 추정하는 것이 가능합니다.</li>\n<li>이러한 기법을 제시한 기존의 연구가 REINFORCE입니다.</li>\n<li>여러 Trajectory를 이용하므로 variance가 높을 수 밖에 없습니다.</li>\n<li>Advantage를 활용하거나 하는 방식으로 이후 여러 연구가 진행되었습니다.</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"DPG-여행하기\"><a href=\"#DPG-여행하기\" class=\"headerlink\" title=\"DPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/27/2_dpg/\">DPG 여행하기</a></h2>"},{"title":"Deterministic Policy Gradient Algorithms","date":"2018-06-27T08:21:48.000Z","author":"김동민, 공민서, 장수영, 차금강","subtitle":"피지여행 2번째 논문","_content":"\n<center> <img src=\"https://www.dropbox.com/s/etwa3hn5c0pegdv/Screen%20Shot%202018-07-18%20at%2012.51.35%20AM.png?dl=1\" width=\"800\"> </center>\n\n논문 저자 : David Silver, Guy Lever, Nicloas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller\n논문 링크 : [main text](http://proceedings.mlr.press/v32/silver14.pdf), [supplementary material](http://proceedings.mlr.press/v32/silver14-supp.pdf)\nProceeding : International Conference on Machine Learning (ICML) 2014\n정리 : 김동민, 공민서, 장수영, 차금강\n\n---\n\n# 1. 들어가며...\n\n- Deterministic Policy Gradient (DPG) Theorem을 제안합니다.\n    - 중요한 점은 DPG는 Expected gradient of the action-value function의 형태라는 것입니다.\n- Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient (SPG)와 동일해집니다. (Theorem 2)\n    - Theorem 2로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG에 적용할 수 있게 됩니다.\n        - 예. Sutton PG, natural gradients, actor-critic, episodic/batch methods\n- 적절한 exploration 을 위해 model-free, off-policy actor-critic algorithm 을 제안합니다\n    - action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility condition을 제공합니다. (Theorem 3)\n- DPG 는 SPG 보다 성능이 좋습니다.\n    - 특히 high dimensional action spaces를 가지는 tasks에서의 성능 향상이 큽니다.\n        - SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는 state spaces에 대해서만 평균을 취합니다.\n        - 결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됩니다.\n        - 무한정 학습을 시키면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정합니다.\n    - 기존 기법들에 비해 computation 양이 많지 않습니다.\n        - Computation 은 action dimensionality 와 policy parameters 수에 비례합니다.\n\n<br><br>\n\n# 2. Background\n\n<br>\n## 2.1 Performance objective function\n\n$$\n\\begin{align}\nJ(\\pi_{\\theta}) &= \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\pi_{\\theta}(s,a)r(s,a)da ds = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[r(s,a)]\n\\end{align}\n$$\n\n<br>\n## 2.2 SPG Theorem\n- State distribution $ \\rho^{\\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없습니다.\n$$\\begin{eqnarray}\\nabla_{\\theta}J(\\pi_{\\theta}) &=& \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\\\ &=& E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]\n\\end{eqnarray}$$\n\n<br>\n## 2.3 Stochastic Actor-Critic Algorithms\n- Actor와 Critic이 번갈아가면서 동작하며 stochastic policy를 최적화하는 기법입니다.\n- Actor: $ Q^{\\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $를 이용해 stochastic policy gradient를 ascent하는 방향으로 policy parameter $ \\theta $를 업데이트함으로써 stochastic policy를 발전시킵니다.\n    - $ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $\n- Critic: SARSA나 Q-learning같은 Temporal-difference (TD) learning을 이용해 action-value function의 parameter, $ w $를 업데이트함으로써 $ Q^w(s,a) $가 $ Q^{\\pi}(s,a) $과 유사해지도록 합니다.\n- 실제 값인 $ Q^{\\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $를 사용하게 되면, 일반적으로 bias가 발생합니다. 하지만, compatible condition에 부합하는 $ Q^w(s,a) $를 사용하게 되면, bias가 발생하지 않습니다.\n\n<br>\n## 2.4 Off-policy Actor-Critic\n- Distinct behavior policy $ \\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic\n- Performance objective function\n    - $\\begin{eqnarray}\n        J_{\\beta}(\\pi_{\\theta}) \n        &=& \\int_{S}\\rho^{\\beta}(s)V^{\\pi}(s)ds \\nonumber \\\\\\\\\n        &=& \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads\n        \\end{eqnarray} $\n- off-policy policy gradient\n    - $ \\begin{eqnarray}\n        \\nabla_{\\theta}J_{\\beta}(\\pi_{\\theta}) &\\approx& \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\end{eqnarray} $\n        $=E_{s \\sim \\rho^{\\beta}, a \\sim \\beta}[\\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)}\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]$\n    - off-policy policy gradient 식에서의 물결 표시는 [Degris, 2012b](https://arxiv.org/abs/1205.4839) 논문에 근거합니다.\n        - Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같습니다. (빨간색 상자에 있는 항목을 삭제함으로써 근사합니다.)\n            - <img src=\"https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1\" width=500px>\n        - [Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\\nabla_{u}𝑄^{\\pi,\\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮습니다.\n            - <img src=\"https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1\" width=500px>\n    - off-policy policy gradient 식에서 $ \\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)} $는 importance sampling ratio 입니다.\n        - off-policy actor-critic에서는 $ \\beta $에 의해 샘플링된 trajectory를 이용해서 stochastic policy $ \\pi $를 예측하는 것이기 때문에 importance sampling이 필요합니다.\n\n\n<br><br>\n\n# 3. Gradient of Deterministic Policies\n\n<br>\n## 3.1 Regularity Conditions\n- 어떠한 이론이 성립하기 위한 전제 조건\n- Regularity conditions A.1\n    - $ p(s'|s,a), \\nabla_{a}p(s'|s,a), \\mu_{\\theta}(s), \\nabla_{\\theta}\\mu_{\\theta}(s), r(s,a), \\nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s' $ and $ x $.\n- regularity conditions A.2\n    - There exists a $ b $ and $ L $ such that $ \\sup_{s}p_{1}(s) < b $, $ \\sup_{a,s,s'}p(s'|s,a) < b $, $ \\sup_{a,s}r(s,a) < b $, $ \\sup_{a,s,s'}\\|\\nabla_{a}p(s'|s,a)\\| < L $, and $ \\sup_{a,s}\\|\\nabla_{a}r(s,a)\\| < L $.\n\n<br>\n## 3.2 Deterministic Policy Gradient Theorem\n- Deterministic policy\n    - $ \\mu_{\\theta} : S \\to A $ with parameter vector $ \\theta \\in \\mathbb{R}^n $\n- Probability distribution\n    - $ p(s \\to s', t, \\mu) $\n- Discounted state distribution\n    - $ \\rho^{\\mu}(s) $\n- Performance objective\n\n$$\nJ(\\mu_{\\theta}) = E[r^{\\gamma}_{1} | \\mu] \n$$\n\n$$\n= \\int_{S}\\rho^{\\mu}(s)r(s,\\mu_{\\theta}(s))ds \n= E_{s \\sim \\rho^{\\mu}}[r(s,\\mu_{\\theta}(s))]\n$$\n\n- DPG Theorem\n    - MDP 가 A.1 만족한다면, 아래 식이 성립합니다.\n    $\\nabla_{\\theta}J(\\mu_{\\theta}) = \\int_{S}\\rho^{\\mu}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}ds \\nonumber$\n    $= E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]   \\nonumber $ \n    \n\t- DPG는 state space에 대해서만 평균을 취하면 되기에, state와 action space 모두에 대해 평균을 취해야 하는 SPG에 비해 data efficiency가 좋습니다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 됩니다.\n\n<br>    \n## 3.3 DPG 형태에 대한 informal intuition\n- Generalized policy iteration\n    - 정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration\n        - 위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴합니다.\n- 정책 평가\n    - action-value function $ Q^{\\pi}(s,a) $ or $ Q^{\\mu}(s,a) $을 estimate 하는 것 입니다.\n- 정책 발전\n    - 위 estimated action-value function에 따라 정책을 update하는 것 입니다.\n    - 주로 action-value function에 대한 greedy maximisation을 사용합니다.\n        - $ \\mu^{k+1}(s) = \\arg\\max\\limits_{a}Q^{\\mu^{k}}(s,a) $\n        - greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이로 인해 continuous action spaces에서 계산량이 급격히 늘어납니다.\n    - 그래서 policy gradient 방법이 나옵니다.\n        - policy 를 $ \\theta $에 대해서 parameterize 합니다.\n        - 매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $마다 policy parameter를 action-value function $ Q $의 $ \\theta $에 대한 gradient $ \\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s)) $ 방향으로 proportional하게 update 합니다.\n        - 하지만 각 state는 다른 방향을 제시할 수 있기에, state distribution $ \\rho^{\\mu}(s) $에 대한 기대값을 취해 policy parameter를 update 할 수도 있습니다.\n            - $ \\theta^{k+1} = \\theta^{k} + \\alpha E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s))] $\n        - 이는 chain-rule에 따라 아래와 같이 분리될 수 있습니다.\n            - $ \\theta^{k+1} = \\theta^{k} + \\alpha E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu^{k}}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $ (7)\n            - chain rule: $ \\frac{\\partial Q}{\\partial \\theta} = \\frac{\\partial a}{\\partial \\theta} \\frac{\\partial Q}{\\partial a} $\n        - 하지만 state distribution $ \\rho^{\\mu} $은 정책에 dependent 합니다.\n            - 정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state가 변하기 때문에 state distribution이 변하게 됩니다.\n        - 그렇기에 정책 update 시 state distribution에 대한 gradient를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있습니다.\n        - deterministic policy gradient theorem은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update해도 performance objective의 gradient를 정확하게 따름을 의미합니다.\n\n<br>\n## 3.4 DPG는 SPG의 limiting case\n- stochastic policy parameterization\n    - $ \\pi_{\\mu_{\\theta},\\sigma} $ by a deterministic policy $ \\mu_{\\theta} : S \\to A $ and a variance parameter $ \\sigma $\n    - $ \\sigma = 0 $ 이면, $ \\pi_{\\mu_{\\theta},\\sigma} \\equiv \\mu_{\\theta} $\n- Theorem 2. Policy의 variance가 0에 수렴하면, 즉, $ \\sigma \\to 0 $, stochastic policy gradient와 deterministic policy gradient는 동일해집니다.\n    - 조건: stochastic policy $ \\pi_{\\mu_{\\theta},\\sigma} = \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $\n        - $ \\sigma $는 variance입니다.\n        - $ \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $는 condition B.1을 만족합니다.\n        - MDP는 conditions A.1과 A.2를 만족합니다.\n    - 결과:\n        - $ \\lim\\limits_{\\sigma\\downarrow0}\\nabla_{\\theta}J(\\pi_{\\mu_{\\theta},\\sigma}) = \\nabla_{\\theta}J(\\mu_{\\theta})  $\n            - 좌변은 standard stochastic gradient이며, 우변은 deterministic gradient입니다.\n    - 의미:\n        - deterministic policy gradient는 stochastic policy gradient의 특수 case 입니다.\n        - 기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있습니다.\n            - 기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)\n\n<br><br>\n\n# 4. Deterministic Actor-Critic Algorithms\n1. SARSA critic를 이용한 on-policy actor-critic\n    - 단점\n        - deterministic policy에 의해 행동하면 exploration이 잘 되지 않기에, sub-optimal에 빠지기 쉽습니다.\n    - 목적\n        - 교훈/정보제공\n        - 환경에서 충분한 noise를 제공하여 exploration을 시킬 수 있다면, deterministic policy를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있습니다.\n            - 예. 바람이 agent의 행동에 영향(noise)을 줌\n    - Remind: 살사(SARSA) update rule\n        - $ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $\n    - Algorithm\n        - Critic은 MSE를 $ \\bf minimize $하는 방향, 즉, action-value function을 stochastic gradient $ \\bf descent $ 방법으로 update합니다.\n            - $ MSE = [Q^{\\mu}(s,a) - Q^{w}(s,a)]^2 $\n                - critic은 실제 $ Q^{\\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $로 대체하여 action-value function을 estimate하며, 이 둘 간 mean square error를 최소화하는 것이 목표입니다.\n            - $ \\nabla_{w}MSE \\approx -2 * [r + \\gamma Q^{w}(s',a') - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $\n                - $ \\nabla_{w}MSE = -2 * [Q^{\\mu}(s,a) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $\n                - $ Q^{\\mu}(s,a) $ 를 $ r + \\gamma Q^{w}(s',a') $로 대체\n                    - $ Q^{\\mu}(s,a) = r + \\gamma Q^{\\mu}(s',a') $\n            - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $\n                - $w_{t+1} = w_{t} - \\alpha_{w}\\nabla_{w}MSE  \\nonumber$\n                $ \\approx w_{t} - \\alpha_{w} * (-2 * [r + \\gamma Q^{w}(s',a') - Q^{w}(s,a)] \\nabla_{w}Q^{w}(s,a)$\n                - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $\n        - Actor는 식(9)에 따라 보상이 $ \\bf maximize $되는 방향, 즉, deterministic policy를 stochastic gradient $ \\bf ascent $ 방법으로 update합니다.\n            - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $\n2. Q-learning 을 이용한 off-policy actor-critic\n    - stochastic behavior policy $ \\beta(a|s) $에 의해 생성된 trajectories로부터 deterministic target policy $ \\mu_{\\theta}(s) $를 학습하는 off-policy actor-critic입니다\n    - performance objective\n        - $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)V^{\\mu}(s)ds \\nonumber \\\\\\\\$\n          $= \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds \\nonumber \\\\\\\\$\n          $= E_{s \\sim \\rho^{\\beta}}[Q^{\\mu}(s,\\mu_{\\theta}(s))]$\n    - off-policy deterministic policy gradient\n        - $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n            - 논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됩니다.\n            - $ \\begin{eqnarray}\n                \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) &\\approx& \\int_{S}\\rho^{\\beta}(s)\\nabla_{\\theta}\\mu_{\\theta}(a|s)Q^{\\mu}(s,a)ds \\nonumber \\\\\n                &=& E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]\n                \\end{eqnarray} $\n            - 근거: Action이 deterministic하기에 stochastic 경우와는 다르게 performance objective에서 action에 대해 평균을 구할 필요가 없습니다. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b]에서 처럼 곱의 미분을 통해 생기는 action-value function에 대한 gradient term을 생략할 필요가 사라집니다.\n    - Remind: 큐러닝(Q-learning) update rule\n        - $ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $\n    - algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)\n        - 살사를 이용한 on-policy deterministic actor-critic과 아래 부분을 제외하고는 같습니다.\n            - target policy는 $ \\beta(a|s) $에 의해 생성된 trajectories를 통해 학습합니다.\n            - 업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $이 아니라 정책으로부터 나온 행동 값 $ \\mu_{\\theta}(s_{t+1}) $을 사용합니다.\n                - $ \\mu_{\\theta}(s_{t+1}) $ 은 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $\n    - Stochastic off-policy actor-critic은 대개 actor와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient에선 importance sampling이 필요없습니다.\n        - Actor 는 deterministic 이기에 sampling 자체가 필요없습니다.\n            - Stochastic policy인 경우, Actor에서 importance sampling이 필요한 이유는 상태 $ s $에서의 가치 함수 값 $ V^{\\pi}(s) $을 estimate하기 위해 $ \\pi $가 아니라 $ \\beta $에 따라 sampling을 한 후, 평균을 내기 때문입니다.\n            - 하지만 deterministic policy인 경우, 상태 $ s $에서의 가치 함수 값 $ V^{\\pi}(s) = Q^{\\pi}(s,\\mu_{\\theta}) $ 즉, action이 상태 s에 대해 deterministic이기에 sampling을 통해 estimate할 필요가 없고, 따라서 importance sampling도 필요없어집니다.\n            - stochastic vs. deterministic performance objective\n                - stochastic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads $\n                - deterministic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds $\n        - Critic이 사용하는 Q-learning은 importance sampling이 필요없는 off policy 알고리즘입니다.\n            - Q-learning도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있습니다.\n3. compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic\n    - 위 살사/Q-learning 기반 on/off-policy는 아래와 같은 문제가 존재합니다.\n        - function approximator에 의한 bias\n            - 일반적으로 $ Q^{\\mu}(s,a) $ 를 $ Q^{w}(s,a) $로 대체하여 deterministic policy gradient를 구하면, 그 gradient는 ascent하는 방향이 아닐 수도 있습니다.\n        - off-policy learning에 의한 instabilities\n    - 그래서 stochastic처럼 $ \\nabla_{a}Q^{\\mu}(s,a) $를 $ \\nabla_{a}Q^{w}(s,a) $로 대체해도 deterministic policy gradient에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $를 찾아야 합니다.\n    - Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $는 deterministic policy $ \\mu_{\\theta}(s) $와 compatible 합니다. 즉, $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n        - $ \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $입니다.\n        - $ w $는 $ MSE(\\theta, w) = E[\\epsilon(s;\\theta,w)^{\\top}\\epsilon(s;\\theta,w)] $를 최소화합니다.\n            - $ \\epsilon(s;\\theta,w) = \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} - \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}  $\n    - Theorem 3은 on-policy 뿐만 아니라 off-policy에도 적용 가능합니다.\n    - $ Q^{w}(s,a) = (a-\\mu_{\\theta}(s))^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top} w + V^{v}(s) $\n        - 어떠한 deterministic policy에 대해서도 위 형태와 같은 compatible function approximator가 존재합니다.\n        - 앞의 term은 advantage를, 뒤의 term은 value로 볼 수 있습니다.\n    - $ Q^{w}(s,a) = \\phi(s,a)^{\\top} w + v^{\\top}\\phi(s) $\n        - 정의 : $ \\phi(s,a) \\overset{\\underset{\\mathrm{def}}{}}{=} \\nabla_{\\theta}\\mu_{\\theta}(s)(a-\\mu_{\\theta}(s)) $\n        - 일례 : $ V^{v}(s) = v^{\\top}\\phi(s) $\n        - Theorem 3 만족 여부\n            - 첫 번째 조건 만족합니다.\n            - 두 번째 조건은 대강 만족합니다.\n                - $ \\nabla_{a}Q^{\\mu}(s,a) $에 대한 unbiased sample을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $를 학습합니다.\n                - 이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \\approx Q^{\\mu}(s,a) $인 reasonable solution을 찾을 수 있기에 대강 $ \\nabla_{a}Q^{w}(s,a) \\approx \\nabla_{a}Q^{\\mu}(s,a) $이 될 것입니다.\n        - action-value function에 대한 linear function approximator는 큰 값을 가지는 actions에 대해선 diverge할 수 있어 global하게 action-values 예측하기에는 좋지 않지만, local critic에 사용할 때는 매우 유용합니다.\n            - 즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\\mu_{\\theta}(s)+\\delta) = \\delta^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $로, diverge하지 않고, 값을 얻을 수 있습니다.\n    - COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)\n        - Critic: 실제 action-value function에 대한 linear function approximator인 $ Q^{w}(s,a) = \\phi(s,a)^{\\top}w $를 estimate합니다.\n            - $ \\phi(s,a) = a^{\\top}\\nabla_{\\theta}\\mu_{\\theta} $\n            - Behavior policy $ \\beta(a|s) $로부터 얻은 samples를 이용하여 Q-learning이나 gradient Q-learning과 같은 off-policy algorithm으로 학습 가능합니다.\n        - Actor: estimated action-value function에 대한 gradient를 $ \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $로 치환 후, 정책을 업데이트합니다.\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $\n    - off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있습니다.\n        - $ \\mu_{\\theta}(s_{t+1}) $이 diverge할 수도 있기 때문으로 판단됩니다.\n        - 그렇기에 simple Q-learning 대신 다른 기법이 필요합니다.\n    - 그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm을 제안합니다.\n        - gradient temporal-difference learning에 기반한 기법들은 true gradient descent algorithm이며, converge가 보장됩니다. (Sutton, 2009)\n            - 기본 아이디어는 stochastic gradient descent로 mean-squared projected Bellman error (MSPBE)를 최소화하는 것입니다.\n            - critic이 actor보다 빠른 time-scale로 update되도록 step size들을 잘 조절하면, critic은 MSPBE를 최소화하는 parameters로 converge하게 됩니다.\n            - critic에 gradient temporal-difference learning의 일종인 gradient Q-learning을 사용한 논문입니다. (Maei, 2010)\n    - COPDAC-GQ algorithm\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) - \\alpha_{w}\\gamma\\phi(s_{t+1}, \\mu_{\\theta}(s_{t+1}))(\\phi(s_{t},a_{t})^{\\top} u_{t}) $\n        - $ v_{t+1} = v_{t} + \\alpha_{v}\\delta_{t}\\phi(s_{t}) - \\alpha_{v}\\gamma\\phi(s_{t+1})(\\phi(s_{t},a_{t})^{\\top} u_{t}) $\n        - $ u_{t+1} = u_{t} + \\alpha_{u}(\\delta_{t}-\\phi(s_{t}, a_{t})^{\\top} u_{t})\\phi(s_{t}, a_{t}) $\n    - stochastic actor-critic과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $입니다.\n        - m은 action dimensions, n은 number of policy parameters\n    - Natural policy gradient를 이용해 deterministic policies를 찾을 수 있습니다.\n        - $ M(\\theta)^{-1}\\nabla_{\\theta}J(\\mu_{\\theta}) $은 any metric $ M(\\theta) $에 대한 performance objective (식(14))의 steepest ascent direction 입니다. (Toussaint, 2012)\n        - Natural gradient는 Fisher information metric $ M_{\\pi}(\\theta) $에 대한 steepest ascent direction 입니다.\n            -  $ M_{\\pi}(\\theta) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}] $\n            - Fisher information metric은 policy reparameterization에 대해 불변입니다. (Bagnell, 2003)\n        - deterministic policies에 대한 metric으로 $ M_{\\mu}(\\theta) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}] $을 사용합니다.\n        \t- 이는 variance가 0인 policy에 대한 Fisher information metric으로 볼 수 있습니다.\n        \t- $ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\theta}(a\\vert s)}$에서 policy variance가 0이면, 특정 s에 대한 $ \\pi_{\\theta}(a|s)$만 1이 되고, 나머지는 0입니다.\n        - deterministic policy gradient theorem과 compatible function approximation을 결합하면 $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] $이 됩니다.\n            - $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n            - $ \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)} \\approx \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $\n        - 그렇기에 steepest ascent direction은 $ M_{\\mu}(\\theta)^{-1}\\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = w $이 됩니다.\n            - $ E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}]^{-1}E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] = w $\n        - 이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ에서 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $ 식을 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}w_{t} $로 바꿔주기만 하면 됩니다.\n\n<br><br>\n\n# 5. Experiments\n\n<br>\n## 5.1. Continuous Bandit\n- Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행합니다.\n    - Action dimension이 커질수록 성능 차이가 심합니다.\n    - 빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있습니다.\n    - <img src=\"https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1\">\n\n<br>\n## 5.2. Continuous Reinforcement Learning\n- COPDAC-Q, SAC, off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행합니다.\n    - COPDAC-Q의 성능이 약간 더 좋습니다.\n    - COPDAC-Q의 학습이 더 빨리 이뤄집니다.\n    - <img src=\"https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1\">\n\n<br>\n## 5.3. Octopus Arm\n- 목표: 6 segments octopus arm (20 action dimensions & 50 state dimensions)을 control하여 target을 맞추는 것입니다.\n    - COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤입니다.\n    - <img src=\"https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1\" width=600px>\n    - 기존 기법들은 action spaces 혹은 action과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않습니다.\n        - 기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 보여줬으면 하지만 실험을 하지 않았습니다.\n    - 8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보입니다.\n- [참고] Octopus Arm 이란?\n    - [OctopusArm Youtube Link](https://www.youtube.com/watch?v=AxeeHif0euY)\n    - <img src=\"https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1\">\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [Sutton PG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/)\n\n<br>\n\n# 다음으로\n\n## [DDPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/)","source":"_posts/2_dpg.md","raw":"---\ntitle: Deterministic Policy Gradient Algorithms\ndate: 2018-06-27 17:21:48\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 김동민, 공민서, 장수영, 차금강\nsubtitle: 피지여행 2번째 논문\n---\n\n<center> <img src=\"https://www.dropbox.com/s/etwa3hn5c0pegdv/Screen%20Shot%202018-07-18%20at%2012.51.35%20AM.png?dl=1\" width=\"800\"> </center>\n\n논문 저자 : David Silver, Guy Lever, Nicloas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller\n논문 링크 : [main text](http://proceedings.mlr.press/v32/silver14.pdf), [supplementary material](http://proceedings.mlr.press/v32/silver14-supp.pdf)\nProceeding : International Conference on Machine Learning (ICML) 2014\n정리 : 김동민, 공민서, 장수영, 차금강\n\n---\n\n# 1. 들어가며...\n\n- Deterministic Policy Gradient (DPG) Theorem을 제안합니다.\n    - 중요한 점은 DPG는 Expected gradient of the action-value function의 형태라는 것입니다.\n- Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient (SPG)와 동일해집니다. (Theorem 2)\n    - Theorem 2로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG에 적용할 수 있게 됩니다.\n        - 예. Sutton PG, natural gradients, actor-critic, episodic/batch methods\n- 적절한 exploration 을 위해 model-free, off-policy actor-critic algorithm 을 제안합니다\n    - action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility condition을 제공합니다. (Theorem 3)\n- DPG 는 SPG 보다 성능이 좋습니다.\n    - 특히 high dimensional action spaces를 가지는 tasks에서의 성능 향상이 큽니다.\n        - SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는 state spaces에 대해서만 평균을 취합니다.\n        - 결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됩니다.\n        - 무한정 학습을 시키면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정합니다.\n    - 기존 기법들에 비해 computation 양이 많지 않습니다.\n        - Computation 은 action dimensionality 와 policy parameters 수에 비례합니다.\n\n<br><br>\n\n# 2. Background\n\n<br>\n## 2.1 Performance objective function\n\n$$\n\\begin{align}\nJ(\\pi_{\\theta}) &= \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\pi_{\\theta}(s,a)r(s,a)da ds = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[r(s,a)]\n\\end{align}\n$$\n\n<br>\n## 2.2 SPG Theorem\n- State distribution $ \\rho^{\\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없습니다.\n$$\\begin{eqnarray}\\nabla_{\\theta}J(\\pi_{\\theta}) &=& \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\\\ &=& E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]\n\\end{eqnarray}$$\n\n<br>\n## 2.3 Stochastic Actor-Critic Algorithms\n- Actor와 Critic이 번갈아가면서 동작하며 stochastic policy를 최적화하는 기법입니다.\n- Actor: $ Q^{\\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $를 이용해 stochastic policy gradient를 ascent하는 방향으로 policy parameter $ \\theta $를 업데이트함으로써 stochastic policy를 발전시킵니다.\n    - $ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $\n- Critic: SARSA나 Q-learning같은 Temporal-difference (TD) learning을 이용해 action-value function의 parameter, $ w $를 업데이트함으로써 $ Q^w(s,a) $가 $ Q^{\\pi}(s,a) $과 유사해지도록 합니다.\n- 실제 값인 $ Q^{\\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $를 사용하게 되면, 일반적으로 bias가 발생합니다. 하지만, compatible condition에 부합하는 $ Q^w(s,a) $를 사용하게 되면, bias가 발생하지 않습니다.\n\n<br>\n## 2.4 Off-policy Actor-Critic\n- Distinct behavior policy $ \\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic\n- Performance objective function\n    - $\\begin{eqnarray}\n        J_{\\beta}(\\pi_{\\theta}) \n        &=& \\int_{S}\\rho^{\\beta}(s)V^{\\pi}(s)ds \\nonumber \\\\\\\\\n        &=& \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads\n        \\end{eqnarray} $\n- off-policy policy gradient\n    - $ \\begin{eqnarray}\n        \\nabla_{\\theta}J_{\\beta}(\\pi_{\\theta}) &\\approx& \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\end{eqnarray} $\n        $=E_{s \\sim \\rho^{\\beta}, a \\sim \\beta}[\\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)}\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]$\n    - off-policy policy gradient 식에서의 물결 표시는 [Degris, 2012b](https://arxiv.org/abs/1205.4839) 논문에 근거합니다.\n        - Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같습니다. (빨간색 상자에 있는 항목을 삭제함으로써 근사합니다.)\n            - <img src=\"https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1\" width=500px>\n        - [Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\\nabla_{u}𝑄^{\\pi,\\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮습니다.\n            - <img src=\"https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1\" width=500px>\n    - off-policy policy gradient 식에서 $ \\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)} $는 importance sampling ratio 입니다.\n        - off-policy actor-critic에서는 $ \\beta $에 의해 샘플링된 trajectory를 이용해서 stochastic policy $ \\pi $를 예측하는 것이기 때문에 importance sampling이 필요합니다.\n\n\n<br><br>\n\n# 3. Gradient of Deterministic Policies\n\n<br>\n## 3.1 Regularity Conditions\n- 어떠한 이론이 성립하기 위한 전제 조건\n- Regularity conditions A.1\n    - $ p(s'|s,a), \\nabla_{a}p(s'|s,a), \\mu_{\\theta}(s), \\nabla_{\\theta}\\mu_{\\theta}(s), r(s,a), \\nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s' $ and $ x $.\n- regularity conditions A.2\n    - There exists a $ b $ and $ L $ such that $ \\sup_{s}p_{1}(s) < b $, $ \\sup_{a,s,s'}p(s'|s,a) < b $, $ \\sup_{a,s}r(s,a) < b $, $ \\sup_{a,s,s'}\\|\\nabla_{a}p(s'|s,a)\\| < L $, and $ \\sup_{a,s}\\|\\nabla_{a}r(s,a)\\| < L $.\n\n<br>\n## 3.2 Deterministic Policy Gradient Theorem\n- Deterministic policy\n    - $ \\mu_{\\theta} : S \\to A $ with parameter vector $ \\theta \\in \\mathbb{R}^n $\n- Probability distribution\n    - $ p(s \\to s', t, \\mu) $\n- Discounted state distribution\n    - $ \\rho^{\\mu}(s) $\n- Performance objective\n\n$$\nJ(\\mu_{\\theta}) = E[r^{\\gamma}_{1} | \\mu] \n$$\n\n$$\n= \\int_{S}\\rho^{\\mu}(s)r(s,\\mu_{\\theta}(s))ds \n= E_{s \\sim \\rho^{\\mu}}[r(s,\\mu_{\\theta}(s))]\n$$\n\n- DPG Theorem\n    - MDP 가 A.1 만족한다면, 아래 식이 성립합니다.\n    $\\nabla_{\\theta}J(\\mu_{\\theta}) = \\int_{S}\\rho^{\\mu}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}ds \\nonumber$\n    $= E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]   \\nonumber $ \n    \n\t- DPG는 state space에 대해서만 평균을 취하면 되기에, state와 action space 모두에 대해 평균을 취해야 하는 SPG에 비해 data efficiency가 좋습니다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 됩니다.\n\n<br>    \n## 3.3 DPG 형태에 대한 informal intuition\n- Generalized policy iteration\n    - 정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration\n        - 위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴합니다.\n- 정책 평가\n    - action-value function $ Q^{\\pi}(s,a) $ or $ Q^{\\mu}(s,a) $을 estimate 하는 것 입니다.\n- 정책 발전\n    - 위 estimated action-value function에 따라 정책을 update하는 것 입니다.\n    - 주로 action-value function에 대한 greedy maximisation을 사용합니다.\n        - $ \\mu^{k+1}(s) = \\arg\\max\\limits_{a}Q^{\\mu^{k}}(s,a) $\n        - greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이로 인해 continuous action spaces에서 계산량이 급격히 늘어납니다.\n    - 그래서 policy gradient 방법이 나옵니다.\n        - policy 를 $ \\theta $에 대해서 parameterize 합니다.\n        - 매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $마다 policy parameter를 action-value function $ Q $의 $ \\theta $에 대한 gradient $ \\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s)) $ 방향으로 proportional하게 update 합니다.\n        - 하지만 각 state는 다른 방향을 제시할 수 있기에, state distribution $ \\rho^{\\mu}(s) $에 대한 기대값을 취해 policy parameter를 update 할 수도 있습니다.\n            - $ \\theta^{k+1} = \\theta^{k} + \\alpha E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s))] $\n        - 이는 chain-rule에 따라 아래와 같이 분리될 수 있습니다.\n            - $ \\theta^{k+1} = \\theta^{k} + \\alpha E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu^{k}}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $ (7)\n            - chain rule: $ \\frac{\\partial Q}{\\partial \\theta} = \\frac{\\partial a}{\\partial \\theta} \\frac{\\partial Q}{\\partial a} $\n        - 하지만 state distribution $ \\rho^{\\mu} $은 정책에 dependent 합니다.\n            - 정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state가 변하기 때문에 state distribution이 변하게 됩니다.\n        - 그렇기에 정책 update 시 state distribution에 대한 gradient를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있습니다.\n        - deterministic policy gradient theorem은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update해도 performance objective의 gradient를 정확하게 따름을 의미합니다.\n\n<br>\n## 3.4 DPG는 SPG의 limiting case\n- stochastic policy parameterization\n    - $ \\pi_{\\mu_{\\theta},\\sigma} $ by a deterministic policy $ \\mu_{\\theta} : S \\to A $ and a variance parameter $ \\sigma $\n    - $ \\sigma = 0 $ 이면, $ \\pi_{\\mu_{\\theta},\\sigma} \\equiv \\mu_{\\theta} $\n- Theorem 2. Policy의 variance가 0에 수렴하면, 즉, $ \\sigma \\to 0 $, stochastic policy gradient와 deterministic policy gradient는 동일해집니다.\n    - 조건: stochastic policy $ \\pi_{\\mu_{\\theta},\\sigma} = \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $\n        - $ \\sigma $는 variance입니다.\n        - $ \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $는 condition B.1을 만족합니다.\n        - MDP는 conditions A.1과 A.2를 만족합니다.\n    - 결과:\n        - $ \\lim\\limits_{\\sigma\\downarrow0}\\nabla_{\\theta}J(\\pi_{\\mu_{\\theta},\\sigma}) = \\nabla_{\\theta}J(\\mu_{\\theta})  $\n            - 좌변은 standard stochastic gradient이며, 우변은 deterministic gradient입니다.\n    - 의미:\n        - deterministic policy gradient는 stochastic policy gradient의 특수 case 입니다.\n        - 기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있습니다.\n            - 기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)\n\n<br><br>\n\n# 4. Deterministic Actor-Critic Algorithms\n1. SARSA critic를 이용한 on-policy actor-critic\n    - 단점\n        - deterministic policy에 의해 행동하면 exploration이 잘 되지 않기에, sub-optimal에 빠지기 쉽습니다.\n    - 목적\n        - 교훈/정보제공\n        - 환경에서 충분한 noise를 제공하여 exploration을 시킬 수 있다면, deterministic policy를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있습니다.\n            - 예. 바람이 agent의 행동에 영향(noise)을 줌\n    - Remind: 살사(SARSA) update rule\n        - $ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $\n    - Algorithm\n        - Critic은 MSE를 $ \\bf minimize $하는 방향, 즉, action-value function을 stochastic gradient $ \\bf descent $ 방법으로 update합니다.\n            - $ MSE = [Q^{\\mu}(s,a) - Q^{w}(s,a)]^2 $\n                - critic은 실제 $ Q^{\\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $로 대체하여 action-value function을 estimate하며, 이 둘 간 mean square error를 최소화하는 것이 목표입니다.\n            - $ \\nabla_{w}MSE \\approx -2 * [r + \\gamma Q^{w}(s',a') - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $\n                - $ \\nabla_{w}MSE = -2 * [Q^{\\mu}(s,a) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $\n                - $ Q^{\\mu}(s,a) $ 를 $ r + \\gamma Q^{w}(s',a') $로 대체\n                    - $ Q^{\\mu}(s,a) = r + \\gamma Q^{\\mu}(s',a') $\n            - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $\n                - $w_{t+1} = w_{t} - \\alpha_{w}\\nabla_{w}MSE  \\nonumber$\n                $ \\approx w_{t} - \\alpha_{w} * (-2 * [r + \\gamma Q^{w}(s',a') - Q^{w}(s,a)] \\nabla_{w}Q^{w}(s,a)$\n                - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $\n        - Actor는 식(9)에 따라 보상이 $ \\bf maximize $되는 방향, 즉, deterministic policy를 stochastic gradient $ \\bf ascent $ 방법으로 update합니다.\n            - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $\n2. Q-learning 을 이용한 off-policy actor-critic\n    - stochastic behavior policy $ \\beta(a|s) $에 의해 생성된 trajectories로부터 deterministic target policy $ \\mu_{\\theta}(s) $를 학습하는 off-policy actor-critic입니다\n    - performance objective\n        - $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)V^{\\mu}(s)ds \\nonumber \\\\\\\\$\n          $= \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds \\nonumber \\\\\\\\$\n          $= E_{s \\sim \\rho^{\\beta}}[Q^{\\mu}(s,\\mu_{\\theta}(s))]$\n    - off-policy deterministic policy gradient\n        - $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n            - 논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됩니다.\n            - $ \\begin{eqnarray}\n                \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) &\\approx& \\int_{S}\\rho^{\\beta}(s)\\nabla_{\\theta}\\mu_{\\theta}(a|s)Q^{\\mu}(s,a)ds \\nonumber \\\\\n                &=& E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]\n                \\end{eqnarray} $\n            - 근거: Action이 deterministic하기에 stochastic 경우와는 다르게 performance objective에서 action에 대해 평균을 구할 필요가 없습니다. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b]에서 처럼 곱의 미분을 통해 생기는 action-value function에 대한 gradient term을 생략할 필요가 사라집니다.\n    - Remind: 큐러닝(Q-learning) update rule\n        - $ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $\n    - algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)\n        - 살사를 이용한 on-policy deterministic actor-critic과 아래 부분을 제외하고는 같습니다.\n            - target policy는 $ \\beta(a|s) $에 의해 생성된 trajectories를 통해 학습합니다.\n            - 업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $이 아니라 정책으로부터 나온 행동 값 $ \\mu_{\\theta}(s_{t+1}) $을 사용합니다.\n                - $ \\mu_{\\theta}(s_{t+1}) $ 은 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $\n    - Stochastic off-policy actor-critic은 대개 actor와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient에선 importance sampling이 필요없습니다.\n        - Actor 는 deterministic 이기에 sampling 자체가 필요없습니다.\n            - Stochastic policy인 경우, Actor에서 importance sampling이 필요한 이유는 상태 $ s $에서의 가치 함수 값 $ V^{\\pi}(s) $을 estimate하기 위해 $ \\pi $가 아니라 $ \\beta $에 따라 sampling을 한 후, 평균을 내기 때문입니다.\n            - 하지만 deterministic policy인 경우, 상태 $ s $에서의 가치 함수 값 $ V^{\\pi}(s) = Q^{\\pi}(s,\\mu_{\\theta}) $ 즉, action이 상태 s에 대해 deterministic이기에 sampling을 통해 estimate할 필요가 없고, 따라서 importance sampling도 필요없어집니다.\n            - stochastic vs. deterministic performance objective\n                - stochastic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads $\n                - deterministic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds $\n        - Critic이 사용하는 Q-learning은 importance sampling이 필요없는 off policy 알고리즘입니다.\n            - Q-learning도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있습니다.\n3. compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic\n    - 위 살사/Q-learning 기반 on/off-policy는 아래와 같은 문제가 존재합니다.\n        - function approximator에 의한 bias\n            - 일반적으로 $ Q^{\\mu}(s,a) $ 를 $ Q^{w}(s,a) $로 대체하여 deterministic policy gradient를 구하면, 그 gradient는 ascent하는 방향이 아닐 수도 있습니다.\n        - off-policy learning에 의한 instabilities\n    - 그래서 stochastic처럼 $ \\nabla_{a}Q^{\\mu}(s,a) $를 $ \\nabla_{a}Q^{w}(s,a) $로 대체해도 deterministic policy gradient에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $를 찾아야 합니다.\n    - Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $는 deterministic policy $ \\mu_{\\theta}(s) $와 compatible 합니다. 즉, $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n        - $ \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $입니다.\n        - $ w $는 $ MSE(\\theta, w) = E[\\epsilon(s;\\theta,w)^{\\top}\\epsilon(s;\\theta,w)] $를 최소화합니다.\n            - $ \\epsilon(s;\\theta,w) = \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} - \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}  $\n    - Theorem 3은 on-policy 뿐만 아니라 off-policy에도 적용 가능합니다.\n    - $ Q^{w}(s,a) = (a-\\mu_{\\theta}(s))^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top} w + V^{v}(s) $\n        - 어떠한 deterministic policy에 대해서도 위 형태와 같은 compatible function approximator가 존재합니다.\n        - 앞의 term은 advantage를, 뒤의 term은 value로 볼 수 있습니다.\n    - $ Q^{w}(s,a) = \\phi(s,a)^{\\top} w + v^{\\top}\\phi(s) $\n        - 정의 : $ \\phi(s,a) \\overset{\\underset{\\mathrm{def}}{}}{=} \\nabla_{\\theta}\\mu_{\\theta}(s)(a-\\mu_{\\theta}(s)) $\n        - 일례 : $ V^{v}(s) = v^{\\top}\\phi(s) $\n        - Theorem 3 만족 여부\n            - 첫 번째 조건 만족합니다.\n            - 두 번째 조건은 대강 만족합니다.\n                - $ \\nabla_{a}Q^{\\mu}(s,a) $에 대한 unbiased sample을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $를 학습합니다.\n                - 이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \\approx Q^{\\mu}(s,a) $인 reasonable solution을 찾을 수 있기에 대강 $ \\nabla_{a}Q^{w}(s,a) \\approx \\nabla_{a}Q^{\\mu}(s,a) $이 될 것입니다.\n        - action-value function에 대한 linear function approximator는 큰 값을 가지는 actions에 대해선 diverge할 수 있어 global하게 action-values 예측하기에는 좋지 않지만, local critic에 사용할 때는 매우 유용합니다.\n            - 즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\\mu_{\\theta}(s)+\\delta) = \\delta^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $로, diverge하지 않고, 값을 얻을 수 있습니다.\n    - COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)\n        - Critic: 실제 action-value function에 대한 linear function approximator인 $ Q^{w}(s,a) = \\phi(s,a)^{\\top}w $를 estimate합니다.\n            - $ \\phi(s,a) = a^{\\top}\\nabla_{\\theta}\\mu_{\\theta} $\n            - Behavior policy $ \\beta(a|s) $로부터 얻은 samples를 이용하여 Q-learning이나 gradient Q-learning과 같은 off-policy algorithm으로 학습 가능합니다.\n        - Actor: estimated action-value function에 대한 gradient를 $ \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $로 치환 후, 정책을 업데이트합니다.\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $\n    - off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있습니다.\n        - $ \\mu_{\\theta}(s_{t+1}) $이 diverge할 수도 있기 때문으로 판단됩니다.\n        - 그렇기에 simple Q-learning 대신 다른 기법이 필요합니다.\n    - 그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm을 제안합니다.\n        - gradient temporal-difference learning에 기반한 기법들은 true gradient descent algorithm이며, converge가 보장됩니다. (Sutton, 2009)\n            - 기본 아이디어는 stochastic gradient descent로 mean-squared projected Bellman error (MSPBE)를 최소화하는 것입니다.\n            - critic이 actor보다 빠른 time-scale로 update되도록 step size들을 잘 조절하면, critic은 MSPBE를 최소화하는 parameters로 converge하게 됩니다.\n            - critic에 gradient temporal-difference learning의 일종인 gradient Q-learning을 사용한 논문입니다. (Maei, 2010)\n    - COPDAC-GQ algorithm\n        - $ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $\n        - $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $\n        - $ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) - \\alpha_{w}\\gamma\\phi(s_{t+1}, \\mu_{\\theta}(s_{t+1}))(\\phi(s_{t},a_{t})^{\\top} u_{t}) $\n        - $ v_{t+1} = v_{t} + \\alpha_{v}\\delta_{t}\\phi(s_{t}) - \\alpha_{v}\\gamma\\phi(s_{t+1})(\\phi(s_{t},a_{t})^{\\top} u_{t}) $\n        - $ u_{t+1} = u_{t} + \\alpha_{u}(\\delta_{t}-\\phi(s_{t}, a_{t})^{\\top} u_{t})\\phi(s_{t}, a_{t}) $\n    - stochastic actor-critic과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $입니다.\n        - m은 action dimensions, n은 number of policy parameters\n    - Natural policy gradient를 이용해 deterministic policies를 찾을 수 있습니다.\n        - $ M(\\theta)^{-1}\\nabla_{\\theta}J(\\mu_{\\theta}) $은 any metric $ M(\\theta) $에 대한 performance objective (식(14))의 steepest ascent direction 입니다. (Toussaint, 2012)\n        - Natural gradient는 Fisher information metric $ M_{\\pi}(\\theta) $에 대한 steepest ascent direction 입니다.\n            -  $ M_{\\pi}(\\theta) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}] $\n            - Fisher information metric은 policy reparameterization에 대해 불변입니다. (Bagnell, 2003)\n        - deterministic policies에 대한 metric으로 $ M_{\\mu}(\\theta) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}] $을 사용합니다.\n        \t- 이는 variance가 0인 policy에 대한 Fisher information metric으로 볼 수 있습니다.\n        \t- $ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\theta}(a\\vert s)}$에서 policy variance가 0이면, 특정 s에 대한 $ \\pi_{\\theta}(a|s)$만 1이 되고, 나머지는 0입니다.\n        - deterministic policy gradient theorem과 compatible function approximation을 결합하면 $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] $이 됩니다.\n            - $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $\n            - $ \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)} \\approx \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $\n        - 그렇기에 steepest ascent direction은 $ M_{\\mu}(\\theta)^{-1}\\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = w $이 됩니다.\n            - $ E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}]^{-1}E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] = w $\n        - 이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ에서 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $ 식을 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}w_{t} $로 바꿔주기만 하면 됩니다.\n\n<br><br>\n\n# 5. Experiments\n\n<br>\n## 5.1. Continuous Bandit\n- Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행합니다.\n    - Action dimension이 커질수록 성능 차이가 심합니다.\n    - 빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있습니다.\n    - <img src=\"https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1\">\n\n<br>\n## 5.2. Continuous Reinforcement Learning\n- COPDAC-Q, SAC, off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행합니다.\n    - COPDAC-Q의 성능이 약간 더 좋습니다.\n    - COPDAC-Q의 학습이 더 빨리 이뤄집니다.\n    - <img src=\"https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1\">\n\n<br>\n## 5.3. Octopus Arm\n- 목표: 6 segments octopus arm (20 action dimensions & 50 state dimensions)을 control하여 target을 맞추는 것입니다.\n    - COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤입니다.\n    - <img src=\"https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1\" width=600px>\n    - 기존 기법들은 action spaces 혹은 action과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않습니다.\n        - 기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 보여줬으면 하지만 실험을 하지 않았습니다.\n    - 8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보입니다.\n- [참고] Octopus Arm 이란?\n    - [OctopusArm Youtube Link](https://www.youtube.com/watch?v=AxeeHif0euY)\n    - <img src=\"https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1\">\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [Sutton PG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/)\n\n<br>\n\n# 다음으로\n\n## [DDPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/)","slug":"2_dpg","published":1,"updated":"2019-02-07T11:21:31.930Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjrujlu0x001g5wfen01yjb8o","content":"<center> <img src=\"https://www.dropbox.com/s/etwa3hn5c0pegdv/Screen%20Shot%202018-07-18%20at%2012.51.35%20AM.png?dl=1\" width=\"800\"> </center>\n\n<p>논문 저자 : David Silver, Guy Lever, Nicloas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller<br>논문 링크 : <a href=\"http://proceedings.mlr.press/v32/silver14.pdf\" target=\"_blank\" rel=\"noopener\">main text</a>, <a href=\"http://proceedings.mlr.press/v32/silver14-supp.pdf\" target=\"_blank\" rel=\"noopener\">supplementary material</a><br>Proceeding : International Conference on Machine Learning (ICML) 2014<br>정리 : 김동민, 공민서, 장수영, 차금강</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><ul>\n<li>Deterministic Policy Gradient (DPG) Theorem을 제안합니다.<ul>\n<li>중요한 점은 DPG는 Expected gradient of the action-value function의 형태라는 것입니다.</li>\n</ul>\n</li>\n<li>Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient (SPG)와 동일해집니다. (Theorem 2)<ul>\n<li>Theorem 2로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG에 적용할 수 있게 됩니다.<ul>\n<li>예. Sutton PG, natural gradients, actor-critic, episodic/batch methods</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>적절한 exploration 을 위해 model-free, off-policy actor-critic algorithm 을 제안합니다<ul>\n<li>action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility condition을 제공합니다. (Theorem 3)</li>\n</ul>\n</li>\n<li>DPG 는 SPG 보다 성능이 좋습니다.<ul>\n<li>특히 high dimensional action spaces를 가지는 tasks에서의 성능 향상이 큽니다.<ul>\n<li>SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는 state spaces에 대해서만 평균을 취합니다.</li>\n<li>결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됩니다.</li>\n<li>무한정 학습을 시키면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정합니다.</li>\n</ul>\n</li>\n<li>기존 기법들에 비해 computation 양이 많지 않습니다.<ul>\n<li>Computation 은 action dimensionality 와 policy parameters 수에 비례합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"2-Background\"><a href=\"#2-Background\" class=\"headerlink\" title=\"2. Background\"></a>2. Background</h1><p><br></p>\n<h2 id=\"2-1-Performance-objective-function\"><a href=\"#2-1-Performance-objective-function\" class=\"headerlink\" title=\"2.1 Performance objective function\"></a>2.1 Performance objective function</h2><p>$$<br>\\begin{align}<br>J(\\pi_{\\theta}) &amp;= \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\pi_{\\theta}(s,a)r(s,a)da ds = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[r(s,a)]<br>\\end{align}<br>$$</p>\n<p><br></p>\n<h2 id=\"2-2-SPG-Theorem\"><a href=\"#2-2-SPG-Theorem\" class=\"headerlink\" title=\"2.2 SPG Theorem\"></a>2.2 SPG Theorem</h2><ul>\n<li>State distribution $ \\rho^{\\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없습니다.<br>$$\\begin{eqnarray}\\nabla_{\\theta}J(\\pi_{\\theta}) &amp;=&amp; \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\ &amp;=&amp; E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]<br>\\end{eqnarray}$$</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-3-Stochastic-Actor-Critic-Algorithms\"><a href=\"#2-3-Stochastic-Actor-Critic-Algorithms\" class=\"headerlink\" title=\"2.3 Stochastic Actor-Critic Algorithms\"></a>2.3 Stochastic Actor-Critic Algorithms</h2><ul>\n<li>Actor와 Critic이 번갈아가면서 동작하며 stochastic policy를 최적화하는 기법입니다.</li>\n<li>Actor: $ Q^{\\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $를 이용해 stochastic policy gradient를 ascent하는 방향으로 policy parameter $ \\theta $를 업데이트함으로써 stochastic policy를 발전시킵니다.<ul>\n<li>$ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $</li>\n</ul>\n</li>\n<li>Critic: SARSA나 Q-learning같은 Temporal-difference (TD) learning을 이용해 action-value function의 parameter, $ w $를 업데이트함으로써 $ Q^w(s,a) $가 $ Q^{\\pi}(s,a) $과 유사해지도록 합니다.</li>\n<li>실제 값인 $ Q^{\\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $를 사용하게 되면, 일반적으로 bias가 발생합니다. 하지만, compatible condition에 부합하는 $ Q^w(s,a) $를 사용하게 되면, bias가 발생하지 않습니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-4-Off-policy-Actor-Critic\"><a href=\"#2-4-Off-policy-Actor-Critic\" class=\"headerlink\" title=\"2.4 Off-policy Actor-Critic\"></a>2.4 Off-policy Actor-Critic</h2><ul>\n<li>Distinct behavior policy $ \\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic</li>\n<li>Performance objective function<ul>\n<li>$\\begin{eqnarray}<br>  J_{\\beta}(\\pi_{\\theta})<br>  &amp;=&amp; \\int_{S}\\rho^{\\beta}(s)V^{\\pi}(s)ds \\nonumber \\\\<br>  &amp;=&amp; \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads<br>  \\end{eqnarray} $</li>\n</ul>\n</li>\n<li>off-policy policy gradient<ul>\n<li>$ \\begin{eqnarray}<br>  \\nabla_{\\theta}J_{\\beta}(\\pi_{\\theta}) &amp;\\approx&amp; \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\end{eqnarray} $<br>  $=E_{s \\sim \\rho^{\\beta}, a \\sim \\beta}[\\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)}\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]$</li>\n<li>off-policy policy gradient 식에서의 물결 표시는 <a href=\"https://arxiv.org/abs/1205.4839\" target=\"_blank\" rel=\"noopener\">Degris, 2012b</a> 논문에 근거합니다.<ul>\n<li>Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같습니다. (빨간색 상자에 있는 항목을 삭제함으로써 근사합니다.)<ul>\n<li><img src=\"https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1\" width=\"500px\"></li>\n</ul>\n</li>\n<li>[Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\\nabla_{u}𝑄^{\\pi,\\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮습니다.<ul>\n<li><img src=\"https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1\" width=\"500px\"></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>off-policy policy gradient 식에서 $ \\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)} $는 importance sampling ratio 입니다.<ul>\n<li>off-policy actor-critic에서는 $ \\beta $에 의해 샘플링된 trajectory를 이용해서 stochastic policy $ \\pi $를 예측하는 것이기 때문에 importance sampling이 필요합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"3-Gradient-of-Deterministic-Policies\"><a href=\"#3-Gradient-of-Deterministic-Policies\" class=\"headerlink\" title=\"3. Gradient of Deterministic Policies\"></a>3. Gradient of Deterministic Policies</h1><p><br></p>\n<h2 id=\"3-1-Regularity-Conditions\"><a href=\"#3-1-Regularity-Conditions\" class=\"headerlink\" title=\"3.1 Regularity Conditions\"></a>3.1 Regularity Conditions</h2><ul>\n<li>어떠한 이론이 성립하기 위한 전제 조건</li>\n<li>Regularity conditions A.1<ul>\n<li>$ p(s’|s,a), \\nabla_{a}p(s’|s,a), \\mu_{\\theta}(s), \\nabla_{\\theta}\\mu_{\\theta}(s), r(s,a), \\nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s’ $ and $ x $.</li>\n</ul>\n</li>\n<li>regularity conditions A.2<ul>\n<li>There exists a $ b $ and $ L $ such that $ \\sup_{s}p_{1}(s) &lt; b $, $ \\sup_{a,s,s’}p(s’|s,a) &lt; b $, $ \\sup_{a,s}r(s,a) &lt; b $, $ \\sup_{a,s,s’}|\\nabla_{a}p(s’|s,a)| &lt; L $, and $ \\sup_{a,s}|\\nabla_{a}r(s,a)| &lt; L $.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-2-Deterministic-Policy-Gradient-Theorem\"><a href=\"#3-2-Deterministic-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"3.2 Deterministic Policy Gradient Theorem\"></a>3.2 Deterministic Policy Gradient Theorem</h2><ul>\n<li>Deterministic policy<ul>\n<li>$ \\mu_{\\theta} : S \\to A $ with parameter vector $ \\theta \\in \\mathbb{R}^n $</li>\n</ul>\n</li>\n<li>Probability distribution<ul>\n<li>$ p(s \\to s’, t, \\mu) $</li>\n</ul>\n</li>\n<li>Discounted state distribution<ul>\n<li>$ \\rho^{\\mu}(s) $</li>\n</ul>\n</li>\n<li>Performance objective</li>\n</ul>\n<p>$$<br>J(\\mu_{\\theta}) = E[r^{\\gamma}_{1} | \\mu]<br>$$</p>\n<p>$$<br>= \\int_{S}\\rho^{\\mu}(s)r(s,\\mu_{\\theta}(s))ds<br>= E_{s \\sim \\rho^{\\mu}}[r(s,\\mu_{\\theta}(s))]<br>$$</p>\n<ul>\n<li><p>DPG Theorem</p>\n<ul>\n<li><p>MDP 가 A.1 만족한다면, 아래 식이 성립합니다.<br>$\\nabla_{\\theta}J(\\mu_{\\theta}) = \\int_{S}\\rho^{\\mu}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}ds \\nonumber$<br>$= E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]   \\nonumber $ </p>\n</li>\n<li><p>DPG는 state space에 대해서만 평균을 취하면 되기에, state와 action space 모두에 대해 평균을 취해야 하는 SPG에 비해 data efficiency가 좋습니다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 됩니다.</p>\n</li>\n</ul>\n</li>\n</ul>\n<p><br>    </p>\n<h2 id=\"3-3-DPG-형태에-대한-informal-intuition\"><a href=\"#3-3-DPG-형태에-대한-informal-intuition\" class=\"headerlink\" title=\"3.3 DPG 형태에 대한 informal intuition\"></a>3.3 DPG 형태에 대한 informal intuition</h2><ul>\n<li>Generalized policy iteration<ul>\n<li>정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration<ul>\n<li>위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>정책 평가<ul>\n<li>action-value function $ Q^{\\pi}(s,a) $ or $ Q^{\\mu}(s,a) $을 estimate 하는 것 입니다.</li>\n</ul>\n</li>\n<li>정책 발전<ul>\n<li>위 estimated action-value function에 따라 정책을 update하는 것 입니다.</li>\n<li>주로 action-value function에 대한 greedy maximisation을 사용합니다.<ul>\n<li>$ \\mu^{k+1}(s) = \\arg\\max\\limits_{a}Q^{\\mu^{k}}(s,a) $</li>\n<li>greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이로 인해 continuous action spaces에서 계산량이 급격히 늘어납니다.</li>\n</ul>\n</li>\n<li>그래서 policy gradient 방법이 나옵니다.<ul>\n<li>policy 를 $ \\theta $에 대해서 parameterize 합니다.</li>\n<li>매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $마다 policy parameter를 action-value function $ Q $의 $ \\theta $에 대한 gradient $ \\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s)) $ 방향으로 proportional하게 update 합니다.</li>\n<li>하지만 각 state는 다른 방향을 제시할 수 있기에, state distribution $ \\rho^{\\mu}(s) $에 대한 기대값을 취해 policy parameter를 update 할 수도 있습니다.<ul>\n<li>$ \\theta^{k+1} = \\theta^{k} + \\alpha E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s))] $</li>\n</ul>\n</li>\n<li>이는 chain-rule에 따라 아래와 같이 분리될 수 있습니다.<ul>\n<li>$ \\theta^{k+1} = \\theta^{k} + \\alpha E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu^{k}}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $ (7)</li>\n<li>chain rule: $ \\frac{\\partial Q}{\\partial \\theta} = \\frac{\\partial a}{\\partial \\theta} \\frac{\\partial Q}{\\partial a} $</li>\n</ul>\n</li>\n<li>하지만 state distribution $ \\rho^{\\mu} $은 정책에 dependent 합니다.<ul>\n<li>정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state가 변하기 때문에 state distribution이 변하게 됩니다.</li>\n</ul>\n</li>\n<li>그렇기에 정책 update 시 state distribution에 대한 gradient를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있습니다.</li>\n<li>deterministic policy gradient theorem은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update해도 performance objective의 gradient를 정확하게 따름을 의미합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-4-DPG는-SPG의-limiting-case\"><a href=\"#3-4-DPG는-SPG의-limiting-case\" class=\"headerlink\" title=\"3.4 DPG는 SPG의 limiting case\"></a>3.4 DPG는 SPG의 limiting case</h2><ul>\n<li>stochastic policy parameterization<ul>\n<li>$ \\pi_{\\mu_{\\theta},\\sigma} $ by a deterministic policy $ \\mu_{\\theta} : S \\to A $ and a variance parameter $ \\sigma $</li>\n<li>$ \\sigma = 0 $ 이면, $ \\pi_{\\mu_{\\theta},\\sigma} \\equiv \\mu_{\\theta} $</li>\n</ul>\n</li>\n<li>Theorem 2. Policy의 variance가 0에 수렴하면, 즉, $ \\sigma \\to 0 $, stochastic policy gradient와 deterministic policy gradient는 동일해집니다.<ul>\n<li>조건: stochastic policy $ \\pi_{\\mu_{\\theta},\\sigma} = \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $<ul>\n<li>$ \\sigma $는 variance입니다.</li>\n<li>$ \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $는 condition B.1을 만족합니다.</li>\n<li>MDP는 conditions A.1과 A.2를 만족합니다.</li>\n</ul>\n</li>\n<li>결과:<ul>\n<li>$ \\lim\\limits_{\\sigma\\downarrow0}\\nabla_{\\theta}J(\\pi_{\\mu_{\\theta},\\sigma}) = \\nabla_{\\theta}J(\\mu_{\\theta})  $<ul>\n<li>좌변은 standard stochastic gradient이며, 우변은 deterministic gradient입니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>의미:<ul>\n<li>deterministic policy gradient는 stochastic policy gradient의 특수 case 입니다.</li>\n<li>기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있습니다.<ul>\n<li>기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"4-Deterministic-Actor-Critic-Algorithms\"><a href=\"#4-Deterministic-Actor-Critic-Algorithms\" class=\"headerlink\" title=\"4. Deterministic Actor-Critic Algorithms\"></a>4. Deterministic Actor-Critic Algorithms</h1><ol>\n<li>SARSA critic를 이용한 on-policy actor-critic<ul>\n<li>단점<ul>\n<li>deterministic policy에 의해 행동하면 exploration이 잘 되지 않기에, sub-optimal에 빠지기 쉽습니다.</li>\n</ul>\n</li>\n<li>목적<ul>\n<li>교훈/정보제공</li>\n<li>환경에서 충분한 noise를 제공하여 exploration을 시킬 수 있다면, deterministic policy를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있습니다.<ul>\n<li>예. 바람이 agent의 행동에 영향(noise)을 줌</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Remind: 살사(SARSA) update rule<ul>\n<li>$ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $</li>\n</ul>\n</li>\n<li>Algorithm<ul>\n<li>Critic은 MSE를 $ \\bf minimize $하는 방향, 즉, action-value function을 stochastic gradient $ \\bf descent $ 방법으로 update합니다.<ul>\n<li>$ MSE = [Q^{\\mu}(s,a) - Q^{w}(s,a)]^2 $<ul>\n<li>critic은 실제 $ Q^{\\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $로 대체하여 action-value function을 estimate하며, 이 둘 간 mean square error를 최소화하는 것이 목표입니다.</li>\n</ul>\n</li>\n<li>$ \\nabla_{w}MSE \\approx -2 * [r + \\gamma Q^{w}(s’,a’) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $<ul>\n<li>$ \\nabla_{w}MSE = -2 * [Q^{\\mu}(s,a) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $</li>\n<li>$ Q^{\\mu}(s,a) $ 를 $ r + \\gamma Q^{w}(s’,a’) $로 대체<ul>\n<li>$ Q^{\\mu}(s,a) = r + \\gamma Q^{\\mu}(s’,a’) $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $<ul>\n<li>$w_{t+1} = w_{t} - \\alpha_{w}\\nabla_{w}MSE  \\nonumber$<br>$ \\approx w_{t} - \\alpha_{w} <em> (-2 </em> [r + \\gamma Q^{w}(s’,a’) - Q^{w}(s,a)] \\nabla_{w}Q^{w}(s,a)$</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Actor는 식(9)에 따라 보상이 $ \\bf maximize $되는 방향, 즉, deterministic policy를 stochastic gradient $ \\bf ascent $ 방법으로 update합니다.<ul>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Q-learning 을 이용한 off-policy actor-critic<ul>\n<li>stochastic behavior policy $ \\beta(a|s) $에 의해 생성된 trajectories로부터 deterministic target policy $ \\mu_{\\theta}(s) $를 학습하는 off-policy actor-critic입니다</li>\n<li>performance objective<ul>\n<li>$ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)V^{\\mu}(s)ds \\nonumber \\\\$<br>$= \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds \\nonumber \\\\$<br>$= E_{s \\sim \\rho^{\\beta}}[Q^{\\mu}(s,\\mu_{\\theta}(s))]$</li>\n</ul>\n</li>\n<li>off-policy deterministic policy gradient<ul>\n<li>$ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $<ul>\n<li>논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됩니다.</li>\n<li>$ \\begin{eqnarray}<br>  \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) &amp;\\approx&amp; \\int_{S}\\rho^{\\beta}(s)\\nabla_{\\theta}\\mu_{\\theta}(a|s)Q^{\\mu}(s,a)ds \\nonumber \\<br>  &amp;=&amp; E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]<br>  \\end{eqnarray} $</li>\n<li>근거: Action이 deterministic하기에 stochastic 경우와는 다르게 performance objective에서 action에 대해 평균을 구할 필요가 없습니다. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b]에서 처럼 곱의 미분을 통해 생기는 action-value function에 대한 gradient term을 생략할 필요가 사라집니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Remind: 큐러닝(Q-learning) update rule<ul>\n<li>$ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $</li>\n</ul>\n</li>\n<li>algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)<ul>\n<li>살사를 이용한 on-policy deterministic actor-critic과 아래 부분을 제외하고는 같습니다.<ul>\n<li>target policy는 $ \\beta(a|s) $에 의해 생성된 trajectories를 통해 학습합니다.</li>\n<li>업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $이 아니라 정책으로부터 나온 행동 값 $ \\mu_{\\theta}(s_{t+1}) $을 사용합니다.<ul>\n<li>$ \\mu_{\\theta}(s_{t+1}) $ 은 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $</li>\n</ul>\n</li>\n<li>Stochastic off-policy actor-critic은 대개 actor와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient에선 importance sampling이 필요없습니다.<ul>\n<li>Actor 는 deterministic 이기에 sampling 자체가 필요없습니다.<ul>\n<li>Stochastic policy인 경우, Actor에서 importance sampling이 필요한 이유는 상태 $ s $에서의 가치 함수 값 $ V^{\\pi}(s) $을 estimate하기 위해 $ \\pi $가 아니라 $ \\beta $에 따라 sampling을 한 후, 평균을 내기 때문입니다.</li>\n<li>하지만 deterministic policy인 경우, 상태 $ s $에서의 가치 함수 값 $ V^{\\pi}(s) = Q^{\\pi}(s,\\mu_{\\theta}) $ 즉, action이 상태 s에 대해 deterministic이기에 sampling을 통해 estimate할 필요가 없고, 따라서 importance sampling도 필요없어집니다.</li>\n<li>stochastic vs. deterministic performance objective<ul>\n<li>stochastic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads $</li>\n<li>deterministic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Critic이 사용하는 Q-learning은 importance sampling이 필요없는 off policy 알고리즘입니다.<ul>\n<li>Q-learning도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic<ul>\n<li>위 살사/Q-learning 기반 on/off-policy는 아래와 같은 문제가 존재합니다.<ul>\n<li>function approximator에 의한 bias<ul>\n<li>일반적으로 $ Q^{\\mu}(s,a) $ 를 $ Q^{w}(s,a) $로 대체하여 deterministic policy gradient를 구하면, 그 gradient는 ascent하는 방향이 아닐 수도 있습니다.</li>\n</ul>\n</li>\n<li>off-policy learning에 의한 instabilities</li>\n</ul>\n</li>\n<li>그래서 stochastic처럼 $ \\nabla_{a}Q^{\\mu}(s,a) $를 $ \\nabla_{a}Q^{w}(s,a) $로 대체해도 deterministic policy gradient에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $를 찾아야 합니다.</li>\n<li>Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $는 deterministic policy $ \\mu_{\\theta}(s) $와 compatible 합니다. 즉, $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $<ul>\n<li>$ \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $입니다.</li>\n<li>$ w $는 $ MSE(\\theta, w) = E[\\epsilon(s;\\theta,w)^{\\top}\\epsilon(s;\\theta,w)] $를 최소화합니다.<ul>\n<li>$ \\epsilon(s;\\theta,w) = \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} - \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}  $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Theorem 3은 on-policy 뿐만 아니라 off-policy에도 적용 가능합니다.</li>\n<li>$ Q^{w}(s,a) = (a-\\mu_{\\theta}(s))^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top} w + V^{v}(s) $<ul>\n<li>어떠한 deterministic policy에 대해서도 위 형태와 같은 compatible function approximator가 존재합니다.</li>\n<li>앞의 term은 advantage를, 뒤의 term은 value로 볼 수 있습니다.</li>\n</ul>\n</li>\n<li>$ Q^{w}(s,a) = \\phi(s,a)^{\\top} w + v^{\\top}\\phi(s) $<ul>\n<li>정의 : $ \\phi(s,a) \\overset{\\underset{\\mathrm{def}}{}}{=} \\nabla_{\\theta}\\mu_{\\theta}(s)(a-\\mu_{\\theta}(s)) $</li>\n<li>일례 : $ V^{v}(s) = v^{\\top}\\phi(s) $</li>\n<li>Theorem 3 만족 여부<ul>\n<li>첫 번째 조건 만족합니다.</li>\n<li>두 번째 조건은 대강 만족합니다.<ul>\n<li>$ \\nabla_{a}Q^{\\mu}(s,a) $에 대한 unbiased sample을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $를 학습합니다.</li>\n<li>이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \\approx Q^{\\mu}(s,a) $인 reasonable solution을 찾을 수 있기에 대강 $ \\nabla_{a}Q^{w}(s,a) \\approx \\nabla_{a}Q^{\\mu}(s,a) $이 될 것입니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>action-value function에 대한 linear function approximator는 큰 값을 가지는 actions에 대해선 diverge할 수 있어 global하게 action-values 예측하기에는 좋지 않지만, local critic에 사용할 때는 매우 유용합니다.<ul>\n<li>즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\\mu_{\\theta}(s)+\\delta) = \\delta^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $로, diverge하지 않고, 값을 얻을 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)<ul>\n<li>Critic: 실제 action-value function에 대한 linear function approximator인 $ Q^{w}(s,a) = \\phi(s,a)^{\\top}w $를 estimate합니다.<ul>\n<li>$ \\phi(s,a) = a^{\\top}\\nabla_{\\theta}\\mu_{\\theta} $</li>\n<li>Behavior policy $ \\beta(a|s) $로부터 얻은 samples를 이용하여 Q-learning이나 gradient Q-learning과 같은 off-policy algorithm으로 학습 가능합니다.</li>\n</ul>\n</li>\n<li>Actor: estimated action-value function에 대한 gradient를 $ \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $로 치환 후, 정책을 업데이트합니다.</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $</li>\n</ul>\n</li>\n<li>off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있습니다.<ul>\n<li>$ \\mu_{\\theta}(s_{t+1}) $이 diverge할 수도 있기 때문으로 판단됩니다.</li>\n<li>그렇기에 simple Q-learning 대신 다른 기법이 필요합니다.</li>\n</ul>\n</li>\n<li>그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm을 제안합니다.<ul>\n<li>gradient temporal-difference learning에 기반한 기법들은 true gradient descent algorithm이며, converge가 보장됩니다. (Sutton, 2009)<ul>\n<li>기본 아이디어는 stochastic gradient descent로 mean-squared projected Bellman error (MSPBE)를 최소화하는 것입니다.</li>\n<li>critic이 actor보다 빠른 time-scale로 update되도록 step size들을 잘 조절하면, critic은 MSPBE를 최소화하는 parameters로 converge하게 됩니다.</li>\n<li>critic에 gradient temporal-difference learning의 일종인 gradient Q-learning을 사용한 논문입니다. (Maei, 2010)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>COPDAC-GQ algorithm<ul>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) - \\alpha_{w}\\gamma\\phi(s_{t+1}, \\mu_{\\theta}(s_{t+1}))(\\phi(s_{t},a_{t})^{\\top} u_{t}) $</li>\n<li>$ v_{t+1} = v_{t} + \\alpha_{v}\\delta_{t}\\phi(s_{t}) - \\alpha_{v}\\gamma\\phi(s_{t+1})(\\phi(s_{t},a_{t})^{\\top} u_{t}) $</li>\n<li>$ u_{t+1} = u_{t} + \\alpha_{u}(\\delta_{t}-\\phi(s_{t}, a_{t})^{\\top} u_{t})\\phi(s_{t}, a_{t}) $</li>\n</ul>\n</li>\n<li>stochastic actor-critic과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $입니다.<ul>\n<li>m은 action dimensions, n은 number of policy parameters</li>\n</ul>\n</li>\n<li>Natural policy gradient를 이용해 deterministic policies를 찾을 수 있습니다.<ul>\n<li>$ M(\\theta)^{-1}\\nabla_{\\theta}J(\\mu_{\\theta}) $은 any metric $ M(\\theta) $에 대한 performance objective (식(14))의 steepest ascent direction 입니다. (Toussaint, 2012)</li>\n<li>Natural gradient는 Fisher information metric $ M_{\\pi}(\\theta) $에 대한 steepest ascent direction 입니다.<ul>\n<li>$ M_{\\pi}(\\theta) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}] $</li>\n<li>Fisher information metric은 policy reparameterization에 대해 불변입니다. (Bagnell, 2003)</li>\n</ul>\n</li>\n<li>deterministic policies에 대한 metric으로 $ M_{\\mu}(\\theta) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}] $을 사용합니다.<ul>\n<li>이는 variance가 0인 policy에 대한 Fisher information metric으로 볼 수 있습니다.</li>\n<li>$ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\theta}(a\\vert s)}$에서 policy variance가 0이면, 특정 s에 대한 $ \\pi_{\\theta}(a|s)$만 1이 되고, 나머지는 0입니다.</li>\n</ul>\n</li>\n<li>deterministic policy gradient theorem과 compatible function approximation을 결합하면 $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] $이 됩니다.<ul>\n<li>$ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $</li>\n<li>$ \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)} \\approx \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $</li>\n</ul>\n</li>\n<li>그렇기에 steepest ascent direction은 $ M_{\\mu}(\\theta)^{-1}\\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = w $이 됩니다.<ul>\n<li>$ E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}]^{-1}E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] = w $</li>\n</ul>\n</li>\n<li>이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ에서 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $ 식을 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}w_{t} $로 바꿔주기만 하면 됩니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p><br><br></p>\n<h1 id=\"5-Experiments\"><a href=\"#5-Experiments\" class=\"headerlink\" title=\"5. Experiments\"></a>5. Experiments</h1><p><br></p>\n<h2 id=\"5-1-Continuous-Bandit\"><a href=\"#5-1-Continuous-Bandit\" class=\"headerlink\" title=\"5.1. Continuous Bandit\"></a>5.1. Continuous Bandit</h2><ul>\n<li>Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행합니다.<ul>\n<li>Action dimension이 커질수록 성능 차이가 심합니다.</li>\n<li>빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있습니다.</li>\n<li><img src=\"https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"5-2-Continuous-Reinforcement-Learning\"><a href=\"#5-2-Continuous-Reinforcement-Learning\" class=\"headerlink\" title=\"5.2. Continuous Reinforcement Learning\"></a>5.2. Continuous Reinforcement Learning</h2><ul>\n<li>COPDAC-Q, SAC, off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행합니다.<ul>\n<li>COPDAC-Q의 성능이 약간 더 좋습니다.</li>\n<li>COPDAC-Q의 학습이 더 빨리 이뤄집니다.</li>\n<li><img src=\"https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"5-3-Octopus-Arm\"><a href=\"#5-3-Octopus-Arm\" class=\"headerlink\" title=\"5.3. Octopus Arm\"></a>5.3. Octopus Arm</h2><ul>\n<li>목표: 6 segments octopus arm (20 action dimensions &amp; 50 state dimensions)을 control하여 target을 맞추는 것입니다.<ul>\n<li>COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤입니다.</li>\n<li><img src=\"https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1\" width=\"600px\"></li>\n<li>기존 기법들은 action spaces 혹은 action과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않습니다.<ul>\n<li>기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 보여줬으면 하지만 실험을 하지 않았습니다.</li>\n</ul>\n</li>\n<li>8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보입니다.</li>\n</ul>\n</li>\n<li>[참고] Octopus Arm 이란?<ul>\n<li><a href=\"https://www.youtube.com/watch?v=AxeeHif0euY\" target=\"_blank\" rel=\"noopener\">OctopusArm Youtube Link</a></li>\n<li><img src=\"https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"Sutton-PG-여행하기\"><a href=\"#Sutton-PG-여행하기\" class=\"headerlink\" title=\"Sutton PG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/\">Sutton PG 여행하기</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"DDPG-여행하기\"><a href=\"#DDPG-여행하기\" class=\"headerlink\" title=\"DDPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/\">DDPG 여행하기</a></h2>","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"https://www.dropbox.com/s/etwa3hn5c0pegdv/Screen%20Shot%202018-07-18%20at%2012.51.35%20AM.png?dl=1\" width=\"800\"> </center>\n\n<p>논문 저자 : David Silver, Guy Lever, Nicloas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller<br>논문 링크 : <a href=\"http://proceedings.mlr.press/v32/silver14.pdf\" target=\"_blank\" rel=\"noopener\">main text</a>, <a href=\"http://proceedings.mlr.press/v32/silver14-supp.pdf\" target=\"_blank\" rel=\"noopener\">supplementary material</a><br>Proceeding : International Conference on Machine Learning (ICML) 2014<br>정리 : 김동민, 공민서, 장수영, 차금강</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><ul>\n<li>Deterministic Policy Gradient (DPG) Theorem을 제안합니다.<ul>\n<li>중요한 점은 DPG는 Expected gradient of the action-value function의 형태라는 것입니다.</li>\n</ul>\n</li>\n<li>Policy variance가 0에 수렴할 경우, DPG는 Stochastic Policy Gradient (SPG)와 동일해집니다. (Theorem 2)<ul>\n<li>Theorem 2로 인해 기존 Policy Gradient (PG) 와 관련된 기법들을 DPG에 적용할 수 있게 됩니다.<ul>\n<li>예. Sutton PG, natural gradients, actor-critic, episodic/batch methods</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>적절한 exploration 을 위해 model-free, off-policy actor-critic algorithm 을 제안합니다<ul>\n<li>action-value function approximator 사용으로 인해 policy gradient가 bias되는 것을 방지하기 위해 compatibility condition을 제공합니다. (Theorem 3)</li>\n</ul>\n</li>\n<li>DPG 는 SPG 보다 성능이 좋습니다.<ul>\n<li>특히 high dimensional action spaces를 가지는 tasks에서의 성능 향상이 큽니다.<ul>\n<li>SPG의 policy gradient는 state와 action spaces 모두에 대해서, DPG의 policy gradient는 state spaces에 대해서만 평균을 취합니다.</li>\n<li>결과적으로, action spaces의 dimension이 커질수록 data efficiency가 높은 DPG의 학습이 더 잘 이뤄지게 됩니다.</li>\n<li>무한정 학습을 시키면, SPG도 최적으로 수렴할 것으로 예상되기에 위 성능 비교는 일정 iteration 내로 한정합니다.</li>\n</ul>\n</li>\n<li>기존 기법들에 비해 computation 양이 많지 않습니다.<ul>\n<li>Computation 은 action dimensionality 와 policy parameters 수에 비례합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"2-Background\"><a href=\"#2-Background\" class=\"headerlink\" title=\"2. Background\"></a>2. Background</h1><p><br></p>\n<h2 id=\"2-1-Performance-objective-function\"><a href=\"#2-1-Performance-objective-function\" class=\"headerlink\" title=\"2.1 Performance objective function\"></a>2.1 Performance objective function</h2><p>$$<br>\\begin{align}<br>J(\\pi_{\\theta}) &amp;= \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\pi_{\\theta}(s,a)r(s,a)da ds = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[r(s,a)]<br>\\end{align}<br>$$</p>\n<p><br></p>\n<h2 id=\"2-2-SPG-Theorem\"><a href=\"#2-2-SPG-Theorem\" class=\"headerlink\" title=\"2.2 SPG Theorem\"></a>2.2 SPG Theorem</h2><ul>\n<li>State distribution $ \\rho^{\\pi}(s) $ 은 policy parameters에 영향을 받지만, policy gradient 를 계산할 때는 state distribution 의 gradient 를 고려할 필요가 없습니다.<br>$$\\begin{eqnarray}\\nabla_{\\theta}J(\\pi_{\\theta}) &amp;=&amp; \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\ &amp;=&amp; E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]<br>\\end{eqnarray}$$</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-3-Stochastic-Actor-Critic-Algorithms\"><a href=\"#2-3-Stochastic-Actor-Critic-Algorithms\" class=\"headerlink\" title=\"2.3 Stochastic Actor-Critic Algorithms\"></a>2.3 Stochastic Actor-Critic Algorithms</h2><ul>\n<li>Actor와 Critic이 번갈아가면서 동작하며 stochastic policy를 최적화하는 기법입니다.</li>\n<li>Actor: $ Q^{\\pi}(s,a) $ 를 근사한 $ Q^w(s,a) $를 이용해 stochastic policy gradient를 ascent하는 방향으로 policy parameter $ \\theta $를 업데이트함으로써 stochastic policy를 발전시킵니다.<ul>\n<li>$ \\nabla_{\\theta}J(\\pi_{\\theta}) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{w}(s,a)] $</li>\n</ul>\n</li>\n<li>Critic: SARSA나 Q-learning같은 Temporal-difference (TD) learning을 이용해 action-value function의 parameter, $ w $를 업데이트함으로써 $ Q^w(s,a) $가 $ Q^{\\pi}(s,a) $과 유사해지도록 합니다.</li>\n<li>실제 값인 $ Q^{\\pi}(s,a) $ 대신 이를 근사한 $ Q^w(s,a) $를 사용하게 되면, 일반적으로 bias가 발생합니다. 하지만, compatible condition에 부합하는 $ Q^w(s,a) $를 사용하게 되면, bias가 발생하지 않습니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-4-Off-policy-Actor-Critic\"><a href=\"#2-4-Off-policy-Actor-Critic\" class=\"headerlink\" title=\"2.4 Off-policy Actor-Critic\"></a>2.4 Off-policy Actor-Critic</h2><ul>\n<li>Distinct behavior policy $ \\beta(a|s) ( \\neq \\pi_{\\theta}(a|s) ) $ 로부터 샘플링된 trajectories 를 이용한 Actor-Critic</li>\n<li>Performance objective function<ul>\n<li>$\\begin{eqnarray}<br>  J_{\\beta}(\\pi_{\\theta})<br>  &amp;=&amp; \\int_{S}\\rho^{\\beta}(s)V^{\\pi}(s)ds \\nonumber \\\\<br>  &amp;=&amp; \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads<br>  \\end{eqnarray} $</li>\n</ul>\n</li>\n<li>off-policy policy gradient<ul>\n<li>$ \\begin{eqnarray}<br>  \\nabla_{\\theta}J_{\\beta}(\\pi_{\\theta}) &amp;\\approx&amp; \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads \\nonumber \\end{eqnarray} $<br>  $=E_{s \\sim \\rho^{\\beta}, a \\sim \\beta}[\\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)}\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)]$</li>\n<li>off-policy policy gradient 식에서의 물결 표시는 <a href=\"https://arxiv.org/abs/1205.4839\" target=\"_blank\" rel=\"noopener\">Degris, 2012b</a> 논문에 근거합니다.<ul>\n<li>Exact off-policy policy gradient 와 이를 approximate 한 policy gradient 는 아래와 같습니다. (빨간색 상자에 있는 항목을 삭제함으로써 근사합니다.)<ul>\n<li><img src=\"https://www.dropbox.com/s/xzpv3okc139c1fs/Screenshot%202018-06-16%2017.48.51.png?dl=1\" width=\"500px\"></li>\n</ul>\n</li>\n<li>[Degris, 2012b] Theorem 1 에 의해 policy parameter 가 approximated policy gradient ( $\\nabla_{u}𝑄^{\\pi,\\gamma}(𝑠,𝑎)$ term 제거)에 따라 업데이트되어도 policy 는 improve 가 됨이 보장되기에 exact off-policy policy gradient 대신 approximated off-policy policy gradient 를 사용해도 괜찮습니다.<ul>\n<li><img src=\"https://www.dropbox.com/s/mk13931r4scjngo/Screenshot%202018-06-16%2017.49.24.png?dl=1\" width=\"500px\"></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>off-policy policy gradient 식에서 $ \\frac{\\pi_{\\theta}(a|s)}{\\beta_{\\theta}(a|s)} $는 importance sampling ratio 입니다.<ul>\n<li>off-policy actor-critic에서는 $ \\beta $에 의해 샘플링된 trajectory를 이용해서 stochastic policy $ \\pi $를 예측하는 것이기 때문에 importance sampling이 필요합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"3-Gradient-of-Deterministic-Policies\"><a href=\"#3-Gradient-of-Deterministic-Policies\" class=\"headerlink\" title=\"3. Gradient of Deterministic Policies\"></a>3. Gradient of Deterministic Policies</h1><p><br></p>\n<h2 id=\"3-1-Regularity-Conditions\"><a href=\"#3-1-Regularity-Conditions\" class=\"headerlink\" title=\"3.1 Regularity Conditions\"></a>3.1 Regularity Conditions</h2><ul>\n<li>어떠한 이론이 성립하기 위한 전제 조건</li>\n<li>Regularity conditions A.1<ul>\n<li>$ p(s’|s,a), \\nabla_{a}p(s’|s,a), \\mu_{\\theta}(s), \\nabla_{\\theta}\\mu_{\\theta}(s), r(s,a), \\nabla_{a}r(s,a), p_{1}(s) $ are continuous in all parameters and variables $ s, a, s’ $ and $ x $.</li>\n</ul>\n</li>\n<li>regularity conditions A.2<ul>\n<li>There exists a $ b $ and $ L $ such that $ \\sup_{s}p_{1}(s) &lt; b $, $ \\sup_{a,s,s’}p(s’|s,a) &lt; b $, $ \\sup_{a,s}r(s,a) &lt; b $, $ \\sup_{a,s,s’}|\\nabla_{a}p(s’|s,a)| &lt; L $, and $ \\sup_{a,s}|\\nabla_{a}r(s,a)| &lt; L $.</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-2-Deterministic-Policy-Gradient-Theorem\"><a href=\"#3-2-Deterministic-Policy-Gradient-Theorem\" class=\"headerlink\" title=\"3.2 Deterministic Policy Gradient Theorem\"></a>3.2 Deterministic Policy Gradient Theorem</h2><ul>\n<li>Deterministic policy<ul>\n<li>$ \\mu_{\\theta} : S \\to A $ with parameter vector $ \\theta \\in \\mathbb{R}^n $</li>\n</ul>\n</li>\n<li>Probability distribution<ul>\n<li>$ p(s \\to s’, t, \\mu) $</li>\n</ul>\n</li>\n<li>Discounted state distribution<ul>\n<li>$ \\rho^{\\mu}(s) $</li>\n</ul>\n</li>\n<li>Performance objective</li>\n</ul>\n<p>$$<br>J(\\mu_{\\theta}) = E[r^{\\gamma}_{1} | \\mu]<br>$$</p>\n<p>$$<br>= \\int_{S}\\rho^{\\mu}(s)r(s,\\mu_{\\theta}(s))ds<br>= E_{s \\sim \\rho^{\\mu}}[r(s,\\mu_{\\theta}(s))]<br>$$</p>\n<ul>\n<li><p>DPG Theorem</p>\n<ul>\n<li><p>MDP 가 A.1 만족한다면, 아래 식이 성립합니다.<br>$\\nabla_{\\theta}J(\\mu_{\\theta}) = \\int_{S}\\rho^{\\mu}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}ds \\nonumber$<br>$= E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]   \\nonumber $ </p>\n</li>\n<li><p>DPG는 state space에 대해서만 평균을 취하면 되기에, state와 action space 모두에 대해 평균을 취해야 하는 SPG에 비해 data efficiency가 좋습니다. 즉, 더 적은 양의 데이터로도 학습이 잘 이뤄지게 됩니다.</p>\n</li>\n</ul>\n</li>\n</ul>\n<p><br>    </p>\n<h2 id=\"3-3-DPG-형태에-대한-informal-intuition\"><a href=\"#3-3-DPG-형태에-대한-informal-intuition\" class=\"headerlink\" title=\"3.3 DPG 형태에 대한 informal intuition\"></a>3.3 DPG 형태에 대한 informal intuition</h2><ul>\n<li>Generalized policy iteration<ul>\n<li>정책 평가와 정책 발전을 한 번 씩 번갈아 가면서 실행하는 정책 iteration<ul>\n<li>위와 같이 해도 정책 평가에서 예측한 가치함수가 최적 가치함수에 수렴합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>정책 평가<ul>\n<li>action-value function $ Q^{\\pi}(s,a) $ or $ Q^{\\mu}(s,a) $을 estimate 하는 것 입니다.</li>\n</ul>\n</li>\n<li>정책 발전<ul>\n<li>위 estimated action-value function에 따라 정책을 update하는 것 입니다.</li>\n<li>주로 action-value function에 대한 greedy maximisation을 사용합니다.<ul>\n<li>$ \\mu^{k+1}(s) = \\arg\\max\\limits_{a}Q^{\\mu^{k}}(s,a) $</li>\n<li>greedy 정책 발전은 매 단계마다 global maximization을 해야하는데, 이로 인해 continuous action spaces에서 계산량이 급격히 늘어납니다.</li>\n</ul>\n</li>\n<li>그래서 policy gradient 방법이 나옵니다.<ul>\n<li>policy 를 $ \\theta $에 대해서 parameterize 합니다.</li>\n<li>매 단계마다 global maximisation 수행하는 대신, 방문하는 state $ s $마다 policy parameter를 action-value function $ Q $의 $ \\theta $에 대한 gradient $ \\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s)) $ 방향으로 proportional하게 update 합니다.</li>\n<li>하지만 각 state는 다른 방향을 제시할 수 있기에, state distribution $ \\rho^{\\mu}(s) $에 대한 기대값을 취해 policy parameter를 update 할 수도 있습니다.<ul>\n<li>$ \\theta^{k+1} = \\theta^{k} + \\alpha E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}Q^{\\mu^{k}}(s,\\mu_{\\theta}(s))] $</li>\n</ul>\n</li>\n<li>이는 chain-rule에 따라 아래와 같이 분리될 수 있습니다.<ul>\n<li>$ \\theta^{k+1} = \\theta^{k} + \\alpha E_{s \\sim \\rho^{\\mu^{k}}} [\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu^{k}}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $ (7)</li>\n<li>chain rule: $ \\frac{\\partial Q}{\\partial \\theta} = \\frac{\\partial a}{\\partial \\theta} \\frac{\\partial Q}{\\partial a} $</li>\n</ul>\n</li>\n<li>하지만 state distribution $ \\rho^{\\mu} $은 정책에 dependent 합니다.<ul>\n<li>정책이 바꾸게 되면, 바뀐 정책에 따라 방문하게 되는 state가 변하기 때문에 state distribution이 변하게 됩니다.</li>\n</ul>\n</li>\n<li>그렇기에 정책 update 시 state distribution에 대한 gradient를 고려하지 않는데 정책 발전이 이뤄진다는 것은 직관적으로 와닿지 않을 수 있습니다.</li>\n<li>deterministic policy gradient theorem은 state distribution에 대한 gradient 계산없이 위 식(7) 대로만 update해도 performance objective의 gradient를 정확하게 따름을 의미합니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-4-DPG는-SPG의-limiting-case\"><a href=\"#3-4-DPG는-SPG의-limiting-case\" class=\"headerlink\" title=\"3.4 DPG는 SPG의 limiting case\"></a>3.4 DPG는 SPG의 limiting case</h2><ul>\n<li>stochastic policy parameterization<ul>\n<li>$ \\pi_{\\mu_{\\theta},\\sigma} $ by a deterministic policy $ \\mu_{\\theta} : S \\to A $ and a variance parameter $ \\sigma $</li>\n<li>$ \\sigma = 0 $ 이면, $ \\pi_{\\mu_{\\theta},\\sigma} \\equiv \\mu_{\\theta} $</li>\n</ul>\n</li>\n<li>Theorem 2. Policy의 variance가 0에 수렴하면, 즉, $ \\sigma \\to 0 $, stochastic policy gradient와 deterministic policy gradient는 동일해집니다.<ul>\n<li>조건: stochastic policy $ \\pi_{\\mu_{\\theta},\\sigma} = \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $<ul>\n<li>$ \\sigma $는 variance입니다.</li>\n<li>$ \\nu_{\\sigma}(\\mu_{\\theta}(s),a) $는 condition B.1을 만족합니다.</li>\n<li>MDP는 conditions A.1과 A.2를 만족합니다.</li>\n</ul>\n</li>\n<li>결과:<ul>\n<li>$ \\lim\\limits_{\\sigma\\downarrow0}\\nabla_{\\theta}J(\\pi_{\\mu_{\\theta},\\sigma}) = \\nabla_{\\theta}J(\\mu_{\\theta})  $<ul>\n<li>좌변은 standard stochastic gradient이며, 우변은 deterministic gradient입니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>의미:<ul>\n<li>deterministic policy gradient는 stochastic policy gradient의 특수 case 입니다.</li>\n<li>기존 유명한 policy gradients 기법들에 deterministic policy gradients 를 적용할 수 있습니다.<ul>\n<li>기존 기법들 예: compatible function approximation (Sutton, 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar, 2007) or episodic/batch methods (Peters, 2005)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"4-Deterministic-Actor-Critic-Algorithms\"><a href=\"#4-Deterministic-Actor-Critic-Algorithms\" class=\"headerlink\" title=\"4. Deterministic Actor-Critic Algorithms\"></a>4. Deterministic Actor-Critic Algorithms</h1><ol>\n<li>SARSA critic를 이용한 on-policy actor-critic<ul>\n<li>단점<ul>\n<li>deterministic policy에 의해 행동하면 exploration이 잘 되지 않기에, sub-optimal에 빠지기 쉽습니다.</li>\n</ul>\n</li>\n<li>목적<ul>\n<li>교훈/정보제공</li>\n<li>환경에서 충분한 noise를 제공하여 exploration을 시킬 수 있다면, deterministic policy를 사용한다고 하여도 좋은 학습 결과를 얻을 수도 있습니다.<ul>\n<li>예. 바람이 agent의 행동에 영향(noise)을 줌</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Remind: 살사(SARSA) update rule<ul>\n<li>$ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})) $</li>\n</ul>\n</li>\n<li>Algorithm<ul>\n<li>Critic은 MSE를 $ \\bf minimize $하는 방향, 즉, action-value function을 stochastic gradient $ \\bf descent $ 방법으로 update합니다.<ul>\n<li>$ MSE = [Q^{\\mu}(s,a) - Q^{w}(s,a)]^2 $<ul>\n<li>critic은 실제 $ Q^{\\mu}(s,a) $ 대신 미분 가능한 $ Q^{w}(s,a) $로 대체하여 action-value function을 estimate하며, 이 둘 간 mean square error를 최소화하는 것이 목표입니다.</li>\n</ul>\n</li>\n<li>$ \\nabla_{w}MSE \\approx -2 * [r + \\gamma Q^{w}(s’,a’) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $<ul>\n<li>$ \\nabla_{w}MSE = -2 * [Q^{\\mu}(s,a) - Q^{w}(s,a)]\\nabla_{w}Q^{w}(s,a)  $</li>\n<li>$ Q^{\\mu}(s,a) $ 를 $ r + \\gamma Q^{w}(s’,a’) $로 대체<ul>\n<li>$ Q^{\\mu}(s,a) = r + \\gamma Q^{\\mu}(s’,a’) $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $<ul>\n<li>$w_{t+1} = w_{t} - \\alpha_{w}\\nabla_{w}MSE  \\nonumber$<br>$ \\approx w_{t} - \\alpha_{w} <em> (-2 </em> [r + \\gamma Q^{w}(s’,a’) - Q^{w}(s,a)] \\nabla_{w}Q^{w}(s,a)$</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},a_{t+1}) - Q^{w}(s_{t},a_{t}) $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Actor는 식(9)에 따라 보상이 $ \\bf maximize $되는 방향, 즉, deterministic policy를 stochastic gradient $ \\bf ascent $ 방법으로 update합니다.<ul>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Q-learning 을 이용한 off-policy actor-critic<ul>\n<li>stochastic behavior policy $ \\beta(a|s) $에 의해 생성된 trajectories로부터 deterministic target policy $ \\mu_{\\theta}(s) $를 학습하는 off-policy actor-critic입니다</li>\n<li>performance objective<ul>\n<li>$ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)V^{\\mu}(s)ds \\nonumber \\\\$<br>$= \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds \\nonumber \\\\$<br>$= E_{s \\sim \\rho^{\\beta}}[Q^{\\mu}(s,\\mu_{\\theta}(s))]$</li>\n</ul>\n</li>\n<li>off-policy deterministic policy gradient<ul>\n<li>$ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $<ul>\n<li>논문에는 아래와 같이 나와있는데, 물결 표시 부분은 오류로 판단됩니다.</li>\n<li>$ \\begin{eqnarray}<br>  \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) &amp;\\approx&amp; \\int_{S}\\rho^{\\beta}(s)\\nabla_{\\theta}\\mu_{\\theta}(a|s)Q^{\\mu}(s,a)ds \\nonumber \\<br>  &amp;=&amp; E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}]<br>  \\end{eqnarray} $</li>\n<li>근거: Action이 deterministic하기에 stochastic 경우와는 다르게 performance objective에서 action에 대해 평균을 구할 필요가 없습니다. 그렇기에, 곱의 미분이 있을 필요가 없고, [Degris, 2012b]에서 처럼 곱의 미분을 통해 생기는 action-value function에 대한 gradient term을 생략할 필요가 사라집니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Remind: 큐러닝(Q-learning) update rule<ul>\n<li>$ Q(s_{t},a_{t}) \\leftarrow Q(s_{t},a_{t}) + \\alpha(r_{t} + \\gamma \\max\\limits_{a}Q(s_{t+1},a) - Q(s_{t},a_{t})) $</li>\n</ul>\n</li>\n<li>algorithm: OPDAC (Off-Policy Deterministic Actor-Critic)<ul>\n<li>살사를 이용한 on-policy deterministic actor-critic과 아래 부분을 제외하고는 같습니다.<ul>\n<li>target policy는 $ \\beta(a|s) $에 의해 생성된 trajectories를 통해 학습합니다.</li>\n<li>업데이트 목표 부분에 실제 행동 값 $ a_{t+1} $이 아니라 정책으로부터 나온 행동 값 $ \\mu_{\\theta}(s_{t+1}) $을 사용합니다.<ul>\n<li>$ \\mu_{\\theta}(s_{t+1}) $ 은 가장 높은 Q 값을 가지는 행동. 즉, Q-learning.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\nabla_{w}Q^{w}(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})\\nabla_{a}Q^{w}(s_{t},a_{t})\\vert_{a=\\mu_{\\theta}(s)} $</li>\n</ul>\n</li>\n<li>Stochastic off-policy actor-critic은 대개 actor와 critic 모두 importance sampling을 필요로 하지만, deterministic policy gradient에선 importance sampling이 필요없습니다.<ul>\n<li>Actor 는 deterministic 이기에 sampling 자체가 필요없습니다.<ul>\n<li>Stochastic policy인 경우, Actor에서 importance sampling이 필요한 이유는 상태 $ s $에서의 가치 함수 값 $ V^{\\pi}(s) $을 estimate하기 위해 $ \\pi $가 아니라 $ \\beta $에 따라 sampling을 한 후, 평균을 내기 때문입니다.</li>\n<li>하지만 deterministic policy인 경우, 상태 $ s $에서의 가치 함수 값 $ V^{\\pi}(s) = Q^{\\pi}(s,\\mu_{\\theta}) $ 즉, action이 상태 s에 대해 deterministic이기에 sampling을 통해 estimate할 필요가 없고, 따라서 importance sampling도 필요없어집니다.</li>\n<li>stochastic vs. deterministic performance objective<ul>\n<li>stochastic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\int_{A}\\rho^{\\beta}(s)\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)dads $</li>\n<li>deterministic : $ J_{\\beta}(\\mu_{\\theta}) = \\int_{S}\\rho^{\\beta}(s)Q^{\\mu}(s,\\mu_{\\theta}(s))ds $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Critic이 사용하는 Q-learning은 importance sampling이 필요없는 off policy 알고리즘입니다.<ul>\n<li>Q-learning도 업데이트 목표를 특정 분포에서 샘플링을 통해 estimate 하는 것이 아니라 Q 함수를 최대화하는 action을 선택하는 것이기에 위 actor 에서의 deterministic 경우와 비슷하게 볼 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>compatible function approximation 및 gradient temporal-difference learning 을 이용한 actor-critic<ul>\n<li>위 살사/Q-learning 기반 on/off-policy는 아래와 같은 문제가 존재합니다.<ul>\n<li>function approximator에 의한 bias<ul>\n<li>일반적으로 $ Q^{\\mu}(s,a) $ 를 $ Q^{w}(s,a) $로 대체하여 deterministic policy gradient를 구하면, 그 gradient는 ascent하는 방향이 아닐 수도 있습니다.</li>\n</ul>\n</li>\n<li>off-policy learning에 의한 instabilities</li>\n</ul>\n</li>\n<li>그래서 stochastic처럼 $ \\nabla_{a}Q^{\\mu}(s,a) $를 $ \\nabla_{a}Q^{w}(s,a) $로 대체해도 deterministic policy gradient에 영향을 미치지 않을 compatible function approximator $ Q^{w}(s,a) $를 찾아야 합니다.</li>\n<li>Theorem 3. 아래 두 조건을 만족하면, $ Q^{w}(s,a) $는 deterministic policy $ \\mu_{\\theta}(s) $와 compatible 합니다. 즉, $ \\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\beta}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $<ul>\n<li>$ \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $입니다.</li>\n<li>$ w $는 $ MSE(\\theta, w) = E[\\epsilon(s;\\theta,w)^{\\top}\\epsilon(s;\\theta,w)] $를 최소화합니다.<ul>\n<li>$ \\epsilon(s;\\theta,w) = \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} - \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}  $</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Theorem 3은 on-policy 뿐만 아니라 off-policy에도 적용 가능합니다.</li>\n<li>$ Q^{w}(s,a) = (a-\\mu_{\\theta}(s))^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top} w + V^{v}(s) $<ul>\n<li>어떠한 deterministic policy에 대해서도 위 형태와 같은 compatible function approximator가 존재합니다.</li>\n<li>앞의 term은 advantage를, 뒤의 term은 value로 볼 수 있습니다.</li>\n</ul>\n</li>\n<li>$ Q^{w}(s,a) = \\phi(s,a)^{\\top} w + v^{\\top}\\phi(s) $<ul>\n<li>정의 : $ \\phi(s,a) \\overset{\\underset{\\mathrm{def}}{}}{=} \\nabla_{\\theta}\\mu_{\\theta}(s)(a-\\mu_{\\theta}(s)) $</li>\n<li>일례 : $ V^{v}(s) = v^{\\top}\\phi(s) $</li>\n<li>Theorem 3 만족 여부<ul>\n<li>첫 번째 조건 만족합니다.</li>\n<li>두 번째 조건은 대강 만족합니다.<ul>\n<li>$ \\nabla_{a}Q^{\\mu}(s,a) $에 대한 unbiased sample을 획득하기는 매우 어렵기에, 일반적인 정책 평가 방법들로 $ w $를 학습합니다.</li>\n<li>이 정책 평가 방법들을 이용하면 $ Q^{w}(s,a) \\approx Q^{\\mu}(s,a) $인 reasonable solution을 찾을 수 있기에 대강 $ \\nabla_{a}Q^{w}(s,a) \\approx \\nabla_{a}Q^{\\mu}(s,a) $이 될 것입니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>action-value function에 대한 linear function approximator는 큰 값을 가지는 actions에 대해선 diverge할 수 있어 global하게 action-values 예측하기에는 좋지 않지만, local critic에 사용할 때는 매우 유용합니다.<ul>\n<li>즉, 절대값이 아니라 작은 변화량을 다루는 gradient method 경우엔 $ A^{w}(s,\\mu_{\\theta}(s)+\\delta) = \\delta^{\\top}\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $로, diverge하지 않고, 값을 얻을 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>COPDAC-Q algorithm (Compatible Off-Policy Deterministic Actor-Critic Q-learning critic)<ul>\n<li>Critic: 실제 action-value function에 대한 linear function approximator인 $ Q^{w}(s,a) = \\phi(s,a)^{\\top}w $를 estimate합니다.<ul>\n<li>$ \\phi(s,a) = a^{\\top}\\nabla_{\\theta}\\mu_{\\theta} $</li>\n<li>Behavior policy $ \\beta(a|s) $로부터 얻은 samples를 이용하여 Q-learning이나 gradient Q-learning과 같은 off-policy algorithm으로 학습 가능합니다.</li>\n</ul>\n</li>\n<li>Actor: estimated action-value function에 대한 gradient를 $ \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $로 치환 후, 정책을 업데이트합니다.</li>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $</li>\n</ul>\n</li>\n<li>off-policy Q-learning은 linear function approximation을 이용하면 diverge 할 수도 있습니다.<ul>\n<li>$ \\mu_{\\theta}(s_{t+1}) $이 diverge할 수도 있기 때문으로 판단됩니다.</li>\n<li>그렇기에 simple Q-learning 대신 다른 기법이 필요합니다.</li>\n</ul>\n</li>\n<li>그렇기에 critic 에 gradient Q-learning 사용한 COPDAC-GQ (Gradient Q-learning critic) algorithm을 제안합니다.<ul>\n<li>gradient temporal-difference learning에 기반한 기법들은 true gradient descent algorithm이며, converge가 보장됩니다. (Sutton, 2009)<ul>\n<li>기본 아이디어는 stochastic gradient descent로 mean-squared projected Bellman error (MSPBE)를 최소화하는 것입니다.</li>\n<li>critic이 actor보다 빠른 time-scale로 update되도록 step size들을 잘 조절하면, critic은 MSPBE를 최소화하는 parameters로 converge하게 됩니다.</li>\n<li>critic에 gradient temporal-difference learning의 일종인 gradient Q-learning을 사용한 논문입니다. (Maei, 2010)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>COPDAC-GQ algorithm<ul>\n<li>$ \\delta_{t} = r_{t} + \\gamma Q^{w}(s_{t+1},\\mu_{\\theta}(s_{t+1})) - Q^{w}(s_{t},a_{t}) $</li>\n<li>$ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $</li>\n<li>$ w_{t+1} = w_{t} + \\alpha_{w}\\delta_{t}\\phi(s_{t},a_{t}) - \\alpha_{w}\\gamma\\phi(s_{t+1}, \\mu_{\\theta}(s_{t+1}))(\\phi(s_{t},a_{t})^{\\top} u_{t}) $</li>\n<li>$ v_{t+1} = v_{t} + \\alpha_{v}\\delta_{t}\\phi(s_{t}) - \\alpha_{v}\\gamma\\phi(s_{t+1})(\\phi(s_{t},a_{t})^{\\top} u_{t}) $</li>\n<li>$ u_{t+1} = u_{t} + \\alpha_{u}(\\delta_{t}-\\phi(s_{t}, a_{t})^{\\top} u_{t})\\phi(s_{t}, a_{t}) $</li>\n</ul>\n</li>\n<li>stochastic actor-critic과 같이 매 time-step 마다 update 시 필요한 계산의 복잡도는 $ O(mn) $입니다.<ul>\n<li>m은 action dimensions, n은 number of policy parameters</li>\n</ul>\n</li>\n<li>Natural policy gradient를 이용해 deterministic policies를 찾을 수 있습니다.<ul>\n<li>$ M(\\theta)^{-1}\\nabla_{\\theta}J(\\mu_{\\theta}) $은 any metric $ M(\\theta) $에 대한 performance objective (식(14))의 steepest ascent direction 입니다. (Toussaint, 2012)</li>\n<li>Natural gradient는 Fisher information metric $ M_{\\pi}(\\theta) $에 대한 steepest ascent direction 입니다.<ul>\n<li>$ M_{\\pi}(\\theta) = E_{s \\sim \\rho^{\\pi}, a \\sim \\pi_{\\theta}}[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}] $</li>\n<li>Fisher information metric은 policy reparameterization에 대해 불변입니다. (Bagnell, 2003)</li>\n</ul>\n</li>\n<li>deterministic policies에 대한 metric으로 $ M_{\\mu}(\\theta) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}] $을 사용합니다.<ul>\n<li>이는 variance가 0인 policy에 대한 Fisher information metric으로 볼 수 있습니다.</li>\n<li>$ \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a\\vert s)}{\\pi_{\\theta}(a\\vert s)}$에서 policy variance가 0이면, 특정 s에 대한 $ \\pi_{\\theta}(a|s)$만 1이 되고, 나머지는 0입니다.</li>\n</ul>\n</li>\n<li>deterministic policy gradient theorem과 compatible function approximation을 결합하면 $ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] $이 됩니다.<ul>\n<li>$ \\nabla_{\\theta}J(\\mu_{\\theta}) = E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)}] $</li>\n<li>$ \\nabla_{a}Q^{\\mu}(s,a)\\vert_{a=\\mu_{\\theta}(s)} \\approx \\nabla_{a}Q^{w}(s,a)\\vert_{a=\\mu_{\\theta}(s)} = \\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w $</li>\n</ul>\n</li>\n<li>그렇기에 steepest ascent direction은 $ M_{\\mu}(\\theta)^{-1}\\nabla_{\\theta}J_{\\beta}(\\mu_{\\theta}) = w $이 됩니다.<ul>\n<li>$ E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}]^{-1}E_{s \\sim \\rho^{\\mu}}[\\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_{\\theta}\\mu_{\\theta}(s)^{\\top}w] = w $</li>\n</ul>\n</li>\n<li>이 알고리즘은 COPDAC-Q 혹은 COPDAC-GQ에서 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta} \\nabla_{\\theta}\\mu_{\\theta}(s_{t})(\\nabla_{\\theta}\\mu_{\\theta}(s_{t})^{\\top} w_{t}) $ 식을 $ \\theta_{t+1} = \\theta_{t} + \\alpha_{\\theta}w_{t} $로 바꿔주기만 하면 됩니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p><br><br></p>\n<h1 id=\"5-Experiments\"><a href=\"#5-Experiments\" class=\"headerlink\" title=\"5. Experiments\"></a>5. Experiments</h1><p><br></p>\n<h2 id=\"5-1-Continuous-Bandit\"><a href=\"#5-1-Continuous-Bandit\" class=\"headerlink\" title=\"5.1. Continuous Bandit\"></a>5.1. Continuous Bandit</h2><ul>\n<li>Stochastic Actor-Critic (SAC)과 COPDAC 간 성능 비교 수행합니다.<ul>\n<li>Action dimension이 커질수록 성능 차이가 심합니다.</li>\n<li>빠르게 수렴하는 것을 통해 DPG의 data efficiency가 SPG에 비해 좋다는 것을 확인할 수 있지만, 반면, time-step이 증가할수록 SAC와 COPDAC 간 성능 차이가 줄어드는 것을 통해 성능 차이가 심하다는 것은 일정 time step 내에서만 해당하는 것이라고 유추해볼 수 있습니다.</li>\n<li><img src=\"https://www.dropbox.com/s/hrkyq0s2f24z66r/Screenshot%202018-06-16%2017.47.38.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"5-2-Continuous-Reinforcement-Learning\"><a href=\"#5-2-Continuous-Reinforcement-Learning\" class=\"headerlink\" title=\"5.2. Continuous Reinforcement Learning\"></a>5.2. Continuous Reinforcement Learning</h2><ul>\n<li>COPDAC-Q, SAC, off-policy stochastic actor-critic(OffPAC-TD) 간 성능 비교 수행합니다.<ul>\n<li>COPDAC-Q의 성능이 약간 더 좋습니다.</li>\n<li>COPDAC-Q의 학습이 더 빨리 이뤄집니다.</li>\n<li><img src=\"https://www.dropbox.com/s/qdca4augapmzsxi/Screenshot%202018-06-16%2017.47.07.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"5-3-Octopus-Arm\"><a href=\"#5-3-Octopus-Arm\" class=\"headerlink\" title=\"5.3. Octopus Arm\"></a>5.3. Octopus Arm</h2><ul>\n<li>목표: 6 segments octopus arm (20 action dimensions &amp; 50 state dimensions)을 control하여 target을 맞추는 것입니다.<ul>\n<li>COPDAC-Q 사용 시, action space dimension이 큰 octopus arm을 잘 control하여 target을 맞춤입니다.</li>\n<li><img src=\"https://www.dropbox.com/s/xrxb0a52wntekld/Screenshot%202018-06-16%2017.46.28.png?dl=1\" width=\"600px\"></li>\n<li>기존 기법들은 action spaces 혹은 action과 state spaces 둘 다 작은 경우들에 대해서만 실험했다고 하며, 비교하고 있지 않습니다.<ul>\n<li>기존 기법들이 6 segments octopus arm에서 동작을 잘 안 했을 것 같긴한데, 그래도 실험해서 보여줬으면 하지만 실험을 하지 않았습니다.</li>\n</ul>\n</li>\n<li>8 segment arm 동영상이 저자 홈페이지에 있다고 하는데, 안 보입니다.</li>\n</ul>\n</li>\n<li>[참고] Octopus Arm 이란?<ul>\n<li><a href=\"https://www.youtube.com/watch?v=AxeeHif0euY\" target=\"_blank\" rel=\"noopener\">OctopusArm Youtube Link</a></li>\n<li><img src=\"https://www.dropbox.com/s/950ycj06sudakjx/Screenshot%202018-06-16%2017.45.52.png?dl=1\"></li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"Sutton-PG-여행하기\"><a href=\"#Sutton-PG-여행하기\" class=\"headerlink\" title=\"Sutton PG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/1_sutton-pg/\">Sutton PG 여행하기</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"DDPG-여행하기\"><a href=\"#DDPG-여행하기\" class=\"headerlink\" title=\"DDPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/\">DDPG 여행하기</a></h2>"},{"title":"Maximum Margin Planning","date":"2019-02-06T15:00:00.000Z","author":"이동민","subtitle":"Inverse RL 3번째 논문","_content":"\n<center> <img src=\"../../../../img/irl/mmp_1.png\" width=\"850\"> </center>\n\nAuthor : Nathan D. Ratliff, J. Andrew Bagnell, Martin A. Zinkevich\nPaper Link : https://www.ri.cmu.edu/pub_files/pub4/ratliff_nathan_2006_1/ratliff_nathan_2006_1.pdf\nProceeding : International Conference on Machine Learning (ICML) 2006\n\n---\n\n# 0. Abstract\n\n일반적으로 Supervised learning techniques를 통해 sequential, goal-directed behavior에 대한 imitation learning은 어렵습니다. 이 논문에서는 maximum margin을 이용한 prediction problem에 대해 sequential, goal-directed behavior을 학습하는 것을 다룹니다. 더 구체적으로 말하자면, 각 features부터 cost까지 한번에 mapping하는 것을 학습해서, 이 cost에 대한 Markov Decision Process(MDP)에서의 optimal policy가 expert's behavior를 모방하도록 하는 것입니다.\n\n또한 inference를 위해 fast algorithms을 이용하여 subgradient method의 기반인 structured maximum margin learning으로서 간단하고 효율적인 접근을 보였습니다. 비록 이러한 fast algorithm technique는 일반적으로 사용하지만, QP formulation의 한계를 벗어난 문제에서는 $A^*$나 Dynamic Programming(DP) 접근들이 policy를 learning하는 것을 다룰 수 있도록 만든다는 것을 보였습니다.\n\n실험에서는 outdoor mobile robot들을 이용하여 route planning에 적용합니다.\n\n<br>\n## 0.1 들어가기에 앞서..\n\n이 논문은 앞서 다뤘던 APP 논문에 더하여 좀 더 효율적인 알고리즘을 만들고자 하였습니다. 그래서 기존에 APP에서 QP를 이용한 SVM 방법에 더하여 Soft Margin term을 추가하여 슬랙변수를 가지는 SVM을 사용하였고, subgradient method를 이용하여 알고리즘을 좀 더 쉽고 빠르게 구할 수 있도록 만들었습니다.\n\nSVM과 Soft Margin SVM에 대해 모르는 분이 계시다면 아래의 링크를 꼭 보시고 이 논문을 보시는 것을 추천해드립니다!\n\n1) 영상 (KAIST 문일철 교수님 강의)\n  - [Lecture 1 Decision boundary with Margin](https://www.youtube.com/watch?v=hK7vNvyCXWc&list=PLbhbGI_ppZISMV4tAWHlytBqNq1-lb8bz&index=23)\n  - [Lecture 2 Maximizing the Margin](https://www.youtube.com/watch?v=tZy3uRv-9mY&list=PLbhbGI_ppZISMV4tAWHlytBqNq1-lb8bz&index=24)\n  - [Lecture 3 SVM with Matlab](https://www.youtube.com/watch?v=sYLuJ_8Qw3s&list=PLbhbGI_ppZISMV4tAWHlytBqNq1-lb8bz&index=25)\n  - [Lecture 4 Error Handling in SVM](https://www.youtube.com/watch?v=vEivqCo-LiU&list=PLbhbGI_ppZISMV4tAWHlytBqNq1-lb8bz&index=26)\n  - [Lecture 5 Soft Margin with SVM](https://www.youtube.com/watch?v=5jOqc7ByMm4)\n\n2) 블로그 글\n  - [SVM (Support Vector Machine) Part 1](https://gentlej90.tistory.com/43)\n  - [SVM (Support Vector Machine) Part 2](https://gentlej90.tistory.com/44)\n\n<br><br>\n\n# 1. Introduction\n\nImitation Learning에서, learner는 expert의 \"behavior\" or \"control strategy\"를 모방하려고 시도합니다. 이러한 imitation learning을 이용하여, robotics에서의 supervised learning 접근은 featrues부터 decision까지 mapping하는 것을 학습하는 것에서 엄청난 결과로서 사용되어 왔지만, Long-range나 Goal-directed behavior에서는 이러한 기술들을 사용하기 어렵습니다.\n\n<br>\n## 1.1 Perception and Planning\n\nMobile robotics에서는 일반적으로 **perception** subsystem과 **planning** subsystem으로 autonomy software를 partition함으로써 Long-horizon goal directed behavior를 찾습니다.\n1) Perception system은 다양한 model과 환경의 features를 계산합니다.\n2) Planning system은 cost-map을 input으로 두고, 그 input을 통해 minimal risk (cost) path를 계산합니다.\n\n<br>\n## 1.2 In this work\n\n하지만 불행하게도, perception의 model부터 planner에 대한 cost까지 하는 것은 종종 어렵습니다.\n\n따라서 이 논문에서는 새로운 방법을 제시합니다. **perception features부터 planner에 대한 cost까지 (Perception + Planning)** mapping하는 것을 자동화하는 방법입니다. 최종 목표는 features부터 cost function까지 mapping하는 것을 한번에 학습하는 것입니다. 그래서 이러한 cost function에 대한 optimal policy가 expert의 behavior을 한번에 모방하도록 하는 것입니다.\n\n<br>\n## 1.3 Three fold\n\n정리해보면, 이 논문에서는 3가지 중요한 언급이 있습니다.\n\n1. planning을 학습하기위한 새로운 방법을 제시합니다.\n2. sutructured maximum-margin classification으로서 효율적이고 간단한 접근을 제시합니다. 또한 batch setting에서도 linear convergence하다는 것을 보여주며, 다른 QP 기법이 없이도 lerge problems에 \u001d적용될 수 있다고 합니다.\n3. 실험적으로 mobile robotics 관련 문제에 적용하였습니다.\n\n<br><br>\n\n# 2. Preliminaries\n\n<br>\n## 2.1 Notations\n\n### 2.1.1 modeling the planning problem with discrete MDPs\n\n이 논문은 discrete MDPs에서의 planning problem을 modeling합니다.\n\n$\\mathcal{X}$ is state spaces. $x$ index the state spaces.\n\n$\\mathcal{A}$ is action spaces. $a$ index the action spaces.\n\n$p(y|x,a)$ is transition probablities.\n\n$s$ is initial state distribution.\n\n$\\gamma$ is a discount factor on rewards. if any, $\\gamma$ is aborbed into the transition probabilities.\n\nReward $R$은 따로 두지 않고, demonstrated behavior를 모방하는 policies를 만들기 위해 supervised examples로부터 학습됩니다.\n\n$v \\in \\mathcal{V}$ is primal variables of value function\n\n$\\mu \\in \\mathcal{G}$ is **dual state-action frequency counts**. equal to $y$.\n\n- 여기서 $\\mu$는 어떠한 상태에서 어떠한 행동을 취했는 지를 count의 개념으로 나타낸 것인데 $y$와 혼용되어 쓸 수 있습니다.\n\n그리고 여기서는 오직 stationary policies만을 고려합니다. The generalization is straightforward.\n\n#### state-action frequency counts란?\n\nInverse RL의 궁극적인 목표를 다르게 말해보면 expert가 어떠한 행동을 했을 때, 여기서의 state-action visitation frequency counts(줄여서 visitation frequency)를 구합니다. 그리고 우리의 목적은 expert의 visitation frequency과 최대한 비슷한 visitation frequency를 만들어내는 어떤 reward를 찾는 것입니다.\n\n일반적으로, RL 문제는 reward가 주어졌을 때, 이 reward의 expected sum을 최대로 하는 policy를 찾는다고 알려져 있습니다. 그런데 이 문제의 dual problem은 visitation frequency를 찾는 것입니다.\n\n다시 말해 optimal policy와 optimal visitation frequency counts는 1:1 관계라는 것입니다.\n\n### 2.1.2 The input to our algorithm\n\nThe input is a set of training instances\n\n<center> <img src=\"../../../../img/irl/mmp_2.png\" width=\"300\"> </center>\n\n$p_i$ is transition probablities.\n\nState-action pairs $(x_i, a_i) \\in \\mathcal{X}_i, \\mathcal{A}_i$ is $d \\times |\\mathcal{X}||\\mathcal{A}|$ Feature matrix (or mapping) $F_i \\in \\mathbb{R}^{d \\times |\\mathcal{X}||\\mathcal{A}|}$.\n\n$y_i$ is expert's demonstration (desired trajectory or full policy). equal to **dual state-action frequency counts** $\\mu_i$.\n\n$f_i (y)$ denote vector of expected feature counts $F_i \\mu$ of the $i$th example.\n\n$\\mathcal{L}_i$ : Some additional loss function (heuristic)\n\n### 2.1.3 Detail description\n\n$\\mu_i^{x, a}$ is the expected state-action frequency for state $x$ and action $a$ of example $i$.\n\n- $\\mu_i$를 자세히보면 $\\mu$에 $i$가 붙은 형태로 되어있습니다. 여기서 example $i$라는 것 총 trajectory(or path)의 length 중에서 하나의 index를 말하는 것입니다. expert의 경우, 보통 전체 trajectory를 한꺼번에 취하기 때문에 이 논문에서는 구별하기 위해 $i$라는 notation을 쓴 것입니다.\n\n$\\mathcal{D} = \\\\{(\\mathcal{X}_i, \\mathcal{A}_i, p_i, f_i, y_i, \\mathcal{L}_i)\\\\} \\equiv \\\\{(\\mathcal{X}_i, \\mathcal{A}_i, \\mathcal{G}_i, F_i, \\mathcal{\\mu}_i, \\mathcal{l}_i)\\\\}$ \n\n- ($\\mathcal{D}$는 $\\mathcal{D}_{i=1}^n$)\n\nLoss function is $\\mathcal{L} : \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}_+$ between solutions. $\\mathcal{L} (y_i, y) = \\mathcal{L}_i (y) = l_i^T \\mu$. \n\n- $l_i \\in \\mathbb{R}_+^{|\\mathcal{X}||\\mathcal{A}|}$\n- $l_i = 1 -$ expert's visitation frequency\n\n### 2.1.4 그래서 이 논문에서 하고 싶은 것 (중요)\n\nThe best policy over the resulting reward function $\\mu^* = arg\\max_{\\mu \\in \\mathcal{G}_i} w^T F_i \\mu$ is “**close**” to the expert's demonstrated policy $\\mu_i$.\n\n<br>\n## 2.2 Loss function\n\n위에서 말했던 loss function은 teacher가 아닌 learner가 방문한 states의 count입니다.\n\n또한 이 논문에서는 teacher가 도달한 어떠한 states에서 teacher와 다른 action들을 고르거나 teacher가 선택하지 않은 states를 도달하는 것을 penalizing할 것입니다.\n\n끝으로, $\\mathcal{L}(y_i, y) \\geq 0$을 가정합니다.\n\n<br>\n## 2.3 Quadratic Programming Formulation\n\n### 2.3.1 Quadratic Program\n\nGiven a training set :\n\n<center> <img src=\"../../../../img/irl/mmp_2.png\" width=\"300\"> </center>\n\nQuadratic Program is\n\n$$\\min_{w, \\zeta_i} \\frac{1}{2} \\parallel w \\parallel^2 + \\frac{\\gamma}{n} \\sum_i \\beta_i \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (1)$$\n\n$$s.t. \\,\\, \\forall i \\,\\,\\,\\, w^T f_i (y_i) \\geq \\max_{\\mu \\in \\mathcal{G}_i} (w^T f_i (y) + \\mathcal{L} (y_i, y)) - \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\, (2)$$\n\ns.t.로 나오는 constraint의 intuition을 보자면, expert's policies가 margin에 대해 다른 모든 policies보다 더 높은 experted reward를 가지도록 하는 only weight vectors를 찾자는 것입니다.\n\n다음으로 위의 수식에 나오는 notation에 대해서 자세히 알아보겠습니다.\n\n$\\frac{\\gamma}{n}\\sum_i \\zeta_i$ is **soft margin term** from soft margin SVM. $\\zeta$의 값을 되도록 최소화하여 오분류의 허용도를 낮추기 위해 추가되었습니다.\n\n$\\zeta_i$ is **slack variable**. The slack variable permit violations of these constraints for a penalty. 여유 변수라고도 하고, $\\zeta_i$만큼의 오류를 인정한다는 의미로 볼 수 있습니다.\n\n$\\gamma \\geq 0$ is scaled for a penalty. 보통 $c$ ($c \\geq 0$)라고도 하는데, 최소화 조건을 반영하는 정도를 결정하는 값으로 우리가 적절히 정해주어야 합니다. c값이 크면 전체 margin도 커지므로 오분류를 적게 허용(엄격)한다는 뜻이고, 반대로 c값이 작으면 margin이 작아지므로 비교적 높은 오분류를 허용(관대)한다는 뜻입니다.\n\n$\\beta_i \\geq 0$는 example들이 다른 length일 때 normalization하기 위해서 사용되는 data dependent scalars입니다.\n\n$w^T f_i(y_i)$ is expert's reward.\n\n$w^T f_i(y)$ is other's reward.\n\n$\\mathcal{L} (y_i, y)$는 $y_i$와 $y$가 일치하지 않는 상태의 수입니다.\n\n### 2.3.2 Maximum Margin Problem(MMP)\n\n만약 $f_i (\\cdot)$ and $\\mathcal{L}_i (\\cdot)$가 state-action frequencies $\\mu$에서 linear하다면, MMP는 다음과 같이 정의할 수 있습니다.\n\n$$\\min_{w, \\zeta_i} \\frac{1}{2} \\parallel w \\parallel^2 + \\frac{\\gamma}{n} \\sum_i \\beta_i \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (3)$$\n\n$$s.t. \\,\\, \\forall i \\,\\,\\,\\, w^T F_i \\mu_i \\geq \\max_{\\mu \\in \\mathcal{G}_i} (w^T F_i \\mu + l_i^T \\mu) - \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (4)$$\n\n그리고 수식 (4)에서, $\\mu \\in \\mathcal{G}_i$를 Bellman-flow constraints로 표현할 수 있습니다. 다시 말해 $\\mu \\geq 0$는 다음을 만족합니다. ($\\mu$의 성질)\n\n<center> <img src=\"../../../../img/irl/mmp_3.png\" width=\"450\"> </center>\n\n- 나중에 GAIL 논문에서도 나오겠지만, 위의 수식으로 $\\mu$인 visitation frequency를 정할 수 있습니다.\n\n### 2.3.3 One compact quadratic program for MMP\n\n이어서 수식 (4)에서 nonlinear, convex constraints는 오른쪽 side의 dual을 계산함으로써 linear constraints의 compact set으로 다음과 같이 변형될 수 있습니다.\n\n$$\\forall i \\,\\,\\,\\, w^T F_i \\mu_i \\geq \\min_{v \\in V_i} \\, (s_i^T v) - \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (5)$$\n\n그리고 수식 (5)에서, $v \\in V_i$는 Bellman primal constraints을 만족하는 value function입니다. Bellman primal constraints는 다음과 같습니다.\n\n$$\\forall i,x,a \\,\\,\\,\\, v^x \\geq (w^T F_i + l_i)^{x,a} + \\sum_{x'} p_i (x'|x,a) v^{x'} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (6)$$\n\n위의 constraints (5), (6)을 combining함으로써 최종적으로 다음과 같이 쓸 수 있습니다.\n\nOne compact quadratic program is\n\n$$\\min_{w, \\zeta_i} \\frac{1}{2} \\parallel w \\parallel^2 + \\frac{\\gamma}{n} \\sum_i \\beta_i \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (7)$$\n\n$$\\forall i \\,\\,\\,\\, w^T F_i \\mu_i \\geq \\min_{v \\in V_i} \\, (s_i^T v) - \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (8)$$\n\n$$\\forall i,x,a \\,\\,\\,\\, v_i^x \\geq (w^T F_i + l_i)^{x,a} + \\sum_{x'} p_i (x'|x,a) v_i^{x'} \\,\\,\\,\\,\\,\\,\\,\\, (9)$$\n\n위의 수식은 MMP 문제를 One compact quadratic program으로써 풀 수 있도록 만든 것입니다. 하지만 아쉽게도 위의 constraints의 수는 state-action pairs와 training examples에 대해 linear하게 scaling됩니다.\n\n이러한 program을 직접적으로 optimize할 수 있도록 이미 만들어져 있는 QP software가 있지만, 뒤이어 나오는 **section 3** 에서 **subgradient methods** 의 이용함으로써 One compact quadratic program에 대해 크게 향상시킬 수 있는 다른 alternative formulation을 이용하고자 합니다.\n\n추가적으로, **Section 4** 에서는 최종 objective function에 유용한 방법들을 생각해 볼 것입니다. 그리고 나서 path planning problems에 대한 적절한 examples를 말할 것입니다.\n\n<br><br>\n\n# 3. Efficient Optimization\n\n실제로, 수식 (9)에서의 quadratic program을 해결하는 것은 적어도 single MDP의 linear programming을 해결하는 것만큼 어렵습니다. 그래서 수식 (9)를 quadratic program으로 해결하려는 것이 적절한 전략이 될 수 있지만, 다르게 보면 많은 문제들에 대해 policy iteration과 $A^*$ algorithm처럼 이론적으로나 실험적으로나 더 빠르게 해결할 수 있도록 특별하게 design된 algorithm이 존재한다는 것으로 볼 수 있습니다.\n\n따라서 이 논문에서는 더 나아가 **fast maximization algorithm을 사용하는 iterative method 기반인 subgradient method로 접근합니다.**\n\n<br>\n## 3.1 Objective function\n\n첫 번째 step은 optimization program(One compact quadratic program)을 \"hinge-loss\" form으로 변형하는 것입니다.\n\n변형된 objective function은 다음과 같습니다.\n\n$$c_q(w) = \\frac{1}{n} \\sum_{i=1}^n \\beta_i \\Big( \\big\\\\{\\max_{\\mu \\in \\mathcal{G}_i} (w^T F_i + l_i^T)\\mu \\big\\\\} - w^T F_i \\mu_i\\Big) + \\frac{1}{2} \\parallel w \\parallel^2 \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (10)$$\n\nhinge-loss 관점에서 보면, 위의 수식에서 $\\big\\\\{\\max_{\\mu \\in \\mathcal{G}_i} (w^T F_i + l_i^T)\\mu\\big\\\\} - w^T F_i \\mu_i$은 slack variable인 $\\zeta_i$과 동일합니다.\n\n또한 기존에 있던 $\\gamma$는 slack variable $\\zeta_i$가 없어졌기 때문에 사라집니다.\n\n위의 objective function은 convex하지만, **max term**이 있기 때문에 미분이 불가능합니다. 따라서 이 논문에서는 subgradient method라 불리는 gradient descent의 generalization을 이용함으로써 optimization을 합니다.\n\nconvex function $c : \\mathcal{W} \\rightarrow \\mathbb{R}$의 subgradient는 vector $g$로 정의합니다.\n\n<center> <img src=\"../../../../img/irl/mmp_4.png\" width=\"400\"> </center>\n\n위의 수식에서 subgradient는 비록 미분 가능한 점에서 필연적으로 gradient와는 같지만, unique할 필요는 없습니다.\n\n### 3.1.1 Subgradient Method란?\n\n아래의 링크를 참고해주시면 감사하겠습니다.\n\n1) [Wikipedia - Subgradient method](https://en.wikipedia.org/wiki/Subgradient_method)\n2) [모두를 위한 컨벡스 최적화 - Subgradient](https://wikidocs.net/18963)\n3) [모두를 위한 컨벡스 최적화 - Subgradient Method](https://wikidocs.net/18953)\n\n<br>\n## 3.2 Four well known properties for subgradient method\n\n최종 objective function을 보기전에, 먼저 $c(w)$의 subgradient를 계산하기 위해, subgradient에 대해 잘 알려진 4가지 속성들에 대해서 알아봅시다. (**3번 중요**)\n1) Subgradient operators are linear.\n2) The gradient is the unique subgradient of a differentiable function.\n3) Denoting $y^∗ = arg\\max_y [f (x, y)]$ for differentiable $f (., y)$, $\\nabla_x f(x,y∗)$ is a subgradient of the piecewise(구분적으로, 구간적으로) differentiable convex function $\\max_y [f (x, y)]$.\n4) An analogous chain rule holds as expected.\n\n3번을 보면, 결국 subgradient method를 통해 하고 싶은 것은 piecewise differentiable convex function인 $f(x,y)$ 중에서 제일 큰 $\\max_y [f (x, y)]$를 subgradient로 구해서, 그 중 가장 큰 값인 $y^∗ = arg\\max_y [f (x, y)]$를 통해 $\\nabla_x f(x,y∗)$를 하겠다는 것입니다.\n\n<br>\n## 3.3 A subgradient method for objective function\n\nWe are now equipped to compute a subgradient $g_w \\in \\partial c(w)$ of our objective function (10):\n\n$$g_w = \\frac{1}{n} \\sum_{i=1}^n \\beta_i \\big( (w^T F_i + l_i^T)\\mu^* - w^T F_i \\mu_i \\big) \\cdot F_i \\Delta^w \\mu_i + \\lambda w \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (12)$$\n\n위의 수식에서 detail한 notation은 다음과 같습니다.\n\n$$\\mu^* = arg \\max_{\\mu \\in \\mathcal{G}} (w^T F_i + l_i^T)\\mu \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (12-1)$$\n\n$$\\Delta^w \\mu_i = \\mu^∗ − \\mu_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (12-2)$$\n\n수식 (12-2)를 직관적으로 보면, subgradient가 현재의 reward function $w^T F_i$에 관하여 **optimal policy와 example policy 사이의 state-action visitation frequency count를 비교한다는 점**을 발견할 수 있습니다.\n\n또한 subgradient를 계산하는 것은 $\\mu^* = arg \\max_{\\mu \\in \\mathcal{G}} (w^T F_i + l_i^T)\\mu$을 해결하는 것과 같습니다. 다시 말해 reward function $w^T F_i + l_i^T$를 해결한다는 것입니다.\n\n<br>\n## 3.4 Algorithm 1. Max Margin Planning\n\n<center> <img src=\"../../../../img/irl/mmp_5.png\" width=\"500\"> </center>\n\n- 5: loss augmented cost map $(w^T F_i + l_i^T)$에 대해서 각각의 input map에 대한 optimal policy $\\mu^*$와 state-action visitation frequencies $\\mu^i$를 계산합니다. 처음에는 w가 0에서 시작하므로 loss augmented cost map $w^T F_i + l_i^T$은 $l_i^T$로 시작하게 됩니다.\n- 6: 수식 (12)에 있는 objective function $g$를 계산합니다.\n- 7: w를 $\\alpha_t g$에 따라 minimize합니다.\n- 8: Option으로 추가적인 constraints를 둘 수도 있습니다. 자세한 내용은 section 4.4인 Incorporating Prior Knowledge를 참고하시기 바랍니다.\n- No RL step!\n\n<br><br>\n\n# 4. Additional section\n\n이전까지는 최종 objective function과 algorithm을 살펴봤습니다. 여기서 더 나아가 유용한 방법들을 통해 우리의 objective function과 algorithm이 더 robust하도록 만들어봅시다.\n\n<br>\n## 4.1 Guarantees in the Batch Setting\n\nsubgradient method로 구성된 algorithm들의 잘 연구된 class 중 하나는 batch setting으로 둘 수 있다는 것입니다.\n\nbatch setting에는 두 가지 key point가 존재합니다.\n1) 이 method에서 step-size sequence $\\\\{ \\alpha_t \\\\}$의 선택은 상당히 중요합니다. $\\\\{ \\alpha_t \\\\}$에 따라서 convergence guarantee가 달라집니다.\n2) 우리의 결과는 objective function을 유지하기 위해 strong convexity assumption이 필요합니다.\n따라서 Given G$\\mathcal{W} \\subseteq \\mathbb{R}^d$, a function $f: \\mathcal{W} \\rightarrow \\mathbb{R}$ is $\\eta$-strongly convex if there exists $g: \\mathcal{W} \\rightarrow \\mathbb{R}$ such that for all $w$, $w' \\in \\mathcal{W}$:\n\n<center> <img src=\"../../../../img/irl/mmp_6.png\" width=\"500\"> </center>\n\n**Theorem 1. Linear convergence of constant stepsize sequence.** Let the stepsize sequence $\\\\{ \\alpha_t \\\\}$ of Algorithm (1) be chosen as $\\alpha_t = \\alpha \\leq \\frac{1}{\\lambda}$. Furthermore, assume for a particular region of radius $R$ around the minimum, $\\forall w,g \\in \\partial c(w), ||g|| \\leq C$. Then the algorithm converges at a linear rate to a region of some minimum point $x^*$ of $c$ bounded by\n\n$$||x_{min} - x^*|| \\leq \\sqrt{\\frac{a C^2}{\\lambda}} \\leq \\frac{C}{\\lambda}$$\n\n<center> <img src=\"../../../../img/irl/mmp_7.png\" width=\"450\"> </center>\n\nTheorem 1은 우리가 충분히 작고 일정한 stepsize를 통해 linear convergence rate를 얻을 수 있다는 것을 보여줍니다. 그러나 이 convergence는 오직 minimum 주변 지역에서만 가능합니다.\n\n대안적으로, 우리는 $t \\geq 1$에 대해 $\\alpha_t = \\frac{r}{t}$ 형태의 감소하는 step size rule을 고를 수 있습니다. 여기서 $r$은 learning rate로 생각할 수 있는 some positive constant입니다.\n\n이러한 rule을 통해, Algorithm 1은 minimum에서 convergence가 보장되지만, 위에서 말했던 strong convexity assumption에서만 오직 sublinear rate로 수렴될 수 있습니다.\n\n<br>\n## 4.2 Optimization in an Online Setting\n\n다양한 optimization techniques와 다르게, subgradient method는 batch setting에서 더 확장됩니다.\n\nonline setting에서는 적절하게 관련된 domain에 대한 다양한 planning problem들을 생각해볼 수 있습니다. 특히, 그 중 하나는 path를 plan하기 위해 필요로 하는 domain을 제공하는 것입니다. 더 정확하게는 \"correct\" path를 제공하는 것입니다.\n\nAt each time step $i$:\n1) We observe $\\mathcal{G}_i$ and $F_i$.\n2) Select a weight vector $w_i$ and using this compute a resulting path.\n3) Finally we observe the true policy $y_i$.\n\n즉, strongly convex cost function(앞서 다뤘던 수식 (10))이 되기 위해 $c_i(w) = \\frac{1}{2} \\parallel w \\parallel^2 + \\\\{ \\max_{\\mu \\in \\mathcal{G}_i} (w^T F_i + l_i)\\mu\\\\} − w^T F_i \\mu_i$를 정의할 수 있다는 것입니다. 그리고 우리는 $y_i$, $\\mathcal{G}_i$, $F_i$가 주어진다면 계산할 수 있습니다.\n\n정리하자면, 앞서 본 cost function(Equation 10)에서 $\\frac{1}{n}\\sum_{i=1}^n \\beta_i$가 없어진 것과 같이 online setting이 가능하다는 것을 보여줍니다.\n\nThis is now an **online convex programming problem**.\n\n<br>\n## 4.3 Modifications for Acyclic Positive Costs (Think of Worst Case)\n\nacyclic(특정 방향이 없는, 사이클이 없는, 비순환적인) domain의 infinite horizon problems에서, $A^*$와 $A^*$의 변종들은 일반적으로 좋은 paln을 찾기위한 가장 효율적인 방법입니다. 이러한 domain에서는 strictly negative한 reward를 생각해볼 필요가 있습니다(동일하게, cost는 strictly positive). 다시 말해 best case에 대해서만 생각해볼 것이 아니라 worst case에 대해서도 생각해볼 필요가 있다는 것입니다. 이렇게 하지 않으면 infinite reward path가 발생할지도 모르기 때문입니다. 이러한 negativity의 strictness는 heuristic의 존재를 더 확실히게 보장하는 것이라고 볼 수 있습니다.\n\n$F_i \\geq 0$이라고 가정하면, 이러한 negativity의 strictness는 두 가지 방법을 통해 쉽게 구현할 수 있습니다.\n1) w에 component-wise negativity constraints를 추가\n2) 각각의 state-action pair에 대한 보상에 negativity를 부여하는 constraints를 추가\n\n이렇게 negativity를 추가할 수 있는 이유는 reward $w^T F_i \\mu$(or $(w^T F_i + l_i^T) \\mu$)에서 $F_i$이 0보다 크기 때문에, 우리는 $w, \\mu$에 negativity를 추가할 수 있습니다. 1의 경우 단순히 w의 violated component를 0으로 설정하여 구현할 수 있고, 2의 경우 가장 violated constraint를 반복적으로 추정함으로써 효율적으로 구현할 수 있습니다.\n\n<br>\n## 4.4 Incorporating Prior Knowledge\n\n이 section은 앞서 Algorithm 1의 Line 8에서와 말한 것과 같이 Option으로 prior knowledge을 통해 추가적인 constraints를 둘 수 있다는 것을 보여줍니다.\n1) 0 vector 대신에 $w$에 prior belief에 대한 solution을 regularizing하는 것\n2) loss function을 통해 특정한 state-action pairs를 poor choices으로 표시하는 것. algorithm이 이러한 요소로 인하여 large magin을 가지도록 강제합니다.\n3) $w$에 constraint 형태로 domain knowledge를 포함시키는 것 (e.g 특정한 영역의 state를 다른 state보다 cost가 적어도 두 배가 되도록 요구.)\n\n이러한 방법들은 training example의 사용 외에도 learner에게 expert의 knowledge를 전달하는 강력한 방법입니다.\n\n<br><br>\n\n# 5. Experimental Results\n\n<br>\n## 5.1 Demonstration of learning to plan based on satellite color imagery\n\n실험에서는 실제 문제(Path planning)에서 논문 개념을 이용하여 유효성 검증할 것입니다.\n1) Section 4에서 보여주었던 batch learning algorithm을 사용\n2) Regularization을 위한 적당한 값을 사용하고, 위에서 다뤘던 우리의 algorithm을 사용\n\n추가적으로 prior knowledge에서의 첫 번째 방법을 적용한 것으로 보입니다.\n\n같은 맵의 영역에서 시연되는 다른 예제 경로는 학습 후에 hold out 영역에서 상당히 다른 결과를 이끌었습니다.\n\n<center> <img src=\"../../../../img/irl/mmp_8.png\" width=\"1200\"> </center>\n\n<center> <img src=\"../../../../img/irl/mmp_9.png\" width=\"600\"> </center>\n\n- 실험 의도\n  - Top : Road에 유지하도록 제안\n  - Bottom : 은밀한 의도를 제시(여기서는 숲을 지나는 의도를 의미)\n\n- 실험 결과\n  - Left : Training 예제\n  - Middle : Training 이후에 hold out 영역에서 학습된 cost map\n  - Right : Hold out 영역에서 $A^*$를 이용하여 생성된 행동 결과\n\n<br>\n## 5.2 Data shown are MMP learned cost maps\n\n<center> <img src=\"../../../../img/irl/mmp_10.png\" width=\"1100\"> </center>\n\nFigure 2은 holdout region으로부터의 결과입니다.\n\n그림에서 loss-augmented path (blue)은 일반적으로 마지막 학습된 경로보다 일반적으로 좋은 결과를 수행하지 못하는 것을 나타났습니다.\n\n왜냐하면 loss-augmentation은 높은 loss의 영역을 최종 학습 지도보다 더욱 desirable하게 만들기 때문입니다. \n\n직관적으로, 만약 학습자가 loss-augmented cost map에 대해서 잘 수행할 수 있다면, loss-augmentation없이도 더욱 잘 수행되어야 한다는 것입니다. 이것은 margin을 가지고 학습된 개념입니다.\n\n<br>\n## 5.3 Results using two alternative approaches\n\n<center> <img src=\"../../../../img/irl/mmp_11.png\" width=\"800\"> </center>\n\n- Left : the result of a next-action classifier applied superimposed on a visualization of the second dataset.\n- Right : a cost map learned by manual training of a regression.\n\n두 개의 경우에서 학습된 경로들은 poor approximations. (not shown on left, red on right).\n\n<br>\n## 5.4 Visualization about losses\n\n<center> <img src=\"../../../../img/irl/mmp_12.png\" width=\"850\"> </center>\n\n- Left : Visualization of inverted Loss function $(1− l(x))$ for a training example path.\n- Right : Comparison of holdout loss of MMP (by number of iterations) to a regression method where a teacher hand-labeled costs.\n\n비교를 위해, 저자는 MMP에 다른 두 개의 접근방법을 사용하여 유사한 학습을 시도하였습니다.\n\n1. (Lecun et al., 2006)$^5$에서 제안한 방법으로 state 특징들을 다음 action으로 취하는 mapping을 사용한 직접적으로 학습하는 알고리즘입니다. 이 경우, traing data에 대해 좋은 결과를 얻지 못했습니다.\n\n2. 다소 더 성공적은 시도는 직접 label을 통해 cost를 학습시킨 알고리즘입니다. 이 알고리즘은 MMP보다 학습자에게 더 많은 명시적 정보를 제공합니다.\n   - 다음을 기반하여 low, medium, high cost로 제공\n     1. Expert knowledge of the planner\n     2. Iterated training and observation\n     3. The trainer had prior knowledge of the cost maps found under MMP batch learning on this dataset.\n   - 추가 정보가 주어진 cost map은 정성적으로 올바른 것처럼 보이지만, 그림 3과 그림 4는 상당히 좋지 않은 성능을 보여줍니다..\n\n<br><br>\n\n# 6. Related and Future Work\n\nMaximum Margin Planning과 직접적으로 연관된 두 가지 work가 있습니다.\n\n그 중 하나가 바로 Inverse RL입니다.\n\nIRL의 목표는 MDP에서 agent의 행동을 관찰하는 하여 agent의 행동으로 부터 reward function를 추출하는 것입니다. 그러나 이것은 기본적으로 ill-posed 문제로 알고 있습니다. 그럼에도 불구하고, MMP와 같이 유사한 효과를 가진 IRL 아이디어들을 시도한 몇 가지 heuristic 시도가 있었습니다.\n\n유용한 heuristic 방법은 바로 이전 논문인 *Abbeel, P., & Ng, A. Y. (2004). Apprenticeship learning via inverse reinforcement learning* APP 논문입니다. 학습자의 정책과 시연되는 예제간의 expected feature counts을 통해 매칭을 시도하는 학습 방법입니다.\n\nMMP의 경우 IRL algorithm의 variant과는 다른 스타일의 algorithm입니다.\n\nMMP는 하나의 MDP 보다 많은 정책 시연들을 허용하도록 설계되어 있습니다. 여러 특징 맵, 다른 시작 지점과 목표 지점, 완전히 새로운 맵과 목표 지점을 가지는 학습자의 목표를 이용하여 예제들을 시연했습니다. 또한 MMP는 상당히 다른 IRL적 알고리즘 접근 방법을 유도합니다.\n\nIRL과 MMP간의 관계는 **generative and discriminative learning** 간의 구별을 연상시킵니다.\n\n일반적인 IRL의 경우, feature matching을 시도합니다. agent가 MDP에서 (거의 최적같이) 행동하고 (거의) feature expectation에 매칭 가능할 때 학습하도록 설계되었습니다(Generative models과 같은 strong 가정). 예를 들어, feature expecatation을 매칭하는 능력은 algorithm의 행동이 feature가 선형인 모든 cost function에 대해서 near-optimal일 것이라는 것을 의미합니다.\n\n반대로 MMP의 경우, 우리의 목표가 직접적으로 output 행동을 모방하는 것이라는 weaker 가정을 하고 실제 MDP나 reward 함수에 대해 agnostic합니다. 여기서 MDP는 output decision들을 구조화하고 경쟁을 하려고 하는 expert가 natual class을 제공합니다.\n\n정리하자면,\nGenerative model : 개별 클래스의 분포를 모델링한다.\nDiscriminative model : Discriminative 모델은 기본 확률 분포 또는 데이터 구조를 모델링하지 않고 기본 데이터를 해당 클래스에 직접 매핑(class 경계를 통해 학습). SVM은 이러한 기준을 만족 시키므로 decision tree와 마찬가지로 discriminative model.\n\n<br><br>\n\n# 처음으로\n\n## [Let's do Inverse RL Guide](https://reinforcement-learning-kr.github.io/2019/01/22/0_lets-do-irl-guide/)\n\n<br>\n\n# 이전으로\n\n## [APP 여행하기]()\n\n<br>\n\n# 다음으로\n\n## [MaxEnt 여행하기]()","source":"_posts/3_mmp.md","raw":"---\ntitle: Maximum Margin Planning\ndate: 2019-02-07\ntags: [\"프로젝트\", \"GAIL하자!\"]\ncategories: 프로젝트\nauthor: 이동민\nsubtitle: Inverse RL 3번째 논문\n---\n\n<center> <img src=\"../../../../img/irl/mmp_1.png\" width=\"850\"> </center>\n\nAuthor : Nathan D. Ratliff, J. Andrew Bagnell, Martin A. Zinkevich\nPaper Link : https://www.ri.cmu.edu/pub_files/pub4/ratliff_nathan_2006_1/ratliff_nathan_2006_1.pdf\nProceeding : International Conference on Machine Learning (ICML) 2006\n\n---\n\n# 0. Abstract\n\n일반적으로 Supervised learning techniques를 통해 sequential, goal-directed behavior에 대한 imitation learning은 어렵습니다. 이 논문에서는 maximum margin을 이용한 prediction problem에 대해 sequential, goal-directed behavior을 학습하는 것을 다룹니다. 더 구체적으로 말하자면, 각 features부터 cost까지 한번에 mapping하는 것을 학습해서, 이 cost에 대한 Markov Decision Process(MDP)에서의 optimal policy가 expert's behavior를 모방하도록 하는 것입니다.\n\n또한 inference를 위해 fast algorithms을 이용하여 subgradient method의 기반인 structured maximum margin learning으로서 간단하고 효율적인 접근을 보였습니다. 비록 이러한 fast algorithm technique는 일반적으로 사용하지만, QP formulation의 한계를 벗어난 문제에서는 $A^*$나 Dynamic Programming(DP) 접근들이 policy를 learning하는 것을 다룰 수 있도록 만든다는 것을 보였습니다.\n\n실험에서는 outdoor mobile robot들을 이용하여 route planning에 적용합니다.\n\n<br>\n## 0.1 들어가기에 앞서..\n\n이 논문은 앞서 다뤘던 APP 논문에 더하여 좀 더 효율적인 알고리즘을 만들고자 하였습니다. 그래서 기존에 APP에서 QP를 이용한 SVM 방법에 더하여 Soft Margin term을 추가하여 슬랙변수를 가지는 SVM을 사용하였고, subgradient method를 이용하여 알고리즘을 좀 더 쉽고 빠르게 구할 수 있도록 만들었습니다.\n\nSVM과 Soft Margin SVM에 대해 모르는 분이 계시다면 아래의 링크를 꼭 보시고 이 논문을 보시는 것을 추천해드립니다!\n\n1) 영상 (KAIST 문일철 교수님 강의)\n  - [Lecture 1 Decision boundary with Margin](https://www.youtube.com/watch?v=hK7vNvyCXWc&list=PLbhbGI_ppZISMV4tAWHlytBqNq1-lb8bz&index=23)\n  - [Lecture 2 Maximizing the Margin](https://www.youtube.com/watch?v=tZy3uRv-9mY&list=PLbhbGI_ppZISMV4tAWHlytBqNq1-lb8bz&index=24)\n  - [Lecture 3 SVM with Matlab](https://www.youtube.com/watch?v=sYLuJ_8Qw3s&list=PLbhbGI_ppZISMV4tAWHlytBqNq1-lb8bz&index=25)\n  - [Lecture 4 Error Handling in SVM](https://www.youtube.com/watch?v=vEivqCo-LiU&list=PLbhbGI_ppZISMV4tAWHlytBqNq1-lb8bz&index=26)\n  - [Lecture 5 Soft Margin with SVM](https://www.youtube.com/watch?v=5jOqc7ByMm4)\n\n2) 블로그 글\n  - [SVM (Support Vector Machine) Part 1](https://gentlej90.tistory.com/43)\n  - [SVM (Support Vector Machine) Part 2](https://gentlej90.tistory.com/44)\n\n<br><br>\n\n# 1. Introduction\n\nImitation Learning에서, learner는 expert의 \"behavior\" or \"control strategy\"를 모방하려고 시도합니다. 이러한 imitation learning을 이용하여, robotics에서의 supervised learning 접근은 featrues부터 decision까지 mapping하는 것을 학습하는 것에서 엄청난 결과로서 사용되어 왔지만, Long-range나 Goal-directed behavior에서는 이러한 기술들을 사용하기 어렵습니다.\n\n<br>\n## 1.1 Perception and Planning\n\nMobile robotics에서는 일반적으로 **perception** subsystem과 **planning** subsystem으로 autonomy software를 partition함으로써 Long-horizon goal directed behavior를 찾습니다.\n1) Perception system은 다양한 model과 환경의 features를 계산합니다.\n2) Planning system은 cost-map을 input으로 두고, 그 input을 통해 minimal risk (cost) path를 계산합니다.\n\n<br>\n## 1.2 In this work\n\n하지만 불행하게도, perception의 model부터 planner에 대한 cost까지 하는 것은 종종 어렵습니다.\n\n따라서 이 논문에서는 새로운 방법을 제시합니다. **perception features부터 planner에 대한 cost까지 (Perception + Planning)** mapping하는 것을 자동화하는 방법입니다. 최종 목표는 features부터 cost function까지 mapping하는 것을 한번에 학습하는 것입니다. 그래서 이러한 cost function에 대한 optimal policy가 expert의 behavior을 한번에 모방하도록 하는 것입니다.\n\n<br>\n## 1.3 Three fold\n\n정리해보면, 이 논문에서는 3가지 중요한 언급이 있습니다.\n\n1. planning을 학습하기위한 새로운 방법을 제시합니다.\n2. sutructured maximum-margin classification으로서 효율적이고 간단한 접근을 제시합니다. 또한 batch setting에서도 linear convergence하다는 것을 보여주며, 다른 QP 기법이 없이도 lerge problems에 \u001d적용될 수 있다고 합니다.\n3. 실험적으로 mobile robotics 관련 문제에 적용하였습니다.\n\n<br><br>\n\n# 2. Preliminaries\n\n<br>\n## 2.1 Notations\n\n### 2.1.1 modeling the planning problem with discrete MDPs\n\n이 논문은 discrete MDPs에서의 planning problem을 modeling합니다.\n\n$\\mathcal{X}$ is state spaces. $x$ index the state spaces.\n\n$\\mathcal{A}$ is action spaces. $a$ index the action spaces.\n\n$p(y|x,a)$ is transition probablities.\n\n$s$ is initial state distribution.\n\n$\\gamma$ is a discount factor on rewards. if any, $\\gamma$ is aborbed into the transition probabilities.\n\nReward $R$은 따로 두지 않고, demonstrated behavior를 모방하는 policies를 만들기 위해 supervised examples로부터 학습됩니다.\n\n$v \\in \\mathcal{V}$ is primal variables of value function\n\n$\\mu \\in \\mathcal{G}$ is **dual state-action frequency counts**. equal to $y$.\n\n- 여기서 $\\mu$는 어떠한 상태에서 어떠한 행동을 취했는 지를 count의 개념으로 나타낸 것인데 $y$와 혼용되어 쓸 수 있습니다.\n\n그리고 여기서는 오직 stationary policies만을 고려합니다. The generalization is straightforward.\n\n#### state-action frequency counts란?\n\nInverse RL의 궁극적인 목표를 다르게 말해보면 expert가 어떠한 행동을 했을 때, 여기서의 state-action visitation frequency counts(줄여서 visitation frequency)를 구합니다. 그리고 우리의 목적은 expert의 visitation frequency과 최대한 비슷한 visitation frequency를 만들어내는 어떤 reward를 찾는 것입니다.\n\n일반적으로, RL 문제는 reward가 주어졌을 때, 이 reward의 expected sum을 최대로 하는 policy를 찾는다고 알려져 있습니다. 그런데 이 문제의 dual problem은 visitation frequency를 찾는 것입니다.\n\n다시 말해 optimal policy와 optimal visitation frequency counts는 1:1 관계라는 것입니다.\n\n### 2.1.2 The input to our algorithm\n\nThe input is a set of training instances\n\n<center> <img src=\"../../../../img/irl/mmp_2.png\" width=\"300\"> </center>\n\n$p_i$ is transition probablities.\n\nState-action pairs $(x_i, a_i) \\in \\mathcal{X}_i, \\mathcal{A}_i$ is $d \\times |\\mathcal{X}||\\mathcal{A}|$ Feature matrix (or mapping) $F_i \\in \\mathbb{R}^{d \\times |\\mathcal{X}||\\mathcal{A}|}$.\n\n$y_i$ is expert's demonstration (desired trajectory or full policy). equal to **dual state-action frequency counts** $\\mu_i$.\n\n$f_i (y)$ denote vector of expected feature counts $F_i \\mu$ of the $i$th example.\n\n$\\mathcal{L}_i$ : Some additional loss function (heuristic)\n\n### 2.1.3 Detail description\n\n$\\mu_i^{x, a}$ is the expected state-action frequency for state $x$ and action $a$ of example $i$.\n\n- $\\mu_i$를 자세히보면 $\\mu$에 $i$가 붙은 형태로 되어있습니다. 여기서 example $i$라는 것 총 trajectory(or path)의 length 중에서 하나의 index를 말하는 것입니다. expert의 경우, 보통 전체 trajectory를 한꺼번에 취하기 때문에 이 논문에서는 구별하기 위해 $i$라는 notation을 쓴 것입니다.\n\n$\\mathcal{D} = \\\\{(\\mathcal{X}_i, \\mathcal{A}_i, p_i, f_i, y_i, \\mathcal{L}_i)\\\\} \\equiv \\\\{(\\mathcal{X}_i, \\mathcal{A}_i, \\mathcal{G}_i, F_i, \\mathcal{\\mu}_i, \\mathcal{l}_i)\\\\}$ \n\n- ($\\mathcal{D}$는 $\\mathcal{D}_{i=1}^n$)\n\nLoss function is $\\mathcal{L} : \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}_+$ between solutions. $\\mathcal{L} (y_i, y) = \\mathcal{L}_i (y) = l_i^T \\mu$. \n\n- $l_i \\in \\mathbb{R}_+^{|\\mathcal{X}||\\mathcal{A}|}$\n- $l_i = 1 -$ expert's visitation frequency\n\n### 2.1.4 그래서 이 논문에서 하고 싶은 것 (중요)\n\nThe best policy over the resulting reward function $\\mu^* = arg\\max_{\\mu \\in \\mathcal{G}_i} w^T F_i \\mu$ is “**close**” to the expert's demonstrated policy $\\mu_i$.\n\n<br>\n## 2.2 Loss function\n\n위에서 말했던 loss function은 teacher가 아닌 learner가 방문한 states의 count입니다.\n\n또한 이 논문에서는 teacher가 도달한 어떠한 states에서 teacher와 다른 action들을 고르거나 teacher가 선택하지 않은 states를 도달하는 것을 penalizing할 것입니다.\n\n끝으로, $\\mathcal{L}(y_i, y) \\geq 0$을 가정합니다.\n\n<br>\n## 2.3 Quadratic Programming Formulation\n\n### 2.3.1 Quadratic Program\n\nGiven a training set :\n\n<center> <img src=\"../../../../img/irl/mmp_2.png\" width=\"300\"> </center>\n\nQuadratic Program is\n\n$$\\min_{w, \\zeta_i} \\frac{1}{2} \\parallel w \\parallel^2 + \\frac{\\gamma}{n} \\sum_i \\beta_i \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (1)$$\n\n$$s.t. \\,\\, \\forall i \\,\\,\\,\\, w^T f_i (y_i) \\geq \\max_{\\mu \\in \\mathcal{G}_i} (w^T f_i (y) + \\mathcal{L} (y_i, y)) - \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\, (2)$$\n\ns.t.로 나오는 constraint의 intuition을 보자면, expert's policies가 margin에 대해 다른 모든 policies보다 더 높은 experted reward를 가지도록 하는 only weight vectors를 찾자는 것입니다.\n\n다음으로 위의 수식에 나오는 notation에 대해서 자세히 알아보겠습니다.\n\n$\\frac{\\gamma}{n}\\sum_i \\zeta_i$ is **soft margin term** from soft margin SVM. $\\zeta$의 값을 되도록 최소화하여 오분류의 허용도를 낮추기 위해 추가되었습니다.\n\n$\\zeta_i$ is **slack variable**. The slack variable permit violations of these constraints for a penalty. 여유 변수라고도 하고, $\\zeta_i$만큼의 오류를 인정한다는 의미로 볼 수 있습니다.\n\n$\\gamma \\geq 0$ is scaled for a penalty. 보통 $c$ ($c \\geq 0$)라고도 하는데, 최소화 조건을 반영하는 정도를 결정하는 값으로 우리가 적절히 정해주어야 합니다. c값이 크면 전체 margin도 커지므로 오분류를 적게 허용(엄격)한다는 뜻이고, 반대로 c값이 작으면 margin이 작아지므로 비교적 높은 오분류를 허용(관대)한다는 뜻입니다.\n\n$\\beta_i \\geq 0$는 example들이 다른 length일 때 normalization하기 위해서 사용되는 data dependent scalars입니다.\n\n$w^T f_i(y_i)$ is expert's reward.\n\n$w^T f_i(y)$ is other's reward.\n\n$\\mathcal{L} (y_i, y)$는 $y_i$와 $y$가 일치하지 않는 상태의 수입니다.\n\n### 2.3.2 Maximum Margin Problem(MMP)\n\n만약 $f_i (\\cdot)$ and $\\mathcal{L}_i (\\cdot)$가 state-action frequencies $\\mu$에서 linear하다면, MMP는 다음과 같이 정의할 수 있습니다.\n\n$$\\min_{w, \\zeta_i} \\frac{1}{2} \\parallel w \\parallel^2 + \\frac{\\gamma}{n} \\sum_i \\beta_i \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (3)$$\n\n$$s.t. \\,\\, \\forall i \\,\\,\\,\\, w^T F_i \\mu_i \\geq \\max_{\\mu \\in \\mathcal{G}_i} (w^T F_i \\mu + l_i^T \\mu) - \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (4)$$\n\n그리고 수식 (4)에서, $\\mu \\in \\mathcal{G}_i$를 Bellman-flow constraints로 표현할 수 있습니다. 다시 말해 $\\mu \\geq 0$는 다음을 만족합니다. ($\\mu$의 성질)\n\n<center> <img src=\"../../../../img/irl/mmp_3.png\" width=\"450\"> </center>\n\n- 나중에 GAIL 논문에서도 나오겠지만, 위의 수식으로 $\\mu$인 visitation frequency를 정할 수 있습니다.\n\n### 2.3.3 One compact quadratic program for MMP\n\n이어서 수식 (4)에서 nonlinear, convex constraints는 오른쪽 side의 dual을 계산함으로써 linear constraints의 compact set으로 다음과 같이 변형될 수 있습니다.\n\n$$\\forall i \\,\\,\\,\\, w^T F_i \\mu_i \\geq \\min_{v \\in V_i} \\, (s_i^T v) - \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (5)$$\n\n그리고 수식 (5)에서, $v \\in V_i$는 Bellman primal constraints을 만족하는 value function입니다. Bellman primal constraints는 다음과 같습니다.\n\n$$\\forall i,x,a \\,\\,\\,\\, v^x \\geq (w^T F_i + l_i)^{x,a} + \\sum_{x'} p_i (x'|x,a) v^{x'} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (6)$$\n\n위의 constraints (5), (6)을 combining함으로써 최종적으로 다음과 같이 쓸 수 있습니다.\n\nOne compact quadratic program is\n\n$$\\min_{w, \\zeta_i} \\frac{1}{2} \\parallel w \\parallel^2 + \\frac{\\gamma}{n} \\sum_i \\beta_i \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (7)$$\n\n$$\\forall i \\,\\,\\,\\, w^T F_i \\mu_i \\geq \\min_{v \\in V_i} \\, (s_i^T v) - \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (8)$$\n\n$$\\forall i,x,a \\,\\,\\,\\, v_i^x \\geq (w^T F_i + l_i)^{x,a} + \\sum_{x'} p_i (x'|x,a) v_i^{x'} \\,\\,\\,\\,\\,\\,\\,\\, (9)$$\n\n위의 수식은 MMP 문제를 One compact quadratic program으로써 풀 수 있도록 만든 것입니다. 하지만 아쉽게도 위의 constraints의 수는 state-action pairs와 training examples에 대해 linear하게 scaling됩니다.\n\n이러한 program을 직접적으로 optimize할 수 있도록 이미 만들어져 있는 QP software가 있지만, 뒤이어 나오는 **section 3** 에서 **subgradient methods** 의 이용함으로써 One compact quadratic program에 대해 크게 향상시킬 수 있는 다른 alternative formulation을 이용하고자 합니다.\n\n추가적으로, **Section 4** 에서는 최종 objective function에 유용한 방법들을 생각해 볼 것입니다. 그리고 나서 path planning problems에 대한 적절한 examples를 말할 것입니다.\n\n<br><br>\n\n# 3. Efficient Optimization\n\n실제로, 수식 (9)에서의 quadratic program을 해결하는 것은 적어도 single MDP의 linear programming을 해결하는 것만큼 어렵습니다. 그래서 수식 (9)를 quadratic program으로 해결하려는 것이 적절한 전략이 될 수 있지만, 다르게 보면 많은 문제들에 대해 policy iteration과 $A^*$ algorithm처럼 이론적으로나 실험적으로나 더 빠르게 해결할 수 있도록 특별하게 design된 algorithm이 존재한다는 것으로 볼 수 있습니다.\n\n따라서 이 논문에서는 더 나아가 **fast maximization algorithm을 사용하는 iterative method 기반인 subgradient method로 접근합니다.**\n\n<br>\n## 3.1 Objective function\n\n첫 번째 step은 optimization program(One compact quadratic program)을 \"hinge-loss\" form으로 변형하는 것입니다.\n\n변형된 objective function은 다음과 같습니다.\n\n$$c_q(w) = \\frac{1}{n} \\sum_{i=1}^n \\beta_i \\Big( \\big\\\\{\\max_{\\mu \\in \\mathcal{G}_i} (w^T F_i + l_i^T)\\mu \\big\\\\} - w^T F_i \\mu_i\\Big) + \\frac{1}{2} \\parallel w \\parallel^2 \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (10)$$\n\nhinge-loss 관점에서 보면, 위의 수식에서 $\\big\\\\{\\max_{\\mu \\in \\mathcal{G}_i} (w^T F_i + l_i^T)\\mu\\big\\\\} - w^T F_i \\mu_i$은 slack variable인 $\\zeta_i$과 동일합니다.\n\n또한 기존에 있던 $\\gamma$는 slack variable $\\zeta_i$가 없어졌기 때문에 사라집니다.\n\n위의 objective function은 convex하지만, **max term**이 있기 때문에 미분이 불가능합니다. 따라서 이 논문에서는 subgradient method라 불리는 gradient descent의 generalization을 이용함으로써 optimization을 합니다.\n\nconvex function $c : \\mathcal{W} \\rightarrow \\mathbb{R}$의 subgradient는 vector $g$로 정의합니다.\n\n<center> <img src=\"../../../../img/irl/mmp_4.png\" width=\"400\"> </center>\n\n위의 수식에서 subgradient는 비록 미분 가능한 점에서 필연적으로 gradient와는 같지만, unique할 필요는 없습니다.\n\n### 3.1.1 Subgradient Method란?\n\n아래의 링크를 참고해주시면 감사하겠습니다.\n\n1) [Wikipedia - Subgradient method](https://en.wikipedia.org/wiki/Subgradient_method)\n2) [모두를 위한 컨벡스 최적화 - Subgradient](https://wikidocs.net/18963)\n3) [모두를 위한 컨벡스 최적화 - Subgradient Method](https://wikidocs.net/18953)\n\n<br>\n## 3.2 Four well known properties for subgradient method\n\n최종 objective function을 보기전에, 먼저 $c(w)$의 subgradient를 계산하기 위해, subgradient에 대해 잘 알려진 4가지 속성들에 대해서 알아봅시다. (**3번 중요**)\n1) Subgradient operators are linear.\n2) The gradient is the unique subgradient of a differentiable function.\n3) Denoting $y^∗ = arg\\max_y [f (x, y)]$ for differentiable $f (., y)$, $\\nabla_x f(x,y∗)$ is a subgradient of the piecewise(구분적으로, 구간적으로) differentiable convex function $\\max_y [f (x, y)]$.\n4) An analogous chain rule holds as expected.\n\n3번을 보면, 결국 subgradient method를 통해 하고 싶은 것은 piecewise differentiable convex function인 $f(x,y)$ 중에서 제일 큰 $\\max_y [f (x, y)]$를 subgradient로 구해서, 그 중 가장 큰 값인 $y^∗ = arg\\max_y [f (x, y)]$를 통해 $\\nabla_x f(x,y∗)$를 하겠다는 것입니다.\n\n<br>\n## 3.3 A subgradient method for objective function\n\nWe are now equipped to compute a subgradient $g_w \\in \\partial c(w)$ of our objective function (10):\n\n$$g_w = \\frac{1}{n} \\sum_{i=1}^n \\beta_i \\big( (w^T F_i + l_i^T)\\mu^* - w^T F_i \\mu_i \\big) \\cdot F_i \\Delta^w \\mu_i + \\lambda w \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (12)$$\n\n위의 수식에서 detail한 notation은 다음과 같습니다.\n\n$$\\mu^* = arg \\max_{\\mu \\in \\mathcal{G}} (w^T F_i + l_i^T)\\mu \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (12-1)$$\n\n$$\\Delta^w \\mu_i = \\mu^∗ − \\mu_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (12-2)$$\n\n수식 (12-2)를 직관적으로 보면, subgradient가 현재의 reward function $w^T F_i$에 관하여 **optimal policy와 example policy 사이의 state-action visitation frequency count를 비교한다는 점**을 발견할 수 있습니다.\n\n또한 subgradient를 계산하는 것은 $\\mu^* = arg \\max_{\\mu \\in \\mathcal{G}} (w^T F_i + l_i^T)\\mu$을 해결하는 것과 같습니다. 다시 말해 reward function $w^T F_i + l_i^T$를 해결한다는 것입니다.\n\n<br>\n## 3.4 Algorithm 1. Max Margin Planning\n\n<center> <img src=\"../../../../img/irl/mmp_5.png\" width=\"500\"> </center>\n\n- 5: loss augmented cost map $(w^T F_i + l_i^T)$에 대해서 각각의 input map에 대한 optimal policy $\\mu^*$와 state-action visitation frequencies $\\mu^i$를 계산합니다. 처음에는 w가 0에서 시작하므로 loss augmented cost map $w^T F_i + l_i^T$은 $l_i^T$로 시작하게 됩니다.\n- 6: 수식 (12)에 있는 objective function $g$를 계산합니다.\n- 7: w를 $\\alpha_t g$에 따라 minimize합니다.\n- 8: Option으로 추가적인 constraints를 둘 수도 있습니다. 자세한 내용은 section 4.4인 Incorporating Prior Knowledge를 참고하시기 바랍니다.\n- No RL step!\n\n<br><br>\n\n# 4. Additional section\n\n이전까지는 최종 objective function과 algorithm을 살펴봤습니다. 여기서 더 나아가 유용한 방법들을 통해 우리의 objective function과 algorithm이 더 robust하도록 만들어봅시다.\n\n<br>\n## 4.1 Guarantees in the Batch Setting\n\nsubgradient method로 구성된 algorithm들의 잘 연구된 class 중 하나는 batch setting으로 둘 수 있다는 것입니다.\n\nbatch setting에는 두 가지 key point가 존재합니다.\n1) 이 method에서 step-size sequence $\\\\{ \\alpha_t \\\\}$의 선택은 상당히 중요합니다. $\\\\{ \\alpha_t \\\\}$에 따라서 convergence guarantee가 달라집니다.\n2) 우리의 결과는 objective function을 유지하기 위해 strong convexity assumption이 필요합니다.\n따라서 Given G$\\mathcal{W} \\subseteq \\mathbb{R}^d$, a function $f: \\mathcal{W} \\rightarrow \\mathbb{R}$ is $\\eta$-strongly convex if there exists $g: \\mathcal{W} \\rightarrow \\mathbb{R}$ such that for all $w$, $w' \\in \\mathcal{W}$:\n\n<center> <img src=\"../../../../img/irl/mmp_6.png\" width=\"500\"> </center>\n\n**Theorem 1. Linear convergence of constant stepsize sequence.** Let the stepsize sequence $\\\\{ \\alpha_t \\\\}$ of Algorithm (1) be chosen as $\\alpha_t = \\alpha \\leq \\frac{1}{\\lambda}$. Furthermore, assume for a particular region of radius $R$ around the minimum, $\\forall w,g \\in \\partial c(w), ||g|| \\leq C$. Then the algorithm converges at a linear rate to a region of some minimum point $x^*$ of $c$ bounded by\n\n$$||x_{min} - x^*|| \\leq \\sqrt{\\frac{a C^2}{\\lambda}} \\leq \\frac{C}{\\lambda}$$\n\n<center> <img src=\"../../../../img/irl/mmp_7.png\" width=\"450\"> </center>\n\nTheorem 1은 우리가 충분히 작고 일정한 stepsize를 통해 linear convergence rate를 얻을 수 있다는 것을 보여줍니다. 그러나 이 convergence는 오직 minimum 주변 지역에서만 가능합니다.\n\n대안적으로, 우리는 $t \\geq 1$에 대해 $\\alpha_t = \\frac{r}{t}$ 형태의 감소하는 step size rule을 고를 수 있습니다. 여기서 $r$은 learning rate로 생각할 수 있는 some positive constant입니다.\n\n이러한 rule을 통해, Algorithm 1은 minimum에서 convergence가 보장되지만, 위에서 말했던 strong convexity assumption에서만 오직 sublinear rate로 수렴될 수 있습니다.\n\n<br>\n## 4.2 Optimization in an Online Setting\n\n다양한 optimization techniques와 다르게, subgradient method는 batch setting에서 더 확장됩니다.\n\nonline setting에서는 적절하게 관련된 domain에 대한 다양한 planning problem들을 생각해볼 수 있습니다. 특히, 그 중 하나는 path를 plan하기 위해 필요로 하는 domain을 제공하는 것입니다. 더 정확하게는 \"correct\" path를 제공하는 것입니다.\n\nAt each time step $i$:\n1) We observe $\\mathcal{G}_i$ and $F_i$.\n2) Select a weight vector $w_i$ and using this compute a resulting path.\n3) Finally we observe the true policy $y_i$.\n\n즉, strongly convex cost function(앞서 다뤘던 수식 (10))이 되기 위해 $c_i(w) = \\frac{1}{2} \\parallel w \\parallel^2 + \\\\{ \\max_{\\mu \\in \\mathcal{G}_i} (w^T F_i + l_i)\\mu\\\\} − w^T F_i \\mu_i$를 정의할 수 있다는 것입니다. 그리고 우리는 $y_i$, $\\mathcal{G}_i$, $F_i$가 주어진다면 계산할 수 있습니다.\n\n정리하자면, 앞서 본 cost function(Equation 10)에서 $\\frac{1}{n}\\sum_{i=1}^n \\beta_i$가 없어진 것과 같이 online setting이 가능하다는 것을 보여줍니다.\n\nThis is now an **online convex programming problem**.\n\n<br>\n## 4.3 Modifications for Acyclic Positive Costs (Think of Worst Case)\n\nacyclic(특정 방향이 없는, 사이클이 없는, 비순환적인) domain의 infinite horizon problems에서, $A^*$와 $A^*$의 변종들은 일반적으로 좋은 paln을 찾기위한 가장 효율적인 방법입니다. 이러한 domain에서는 strictly negative한 reward를 생각해볼 필요가 있습니다(동일하게, cost는 strictly positive). 다시 말해 best case에 대해서만 생각해볼 것이 아니라 worst case에 대해서도 생각해볼 필요가 있다는 것입니다. 이렇게 하지 않으면 infinite reward path가 발생할지도 모르기 때문입니다. 이러한 negativity의 strictness는 heuristic의 존재를 더 확실히게 보장하는 것이라고 볼 수 있습니다.\n\n$F_i \\geq 0$이라고 가정하면, 이러한 negativity의 strictness는 두 가지 방법을 통해 쉽게 구현할 수 있습니다.\n1) w에 component-wise negativity constraints를 추가\n2) 각각의 state-action pair에 대한 보상에 negativity를 부여하는 constraints를 추가\n\n이렇게 negativity를 추가할 수 있는 이유는 reward $w^T F_i \\mu$(or $(w^T F_i + l_i^T) \\mu$)에서 $F_i$이 0보다 크기 때문에, 우리는 $w, \\mu$에 negativity를 추가할 수 있습니다. 1의 경우 단순히 w의 violated component를 0으로 설정하여 구현할 수 있고, 2의 경우 가장 violated constraint를 반복적으로 추정함으로써 효율적으로 구현할 수 있습니다.\n\n<br>\n## 4.4 Incorporating Prior Knowledge\n\n이 section은 앞서 Algorithm 1의 Line 8에서와 말한 것과 같이 Option으로 prior knowledge을 통해 추가적인 constraints를 둘 수 있다는 것을 보여줍니다.\n1) 0 vector 대신에 $w$에 prior belief에 대한 solution을 regularizing하는 것\n2) loss function을 통해 특정한 state-action pairs를 poor choices으로 표시하는 것. algorithm이 이러한 요소로 인하여 large magin을 가지도록 강제합니다.\n3) $w$에 constraint 형태로 domain knowledge를 포함시키는 것 (e.g 특정한 영역의 state를 다른 state보다 cost가 적어도 두 배가 되도록 요구.)\n\n이러한 방법들은 training example의 사용 외에도 learner에게 expert의 knowledge를 전달하는 강력한 방법입니다.\n\n<br><br>\n\n# 5. Experimental Results\n\n<br>\n## 5.1 Demonstration of learning to plan based on satellite color imagery\n\n실험에서는 실제 문제(Path planning)에서 논문 개념을 이용하여 유효성 검증할 것입니다.\n1) Section 4에서 보여주었던 batch learning algorithm을 사용\n2) Regularization을 위한 적당한 값을 사용하고, 위에서 다뤘던 우리의 algorithm을 사용\n\n추가적으로 prior knowledge에서의 첫 번째 방법을 적용한 것으로 보입니다.\n\n같은 맵의 영역에서 시연되는 다른 예제 경로는 학습 후에 hold out 영역에서 상당히 다른 결과를 이끌었습니다.\n\n<center> <img src=\"../../../../img/irl/mmp_8.png\" width=\"1200\"> </center>\n\n<center> <img src=\"../../../../img/irl/mmp_9.png\" width=\"600\"> </center>\n\n- 실험 의도\n  - Top : Road에 유지하도록 제안\n  - Bottom : 은밀한 의도를 제시(여기서는 숲을 지나는 의도를 의미)\n\n- 실험 결과\n  - Left : Training 예제\n  - Middle : Training 이후에 hold out 영역에서 학습된 cost map\n  - Right : Hold out 영역에서 $A^*$를 이용하여 생성된 행동 결과\n\n<br>\n## 5.2 Data shown are MMP learned cost maps\n\n<center> <img src=\"../../../../img/irl/mmp_10.png\" width=\"1100\"> </center>\n\nFigure 2은 holdout region으로부터의 결과입니다.\n\n그림에서 loss-augmented path (blue)은 일반적으로 마지막 학습된 경로보다 일반적으로 좋은 결과를 수행하지 못하는 것을 나타났습니다.\n\n왜냐하면 loss-augmentation은 높은 loss의 영역을 최종 학습 지도보다 더욱 desirable하게 만들기 때문입니다. \n\n직관적으로, 만약 학습자가 loss-augmented cost map에 대해서 잘 수행할 수 있다면, loss-augmentation없이도 더욱 잘 수행되어야 한다는 것입니다. 이것은 margin을 가지고 학습된 개념입니다.\n\n<br>\n## 5.3 Results using two alternative approaches\n\n<center> <img src=\"../../../../img/irl/mmp_11.png\" width=\"800\"> </center>\n\n- Left : the result of a next-action classifier applied superimposed on a visualization of the second dataset.\n- Right : a cost map learned by manual training of a regression.\n\n두 개의 경우에서 학습된 경로들은 poor approximations. (not shown on left, red on right).\n\n<br>\n## 5.4 Visualization about losses\n\n<center> <img src=\"../../../../img/irl/mmp_12.png\" width=\"850\"> </center>\n\n- Left : Visualization of inverted Loss function $(1− l(x))$ for a training example path.\n- Right : Comparison of holdout loss of MMP (by number of iterations) to a regression method where a teacher hand-labeled costs.\n\n비교를 위해, 저자는 MMP에 다른 두 개의 접근방법을 사용하여 유사한 학습을 시도하였습니다.\n\n1. (Lecun et al., 2006)$^5$에서 제안한 방법으로 state 특징들을 다음 action으로 취하는 mapping을 사용한 직접적으로 학습하는 알고리즘입니다. 이 경우, traing data에 대해 좋은 결과를 얻지 못했습니다.\n\n2. 다소 더 성공적은 시도는 직접 label을 통해 cost를 학습시킨 알고리즘입니다. 이 알고리즘은 MMP보다 학습자에게 더 많은 명시적 정보를 제공합니다.\n   - 다음을 기반하여 low, medium, high cost로 제공\n     1. Expert knowledge of the planner\n     2. Iterated training and observation\n     3. The trainer had prior knowledge of the cost maps found under MMP batch learning on this dataset.\n   - 추가 정보가 주어진 cost map은 정성적으로 올바른 것처럼 보이지만, 그림 3과 그림 4는 상당히 좋지 않은 성능을 보여줍니다..\n\n<br><br>\n\n# 6. Related and Future Work\n\nMaximum Margin Planning과 직접적으로 연관된 두 가지 work가 있습니다.\n\n그 중 하나가 바로 Inverse RL입니다.\n\nIRL의 목표는 MDP에서 agent의 행동을 관찰하는 하여 agent의 행동으로 부터 reward function를 추출하는 것입니다. 그러나 이것은 기본적으로 ill-posed 문제로 알고 있습니다. 그럼에도 불구하고, MMP와 같이 유사한 효과를 가진 IRL 아이디어들을 시도한 몇 가지 heuristic 시도가 있었습니다.\n\n유용한 heuristic 방법은 바로 이전 논문인 *Abbeel, P., & Ng, A. Y. (2004). Apprenticeship learning via inverse reinforcement learning* APP 논문입니다. 학습자의 정책과 시연되는 예제간의 expected feature counts을 통해 매칭을 시도하는 학습 방법입니다.\n\nMMP의 경우 IRL algorithm의 variant과는 다른 스타일의 algorithm입니다.\n\nMMP는 하나의 MDP 보다 많은 정책 시연들을 허용하도록 설계되어 있습니다. 여러 특징 맵, 다른 시작 지점과 목표 지점, 완전히 새로운 맵과 목표 지점을 가지는 학습자의 목표를 이용하여 예제들을 시연했습니다. 또한 MMP는 상당히 다른 IRL적 알고리즘 접근 방법을 유도합니다.\n\nIRL과 MMP간의 관계는 **generative and discriminative learning** 간의 구별을 연상시킵니다.\n\n일반적인 IRL의 경우, feature matching을 시도합니다. agent가 MDP에서 (거의 최적같이) 행동하고 (거의) feature expectation에 매칭 가능할 때 학습하도록 설계되었습니다(Generative models과 같은 strong 가정). 예를 들어, feature expecatation을 매칭하는 능력은 algorithm의 행동이 feature가 선형인 모든 cost function에 대해서 near-optimal일 것이라는 것을 의미합니다.\n\n반대로 MMP의 경우, 우리의 목표가 직접적으로 output 행동을 모방하는 것이라는 weaker 가정을 하고 실제 MDP나 reward 함수에 대해 agnostic합니다. 여기서 MDP는 output decision들을 구조화하고 경쟁을 하려고 하는 expert가 natual class을 제공합니다.\n\n정리하자면,\nGenerative model : 개별 클래스의 분포를 모델링한다.\nDiscriminative model : Discriminative 모델은 기본 확률 분포 또는 데이터 구조를 모델링하지 않고 기본 데이터를 해당 클래스에 직접 매핑(class 경계를 통해 학습). SVM은 이러한 기준을 만족 시키므로 decision tree와 마찬가지로 discriminative model.\n\n<br><br>\n\n# 처음으로\n\n## [Let's do Inverse RL Guide](https://reinforcement-learning-kr.github.io/2019/01/22/0_lets-do-irl-guide/)\n\n<br>\n\n# 이전으로\n\n## [APP 여행하기]()\n\n<br>\n\n# 다음으로\n\n## [MaxEnt 여행하기]()","slug":"3_mmp","published":1,"updated":"2019-02-07T11:21:31.938Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjrujlu0z001i5wfeqx7nxq57","content":"<center> <img src=\"../../../../img/irl/mmp_1.png\" width=\"850\"> </center>\n\n<p>Author : Nathan D. Ratliff, J. Andrew Bagnell, Martin A. Zinkevich<br>Paper Link : <a href=\"https://www.ri.cmu.edu/pub_files/pub4/ratliff_nathan_2006_1/ratliff_nathan_2006_1.pdf\" target=\"_blank\" rel=\"noopener\">https://www.ri.cmu.edu/pub_files/pub4/ratliff_nathan_2006_1/ratliff_nathan_2006_1.pdf</a><br>Proceeding : International Conference on Machine Learning (ICML) 2006</p>\n<hr>\n<h1 id=\"0-Abstract\"><a href=\"#0-Abstract\" class=\"headerlink\" title=\"0. Abstract\"></a>0. Abstract</h1><p>일반적으로 Supervised learning techniques를 통해 sequential, goal-directed behavior에 대한 imitation learning은 어렵습니다. 이 논문에서는 maximum margin을 이용한 prediction problem에 대해 sequential, goal-directed behavior을 학습하는 것을 다룹니다. 더 구체적으로 말하자면, 각 features부터 cost까지 한번에 mapping하는 것을 학습해서, 이 cost에 대한 Markov Decision Process(MDP)에서의 optimal policy가 expert’s behavior를 모방하도록 하는 것입니다.</p>\n<p>또한 inference를 위해 fast algorithms을 이용하여 subgradient method의 기반인 structured maximum margin learning으로서 간단하고 효율적인 접근을 보였습니다. 비록 이러한 fast algorithm technique는 일반적으로 사용하지만, QP formulation의 한계를 벗어난 문제에서는 $A^*$나 Dynamic Programming(DP) 접근들이 policy를 learning하는 것을 다룰 수 있도록 만든다는 것을 보였습니다.</p>\n<p>실험에서는 outdoor mobile robot들을 이용하여 route planning에 적용합니다.</p>\n<p><br></p>\n<h2 id=\"0-1-들어가기에-앞서\"><a href=\"#0-1-들어가기에-앞서\" class=\"headerlink\" title=\"0.1 들어가기에 앞서..\"></a>0.1 들어가기에 앞서..</h2><p>이 논문은 앞서 다뤘던 APP 논문에 더하여 좀 더 효율적인 알고리즘을 만들고자 하였습니다. 그래서 기존에 APP에서 QP를 이용한 SVM 방법에 더하여 Soft Margin term을 추가하여 슬랙변수를 가지는 SVM을 사용하였고, subgradient method를 이용하여 알고리즘을 좀 더 쉽고 빠르게 구할 수 있도록 만들었습니다.</p>\n<p>SVM과 Soft Margin SVM에 대해 모르는 분이 계시다면 아래의 링크를 꼭 보시고 이 논문을 보시는 것을 추천해드립니다!</p>\n<p>1) 영상 (KAIST 문일철 교수님 강의)</p>\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=hK7vNvyCXWc&amp;list=PLbhbGI_ppZISMV4tAWHlytBqNq1-lb8bz&amp;index=23\" target=\"_blank\" rel=\"noopener\">Lecture 1 Decision boundary with Margin</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=tZy3uRv-9mY&amp;list=PLbhbGI_ppZISMV4tAWHlytBqNq1-lb8bz&amp;index=24\" target=\"_blank\" rel=\"noopener\">Lecture 2 Maximizing the Margin</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=sYLuJ_8Qw3s&amp;list=PLbhbGI_ppZISMV4tAWHlytBqNq1-lb8bz&amp;index=25\" target=\"_blank\" rel=\"noopener\">Lecture 3 SVM with Matlab</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=vEivqCo-LiU&amp;list=PLbhbGI_ppZISMV4tAWHlytBqNq1-lb8bz&amp;index=26\" target=\"_blank\" rel=\"noopener\">Lecture 4 Error Handling in SVM</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=5jOqc7ByMm4\" target=\"_blank\" rel=\"noopener\">Lecture 5 Soft Margin with SVM</a></li>\n</ul>\n<p>2) 블로그 글</p>\n<ul>\n<li><a href=\"https://gentlej90.tistory.com/43\" target=\"_blank\" rel=\"noopener\">SVM (Support Vector Machine) Part 1</a></li>\n<li><a href=\"https://gentlej90.tistory.com/44\" target=\"_blank\" rel=\"noopener\">SVM (Support Vector Machine) Part 2</a></li>\n</ul>\n<p><br><br></p>\n<h1 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h1><p>Imitation Learning에서, learner는 expert의 “behavior” or “control strategy”를 모방하려고 시도합니다. 이러한 imitation learning을 이용하여, robotics에서의 supervised learning 접근은 featrues부터 decision까지 mapping하는 것을 학습하는 것에서 엄청난 결과로서 사용되어 왔지만, Long-range나 Goal-directed behavior에서는 이러한 기술들을 사용하기 어렵습니다.</p>\n<p><br></p>\n<h2 id=\"1-1-Perception-and-Planning\"><a href=\"#1-1-Perception-and-Planning\" class=\"headerlink\" title=\"1.1 Perception and Planning\"></a>1.1 Perception and Planning</h2><p>Mobile robotics에서는 일반적으로 <strong>perception</strong> subsystem과 <strong>planning</strong> subsystem으로 autonomy software를 partition함으로써 Long-horizon goal directed behavior를 찾습니다.<br>1) Perception system은 다양한 model과 환경의 features를 계산합니다.<br>2) Planning system은 cost-map을 input으로 두고, 그 input을 통해 minimal risk (cost) path를 계산합니다.</p>\n<p><br></p>\n<h2 id=\"1-2-In-this-work\"><a href=\"#1-2-In-this-work\" class=\"headerlink\" title=\"1.2 In this work\"></a>1.2 In this work</h2><p>하지만 불행하게도, perception의 model부터 planner에 대한 cost까지 하는 것은 종종 어렵습니다.</p>\n<p>따라서 이 논문에서는 새로운 방법을 제시합니다. <strong>perception features부터 planner에 대한 cost까지 (Perception + Planning)</strong> mapping하는 것을 자동화하는 방법입니다. 최종 목표는 features부터 cost function까지 mapping하는 것을 한번에 학습하는 것입니다. 그래서 이러한 cost function에 대한 optimal policy가 expert의 behavior을 한번에 모방하도록 하는 것입니다.</p>\n<p><br></p>\n<h2 id=\"1-3-Three-fold\"><a href=\"#1-3-Three-fold\" class=\"headerlink\" title=\"1.3 Three fold\"></a>1.3 Three fold</h2><p>정리해보면, 이 논문에서는 3가지 중요한 언급이 있습니다.</p>\n<ol>\n<li>planning을 학습하기위한 새로운 방법을 제시합니다.</li>\n<li>sutructured maximum-margin classification으로서 효율적이고 간단한 접근을 제시합니다. 또한 batch setting에서도 linear convergence하다는 것을 보여주며, 다른 QP 기법이 없이도 lerge problems에 \u001d적용될 수 있다고 합니다.</li>\n<li>실험적으로 mobile robotics 관련 문제에 적용하였습니다.</li>\n</ol>\n<p><br><br></p>\n<h1 id=\"2-Preliminaries\"><a href=\"#2-Preliminaries\" class=\"headerlink\" title=\"2. Preliminaries\"></a>2. Preliminaries</h1><p><br></p>\n<h2 id=\"2-1-Notations\"><a href=\"#2-1-Notations\" class=\"headerlink\" title=\"2.1 Notations\"></a>2.1 Notations</h2><h3 id=\"2-1-1-modeling-the-planning-problem-with-discrete-MDPs\"><a href=\"#2-1-1-modeling-the-planning-problem-with-discrete-MDPs\" class=\"headerlink\" title=\"2.1.1 modeling the planning problem with discrete MDPs\"></a>2.1.1 modeling the planning problem with discrete MDPs</h3><p>이 논문은 discrete MDPs에서의 planning problem을 modeling합니다.</p>\n<p>$\\mathcal{X}$ is state spaces. $x$ index the state spaces.</p>\n<p>$\\mathcal{A}$ is action spaces. $a$ index the action spaces.</p>\n<p>$p(y|x,a)$ is transition probablities.</p>\n<p>$s$ is initial state distribution.</p>\n<p>$\\gamma$ is a discount factor on rewards. if any, $\\gamma$ is aborbed into the transition probabilities.</p>\n<p>Reward $R$은 따로 두지 않고, demonstrated behavior를 모방하는 policies를 만들기 위해 supervised examples로부터 학습됩니다.</p>\n<p>$v \\in \\mathcal{V}$ is primal variables of value function</p>\n<p>$\\mu \\in \\mathcal{G}$ is <strong>dual state-action frequency counts</strong>. equal to $y$.</p>\n<ul>\n<li>여기서 $\\mu$는 어떠한 상태에서 어떠한 행동을 취했는 지를 count의 개념으로 나타낸 것인데 $y$와 혼용되어 쓸 수 있습니다.</li>\n</ul>\n<p>그리고 여기서는 오직 stationary policies만을 고려합니다. The generalization is straightforward.</p>\n<h4 id=\"state-action-frequency-counts란\"><a href=\"#state-action-frequency-counts란\" class=\"headerlink\" title=\"state-action frequency counts란?\"></a>state-action frequency counts란?</h4><p>Inverse RL의 궁극적인 목표를 다르게 말해보면 expert가 어떠한 행동을 했을 때, 여기서의 state-action visitation frequency counts(줄여서 visitation frequency)를 구합니다. 그리고 우리의 목적은 expert의 visitation frequency과 최대한 비슷한 visitation frequency를 만들어내는 어떤 reward를 찾는 것입니다.</p>\n<p>일반적으로, RL 문제는 reward가 주어졌을 때, 이 reward의 expected sum을 최대로 하는 policy를 찾는다고 알려져 있습니다. 그런데 이 문제의 dual problem은 visitation frequency를 찾는 것입니다.</p>\n<p>다시 말해 optimal policy와 optimal visitation frequency counts는 1:1 관계라는 것입니다.</p>\n<h3 id=\"2-1-2-The-input-to-our-algorithm\"><a href=\"#2-1-2-The-input-to-our-algorithm\" class=\"headerlink\" title=\"2.1.2 The input to our algorithm\"></a>2.1.2 The input to our algorithm</h3><p>The input is a set of training instances</p>\n<center> <img src=\"../../../../img/irl/mmp_2.png\" width=\"300\"> </center>\n\n<p>$p_i$ is transition probablities.</p>\n<p>State-action pairs $(x_i, a_i) \\in \\mathcal{X}_i, \\mathcal{A}_i$ is $d \\times |\\mathcal{X}||\\mathcal{A}|$ Feature matrix (or mapping) $F_i \\in \\mathbb{R}^{d \\times |\\mathcal{X}||\\mathcal{A}|}$.</p>\n<p>$y_i$ is expert’s demonstration (desired trajectory or full policy). equal to <strong>dual state-action frequency counts</strong> $\\mu_i$.</p>\n<p>$f_i (y)$ denote vector of expected feature counts $F_i \\mu$ of the $i$th example.</p>\n<p>$\\mathcal{L}_i$ : Some additional loss function (heuristic)</p>\n<h3 id=\"2-1-3-Detail-description\"><a href=\"#2-1-3-Detail-description\" class=\"headerlink\" title=\"2.1.3 Detail description\"></a>2.1.3 Detail description</h3><p>$\\mu_i^{x, a}$ is the expected state-action frequency for state $x$ and action $a$ of example $i$.</p>\n<ul>\n<li>$\\mu_i$를 자세히보면 $\\mu$에 $i$가 붙은 형태로 되어있습니다. 여기서 example $i$라는 것 총 trajectory(or path)의 length 중에서 하나의 index를 말하는 것입니다. expert의 경우, 보통 전체 trajectory를 한꺼번에 취하기 때문에 이 논문에서는 구별하기 위해 $i$라는 notation을 쓴 것입니다.</li>\n</ul>\n<p>$\\mathcal{D} = \\{(\\mathcal{X}_i, \\mathcal{A}_i, p_i, f_i, y_i, \\mathcal{L}_i)\\} \\equiv \\{(\\mathcal{X}_i, \\mathcal{A}_i, \\mathcal{G}_i, F_i, \\mathcal{\\mu}_i, \\mathcal{l}_i)\\}$ </p>\n<ul>\n<li>($\\mathcal{D}$는 $\\mathcal{D}_{i=1}^n$)</li>\n</ul>\n<p>Loss function is $\\mathcal{L} : \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}_+$ between solutions. $\\mathcal{L} (y_i, y) = \\mathcal{L}_i (y) = l_i^T \\mu$. </p>\n<ul>\n<li>$l_i \\in \\mathbb{R}_+^{|\\mathcal{X}||\\mathcal{A}|}$</li>\n<li>$l_i = 1 -$ expert’s visitation frequency</li>\n</ul>\n<h3 id=\"2-1-4-그래서-이-논문에서-하고-싶은-것-중요\"><a href=\"#2-1-4-그래서-이-논문에서-하고-싶은-것-중요\" class=\"headerlink\" title=\"2.1.4 그래서 이 논문에서 하고 싶은 것 (중요)\"></a>2.1.4 그래서 이 논문에서 하고 싶은 것 (중요)</h3><p>The best policy over the resulting reward function $\\mu^* = arg\\max_{\\mu \\in \\mathcal{G}_i} w^T F_i \\mu$ is “<strong>close</strong>” to the expert’s demonstrated policy $\\mu_i$.</p>\n<p><br></p>\n<h2 id=\"2-2-Loss-function\"><a href=\"#2-2-Loss-function\" class=\"headerlink\" title=\"2.2 Loss function\"></a>2.2 Loss function</h2><p>위에서 말했던 loss function은 teacher가 아닌 learner가 방문한 states의 count입니다.</p>\n<p>또한 이 논문에서는 teacher가 도달한 어떠한 states에서 teacher와 다른 action들을 고르거나 teacher가 선택하지 않은 states를 도달하는 것을 penalizing할 것입니다.</p>\n<p>끝으로, $\\mathcal{L}(y_i, y) \\geq 0$을 가정합니다.</p>\n<p><br></p>\n<h2 id=\"2-3-Quadratic-Programming-Formulation\"><a href=\"#2-3-Quadratic-Programming-Formulation\" class=\"headerlink\" title=\"2.3 Quadratic Programming Formulation\"></a>2.3 Quadratic Programming Formulation</h2><h3 id=\"2-3-1-Quadratic-Program\"><a href=\"#2-3-1-Quadratic-Program\" class=\"headerlink\" title=\"2.3.1 Quadratic Program\"></a>2.3.1 Quadratic Program</h3><p>Given a training set :</p>\n<center> <img src=\"../../../../img/irl/mmp_2.png\" width=\"300\"> </center>\n\n<p>Quadratic Program is</p>\n<p>$$\\min_{w, \\zeta_i} \\frac{1}{2} \\parallel w \\parallel^2 + \\frac{\\gamma}{n} \\sum_i \\beta_i \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (1)$$</p>\n<p>$$s.t. \\,\\, \\forall i \\,\\,\\,\\, w^T f_i (y_i) \\geq \\max_{\\mu \\in \\mathcal{G}_i} (w^T f_i (y) + \\mathcal{L} (y_i, y)) - \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\, (2)$$</p>\n<p>s.t.로 나오는 constraint의 intuition을 보자면, expert’s policies가 margin에 대해 다른 모든 policies보다 더 높은 experted reward를 가지도록 하는 only weight vectors를 찾자는 것입니다.</p>\n<p>다음으로 위의 수식에 나오는 notation에 대해서 자세히 알아보겠습니다.</p>\n<p>$\\frac{\\gamma}{n}\\sum_i \\zeta_i$ is <strong>soft margin term</strong> from soft margin SVM. $\\zeta$의 값을 되도록 최소화하여 오분류의 허용도를 낮추기 위해 추가되었습니다.</p>\n<p>$\\zeta_i$ is <strong>slack variable</strong>. The slack variable permit violations of these constraints for a penalty. 여유 변수라고도 하고, $\\zeta_i$만큼의 오류를 인정한다는 의미로 볼 수 있습니다.</p>\n<p>$\\gamma \\geq 0$ is scaled for a penalty. 보통 $c$ ($c \\geq 0$)라고도 하는데, 최소화 조건을 반영하는 정도를 결정하는 값으로 우리가 적절히 정해주어야 합니다. c값이 크면 전체 margin도 커지므로 오분류를 적게 허용(엄격)한다는 뜻이고, 반대로 c값이 작으면 margin이 작아지므로 비교적 높은 오분류를 허용(관대)한다는 뜻입니다.</p>\n<p>$\\beta_i \\geq 0$는 example들이 다른 length일 때 normalization하기 위해서 사용되는 data dependent scalars입니다.</p>\n<p>$w^T f_i(y_i)$ is expert’s reward.</p>\n<p>$w^T f_i(y)$ is other’s reward.</p>\n<p>$\\mathcal{L} (y_i, y)$는 $y_i$와 $y$가 일치하지 않는 상태의 수입니다.</p>\n<h3 id=\"2-3-2-Maximum-Margin-Problem-MMP\"><a href=\"#2-3-2-Maximum-Margin-Problem-MMP\" class=\"headerlink\" title=\"2.3.2 Maximum Margin Problem(MMP)\"></a>2.3.2 Maximum Margin Problem(MMP)</h3><p>만약 $f_i (\\cdot)$ and $\\mathcal{L}_i (\\cdot)$가 state-action frequencies $\\mu$에서 linear하다면, MMP는 다음과 같이 정의할 수 있습니다.</p>\n<p>$$\\min_{w, \\zeta_i} \\frac{1}{2} \\parallel w \\parallel^2 + \\frac{\\gamma}{n} \\sum_i \\beta_i \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (3)$$</p>\n<p>$$s.t. \\,\\, \\forall i \\,\\,\\,\\, w^T F_i \\mu_i \\geq \\max_{\\mu \\in \\mathcal{G}_i} (w^T F_i \\mu + l_i^T \\mu) - \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (4)$$</p>\n<p>그리고 수식 (4)에서, $\\mu \\in \\mathcal{G}_i$를 Bellman-flow constraints로 표현할 수 있습니다. 다시 말해 $\\mu \\geq 0$는 다음을 만족합니다. ($\\mu$의 성질)</p>\n<center> <img src=\"../../../../img/irl/mmp_3.png\" width=\"450\"> </center>\n\n<ul>\n<li>나중에 GAIL 논문에서도 나오겠지만, 위의 수식으로 $\\mu$인 visitation frequency를 정할 수 있습니다.</li>\n</ul>\n<h3 id=\"2-3-3-One-compact-quadratic-program-for-MMP\"><a href=\"#2-3-3-One-compact-quadratic-program-for-MMP\" class=\"headerlink\" title=\"2.3.3 One compact quadratic program for MMP\"></a>2.3.3 One compact quadratic program for MMP</h3><p>이어서 수식 (4)에서 nonlinear, convex constraints는 오른쪽 side의 dual을 계산함으로써 linear constraints의 compact set으로 다음과 같이 변형될 수 있습니다.</p>\n<p>$$\\forall i \\,\\,\\,\\, w^T F_i \\mu_i \\geq \\min_{v \\in V_i} \\, (s_i^T v) - \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (5)$$</p>\n<p>그리고 수식 (5)에서, $v \\in V_i$는 Bellman primal constraints을 만족하는 value function입니다. Bellman primal constraints는 다음과 같습니다.</p>\n<p>$$\\forall i,x,a \\,\\,\\,\\, v^x \\geq (w^T F_i + l_i)^{x,a} + \\sum_{x’} p_i (x’|x,a) v^{x’} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (6)$$</p>\n<p>위의 constraints (5), (6)을 combining함으로써 최종적으로 다음과 같이 쓸 수 있습니다.</p>\n<p>One compact quadratic program is</p>\n<p>$$\\min_{w, \\zeta_i} \\frac{1}{2} \\parallel w \\parallel^2 + \\frac{\\gamma}{n} \\sum_i \\beta_i \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (7)$$</p>\n<p>$$\\forall i \\,\\,\\,\\, w^T F_i \\mu_i \\geq \\min_{v \\in V_i} \\, (s_i^T v) - \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (8)$$</p>\n<p>$$\\forall i,x,a \\,\\,\\,\\, v_i^x \\geq (w^T F_i + l_i)^{x,a} + \\sum_{x’} p_i (x’|x,a) v_i^{x’} \\,\\,\\,\\,\\,\\,\\,\\, (9)$$</p>\n<p>위의 수식은 MMP 문제를 One compact quadratic program으로써 풀 수 있도록 만든 것입니다. 하지만 아쉽게도 위의 constraints의 수는 state-action pairs와 training examples에 대해 linear하게 scaling됩니다.</p>\n<p>이러한 program을 직접적으로 optimize할 수 있도록 이미 만들어져 있는 QP software가 있지만, 뒤이어 나오는 <strong>section 3</strong> 에서 <strong>subgradient methods</strong> 의 이용함으로써 One compact quadratic program에 대해 크게 향상시킬 수 있는 다른 alternative formulation을 이용하고자 합니다.</p>\n<p>추가적으로, <strong>Section 4</strong> 에서는 최종 objective function에 유용한 방법들을 생각해 볼 것입니다. 그리고 나서 path planning problems에 대한 적절한 examples를 말할 것입니다.</p>\n<p><br><br></p>\n<h1 id=\"3-Efficient-Optimization\"><a href=\"#3-Efficient-Optimization\" class=\"headerlink\" title=\"3. Efficient Optimization\"></a>3. Efficient Optimization</h1><p>실제로, 수식 (9)에서의 quadratic program을 해결하는 것은 적어도 single MDP의 linear programming을 해결하는 것만큼 어렵습니다. 그래서 수식 (9)를 quadratic program으로 해결하려는 것이 적절한 전략이 될 수 있지만, 다르게 보면 많은 문제들에 대해 policy iteration과 $A^*$ algorithm처럼 이론적으로나 실험적으로나 더 빠르게 해결할 수 있도록 특별하게 design된 algorithm이 존재한다는 것으로 볼 수 있습니다.</p>\n<p>따라서 이 논문에서는 더 나아가 <strong>fast maximization algorithm을 사용하는 iterative method 기반인 subgradient method로 접근합니다.</strong></p>\n<p><br></p>\n<h2 id=\"3-1-Objective-function\"><a href=\"#3-1-Objective-function\" class=\"headerlink\" title=\"3.1 Objective function\"></a>3.1 Objective function</h2><p>첫 번째 step은 optimization program(One compact quadratic program)을 “hinge-loss” form으로 변형하는 것입니다.</p>\n<p>변형된 objective function은 다음과 같습니다.</p>\n<p>$$c_q(w) = \\frac{1}{n} \\sum_{i=1}^n \\beta_i \\Big( \\big\\{\\max_{\\mu \\in \\mathcal{G}_i} (w^T F_i + l_i^T)\\mu \\big\\} - w^T F_i \\mu_i\\Big) + \\frac{1}{2} \\parallel w \\parallel^2 \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (10)$$</p>\n<p>hinge-loss 관점에서 보면, 위의 수식에서 $\\big\\{\\max_{\\mu \\in \\mathcal{G}_i} (w^T F_i + l_i^T)\\mu\\big\\} - w^T F_i \\mu_i$은 slack variable인 $\\zeta_i$과 동일합니다.</p>\n<p>또한 기존에 있던 $\\gamma$는 slack variable $\\zeta_i$가 없어졌기 때문에 사라집니다.</p>\n<p>위의 objective function은 convex하지만, <strong>max term</strong>이 있기 때문에 미분이 불가능합니다. 따라서 이 논문에서는 subgradient method라 불리는 gradient descent의 generalization을 이용함으로써 optimization을 합니다.</p>\n<p>convex function $c : \\mathcal{W} \\rightarrow \\mathbb{R}$의 subgradient는 vector $g$로 정의합니다.</p>\n<center> <img src=\"../../../../img/irl/mmp_4.png\" width=\"400\"> </center>\n\n<p>위의 수식에서 subgradient는 비록 미분 가능한 점에서 필연적으로 gradient와는 같지만, unique할 필요는 없습니다.</p>\n<h3 id=\"3-1-1-Subgradient-Method란\"><a href=\"#3-1-1-Subgradient-Method란\" class=\"headerlink\" title=\"3.1.1 Subgradient Method란?\"></a>3.1.1 Subgradient Method란?</h3><p>아래의 링크를 참고해주시면 감사하겠습니다.</p>\n<p>1) <a href=\"https://en.wikipedia.org/wiki/Subgradient_method\" target=\"_blank\" rel=\"noopener\">Wikipedia - Subgradient method</a><br>2) <a href=\"https://wikidocs.net/18963\" target=\"_blank\" rel=\"noopener\">모두를 위한 컨벡스 최적화 - Subgradient</a><br>3) <a href=\"https://wikidocs.net/18953\" target=\"_blank\" rel=\"noopener\">모두를 위한 컨벡스 최적화 - Subgradient Method</a></p>\n<p><br></p>\n<h2 id=\"3-2-Four-well-known-properties-for-subgradient-method\"><a href=\"#3-2-Four-well-known-properties-for-subgradient-method\" class=\"headerlink\" title=\"3.2 Four well known properties for subgradient method\"></a>3.2 Four well known properties for subgradient method</h2><p>최종 objective function을 보기전에, 먼저 $c(w)$의 subgradient를 계산하기 위해, subgradient에 대해 잘 알려진 4가지 속성들에 대해서 알아봅시다. (<strong>3번 중요</strong>)<br>1) Subgradient operators are linear.<br>2) The gradient is the unique subgradient of a differentiable function.<br>3) Denoting $y^∗ = arg\\max_y [f (x, y)]$ for differentiable $f (., y)$, $\\nabla_x f(x,y∗)$ is a subgradient of the piecewise(구분적으로, 구간적으로) differentiable convex function $\\max_y [f (x, y)]$.<br>4) An analogous chain rule holds as expected.</p>\n<p>3번을 보면, 결국 subgradient method를 통해 하고 싶은 것은 piecewise differentiable convex function인 $f(x,y)$ 중에서 제일 큰 $\\max_y [f (x, y)]$를 subgradient로 구해서, 그 중 가장 큰 값인 $y^∗ = arg\\max_y [f (x, y)]$를 통해 $\\nabla_x f(x,y∗)$를 하겠다는 것입니다.</p>\n<p><br></p>\n<h2 id=\"3-3-A-subgradient-method-for-objective-function\"><a href=\"#3-3-A-subgradient-method-for-objective-function\" class=\"headerlink\" title=\"3.3 A subgradient method for objective function\"></a>3.3 A subgradient method for objective function</h2><p>We are now equipped to compute a subgradient $g_w \\in \\partial c(w)$ of our objective function (10):</p>\n<p>$$g_w = \\frac{1}{n} \\sum_{i=1}^n \\beta_i \\big( (w^T F_i + l_i^T)\\mu^* - w^T F_i \\mu_i \\big) \\cdot F_i \\Delta^w \\mu_i + \\lambda w \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (12)$$</p>\n<p>위의 수식에서 detail한 notation은 다음과 같습니다.</p>\n<p>$$\\mu^* = arg \\max_{\\mu \\in \\mathcal{G}} (w^T F_i + l_i^T)\\mu \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (12-1)$$</p>\n<p>$$\\Delta^w \\mu_i = \\mu^∗ − \\mu_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (12-2)$$</p>\n<p>수식 (12-2)를 직관적으로 보면, subgradient가 현재의 reward function $w^T F_i$에 관하여 <strong>optimal policy와 example policy 사이의 state-action visitation frequency count를 비교한다는 점</strong>을 발견할 수 있습니다.</p>\n<p>또한 subgradient를 계산하는 것은 $\\mu^* = arg \\max_{\\mu \\in \\mathcal{G}} (w^T F_i + l_i^T)\\mu$을 해결하는 것과 같습니다. 다시 말해 reward function $w^T F_i + l_i^T$를 해결한다는 것입니다.</p>\n<p><br></p>\n<h2 id=\"3-4-Algorithm-1-Max-Margin-Planning\"><a href=\"#3-4-Algorithm-1-Max-Margin-Planning\" class=\"headerlink\" title=\"3.4 Algorithm 1. Max Margin Planning\"></a>3.4 Algorithm 1. Max Margin Planning</h2><center> <img src=\"../../../../img/irl/mmp_5.png\" width=\"500\"> </center>\n\n<ul>\n<li>5: loss augmented cost map $(w^T F_i + l_i^T)$에 대해서 각각의 input map에 대한 optimal policy $\\mu^*$와 state-action visitation frequencies $\\mu^i$를 계산합니다. 처음에는 w가 0에서 시작하므로 loss augmented cost map $w^T F_i + l_i^T$은 $l_i^T$로 시작하게 됩니다.</li>\n<li>6: 수식 (12)에 있는 objective function $g$를 계산합니다.</li>\n<li>7: w를 $\\alpha_t g$에 따라 minimize합니다.</li>\n<li>8: Option으로 추가적인 constraints를 둘 수도 있습니다. 자세한 내용은 section 4.4인 Incorporating Prior Knowledge를 참고하시기 바랍니다.</li>\n<li>No RL step!</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"4-Additional-section\"><a href=\"#4-Additional-section\" class=\"headerlink\" title=\"4. Additional section\"></a>4. Additional section</h1><p>이전까지는 최종 objective function과 algorithm을 살펴봤습니다. 여기서 더 나아가 유용한 방법들을 통해 우리의 objective function과 algorithm이 더 robust하도록 만들어봅시다.</p>\n<p><br></p>\n<h2 id=\"4-1-Guarantees-in-the-Batch-Setting\"><a href=\"#4-1-Guarantees-in-the-Batch-Setting\" class=\"headerlink\" title=\"4.1 Guarantees in the Batch Setting\"></a>4.1 Guarantees in the Batch Setting</h2><p>subgradient method로 구성된 algorithm들의 잘 연구된 class 중 하나는 batch setting으로 둘 수 있다는 것입니다.</p>\n<p>batch setting에는 두 가지 key point가 존재합니다.<br>1) 이 method에서 step-size sequence $\\{ \\alpha_t \\}$의 선택은 상당히 중요합니다. $\\{ \\alpha_t \\}$에 따라서 convergence guarantee가 달라집니다.<br>2) 우리의 결과는 objective function을 유지하기 위해 strong convexity assumption이 필요합니다.<br>따라서 Given G$\\mathcal{W} \\subseteq \\mathbb{R}^d$, a function $f: \\mathcal{W} \\rightarrow \\mathbb{R}$ is $\\eta$-strongly convex if there exists $g: \\mathcal{W} \\rightarrow \\mathbb{R}$ such that for all $w$, $w’ \\in \\mathcal{W}$:</p>\n<center> <img src=\"../../../../img/irl/mmp_6.png\" width=\"500\"> </center>\n\n<p><strong>Theorem 1. Linear convergence of constant stepsize sequence.</strong> Let the stepsize sequence $\\{ \\alpha_t \\}$ of Algorithm (1) be chosen as $\\alpha_t = \\alpha \\leq \\frac{1}{\\lambda}$. Furthermore, assume for a particular region of radius $R$ around the minimum, $\\forall w,g \\in \\partial c(w), ||g|| \\leq C$. Then the algorithm converges at a linear rate to a region of some minimum point $x^*$ of $c$ bounded by</p>\n<p>$$||x_{min} - x^*|| \\leq \\sqrt{\\frac{a C^2}{\\lambda}} \\leq \\frac{C}{\\lambda}$$</p>\n<center> <img src=\"../../../../img/irl/mmp_7.png\" width=\"450\"> </center>\n\n<p>Theorem 1은 우리가 충분히 작고 일정한 stepsize를 통해 linear convergence rate를 얻을 수 있다는 것을 보여줍니다. 그러나 이 convergence는 오직 minimum 주변 지역에서만 가능합니다.</p>\n<p>대안적으로, 우리는 $t \\geq 1$에 대해 $\\alpha_t = \\frac{r}{t}$ 형태의 감소하는 step size rule을 고를 수 있습니다. 여기서 $r$은 learning rate로 생각할 수 있는 some positive constant입니다.</p>\n<p>이러한 rule을 통해, Algorithm 1은 minimum에서 convergence가 보장되지만, 위에서 말했던 strong convexity assumption에서만 오직 sublinear rate로 수렴될 수 있습니다.</p>\n<p><br></p>\n<h2 id=\"4-2-Optimization-in-an-Online-Setting\"><a href=\"#4-2-Optimization-in-an-Online-Setting\" class=\"headerlink\" title=\"4.2 Optimization in an Online Setting\"></a>4.2 Optimization in an Online Setting</h2><p>다양한 optimization techniques와 다르게, subgradient method는 batch setting에서 더 확장됩니다.</p>\n<p>online setting에서는 적절하게 관련된 domain에 대한 다양한 planning problem들을 생각해볼 수 있습니다. 특히, 그 중 하나는 path를 plan하기 위해 필요로 하는 domain을 제공하는 것입니다. 더 정확하게는 “correct” path를 제공하는 것입니다.</p>\n<p>At each time step $i$:<br>1) We observe $\\mathcal{G}_i$ and $F_i$.<br>2) Select a weight vector $w_i$ and using this compute a resulting path.<br>3) Finally we observe the true policy $y_i$.</p>\n<p>즉, strongly convex cost function(앞서 다뤘던 수식 (10))이 되기 위해 $c_i(w) = \\frac{1}{2} \\parallel w \\parallel^2 + \\{ \\max_{\\mu \\in \\mathcal{G}_i} (w^T F_i + l_i)\\mu\\} − w^T F_i \\mu_i$를 정의할 수 있다는 것입니다. 그리고 우리는 $y_i$, $\\mathcal{G}_i$, $F_i$가 주어진다면 계산할 수 있습니다.</p>\n<p>정리하자면, 앞서 본 cost function(Equation 10)에서 $\\frac{1}{n}\\sum_{i=1}^n \\beta_i$가 없어진 것과 같이 online setting이 가능하다는 것을 보여줍니다.</p>\n<p>This is now an <strong>online convex programming problem</strong>.</p>\n<p><br></p>\n<h2 id=\"4-3-Modifications-for-Acyclic-Positive-Costs-Think-of-Worst-Case\"><a href=\"#4-3-Modifications-for-Acyclic-Positive-Costs-Think-of-Worst-Case\" class=\"headerlink\" title=\"4.3 Modifications for Acyclic Positive Costs (Think of Worst Case)\"></a>4.3 Modifications for Acyclic Positive Costs (Think of Worst Case)</h2><p>acyclic(특정 방향이 없는, 사이클이 없는, 비순환적인) domain의 infinite horizon problems에서, $A^<em>$와 $A^</em>$의 변종들은 일반적으로 좋은 paln을 찾기위한 가장 효율적인 방법입니다. 이러한 domain에서는 strictly negative한 reward를 생각해볼 필요가 있습니다(동일하게, cost는 strictly positive). 다시 말해 best case에 대해서만 생각해볼 것이 아니라 worst case에 대해서도 생각해볼 필요가 있다는 것입니다. 이렇게 하지 않으면 infinite reward path가 발생할지도 모르기 때문입니다. 이러한 negativity의 strictness는 heuristic의 존재를 더 확실히게 보장하는 것이라고 볼 수 있습니다.</p>\n<p>$F_i \\geq 0$이라고 가정하면, 이러한 negativity의 strictness는 두 가지 방법을 통해 쉽게 구현할 수 있습니다.<br>1) w에 component-wise negativity constraints를 추가<br>2) 각각의 state-action pair에 대한 보상에 negativity를 부여하는 constraints를 추가</p>\n<p>이렇게 negativity를 추가할 수 있는 이유는 reward $w^T F_i \\mu$(or $(w^T F_i + l_i^T) \\mu$)에서 $F_i$이 0보다 크기 때문에, 우리는 $w, \\mu$에 negativity를 추가할 수 있습니다. 1의 경우 단순히 w의 violated component를 0으로 설정하여 구현할 수 있고, 2의 경우 가장 violated constraint를 반복적으로 추정함으로써 효율적으로 구현할 수 있습니다.</p>\n<p><br></p>\n<h2 id=\"4-4-Incorporating-Prior-Knowledge\"><a href=\"#4-4-Incorporating-Prior-Knowledge\" class=\"headerlink\" title=\"4.4 Incorporating Prior Knowledge\"></a>4.4 Incorporating Prior Knowledge</h2><p>이 section은 앞서 Algorithm 1의 Line 8에서와 말한 것과 같이 Option으로 prior knowledge을 통해 추가적인 constraints를 둘 수 있다는 것을 보여줍니다.<br>1) 0 vector 대신에 $w$에 prior belief에 대한 solution을 regularizing하는 것<br>2) loss function을 통해 특정한 state-action pairs를 poor choices으로 표시하는 것. algorithm이 이러한 요소로 인하여 large magin을 가지도록 강제합니다.<br>3) $w$에 constraint 형태로 domain knowledge를 포함시키는 것 (e.g 특정한 영역의 state를 다른 state보다 cost가 적어도 두 배가 되도록 요구.)</p>\n<p>이러한 방법들은 training example의 사용 외에도 learner에게 expert의 knowledge를 전달하는 강력한 방법입니다.</p>\n<p><br><br></p>\n<h1 id=\"5-Experimental-Results\"><a href=\"#5-Experimental-Results\" class=\"headerlink\" title=\"5. Experimental Results\"></a>5. Experimental Results</h1><p><br></p>\n<h2 id=\"5-1-Demonstration-of-learning-to-plan-based-on-satellite-color-imagery\"><a href=\"#5-1-Demonstration-of-learning-to-plan-based-on-satellite-color-imagery\" class=\"headerlink\" title=\"5.1 Demonstration of learning to plan based on satellite color imagery\"></a>5.1 Demonstration of learning to plan based on satellite color imagery</h2><p>실험에서는 실제 문제(Path planning)에서 논문 개념을 이용하여 유효성 검증할 것입니다.<br>1) Section 4에서 보여주었던 batch learning algorithm을 사용<br>2) Regularization을 위한 적당한 값을 사용하고, 위에서 다뤘던 우리의 algorithm을 사용</p>\n<p>추가적으로 prior knowledge에서의 첫 번째 방법을 적용한 것으로 보입니다.</p>\n<p>같은 맵의 영역에서 시연되는 다른 예제 경로는 학습 후에 hold out 영역에서 상당히 다른 결과를 이끌었습니다.</p>\n<center> <img src=\"../../../../img/irl/mmp_8.png\" width=\"1200\"> </center>\n\n<center> <img src=\"../../../../img/irl/mmp_9.png\" width=\"600\"> </center>\n\n<ul>\n<li><p>실험 의도</p>\n<ul>\n<li>Top : Road에 유지하도록 제안</li>\n<li>Bottom : 은밀한 의도를 제시(여기서는 숲을 지나는 의도를 의미)</li>\n</ul>\n</li>\n<li><p>실험 결과</p>\n<ul>\n<li>Left : Training 예제</li>\n<li>Middle : Training 이후에 hold out 영역에서 학습된 cost map</li>\n<li>Right : Hold out 영역에서 $A^*$를 이용하여 생성된 행동 결과</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"5-2-Data-shown-are-MMP-learned-cost-maps\"><a href=\"#5-2-Data-shown-are-MMP-learned-cost-maps\" class=\"headerlink\" title=\"5.2 Data shown are MMP learned cost maps\"></a>5.2 Data shown are MMP learned cost maps</h2><center> <img src=\"../../../../img/irl/mmp_10.png\" width=\"1100\"> </center>\n\n<p>Figure 2은 holdout region으로부터의 결과입니다.</p>\n<p>그림에서 loss-augmented path (blue)은 일반적으로 마지막 학습된 경로보다 일반적으로 좋은 결과를 수행하지 못하는 것을 나타났습니다.</p>\n<p>왜냐하면 loss-augmentation은 높은 loss의 영역을 최종 학습 지도보다 더욱 desirable하게 만들기 때문입니다. </p>\n<p>직관적으로, 만약 학습자가 loss-augmented cost map에 대해서 잘 수행할 수 있다면, loss-augmentation없이도 더욱 잘 수행되어야 한다는 것입니다. 이것은 margin을 가지고 학습된 개념입니다.</p>\n<p><br></p>\n<h2 id=\"5-3-Results-using-two-alternative-approaches\"><a href=\"#5-3-Results-using-two-alternative-approaches\" class=\"headerlink\" title=\"5.3 Results using two alternative approaches\"></a>5.3 Results using two alternative approaches</h2><center> <img src=\"../../../../img/irl/mmp_11.png\" width=\"800\"> </center>\n\n<ul>\n<li>Left : the result of a next-action classifier applied superimposed on a visualization of the second dataset.</li>\n<li>Right : a cost map learned by manual training of a regression.</li>\n</ul>\n<p>두 개의 경우에서 학습된 경로들은 poor approximations. (not shown on left, red on right).</p>\n<p><br></p>\n<h2 id=\"5-4-Visualization-about-losses\"><a href=\"#5-4-Visualization-about-losses\" class=\"headerlink\" title=\"5.4 Visualization about losses\"></a>5.4 Visualization about losses</h2><center> <img src=\"../../../../img/irl/mmp_12.png\" width=\"850\"> </center>\n\n<ul>\n<li>Left : Visualization of inverted Loss function $(1− l(x))$ for a training example path.</li>\n<li>Right : Comparison of holdout loss of MMP (by number of iterations) to a regression method where a teacher hand-labeled costs.</li>\n</ul>\n<p>비교를 위해, 저자는 MMP에 다른 두 개의 접근방법을 사용하여 유사한 학습을 시도하였습니다.</p>\n<ol>\n<li><p>(Lecun et al., 2006)$^5$에서 제안한 방법으로 state 특징들을 다음 action으로 취하는 mapping을 사용한 직접적으로 학습하는 알고리즘입니다. 이 경우, traing data에 대해 좋은 결과를 얻지 못했습니다.</p>\n</li>\n<li><p>다소 더 성공적은 시도는 직접 label을 통해 cost를 학습시킨 알고리즘입니다. 이 알고리즘은 MMP보다 학습자에게 더 많은 명시적 정보를 제공합니다.</p>\n<ul>\n<li>다음을 기반하여 low, medium, high cost로 제공<ol>\n<li>Expert knowledge of the planner</li>\n<li>Iterated training and observation</li>\n<li>The trainer had prior knowledge of the cost maps found under MMP batch learning on this dataset.</li>\n</ol>\n</li>\n<li>추가 정보가 주어진 cost map은 정성적으로 올바른 것처럼 보이지만, 그림 3과 그림 4는 상당히 좋지 않은 성능을 보여줍니다..</li>\n</ul>\n</li>\n</ol>\n<p><br><br></p>\n<h1 id=\"6-Related-and-Future-Work\"><a href=\"#6-Related-and-Future-Work\" class=\"headerlink\" title=\"6. Related and Future Work\"></a>6. Related and Future Work</h1><p>Maximum Margin Planning과 직접적으로 연관된 두 가지 work가 있습니다.</p>\n<p>그 중 하나가 바로 Inverse RL입니다.</p>\n<p>IRL의 목표는 MDP에서 agent의 행동을 관찰하는 하여 agent의 행동으로 부터 reward function를 추출하는 것입니다. 그러나 이것은 기본적으로 ill-posed 문제로 알고 있습니다. 그럼에도 불구하고, MMP와 같이 유사한 효과를 가진 IRL 아이디어들을 시도한 몇 가지 heuristic 시도가 있었습니다.</p>\n<p>유용한 heuristic 방법은 바로 이전 논문인 <em>Abbeel, P., &amp; Ng, A. Y. (2004). Apprenticeship learning via inverse reinforcement learning</em> APP 논문입니다. 학습자의 정책과 시연되는 예제간의 expected feature counts을 통해 매칭을 시도하는 학습 방법입니다.</p>\n<p>MMP의 경우 IRL algorithm의 variant과는 다른 스타일의 algorithm입니다.</p>\n<p>MMP는 하나의 MDP 보다 많은 정책 시연들을 허용하도록 설계되어 있습니다. 여러 특징 맵, 다른 시작 지점과 목표 지점, 완전히 새로운 맵과 목표 지점을 가지는 학습자의 목표를 이용하여 예제들을 시연했습니다. 또한 MMP는 상당히 다른 IRL적 알고리즘 접근 방법을 유도합니다.</p>\n<p>IRL과 MMP간의 관계는 <strong>generative and discriminative learning</strong> 간의 구별을 연상시킵니다.</p>\n<p>일반적인 IRL의 경우, feature matching을 시도합니다. agent가 MDP에서 (거의 최적같이) 행동하고 (거의) feature expectation에 매칭 가능할 때 학습하도록 설계되었습니다(Generative models과 같은 strong 가정). 예를 들어, feature expecatation을 매칭하는 능력은 algorithm의 행동이 feature가 선형인 모든 cost function에 대해서 near-optimal일 것이라는 것을 의미합니다.</p>\n<p>반대로 MMP의 경우, 우리의 목표가 직접적으로 output 행동을 모방하는 것이라는 weaker 가정을 하고 실제 MDP나 reward 함수에 대해 agnostic합니다. 여기서 MDP는 output decision들을 구조화하고 경쟁을 하려고 하는 expert가 natual class을 제공합니다.</p>\n<p>정리하자면,<br>Generative model : 개별 클래스의 분포를 모델링한다.<br>Discriminative model : Discriminative 모델은 기본 확률 분포 또는 데이터 구조를 모델링하지 않고 기본 데이터를 해당 클래스에 직접 매핑(class 경계를 통해 학습). SVM은 이러한 기준을 만족 시키므로 decision tree와 마찬가지로 discriminative model.</p>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"Let’s-do-Inverse-RL-Guide\"><a href=\"#Let’s-do-Inverse-RL-Guide\" class=\"headerlink\" title=\"Let’s do Inverse RL Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2019/01/22/0_lets-do-irl-guide/\">Let’s do Inverse RL Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"APP-여행하기\"><a href=\"#APP-여행하기\" class=\"headerlink\" title=\"APP 여행하기\"></a><a href=\"\">APP 여행하기</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"MaxEnt-여행하기\"><a href=\"#MaxEnt-여행하기\" class=\"headerlink\" title=\"MaxEnt 여행하기\"></a><a href=\"\">MaxEnt 여행하기</a></h2>","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"../../../../img/irl/mmp_1.png\" width=\"850\"> </center>\n\n<p>Author : Nathan D. Ratliff, J. Andrew Bagnell, Martin A. Zinkevich<br>Paper Link : <a href=\"https://www.ri.cmu.edu/pub_files/pub4/ratliff_nathan_2006_1/ratliff_nathan_2006_1.pdf\" target=\"_blank\" rel=\"noopener\">https://www.ri.cmu.edu/pub_files/pub4/ratliff_nathan_2006_1/ratliff_nathan_2006_1.pdf</a><br>Proceeding : International Conference on Machine Learning (ICML) 2006</p>\n<hr>\n<h1 id=\"0-Abstract\"><a href=\"#0-Abstract\" class=\"headerlink\" title=\"0. Abstract\"></a>0. Abstract</h1><p>일반적으로 Supervised learning techniques를 통해 sequential, goal-directed behavior에 대한 imitation learning은 어렵습니다. 이 논문에서는 maximum margin을 이용한 prediction problem에 대해 sequential, goal-directed behavior을 학습하는 것을 다룹니다. 더 구체적으로 말하자면, 각 features부터 cost까지 한번에 mapping하는 것을 학습해서, 이 cost에 대한 Markov Decision Process(MDP)에서의 optimal policy가 expert’s behavior를 모방하도록 하는 것입니다.</p>\n<p>또한 inference를 위해 fast algorithms을 이용하여 subgradient method의 기반인 structured maximum margin learning으로서 간단하고 효율적인 접근을 보였습니다. 비록 이러한 fast algorithm technique는 일반적으로 사용하지만, QP formulation의 한계를 벗어난 문제에서는 $A^*$나 Dynamic Programming(DP) 접근들이 policy를 learning하는 것을 다룰 수 있도록 만든다는 것을 보였습니다.</p>\n<p>실험에서는 outdoor mobile robot들을 이용하여 route planning에 적용합니다.</p>\n<p><br></p>\n<h2 id=\"0-1-들어가기에-앞서\"><a href=\"#0-1-들어가기에-앞서\" class=\"headerlink\" title=\"0.1 들어가기에 앞서..\"></a>0.1 들어가기에 앞서..</h2><p>이 논문은 앞서 다뤘던 APP 논문에 더하여 좀 더 효율적인 알고리즘을 만들고자 하였습니다. 그래서 기존에 APP에서 QP를 이용한 SVM 방법에 더하여 Soft Margin term을 추가하여 슬랙변수를 가지는 SVM을 사용하였고, subgradient method를 이용하여 알고리즘을 좀 더 쉽고 빠르게 구할 수 있도록 만들었습니다.</p>\n<p>SVM과 Soft Margin SVM에 대해 모르는 분이 계시다면 아래의 링크를 꼭 보시고 이 논문을 보시는 것을 추천해드립니다!</p>\n<p>1) 영상 (KAIST 문일철 교수님 강의)</p>\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=hK7vNvyCXWc&amp;list=PLbhbGI_ppZISMV4tAWHlytBqNq1-lb8bz&amp;index=23\" target=\"_blank\" rel=\"noopener\">Lecture 1 Decision boundary with Margin</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=tZy3uRv-9mY&amp;list=PLbhbGI_ppZISMV4tAWHlytBqNq1-lb8bz&amp;index=24\" target=\"_blank\" rel=\"noopener\">Lecture 2 Maximizing the Margin</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=sYLuJ_8Qw3s&amp;list=PLbhbGI_ppZISMV4tAWHlytBqNq1-lb8bz&amp;index=25\" target=\"_blank\" rel=\"noopener\">Lecture 3 SVM with Matlab</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=vEivqCo-LiU&amp;list=PLbhbGI_ppZISMV4tAWHlytBqNq1-lb8bz&amp;index=26\" target=\"_blank\" rel=\"noopener\">Lecture 4 Error Handling in SVM</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=5jOqc7ByMm4\" target=\"_blank\" rel=\"noopener\">Lecture 5 Soft Margin with SVM</a></li>\n</ul>\n<p>2) 블로그 글</p>\n<ul>\n<li><a href=\"https://gentlej90.tistory.com/43\" target=\"_blank\" rel=\"noopener\">SVM (Support Vector Machine) Part 1</a></li>\n<li><a href=\"https://gentlej90.tistory.com/44\" target=\"_blank\" rel=\"noopener\">SVM (Support Vector Machine) Part 2</a></li>\n</ul>\n<p><br><br></p>\n<h1 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h1><p>Imitation Learning에서, learner는 expert의 “behavior” or “control strategy”를 모방하려고 시도합니다. 이러한 imitation learning을 이용하여, robotics에서의 supervised learning 접근은 featrues부터 decision까지 mapping하는 것을 학습하는 것에서 엄청난 결과로서 사용되어 왔지만, Long-range나 Goal-directed behavior에서는 이러한 기술들을 사용하기 어렵습니다.</p>\n<p><br></p>\n<h2 id=\"1-1-Perception-and-Planning\"><a href=\"#1-1-Perception-and-Planning\" class=\"headerlink\" title=\"1.1 Perception and Planning\"></a>1.1 Perception and Planning</h2><p>Mobile robotics에서는 일반적으로 <strong>perception</strong> subsystem과 <strong>planning</strong> subsystem으로 autonomy software를 partition함으로써 Long-horizon goal directed behavior를 찾습니다.<br>1) Perception system은 다양한 model과 환경의 features를 계산합니다.<br>2) Planning system은 cost-map을 input으로 두고, 그 input을 통해 minimal risk (cost) path를 계산합니다.</p>\n<p><br></p>\n<h2 id=\"1-2-In-this-work\"><a href=\"#1-2-In-this-work\" class=\"headerlink\" title=\"1.2 In this work\"></a>1.2 In this work</h2><p>하지만 불행하게도, perception의 model부터 planner에 대한 cost까지 하는 것은 종종 어렵습니다.</p>\n<p>따라서 이 논문에서는 새로운 방법을 제시합니다. <strong>perception features부터 planner에 대한 cost까지 (Perception + Planning)</strong> mapping하는 것을 자동화하는 방법입니다. 최종 목표는 features부터 cost function까지 mapping하는 것을 한번에 학습하는 것입니다. 그래서 이러한 cost function에 대한 optimal policy가 expert의 behavior을 한번에 모방하도록 하는 것입니다.</p>\n<p><br></p>\n<h2 id=\"1-3-Three-fold\"><a href=\"#1-3-Three-fold\" class=\"headerlink\" title=\"1.3 Three fold\"></a>1.3 Three fold</h2><p>정리해보면, 이 논문에서는 3가지 중요한 언급이 있습니다.</p>\n<ol>\n<li>planning을 학습하기위한 새로운 방법을 제시합니다.</li>\n<li>sutructured maximum-margin classification으로서 효율적이고 간단한 접근을 제시합니다. 또한 batch setting에서도 linear convergence하다는 것을 보여주며, 다른 QP 기법이 없이도 lerge problems에 \u001d적용될 수 있다고 합니다.</li>\n<li>실험적으로 mobile robotics 관련 문제에 적용하였습니다.</li>\n</ol>\n<p><br><br></p>\n<h1 id=\"2-Preliminaries\"><a href=\"#2-Preliminaries\" class=\"headerlink\" title=\"2. Preliminaries\"></a>2. Preliminaries</h1><p><br></p>\n<h2 id=\"2-1-Notations\"><a href=\"#2-1-Notations\" class=\"headerlink\" title=\"2.1 Notations\"></a>2.1 Notations</h2><h3 id=\"2-1-1-modeling-the-planning-problem-with-discrete-MDPs\"><a href=\"#2-1-1-modeling-the-planning-problem-with-discrete-MDPs\" class=\"headerlink\" title=\"2.1.1 modeling the planning problem with discrete MDPs\"></a>2.1.1 modeling the planning problem with discrete MDPs</h3><p>이 논문은 discrete MDPs에서의 planning problem을 modeling합니다.</p>\n<p>$\\mathcal{X}$ is state spaces. $x$ index the state spaces.</p>\n<p>$\\mathcal{A}$ is action spaces. $a$ index the action spaces.</p>\n<p>$p(y|x,a)$ is transition probablities.</p>\n<p>$s$ is initial state distribution.</p>\n<p>$\\gamma$ is a discount factor on rewards. if any, $\\gamma$ is aborbed into the transition probabilities.</p>\n<p>Reward $R$은 따로 두지 않고, demonstrated behavior를 모방하는 policies를 만들기 위해 supervised examples로부터 학습됩니다.</p>\n<p>$v \\in \\mathcal{V}$ is primal variables of value function</p>\n<p>$\\mu \\in \\mathcal{G}$ is <strong>dual state-action frequency counts</strong>. equal to $y$.</p>\n<ul>\n<li>여기서 $\\mu$는 어떠한 상태에서 어떠한 행동을 취했는 지를 count의 개념으로 나타낸 것인데 $y$와 혼용되어 쓸 수 있습니다.</li>\n</ul>\n<p>그리고 여기서는 오직 stationary policies만을 고려합니다. The generalization is straightforward.</p>\n<h4 id=\"state-action-frequency-counts란\"><a href=\"#state-action-frequency-counts란\" class=\"headerlink\" title=\"state-action frequency counts란?\"></a>state-action frequency counts란?</h4><p>Inverse RL의 궁극적인 목표를 다르게 말해보면 expert가 어떠한 행동을 했을 때, 여기서의 state-action visitation frequency counts(줄여서 visitation frequency)를 구합니다. 그리고 우리의 목적은 expert의 visitation frequency과 최대한 비슷한 visitation frequency를 만들어내는 어떤 reward를 찾는 것입니다.</p>\n<p>일반적으로, RL 문제는 reward가 주어졌을 때, 이 reward의 expected sum을 최대로 하는 policy를 찾는다고 알려져 있습니다. 그런데 이 문제의 dual problem은 visitation frequency를 찾는 것입니다.</p>\n<p>다시 말해 optimal policy와 optimal visitation frequency counts는 1:1 관계라는 것입니다.</p>\n<h3 id=\"2-1-2-The-input-to-our-algorithm\"><a href=\"#2-1-2-The-input-to-our-algorithm\" class=\"headerlink\" title=\"2.1.2 The input to our algorithm\"></a>2.1.2 The input to our algorithm</h3><p>The input is a set of training instances</p>\n<center> <img src=\"../../../../img/irl/mmp_2.png\" width=\"300\"> </center>\n\n<p>$p_i$ is transition probablities.</p>\n<p>State-action pairs $(x_i, a_i) \\in \\mathcal{X}_i, \\mathcal{A}_i$ is $d \\times |\\mathcal{X}||\\mathcal{A}|$ Feature matrix (or mapping) $F_i \\in \\mathbb{R}^{d \\times |\\mathcal{X}||\\mathcal{A}|}$.</p>\n<p>$y_i$ is expert’s demonstration (desired trajectory or full policy). equal to <strong>dual state-action frequency counts</strong> $\\mu_i$.</p>\n<p>$f_i (y)$ denote vector of expected feature counts $F_i \\mu$ of the $i$th example.</p>\n<p>$\\mathcal{L}_i$ : Some additional loss function (heuristic)</p>\n<h3 id=\"2-1-3-Detail-description\"><a href=\"#2-1-3-Detail-description\" class=\"headerlink\" title=\"2.1.3 Detail description\"></a>2.1.3 Detail description</h3><p>$\\mu_i^{x, a}$ is the expected state-action frequency for state $x$ and action $a$ of example $i$.</p>\n<ul>\n<li>$\\mu_i$를 자세히보면 $\\mu$에 $i$가 붙은 형태로 되어있습니다. 여기서 example $i$라는 것 총 trajectory(or path)의 length 중에서 하나의 index를 말하는 것입니다. expert의 경우, 보통 전체 trajectory를 한꺼번에 취하기 때문에 이 논문에서는 구별하기 위해 $i$라는 notation을 쓴 것입니다.</li>\n</ul>\n<p>$\\mathcal{D} = \\{(\\mathcal{X}_i, \\mathcal{A}_i, p_i, f_i, y_i, \\mathcal{L}_i)\\} \\equiv \\{(\\mathcal{X}_i, \\mathcal{A}_i, \\mathcal{G}_i, F_i, \\mathcal{\\mu}_i, \\mathcal{l}_i)\\}$ </p>\n<ul>\n<li>($\\mathcal{D}$는 $\\mathcal{D}_{i=1}^n$)</li>\n</ul>\n<p>Loss function is $\\mathcal{L} : \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}_+$ between solutions. $\\mathcal{L} (y_i, y) = \\mathcal{L}_i (y) = l_i^T \\mu$. </p>\n<ul>\n<li>$l_i \\in \\mathbb{R}_+^{|\\mathcal{X}||\\mathcal{A}|}$</li>\n<li>$l_i = 1 -$ expert’s visitation frequency</li>\n</ul>\n<h3 id=\"2-1-4-그래서-이-논문에서-하고-싶은-것-중요\"><a href=\"#2-1-4-그래서-이-논문에서-하고-싶은-것-중요\" class=\"headerlink\" title=\"2.1.4 그래서 이 논문에서 하고 싶은 것 (중요)\"></a>2.1.4 그래서 이 논문에서 하고 싶은 것 (중요)</h3><p>The best policy over the resulting reward function $\\mu^* = arg\\max_{\\mu \\in \\mathcal{G}_i} w^T F_i \\mu$ is “<strong>close</strong>” to the expert’s demonstrated policy $\\mu_i$.</p>\n<p><br></p>\n<h2 id=\"2-2-Loss-function\"><a href=\"#2-2-Loss-function\" class=\"headerlink\" title=\"2.2 Loss function\"></a>2.2 Loss function</h2><p>위에서 말했던 loss function은 teacher가 아닌 learner가 방문한 states의 count입니다.</p>\n<p>또한 이 논문에서는 teacher가 도달한 어떠한 states에서 teacher와 다른 action들을 고르거나 teacher가 선택하지 않은 states를 도달하는 것을 penalizing할 것입니다.</p>\n<p>끝으로, $\\mathcal{L}(y_i, y) \\geq 0$을 가정합니다.</p>\n<p><br></p>\n<h2 id=\"2-3-Quadratic-Programming-Formulation\"><a href=\"#2-3-Quadratic-Programming-Formulation\" class=\"headerlink\" title=\"2.3 Quadratic Programming Formulation\"></a>2.3 Quadratic Programming Formulation</h2><h3 id=\"2-3-1-Quadratic-Program\"><a href=\"#2-3-1-Quadratic-Program\" class=\"headerlink\" title=\"2.3.1 Quadratic Program\"></a>2.3.1 Quadratic Program</h3><p>Given a training set :</p>\n<center> <img src=\"../../../../img/irl/mmp_2.png\" width=\"300\"> </center>\n\n<p>Quadratic Program is</p>\n<p>$$\\min_{w, \\zeta_i} \\frac{1}{2} \\parallel w \\parallel^2 + \\frac{\\gamma}{n} \\sum_i \\beta_i \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (1)$$</p>\n<p>$$s.t. \\,\\, \\forall i \\,\\,\\,\\, w^T f_i (y_i) \\geq \\max_{\\mu \\in \\mathcal{G}_i} (w^T f_i (y) + \\mathcal{L} (y_i, y)) - \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\, (2)$$</p>\n<p>s.t.로 나오는 constraint의 intuition을 보자면, expert’s policies가 margin에 대해 다른 모든 policies보다 더 높은 experted reward를 가지도록 하는 only weight vectors를 찾자는 것입니다.</p>\n<p>다음으로 위의 수식에 나오는 notation에 대해서 자세히 알아보겠습니다.</p>\n<p>$\\frac{\\gamma}{n}\\sum_i \\zeta_i$ is <strong>soft margin term</strong> from soft margin SVM. $\\zeta$의 값을 되도록 최소화하여 오분류의 허용도를 낮추기 위해 추가되었습니다.</p>\n<p>$\\zeta_i$ is <strong>slack variable</strong>. The slack variable permit violations of these constraints for a penalty. 여유 변수라고도 하고, $\\zeta_i$만큼의 오류를 인정한다는 의미로 볼 수 있습니다.</p>\n<p>$\\gamma \\geq 0$ is scaled for a penalty. 보통 $c$ ($c \\geq 0$)라고도 하는데, 최소화 조건을 반영하는 정도를 결정하는 값으로 우리가 적절히 정해주어야 합니다. c값이 크면 전체 margin도 커지므로 오분류를 적게 허용(엄격)한다는 뜻이고, 반대로 c값이 작으면 margin이 작아지므로 비교적 높은 오분류를 허용(관대)한다는 뜻입니다.</p>\n<p>$\\beta_i \\geq 0$는 example들이 다른 length일 때 normalization하기 위해서 사용되는 data dependent scalars입니다.</p>\n<p>$w^T f_i(y_i)$ is expert’s reward.</p>\n<p>$w^T f_i(y)$ is other’s reward.</p>\n<p>$\\mathcal{L} (y_i, y)$는 $y_i$와 $y$가 일치하지 않는 상태의 수입니다.</p>\n<h3 id=\"2-3-2-Maximum-Margin-Problem-MMP\"><a href=\"#2-3-2-Maximum-Margin-Problem-MMP\" class=\"headerlink\" title=\"2.3.2 Maximum Margin Problem(MMP)\"></a>2.3.2 Maximum Margin Problem(MMP)</h3><p>만약 $f_i (\\cdot)$ and $\\mathcal{L}_i (\\cdot)$가 state-action frequencies $\\mu$에서 linear하다면, MMP는 다음과 같이 정의할 수 있습니다.</p>\n<p>$$\\min_{w, \\zeta_i} \\frac{1}{2} \\parallel w \\parallel^2 + \\frac{\\gamma}{n} \\sum_i \\beta_i \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (3)$$</p>\n<p>$$s.t. \\,\\, \\forall i \\,\\,\\,\\, w^T F_i \\mu_i \\geq \\max_{\\mu \\in \\mathcal{G}_i} (w^T F_i \\mu + l_i^T \\mu) - \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (4)$$</p>\n<p>그리고 수식 (4)에서, $\\mu \\in \\mathcal{G}_i$를 Bellman-flow constraints로 표현할 수 있습니다. 다시 말해 $\\mu \\geq 0$는 다음을 만족합니다. ($\\mu$의 성질)</p>\n<center> <img src=\"../../../../img/irl/mmp_3.png\" width=\"450\"> </center>\n\n<ul>\n<li>나중에 GAIL 논문에서도 나오겠지만, 위의 수식으로 $\\mu$인 visitation frequency를 정할 수 있습니다.</li>\n</ul>\n<h3 id=\"2-3-3-One-compact-quadratic-program-for-MMP\"><a href=\"#2-3-3-One-compact-quadratic-program-for-MMP\" class=\"headerlink\" title=\"2.3.3 One compact quadratic program for MMP\"></a>2.3.3 One compact quadratic program for MMP</h3><p>이어서 수식 (4)에서 nonlinear, convex constraints는 오른쪽 side의 dual을 계산함으로써 linear constraints의 compact set으로 다음과 같이 변형될 수 있습니다.</p>\n<p>$$\\forall i \\,\\,\\,\\, w^T F_i \\mu_i \\geq \\min_{v \\in V_i} \\, (s_i^T v) - \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (5)$$</p>\n<p>그리고 수식 (5)에서, $v \\in V_i$는 Bellman primal constraints을 만족하는 value function입니다. Bellman primal constraints는 다음과 같습니다.</p>\n<p>$$\\forall i,x,a \\,\\,\\,\\, v^x \\geq (w^T F_i + l_i)^{x,a} + \\sum_{x’} p_i (x’|x,a) v^{x’} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (6)$$</p>\n<p>위의 constraints (5), (6)을 combining함으로써 최종적으로 다음과 같이 쓸 수 있습니다.</p>\n<p>One compact quadratic program is</p>\n<p>$$\\min_{w, \\zeta_i} \\frac{1}{2} \\parallel w \\parallel^2 + \\frac{\\gamma}{n} \\sum_i \\beta_i \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (7)$$</p>\n<p>$$\\forall i \\,\\,\\,\\, w^T F_i \\mu_i \\geq \\min_{v \\in V_i} \\, (s_i^T v) - \\zeta_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (8)$$</p>\n<p>$$\\forall i,x,a \\,\\,\\,\\, v_i^x \\geq (w^T F_i + l_i)^{x,a} + \\sum_{x’} p_i (x’|x,a) v_i^{x’} \\,\\,\\,\\,\\,\\,\\,\\, (9)$$</p>\n<p>위의 수식은 MMP 문제를 One compact quadratic program으로써 풀 수 있도록 만든 것입니다. 하지만 아쉽게도 위의 constraints의 수는 state-action pairs와 training examples에 대해 linear하게 scaling됩니다.</p>\n<p>이러한 program을 직접적으로 optimize할 수 있도록 이미 만들어져 있는 QP software가 있지만, 뒤이어 나오는 <strong>section 3</strong> 에서 <strong>subgradient methods</strong> 의 이용함으로써 One compact quadratic program에 대해 크게 향상시킬 수 있는 다른 alternative formulation을 이용하고자 합니다.</p>\n<p>추가적으로, <strong>Section 4</strong> 에서는 최종 objective function에 유용한 방법들을 생각해 볼 것입니다. 그리고 나서 path planning problems에 대한 적절한 examples를 말할 것입니다.</p>\n<p><br><br></p>\n<h1 id=\"3-Efficient-Optimization\"><a href=\"#3-Efficient-Optimization\" class=\"headerlink\" title=\"3. Efficient Optimization\"></a>3. Efficient Optimization</h1><p>실제로, 수식 (9)에서의 quadratic program을 해결하는 것은 적어도 single MDP의 linear programming을 해결하는 것만큼 어렵습니다. 그래서 수식 (9)를 quadratic program으로 해결하려는 것이 적절한 전략이 될 수 있지만, 다르게 보면 많은 문제들에 대해 policy iteration과 $A^*$ algorithm처럼 이론적으로나 실험적으로나 더 빠르게 해결할 수 있도록 특별하게 design된 algorithm이 존재한다는 것으로 볼 수 있습니다.</p>\n<p>따라서 이 논문에서는 더 나아가 <strong>fast maximization algorithm을 사용하는 iterative method 기반인 subgradient method로 접근합니다.</strong></p>\n<p><br></p>\n<h2 id=\"3-1-Objective-function\"><a href=\"#3-1-Objective-function\" class=\"headerlink\" title=\"3.1 Objective function\"></a>3.1 Objective function</h2><p>첫 번째 step은 optimization program(One compact quadratic program)을 “hinge-loss” form으로 변형하는 것입니다.</p>\n<p>변형된 objective function은 다음과 같습니다.</p>\n<p>$$c_q(w) = \\frac{1}{n} \\sum_{i=1}^n \\beta_i \\Big( \\big\\{\\max_{\\mu \\in \\mathcal{G}_i} (w^T F_i + l_i^T)\\mu \\big\\} - w^T F_i \\mu_i\\Big) + \\frac{1}{2} \\parallel w \\parallel^2 \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (10)$$</p>\n<p>hinge-loss 관점에서 보면, 위의 수식에서 $\\big\\{\\max_{\\mu \\in \\mathcal{G}_i} (w^T F_i + l_i^T)\\mu\\big\\} - w^T F_i \\mu_i$은 slack variable인 $\\zeta_i$과 동일합니다.</p>\n<p>또한 기존에 있던 $\\gamma$는 slack variable $\\zeta_i$가 없어졌기 때문에 사라집니다.</p>\n<p>위의 objective function은 convex하지만, <strong>max term</strong>이 있기 때문에 미분이 불가능합니다. 따라서 이 논문에서는 subgradient method라 불리는 gradient descent의 generalization을 이용함으로써 optimization을 합니다.</p>\n<p>convex function $c : \\mathcal{W} \\rightarrow \\mathbb{R}$의 subgradient는 vector $g$로 정의합니다.</p>\n<center> <img src=\"../../../../img/irl/mmp_4.png\" width=\"400\"> </center>\n\n<p>위의 수식에서 subgradient는 비록 미분 가능한 점에서 필연적으로 gradient와는 같지만, unique할 필요는 없습니다.</p>\n<h3 id=\"3-1-1-Subgradient-Method란\"><a href=\"#3-1-1-Subgradient-Method란\" class=\"headerlink\" title=\"3.1.1 Subgradient Method란?\"></a>3.1.1 Subgradient Method란?</h3><p>아래의 링크를 참고해주시면 감사하겠습니다.</p>\n<p>1) <a href=\"https://en.wikipedia.org/wiki/Subgradient_method\" target=\"_blank\" rel=\"noopener\">Wikipedia - Subgradient method</a><br>2) <a href=\"https://wikidocs.net/18963\" target=\"_blank\" rel=\"noopener\">모두를 위한 컨벡스 최적화 - Subgradient</a><br>3) <a href=\"https://wikidocs.net/18953\" target=\"_blank\" rel=\"noopener\">모두를 위한 컨벡스 최적화 - Subgradient Method</a></p>\n<p><br></p>\n<h2 id=\"3-2-Four-well-known-properties-for-subgradient-method\"><a href=\"#3-2-Four-well-known-properties-for-subgradient-method\" class=\"headerlink\" title=\"3.2 Four well known properties for subgradient method\"></a>3.2 Four well known properties for subgradient method</h2><p>최종 objective function을 보기전에, 먼저 $c(w)$의 subgradient를 계산하기 위해, subgradient에 대해 잘 알려진 4가지 속성들에 대해서 알아봅시다. (<strong>3번 중요</strong>)<br>1) Subgradient operators are linear.<br>2) The gradient is the unique subgradient of a differentiable function.<br>3) Denoting $y^∗ = arg\\max_y [f (x, y)]$ for differentiable $f (., y)$, $\\nabla_x f(x,y∗)$ is a subgradient of the piecewise(구분적으로, 구간적으로) differentiable convex function $\\max_y [f (x, y)]$.<br>4) An analogous chain rule holds as expected.</p>\n<p>3번을 보면, 결국 subgradient method를 통해 하고 싶은 것은 piecewise differentiable convex function인 $f(x,y)$ 중에서 제일 큰 $\\max_y [f (x, y)]$를 subgradient로 구해서, 그 중 가장 큰 값인 $y^∗ = arg\\max_y [f (x, y)]$를 통해 $\\nabla_x f(x,y∗)$를 하겠다는 것입니다.</p>\n<p><br></p>\n<h2 id=\"3-3-A-subgradient-method-for-objective-function\"><a href=\"#3-3-A-subgradient-method-for-objective-function\" class=\"headerlink\" title=\"3.3 A subgradient method for objective function\"></a>3.3 A subgradient method for objective function</h2><p>We are now equipped to compute a subgradient $g_w \\in \\partial c(w)$ of our objective function (10):</p>\n<p>$$g_w = \\frac{1}{n} \\sum_{i=1}^n \\beta_i \\big( (w^T F_i + l_i^T)\\mu^* - w^T F_i \\mu_i \\big) \\cdot F_i \\Delta^w \\mu_i + \\lambda w \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (12)$$</p>\n<p>위의 수식에서 detail한 notation은 다음과 같습니다.</p>\n<p>$$\\mu^* = arg \\max_{\\mu \\in \\mathcal{G}} (w^T F_i + l_i^T)\\mu \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (12-1)$$</p>\n<p>$$\\Delta^w \\mu_i = \\mu^∗ − \\mu_i \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (12-2)$$</p>\n<p>수식 (12-2)를 직관적으로 보면, subgradient가 현재의 reward function $w^T F_i$에 관하여 <strong>optimal policy와 example policy 사이의 state-action visitation frequency count를 비교한다는 점</strong>을 발견할 수 있습니다.</p>\n<p>또한 subgradient를 계산하는 것은 $\\mu^* = arg \\max_{\\mu \\in \\mathcal{G}} (w^T F_i + l_i^T)\\mu$을 해결하는 것과 같습니다. 다시 말해 reward function $w^T F_i + l_i^T$를 해결한다는 것입니다.</p>\n<p><br></p>\n<h2 id=\"3-4-Algorithm-1-Max-Margin-Planning\"><a href=\"#3-4-Algorithm-1-Max-Margin-Planning\" class=\"headerlink\" title=\"3.4 Algorithm 1. Max Margin Planning\"></a>3.4 Algorithm 1. Max Margin Planning</h2><center> <img src=\"../../../../img/irl/mmp_5.png\" width=\"500\"> </center>\n\n<ul>\n<li>5: loss augmented cost map $(w^T F_i + l_i^T)$에 대해서 각각의 input map에 대한 optimal policy $\\mu^*$와 state-action visitation frequencies $\\mu^i$를 계산합니다. 처음에는 w가 0에서 시작하므로 loss augmented cost map $w^T F_i + l_i^T$은 $l_i^T$로 시작하게 됩니다.</li>\n<li>6: 수식 (12)에 있는 objective function $g$를 계산합니다.</li>\n<li>7: w를 $\\alpha_t g$에 따라 minimize합니다.</li>\n<li>8: Option으로 추가적인 constraints를 둘 수도 있습니다. 자세한 내용은 section 4.4인 Incorporating Prior Knowledge를 참고하시기 바랍니다.</li>\n<li>No RL step!</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"4-Additional-section\"><a href=\"#4-Additional-section\" class=\"headerlink\" title=\"4. Additional section\"></a>4. Additional section</h1><p>이전까지는 최종 objective function과 algorithm을 살펴봤습니다. 여기서 더 나아가 유용한 방법들을 통해 우리의 objective function과 algorithm이 더 robust하도록 만들어봅시다.</p>\n<p><br></p>\n<h2 id=\"4-1-Guarantees-in-the-Batch-Setting\"><a href=\"#4-1-Guarantees-in-the-Batch-Setting\" class=\"headerlink\" title=\"4.1 Guarantees in the Batch Setting\"></a>4.1 Guarantees in the Batch Setting</h2><p>subgradient method로 구성된 algorithm들의 잘 연구된 class 중 하나는 batch setting으로 둘 수 있다는 것입니다.</p>\n<p>batch setting에는 두 가지 key point가 존재합니다.<br>1) 이 method에서 step-size sequence $\\{ \\alpha_t \\}$의 선택은 상당히 중요합니다. $\\{ \\alpha_t \\}$에 따라서 convergence guarantee가 달라집니다.<br>2) 우리의 결과는 objective function을 유지하기 위해 strong convexity assumption이 필요합니다.<br>따라서 Given G$\\mathcal{W} \\subseteq \\mathbb{R}^d$, a function $f: \\mathcal{W} \\rightarrow \\mathbb{R}$ is $\\eta$-strongly convex if there exists $g: \\mathcal{W} \\rightarrow \\mathbb{R}$ such that for all $w$, $w’ \\in \\mathcal{W}$:</p>\n<center> <img src=\"../../../../img/irl/mmp_6.png\" width=\"500\"> </center>\n\n<p><strong>Theorem 1. Linear convergence of constant stepsize sequence.</strong> Let the stepsize sequence $\\{ \\alpha_t \\}$ of Algorithm (1) be chosen as $\\alpha_t = \\alpha \\leq \\frac{1}{\\lambda}$. Furthermore, assume for a particular region of radius $R$ around the minimum, $\\forall w,g \\in \\partial c(w), ||g|| \\leq C$. Then the algorithm converges at a linear rate to a region of some minimum point $x^*$ of $c$ bounded by</p>\n<p>$$||x_{min} - x^*|| \\leq \\sqrt{\\frac{a C^2}{\\lambda}} \\leq \\frac{C}{\\lambda}$$</p>\n<center> <img src=\"../../../../img/irl/mmp_7.png\" width=\"450\"> </center>\n\n<p>Theorem 1은 우리가 충분히 작고 일정한 stepsize를 통해 linear convergence rate를 얻을 수 있다는 것을 보여줍니다. 그러나 이 convergence는 오직 minimum 주변 지역에서만 가능합니다.</p>\n<p>대안적으로, 우리는 $t \\geq 1$에 대해 $\\alpha_t = \\frac{r}{t}$ 형태의 감소하는 step size rule을 고를 수 있습니다. 여기서 $r$은 learning rate로 생각할 수 있는 some positive constant입니다.</p>\n<p>이러한 rule을 통해, Algorithm 1은 minimum에서 convergence가 보장되지만, 위에서 말했던 strong convexity assumption에서만 오직 sublinear rate로 수렴될 수 있습니다.</p>\n<p><br></p>\n<h2 id=\"4-2-Optimization-in-an-Online-Setting\"><a href=\"#4-2-Optimization-in-an-Online-Setting\" class=\"headerlink\" title=\"4.2 Optimization in an Online Setting\"></a>4.2 Optimization in an Online Setting</h2><p>다양한 optimization techniques와 다르게, subgradient method는 batch setting에서 더 확장됩니다.</p>\n<p>online setting에서는 적절하게 관련된 domain에 대한 다양한 planning problem들을 생각해볼 수 있습니다. 특히, 그 중 하나는 path를 plan하기 위해 필요로 하는 domain을 제공하는 것입니다. 더 정확하게는 “correct” path를 제공하는 것입니다.</p>\n<p>At each time step $i$:<br>1) We observe $\\mathcal{G}_i$ and $F_i$.<br>2) Select a weight vector $w_i$ and using this compute a resulting path.<br>3) Finally we observe the true policy $y_i$.</p>\n<p>즉, strongly convex cost function(앞서 다뤘던 수식 (10))이 되기 위해 $c_i(w) = \\frac{1}{2} \\parallel w \\parallel^2 + \\{ \\max_{\\mu \\in \\mathcal{G}_i} (w^T F_i + l_i)\\mu\\} − w^T F_i \\mu_i$를 정의할 수 있다는 것입니다. 그리고 우리는 $y_i$, $\\mathcal{G}_i$, $F_i$가 주어진다면 계산할 수 있습니다.</p>\n<p>정리하자면, 앞서 본 cost function(Equation 10)에서 $\\frac{1}{n}\\sum_{i=1}^n \\beta_i$가 없어진 것과 같이 online setting이 가능하다는 것을 보여줍니다.</p>\n<p>This is now an <strong>online convex programming problem</strong>.</p>\n<p><br></p>\n<h2 id=\"4-3-Modifications-for-Acyclic-Positive-Costs-Think-of-Worst-Case\"><a href=\"#4-3-Modifications-for-Acyclic-Positive-Costs-Think-of-Worst-Case\" class=\"headerlink\" title=\"4.3 Modifications for Acyclic Positive Costs (Think of Worst Case)\"></a>4.3 Modifications for Acyclic Positive Costs (Think of Worst Case)</h2><p>acyclic(특정 방향이 없는, 사이클이 없는, 비순환적인) domain의 infinite horizon problems에서, $A^<em>$와 $A^</em>$의 변종들은 일반적으로 좋은 paln을 찾기위한 가장 효율적인 방법입니다. 이러한 domain에서는 strictly negative한 reward를 생각해볼 필요가 있습니다(동일하게, cost는 strictly positive). 다시 말해 best case에 대해서만 생각해볼 것이 아니라 worst case에 대해서도 생각해볼 필요가 있다는 것입니다. 이렇게 하지 않으면 infinite reward path가 발생할지도 모르기 때문입니다. 이러한 negativity의 strictness는 heuristic의 존재를 더 확실히게 보장하는 것이라고 볼 수 있습니다.</p>\n<p>$F_i \\geq 0$이라고 가정하면, 이러한 negativity의 strictness는 두 가지 방법을 통해 쉽게 구현할 수 있습니다.<br>1) w에 component-wise negativity constraints를 추가<br>2) 각각의 state-action pair에 대한 보상에 negativity를 부여하는 constraints를 추가</p>\n<p>이렇게 negativity를 추가할 수 있는 이유는 reward $w^T F_i \\mu$(or $(w^T F_i + l_i^T) \\mu$)에서 $F_i$이 0보다 크기 때문에, 우리는 $w, \\mu$에 negativity를 추가할 수 있습니다. 1의 경우 단순히 w의 violated component를 0으로 설정하여 구현할 수 있고, 2의 경우 가장 violated constraint를 반복적으로 추정함으로써 효율적으로 구현할 수 있습니다.</p>\n<p><br></p>\n<h2 id=\"4-4-Incorporating-Prior-Knowledge\"><a href=\"#4-4-Incorporating-Prior-Knowledge\" class=\"headerlink\" title=\"4.4 Incorporating Prior Knowledge\"></a>4.4 Incorporating Prior Knowledge</h2><p>이 section은 앞서 Algorithm 1의 Line 8에서와 말한 것과 같이 Option으로 prior knowledge을 통해 추가적인 constraints를 둘 수 있다는 것을 보여줍니다.<br>1) 0 vector 대신에 $w$에 prior belief에 대한 solution을 regularizing하는 것<br>2) loss function을 통해 특정한 state-action pairs를 poor choices으로 표시하는 것. algorithm이 이러한 요소로 인하여 large magin을 가지도록 강제합니다.<br>3) $w$에 constraint 형태로 domain knowledge를 포함시키는 것 (e.g 특정한 영역의 state를 다른 state보다 cost가 적어도 두 배가 되도록 요구.)</p>\n<p>이러한 방법들은 training example의 사용 외에도 learner에게 expert의 knowledge를 전달하는 강력한 방법입니다.</p>\n<p><br><br></p>\n<h1 id=\"5-Experimental-Results\"><a href=\"#5-Experimental-Results\" class=\"headerlink\" title=\"5. Experimental Results\"></a>5. Experimental Results</h1><p><br></p>\n<h2 id=\"5-1-Demonstration-of-learning-to-plan-based-on-satellite-color-imagery\"><a href=\"#5-1-Demonstration-of-learning-to-plan-based-on-satellite-color-imagery\" class=\"headerlink\" title=\"5.1 Demonstration of learning to plan based on satellite color imagery\"></a>5.1 Demonstration of learning to plan based on satellite color imagery</h2><p>실험에서는 실제 문제(Path planning)에서 논문 개념을 이용하여 유효성 검증할 것입니다.<br>1) Section 4에서 보여주었던 batch learning algorithm을 사용<br>2) Regularization을 위한 적당한 값을 사용하고, 위에서 다뤘던 우리의 algorithm을 사용</p>\n<p>추가적으로 prior knowledge에서의 첫 번째 방법을 적용한 것으로 보입니다.</p>\n<p>같은 맵의 영역에서 시연되는 다른 예제 경로는 학습 후에 hold out 영역에서 상당히 다른 결과를 이끌었습니다.</p>\n<center> <img src=\"../../../../img/irl/mmp_8.png\" width=\"1200\"> </center>\n\n<center> <img src=\"../../../../img/irl/mmp_9.png\" width=\"600\"> </center>\n\n<ul>\n<li><p>실험 의도</p>\n<ul>\n<li>Top : Road에 유지하도록 제안</li>\n<li>Bottom : 은밀한 의도를 제시(여기서는 숲을 지나는 의도를 의미)</li>\n</ul>\n</li>\n<li><p>실험 결과</p>\n<ul>\n<li>Left : Training 예제</li>\n<li>Middle : Training 이후에 hold out 영역에서 학습된 cost map</li>\n<li>Right : Hold out 영역에서 $A^*$를 이용하여 생성된 행동 결과</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"5-2-Data-shown-are-MMP-learned-cost-maps\"><a href=\"#5-2-Data-shown-are-MMP-learned-cost-maps\" class=\"headerlink\" title=\"5.2 Data shown are MMP learned cost maps\"></a>5.2 Data shown are MMP learned cost maps</h2><center> <img src=\"../../../../img/irl/mmp_10.png\" width=\"1100\"> </center>\n\n<p>Figure 2은 holdout region으로부터의 결과입니다.</p>\n<p>그림에서 loss-augmented path (blue)은 일반적으로 마지막 학습된 경로보다 일반적으로 좋은 결과를 수행하지 못하는 것을 나타났습니다.</p>\n<p>왜냐하면 loss-augmentation은 높은 loss의 영역을 최종 학습 지도보다 더욱 desirable하게 만들기 때문입니다. </p>\n<p>직관적으로, 만약 학습자가 loss-augmented cost map에 대해서 잘 수행할 수 있다면, loss-augmentation없이도 더욱 잘 수행되어야 한다는 것입니다. 이것은 margin을 가지고 학습된 개념입니다.</p>\n<p><br></p>\n<h2 id=\"5-3-Results-using-two-alternative-approaches\"><a href=\"#5-3-Results-using-two-alternative-approaches\" class=\"headerlink\" title=\"5.3 Results using two alternative approaches\"></a>5.3 Results using two alternative approaches</h2><center> <img src=\"../../../../img/irl/mmp_11.png\" width=\"800\"> </center>\n\n<ul>\n<li>Left : the result of a next-action classifier applied superimposed on a visualization of the second dataset.</li>\n<li>Right : a cost map learned by manual training of a regression.</li>\n</ul>\n<p>두 개의 경우에서 학습된 경로들은 poor approximations. (not shown on left, red on right).</p>\n<p><br></p>\n<h2 id=\"5-4-Visualization-about-losses\"><a href=\"#5-4-Visualization-about-losses\" class=\"headerlink\" title=\"5.4 Visualization about losses\"></a>5.4 Visualization about losses</h2><center> <img src=\"../../../../img/irl/mmp_12.png\" width=\"850\"> </center>\n\n<ul>\n<li>Left : Visualization of inverted Loss function $(1− l(x))$ for a training example path.</li>\n<li>Right : Comparison of holdout loss of MMP (by number of iterations) to a regression method where a teacher hand-labeled costs.</li>\n</ul>\n<p>비교를 위해, 저자는 MMP에 다른 두 개의 접근방법을 사용하여 유사한 학습을 시도하였습니다.</p>\n<ol>\n<li><p>(Lecun et al., 2006)$^5$에서 제안한 방법으로 state 특징들을 다음 action으로 취하는 mapping을 사용한 직접적으로 학습하는 알고리즘입니다. 이 경우, traing data에 대해 좋은 결과를 얻지 못했습니다.</p>\n</li>\n<li><p>다소 더 성공적은 시도는 직접 label을 통해 cost를 학습시킨 알고리즘입니다. 이 알고리즘은 MMP보다 학습자에게 더 많은 명시적 정보를 제공합니다.</p>\n<ul>\n<li>다음을 기반하여 low, medium, high cost로 제공<ol>\n<li>Expert knowledge of the planner</li>\n<li>Iterated training and observation</li>\n<li>The trainer had prior knowledge of the cost maps found under MMP batch learning on this dataset.</li>\n</ol>\n</li>\n<li>추가 정보가 주어진 cost map은 정성적으로 올바른 것처럼 보이지만, 그림 3과 그림 4는 상당히 좋지 않은 성능을 보여줍니다..</li>\n</ul>\n</li>\n</ol>\n<p><br><br></p>\n<h1 id=\"6-Related-and-Future-Work\"><a href=\"#6-Related-and-Future-Work\" class=\"headerlink\" title=\"6. Related and Future Work\"></a>6. Related and Future Work</h1><p>Maximum Margin Planning과 직접적으로 연관된 두 가지 work가 있습니다.</p>\n<p>그 중 하나가 바로 Inverse RL입니다.</p>\n<p>IRL의 목표는 MDP에서 agent의 행동을 관찰하는 하여 agent의 행동으로 부터 reward function를 추출하는 것입니다. 그러나 이것은 기본적으로 ill-posed 문제로 알고 있습니다. 그럼에도 불구하고, MMP와 같이 유사한 효과를 가진 IRL 아이디어들을 시도한 몇 가지 heuristic 시도가 있었습니다.</p>\n<p>유용한 heuristic 방법은 바로 이전 논문인 <em>Abbeel, P., &amp; Ng, A. Y. (2004). Apprenticeship learning via inverse reinforcement learning</em> APP 논문입니다. 학습자의 정책과 시연되는 예제간의 expected feature counts을 통해 매칭을 시도하는 학습 방법입니다.</p>\n<p>MMP의 경우 IRL algorithm의 variant과는 다른 스타일의 algorithm입니다.</p>\n<p>MMP는 하나의 MDP 보다 많은 정책 시연들을 허용하도록 설계되어 있습니다. 여러 특징 맵, 다른 시작 지점과 목표 지점, 완전히 새로운 맵과 목표 지점을 가지는 학습자의 목표를 이용하여 예제들을 시연했습니다. 또한 MMP는 상당히 다른 IRL적 알고리즘 접근 방법을 유도합니다.</p>\n<p>IRL과 MMP간의 관계는 <strong>generative and discriminative learning</strong> 간의 구별을 연상시킵니다.</p>\n<p>일반적인 IRL의 경우, feature matching을 시도합니다. agent가 MDP에서 (거의 최적같이) 행동하고 (거의) feature expectation에 매칭 가능할 때 학습하도록 설계되었습니다(Generative models과 같은 strong 가정). 예를 들어, feature expecatation을 매칭하는 능력은 algorithm의 행동이 feature가 선형인 모든 cost function에 대해서 near-optimal일 것이라는 것을 의미합니다.</p>\n<p>반대로 MMP의 경우, 우리의 목표가 직접적으로 output 행동을 모방하는 것이라는 weaker 가정을 하고 실제 MDP나 reward 함수에 대해 agnostic합니다. 여기서 MDP는 output decision들을 구조화하고 경쟁을 하려고 하는 expert가 natual class을 제공합니다.</p>\n<p>정리하자면,<br>Generative model : 개별 클래스의 분포를 모델링한다.<br>Discriminative model : Discriminative 모델은 기본 확률 분포 또는 데이터 구조를 모델링하지 않고 기본 데이터를 해당 클래스에 직접 매핑(class 경계를 통해 학습). SVM은 이러한 기준을 만족 시키므로 decision tree와 마찬가지로 discriminative model.</p>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"Let’s-do-Inverse-RL-Guide\"><a href=\"#Let’s-do-Inverse-RL-Guide\" class=\"headerlink\" title=\"Let’s do Inverse RL Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2019/01/22/0_lets-do-irl-guide/\">Let’s do Inverse RL Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"APP-여행하기\"><a href=\"#APP-여행하기\" class=\"headerlink\" title=\"APP 여행하기\"></a><a href=\"\">APP 여행하기</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"MaxEnt-여행하기\"><a href=\"#MaxEnt-여행하기\" class=\"headerlink\" title=\"MaxEnt 여행하기\"></a><a href=\"\">MaxEnt 여행하기</a></h2>"},{"title":"Distributional Reinforcement Learning with Quantile Regression","date":"2018-10-22T08:22:40.000Z","author":"민규식","subtitle":"Distributional RL 2번째 논문","_content":"\n<center> <img src=\"https://www.dropbox.com/s/ftl2dcwz274qkh5/paper_qrdqn.png?dl=1\" width=\"800\"> </center>\n\n논문 저자 : [Will Dabney](https://arxiv.org/search/cs?searchtype=author&query=Dabney%2C+W), [Mark Rowland](https://arxiv.org/search/cs?searchtype=author&query=Rowland%2C+M), [Marc G. Bellemare](https://arxiv.org/search/cs?searchtype=author&query=Bellemare%2C+M+G), [Rémi Munos](https://arxiv.org/search/cs?searchtype=author&query=Munos%2C+R)    \n논문 링크 : [ArXiv](https://arxiv.org/abs/1710.10044)\nProceeding : The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)         \n정리 : 민규식\n\n---\n\n## Introduction\n\n본 게시물은 2017년 10월에 발표된 논문 [Distributional Reinforcement Learning with Quantile Regression(QR-DQN)](https://arxiv.org/abs/1710.10044) 의 내용에 대해 설명합니다.\n\n<p align= \"center\">\n<img src=\"https://www.dropbox.com/s/ftl2dcwz274qkh5/paper_qrdqn.png?dl=1\" alt=\"paper\" style=\"width: 800px;\"/>\n\n </p>\n\n<br>\n\n## Algorithm \n\nQR-DQN의 경우 C51과 비교했을 때 다음의 내용들에서 차이를 가집니다. \n\n- Network의 Output\n- Loss\n\n\n\n위와 같이 사실상 별로 다른 점은 없습니다. 위의 내용들에 대해 하나하나 살펴보도록 하겠습니다. \n\n<br>\n\n### 1. Network의 Output\n\n이 파트에서는 QR-DQN이 output으로 어떤 값들을 추정하는지 알아봅니다. 왜 Quantile regression을 이용하는지, 이를 이용하면 어떻게 Wasserstein distance를 줄일 수 있고 이에 따라 distributional RL의 수렴성을 증명하게 되는지, 어떤 quantile을 이용해서 Wasserstein distance를 최소화 할 수 있는지 알아보도록 하겠습니다. \n\n<br>\n\n#### C51 vs QR-DQN\n\n QR-DQN의 경우도 C51과 같이 distributional RL 알고리즘입니다. 이에 따라 QR-DQN에서도 network의 output은 각 action에 대한 value distribution 입니다. C51 게시물에서 value distribution을 구성하는 것은 아래 그림과 같이 **support**와 해당 support의 **확률**, 2가지였습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/hfy4ynt2ic9tgtx/support_and_prob.png?dl=1\" alt=\"value distribution\" width=\"800\"/>\n\n</p>\n\n\n\nQR-DQN과 C51의 경우 output을 구하는데 차이가 있습니다. 해당 차이를 그림으로 나타낸 것이 다음과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/7qkn9a4fz37th6m/c51_qrdqn.png?dl=1\" width=\"600\"/>\n\n</p>\n\n즉 다음과 같은 차이가 있습니다. \n\n- C51: support를 동일한 간격으로 고정, network의 output으로 확률을 구함 \n- QR-DQN: 확률을 동일하게 설정, network의 output으로 support를 구함 \n\n\n\nC51의 경우 support를 구하기 위해서 다음의 parameter들을 결정해줘야 했습니다. \n\n- Support의 수\n- Support의 최대값\n- Support의 최소값\n\n\n\n하지만 QR-DQN의 경우 network가 바로 supports를 구하기 때문에 support의 최대값이나 최소값은 정해줄 필요가 없습니다. 이에 따라 QR-DQN은 support의 수만 추가적인 parameter로 결정해주면 됩니다. QR-DQN에서 확률은 모두 동일하게 결정해주기 때문에 (1/support의 수) 로 단순하게 결정해주면 됩니다. \n\n\n\n#### Quantile Regression\n\n 그럼 QR-DQN은 왜 확률은 고정하고 network를 통해 supports를 선택하는 방법을 취할까요?? 단순히 support와 관련된 parameter들의 수를 줄이기 위함은 아닙니다! 바로 QR-DQN은 **Quantile Regression**이라는 기법을 사용하기 때문입니다. Quantile regression이 무엇인지, 왜 사용하는지 한번 알아보도록 하겠습니다. \n\n\n\n그럼 일단 **Quantile**이 무엇인지부터 알아보겠습니다. 우선 논문에서 사용된 Quantile은 확률분포는 몇 등분 했는가를 나타냅니다. 예를 들어 4-quantiles 라고 하면 아래와 같이 확률분포를 25%씩 4등분 하게 되는 것입니다. 그리고 이때 quantile의 값들은 [0.25, 0.5, 0.75, 1]이 됩니다. \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/dint0ee5q2kkpcm/quantile.png?dl=1\" alt=\"quantile\" width=\"400\"/>\n\n</p>\n\n Quantile regression은 [Cumulative Distribution Function (CDF)](https://en.wikipedia.org/wiki/Cumulative_distribution_function)에서 적용하는 알고리즘이므로 Quantile의 예시를 CDF로 나타낸 결과가 다음과 같습니다. \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/dzkhyy77oljdi7o/cdf.png?dl=1\" alt=\"cdf\" width=\"400\"/>\n\n</p>\n\n\n\nCDF의 함수를 F, distribution function을 Z라고 했을 때 다음과 같이 식을 표시합니다. \n\n <img src=\"https://www.dropbox.com/s/dsone4lqernj54p/cdf_function.png?dl=1\" alt=\"cdf\" width=\"200\"/>\n\n\n\nQuantile regression은 모든 quantile에 대한 CDF의 역함수입니다. 그렇기 때문에 위의 식을 다음과 같이 역함수의 형태로 나타낼 수 있습니다. \n\n<img src=\"https://www.dropbox.com/s/7kgktid1hpc7nba/quantile_regression.png?dl=1\" alt=\"quantile regression\" width=\"200\"/>\n\n즉 Quantile regression은 동일하게 나눈 확률들을 input으로 하여 각각의 support를 구하는 것입니다. 그럼 왜 본 논문에서는 quantile regression을 통해 구한 support들간의 차이를 줄이는 방향으로 학습을 수행할까요? 이것은 **Wasserstein Distance**와 관련이 있습니다. \n\n<br>\n\n#### Wasserstein Distance\n\nC51논문에서 언급하였듯이 Distributional RL은 다음의 contraction 조건을 만족할 때 알고리즘의 수렴성을 보장합니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/wb2vqtia3064ydw/contraction.png?dl=1\" alt=\"cdf\" width=\"600\"/>\n\n</p>\n\n이때 distribution간의 거리를 나타내는 d_p가 Wasserstein distance일때는 수학적으로 위의 조건을 만족하지만 C51은 cross entropy를 이용했기 때문에 수학적으로 위의 조건을 만족한다는 것을 증명할 수 없었습니다. \n\np-Wasserstein distance의 식은 아래와 같습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/jja6ahq9l0q5a78/wasserstein.png?dl=1\" alt=\"cdf\" width=\"600\"/>\n\n</p> \n\n위의 식에서 볼 수 있듯이 p-Wasserstein distance는 CDF의 역함수의 L^p metric입니다. 본 논문에서는 1-Wasserstein distance를 이용합니다. 1-Wasserstein distance는 두 확률분포에 대한 CDF의 역함수간의 차이입니다. 아래의 그래프는 [Distributional RL 블로그](https://mtomassoli.github.io/2017/12/08/distributional_rl/)에서 참고한 그래프입니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/l1zx9hgb271kzar/wasserstein_graph.png?dl=1\" alt=\"cdf\" width=\"500\"/>\n\n</p> \n\n위의 graph에서 하늘색으로 된 부분이 1-Wasserstein distance를 나타냅니다. 이 부분은 두 확률 분포의 CDF의 역함수간 차이입니다. 그런데 아까 Quantile regression에 대해 이야기할 때 quantile regression의 정의가 바로 모든 quantile에 대한 CDF의 역함수였습니다. 즉 이 quantile regression을 통해 구한 support 간의 차이를 줄어들게 되면 wasserstein distance 또한 줄어들게 되는 것입니다. 본 논문에서는 quantile regression을 통해 구한 support를 이용하여 확률분포를 추정하고 이를 target distribution과 유사해지도록 학습을 수행합니다. 그렇기 때문에 본 논문의 방식을 이용하면 distribution간의 Wasserstein distance를 줄이는 방향으로 학습할 수 있는 것이고 이에 따라 contraction 조건을 만족하게 되어 수학적으로 distributional RL의 수렴성 또한 증명하게 되는 것입니다. \n\n\n\n#### Unique Minimizer\n\n하지만 이 논문에서 quantile은 단순히 (1/quantile의 수)를 이용하지 않습니다. 바로 각 quantiles의 중간값인 **quantile midpoint**를 이용합니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/yghdhpm4d2wk94d/midpoint.png?dl=1\" alt=\"cdf\" width=\"500\"/>\n\n</p> \n\n왜 이렇게 할까요? 바로 아래의 Lemma와 같이 두 확률 사이의 중간 지점이 해당 구간에서 Wasserstein distance를 최소로 하는 **unique minimizer**이기 때문입니다.\n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/z75aewiscsojdhl/minimizer_lemma.png?dl=1\" alt=\"cdf\" width=\"700\"/>\n\n</p> \n\n해당 내용을 그래프로 표현한 결과가 다음과 같습니다. 해당 내용은 [Distributional RL 블로그](https://mtomassoli.github.io/2017/12/08/distributional_rl/)의 내용을 참고하였습니다. \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/k7bes1ujqui8vss/midpoint_graph.png?dl=1\" alt=\"cdf\" width=\"800\"/>\n\n</p> \n\n위의 그래프들을 통해 볼 수 있듯이 단순히 (1/Number of quantiles) 를 통해 구한 Wasserstein distance보다 해당 값들의 중간값들을 이용하여 구한 Wasserstein distance의 크기가 더 작은 것을 확인할 수 있습니다. 오른쪽 그래프에서 하늘색 영역이 많이 줄어든 것을 확인할 수 있습니다. 이에 따라 본 논문에서는 quantile regression을 적용할 때 **quantile midpoint**를 이용하여 그때의 support들을 추정합니다. \n\n<br>\n\n이번 파트에서는 왜 QR-DQN이 C51과는 반대로 확률을 고정하고 support들을 추정하는지 살펴보았습니다. 다음 파트에서는 quantile regression 적용에 따라 사용되는 loss인 **Quantile Huber Loss**에 대해서 살펴보도록 하겠습니다. \n\n\n\n<br>\n\n### 2. Quantile Huber Loss\n\n이번 파트에서는 Quantile regression 사용에 따른 quantile regression loss와 여기에 Huber loss를 적용한 Quantile Huber loss에 대해서 살펴보도록 하겠습니다. \n\n<br>\n\n#### Quantile Regression Loss\n\n위에서 보셨듯이 QR-DQN은 Quantile Regression이라는 기법을 이용하여 value distribution을 정의합니다. 이에 따라 **Quantile Regression Loss**라는 특별한 loss를 이용하여 학습을 수행합니다. 우선 quantile regression loss의 목적은 다음의 2가지입니다. \n\n1. Target value distribution과 네트워크를 통해 예측된 value distribution간 차이를 줄이도록 네트워크 학습 \n2. 네트워크가 낮은 quantile에 대해서는 낮은 support값을, 높은 quantile에 대해서는 높은 support를 도출하도록 학습  \n\n\n\n위의 상황에서 1의 경우 일반적인 loss의 목표입니다. Target distribution과 network를 통해 예측된 distribution간의 차이를 최소화 하도록 network를 학습시키는 것이죠. 하지만 2의 경우 quantile regression의 적용 때문에 필요한 부분입니다. 일단 2의 내용에 대해서 살펴보도록 하겠습니다. \n\nQR-DQN은 아래와 같이 CDF를 동일한 수의 quantile로 나누고 그때의 support를 찾는 기법입니다. 한번 예시를 들어보겠습니다. Quantile의 수가 4인 경우 중 tau=[0.25, 0.5, 0.75, 1]이 될 것이고 그 중앙값들은 [0.125, 0.375, 0.625, 0.875]가 될 것입니다. 중앙값들에 대해 network가 도출한 support들이 [1, 4, 5, 7]이라고 해보겠습니다. 위의 결과를 CDF로 나타낸 것이 아래의 그림과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/xk1eey4bh35w0qc/QR_cdf_normal.png?dl=1\" alt=\"cdf normal\" width=\"400\"/>\n\n</p>\n\n 위의 경우 정상적인 형태의 CDF입니다. 결과를 보면 tau의 중앙값 중 낮은 값들은 작은 값의 support를, 높은 값들은 큰 값의 support를 추정하여 cdf의 형태가 그 정의에 맞게 단조 증가하는 형태를 볼 수 있습니다. \n\n하지만 만약 위와 같은 상황에서 network가 도출한 support가 [1, **5**, **4**, 7] 이라고 생각해보겠습니다. 4와 5의 위치만 바뀌었죠? 이 결과를 CDF로 표현한 것이 아래와 같습니다. \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/68ng0senkvdjp7n/QR_cdf_abnormal.png?dl=1\" alt=\"cdf abnormal\" width=\"400\"/>\n\n</p>\n\nCDF는 확률변수 값에 따른 확률을 누적해서 더하기 때문에 확률변수 값이 커질수록 누적확률값이 커지다가 최종적으로 누적 확률이 1이 되는 **단조 증가** 특성을 가집니다. 위의 경우는 확률변수가 증가하는데 반해 누적 확률값은 오르락 내리락 하기 때문에 CDF의 기본 특성 중 하나인 단조 증가 특성을 지키지 못한 형태입니다. Quantile regression이 CDF의 역함수인데 network를 통해 구한 결과가 CDF의 기본적인 특성가지지 못한 이상한 형태로 나오면 quantile regression을 사용하는 의미가 없어지게 됩니다. \n\n위와 같은 이유로 network의 결과는 CDF가 단조증가 특성을 가질 수 있도록 낮은 값부터 높은 값의 순서로 도출되어야 합니다. Quantile regression loss의 경우 낮은 quantile이 높은 값의 support를 추정할수록, 혹은 높은 quantile이 낮은 값의 support를 추정할수록 큰 패널티를 주는 방식으로 설계되어 있습니다. 한번 Quantile regression loss의 계산 과정은 어떻게 되는지, quantile regression loss의 식을 통해 어떻게 penalty를 주는지 한번 알아보도록 하겠습니다. Quantile regression loss의 식은 아래와 같습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/md1l7vbvt7w0brq/quantile_regression_loss.png?dl=1\" alt=\"Quantile regression loss\" width=\"400\"/>\n\n</p>\n\n위의 식은 다음의 과정을 거쳐서 진행됩니다. \n\n1. Target network를 통해 구한 target support들과 network를 통해 추정한 support들의 차이를 구한다. \n   (각 target support와 추정된 support의 차이를 모두 구해야함)\n2. 차이 값이 0보다 작은 경우 (tau-1)을, 0보다 크거나 같은 경우 (tau)를 곱해준다. \n3. 해당 결과를 target에 대해서는 평균을 (E), prediction에 대해서는 sum을 해주어 최종 loss를 도출   \n\n<br>\n\n위의 과정만 봤을때는 어떻게 loss를 구해야 될지 직관적으로 이해되지 않을 수 있기 때문에 한번 예시를 들어보도록 하겠습니다. \n\nTarget supports가 [2, 4, 8, 9]이고 추정된 support가 [1, 4, 5, 8]이라고 해보겠습니다. 예시를 위한 값들을 이용하여 위의 1, 2, 3 과정을 순서대로 살펴보겠습니다. \n\n우선 **과정 1**의 경우, 먼저 target support와 추정된 support 각각 모든 값에 대해 차이를 구해야합니다. 이를 구현하기 위해 target support와 추정된 support를 각각 다른 축으로 쌓아서 matrix의 형태로 만든 다음에 빼주도록 하겠습니다. 위 내용을 아래와 같이 표현할 수 있습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/itc14ozvvglr4xc/qr_dqn_loss1.png?dl=1\" alt=\"Quantile regression loss1\" width=\"800\"/>\n\n</p>\n\n현재 quantile의 수는 4이므로 tau = [0.25, 0.5, 0.75, 1]이고 해당 tau의 중앙값들은 [0.125, 0.375, 0.625, 0.875] 입니다. \n\nQuantile regression loss 중 **과정 2**에 해당하는 부분이 다음과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/fns0p28m7utsyb0/qr_dqn_loss2.png?dl=1\" alt=\"Quantile regression loss\" width=\"300\"/>\n\n</p>\n\nError의 각 column에 해당하는 quantile의 중앙값들을 나타낸 것이 아래의 그림과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/kavn0uisgwdityo/qr_dqn_loss2_1.png?dl=1\" alt=\"Quantile regression loss\" width=\"800\"/>\n\n</p>\n\n이제 과정 2의 연산을 수행한 결과가 아래의 그림과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/6whtc7t28e7dyz9/qr_dqn_loss2_2.png?dl=1\" alt=\"Quantile regression loss\" width=\"800\"/>\n\n</p>\n\n이제 **과정 3**에 해당하는 부분을 살펴보도록 하겠습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/0hlpemekpg16cgr/qr_dqn_loss3_1.png?dl=1\" alt=\"Quantile regression loss\" width=\"300\"/>\n\n</p>\n\n과정 3에서는 과정 2를 제외한 나머지 부분, 즉 **j (target)**에 대해서 평균하고 **i (prediction)**에 대해서 더해주는 부분에 대한 연산만 수행해주면 됩니다. 다음에 살펴볼 논문인 IQN에서는 위의 식을 아래와 같이 표현하기도 합니다. \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/efckt3pgrewclqn/qr_dqn_loss3_2.png?dl=1\" alt=\"Quantile regression loss\" width=\"300\"/>\n\n</p>\n\n이는 과정 2를 통해 구한 matrix의 row들에 대해서는 평균을, column들에 대해서는 sum을 해주면 됩니다. \n\n해당 연산의 결과가 아래와 같습니다. \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/wfjzmvdgx8zxsf1/qr_dqn_loss4.png?dl=1\" alt=\"Quantile regression loss\" width=\"800\"/>\n\n</p>\n\n위와 같이 최종적으로 구한 Quantile regression loss가 3.6525 입니다!! \n\n<br>\n\n그렇다면 Quantile regression loss를 이용하면 어떻게 cdf가 단조증가할 수 있게 support를 추정하게 되는 것일까요? 바로 낮은 quantile이 높은 support를 추정하거나, 높은 quantile이 낮은 support를 추정하는 경우 더 penalty를 많이 줘서 loss의 값이 커지도록 하는 것입니다. \n\n이 경우 또한 예를 들어 설명해보도록 하겠습니다. 위에서 들었던 예시와 동일하게 Target support가 [2,4,8,9]이고 predicted support가 아래와 같이 두 경우일때를 비교해보겠습니다. \n\n1. [2, 4, 8, 3]\n2. [2, 4, 8, 15]\n\n\n\n1과 2의 경우 마지막 추정된 support 이외에는 모두 target값과 동일합니다. 그리고 마지막으로 추정된 support는 target support와 비교했을 때 그 오차의 크기가 6으로 동일합니다. 하지만 **quantile regression의 입장에서는 1번의 경우 2번보다 문제가 큽니다.** 왜냐하면 큰 quantile에 대한 support값이 상대적으로 매우 작은 값을 추정했기 때문입니다. 위의 두 경우에 대한 quantile regression loss 계산 결과는 다음과 같습니다. \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/ikzy0i7k78nmr51/qr_dqn_compare.png?dl=1\" alt=\"Quantile regression loss\" width=\"800\"/>\n\n</p>\n\n위의 결과 중 왼쪽이 predicted support = [2, 4, 8, 3] 일때, 오른쪽이 predicted support = [2, 4, 8, 15] 일 때 입니다. Quantile값 [0.25, 0.5, 0.75, 1] 중 가장 큰 quantile값인 1에 해당하는 support가 작게 나온 경우 quantile huber loss의 값이 더 크게 도출되었습니다!! 이런 차이를 만들어 낸 것은 위의 비교 중에서 다음에 해당하는 부분입니다. \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/vpjidvf7a1dkqry/qr_dqn_compare2.png?dl=1\" alt=\"Quantile regression loss\" width=\"800\"/>\n\n</p>\n\n위 부분에서 볼 수 있듯이 높은 quantile에 대해서 낮은 support값을 추정하는 경우 error (u)가 대부분 양수인 것을 확인할 수 있습니다. 가장 마지막 support에 대해서 u가 0보다 크거나 같은 경우 곱해지는 tau값은 0.875입니다. 반대로 u가 0보다 작은 경우 곱해지는 값은  (tau-1)인 0.125입니다. \n\n이렇게 높은 quantile에 대한 support가 낮은 값을 추정하는 경우 위와 같은 연산 때문에 penalty가 생기게 되고 loss를 줄이는 방향으로 학습하다보면 높은 quantile에 대한 support가 다른 support들에 비해 높은 값을 추정할 수 있도록 학습되는 것입니다!! 위의 예시는 낮은 quantile에 대한 support가 높은 값을 추정하는 경우에도 유사하게 적용할 수 있습니다. \n\n위의 과정을 통해서 낮은 quantile에 대해서는 낮은 support가, 높은 quantile에 대해서는 높은 support가 추정되는 것입니다. 즉, 단조증가의 형태로 cdf를 추정할 수 있게 되는 것입니다. \n\n\n\n#### Quantile Huber Loss\n\n하지만 QR-DQN 논문에서는 quantile regression loss를 그대로 이용하지 않고 **Quantile Huber Loss** 를 사용합니다. \n\n아래의 그림은 [Distributional RL 블로그](https://mtomassoli.github.io/2017/12/08/distributional_rl/)의 자료입니다. \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/ik1v03jcxj1crhi/quantile_huber.png?dl=1\" alt=\"Huber loss\" width=\"500\"/>\n\n</p>\n\n위의 결과에서 주황색선이 일반적인 quantile regression loss를 적용하였을 때 loss의 결과이며 파란색선이 Quantile huber loss를 사용하였을 때 loss의 결과입니다. Deep learning은 일반적으로 gradient를 기반으로 최적화를 수행하며 학습하는 알고리즘입니다. 하지만 quantile regression loss를 그냥 이용하는 경우 0에서 smooth하지 않아 미분이 불가능해집니다. 이런 이유로 [Huber loss](https://en.wikipedia.org/wiki/Huber_loss) 를 추가해주어 학습의 안정성을 높여줍니다. \n\n우선 Huber Loss의 식은 아래와 같습니다. \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/dtfi0y1retnchwa/HuberLoss.png?dl=1\" alt=\"Huber loss\" width=\"500\"/>\n\n</p>\n\n이를 이용하여 Quantile regression loss의 식 중 rho 부분을 다음과 같이 변경합니다. \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/0b1dim8bafuig9p/Quantile_Huber_Loss.png?dl=1\" alt=\"Huber loss\" width=\"500\"/>\n\n</p>\n\n위의 부분에서 기존에는 u<0 일 때 (tau-1) 이었던 것이 (1-tau)로 바뀌었습니다. 이는 Huber loss가 적용되면서 L(u)가 0보다 커졌기 때문에 (1-tau)를 해줘야 최종적으로 loss값이 양수가 됩니다. \n\nRho의 부분을 위와 같이 수정하여 최종적으로 아래와 같은 Quantile Huber Loss에 적용해주면 Quantile Huber Loss에 대한 모든 설명이 마무리됩니다!! :) \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/g834dg6uq8kvtce/Quantile_Huber_Loss_final.png?dl=1\" alt=\"Huber loss\" width=\"500\"/>\n\n</p>\n\n<br>\n\n본 논문의 알고리즘은 다음과 같습니다. \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/wt2ohbo1cifwvw4/QR-DQN_algorithm.png?dl=1\" alt=\"Algorithm\" width=\"600\"/>\n\n</p>\n\n위의 알고리즘에서 볼 수 있듯이 Q-value를 계산하는 방법은 C51과 동일합니다. \n\nTarget distribution을 계산할 때는 target network를 통해 추정한 support에 discount factor gamma를 곱하고 reward를 더하는 방식을 이용합니다. \n\n마지막으로 loss는 Quantile Huber Loss를 이용하여 이를 최소화 하는 방향으로 학습을 수행합니다. \n\n<br>\n\n## Result\n\n본 알고리즘의 성능은 다음의 두 종류의 환경을 이용하여 검증됩니다. \n\n- Two-room windy gridworld\n- Atari 2600 games\n\n각각의 결과에 대해 살펴보겠습니다. \n\n<br>\n\n### Two-room Windy Gridworld\n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/xcy1im29xjc1rel/env1.png?dl=1\" alt=\"Two room windy grid world\" width=\"300\"/>\n\n</p>\n\n해당 환경은 확률적인 환경을 구성하기 위해 gridworld에 몇가지 장치를 추가해주었습니다. Randomness를 추가하기 위해 사용한 장치들은 다음과 같습니다. \n\n- Doorway\n- Wind (바람이 agent를 위쪽 방향으로 밀어냄)\n- Transition stochasticity (0.1 확률로 random한 방향으로 움직임)\n\nAgent는 x_s에서 출발하며 x_G에 도달하면 1의 reward를 얻습니다. 해당 환경에서는 실제로 각 state에서 1000 step 만큼 Monte-Carlo rollout을 수행 후 직접적인 경험을 통해 확률변수를 만들어내고 QR-DQN이 이를 잘 추정하는지 검증합니다.  \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/xivl1qhtfmr8z7b/QR_result_value.png?dl=1\" alt=\"Two room windy grid world\" width=\"700\"/>\n\n</p>\n\n위의 결과가 Monte-Carlo rollout을 통해 구한 value distribution과 QR-DQN을 통해 추정한 value distribution간의 차이를 보여줍니다. 왼쪽의 경우 value-distribution으로, 오른쪽의 경우 CDF로 나타낸 결과입니다. 위 결과와 같이 QR-DQN은 value distribution을 실제와 유사하게 추정한다는 것을 확인할 수 있습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/tzav16rkhtt07cz/result_value_wasserstein.png?dl=1\" alt=\"Two room windy grid world\" width=\"700\"/>\n\n</p>\n\n위의 결과는 Monte-Carlo rollout으로 구한 value 간 차이와 Wasserstein distance를 나타낸 그래프입니다. 위에서 볼 수 있듯이 학습이 진행될수록 Value간의 오차뿐 아니라 Wasserstein distance도 감소하는 것을 확인할 수 있습니다. 이렇게 Wasserstein distance가 줄어드는 것을 통해서 QR-DQN을 이용하는 경우 contraction 조건을 만족하며 distributional RL의 수렴성을 수학적으로 만족함을 확인할 수 있습니다. \n\n\n\n### Atari 2600 \n\nAtari 환경에서 성능을 검증하기 위해 본 논문에서 설정한 파라미터들은 다음과 같습니다. \n\n- Learning rate = 0.00005\n- Epsilon(adam) = 0.01/32\n- Number of Quantiles = 200\n\n본 논문에서는 다양한 deep reinforcement learning 알고리즘 (DQN, Double DQN, Prioritized DQN, C51) 및 Quantile huber loss의 kappa = 0, 1를 적용하였을때 결과를 비교합니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/of3x9s2xkx7y4yo/QR_DQN_Atari3.png?dl=1\" class =\"center\" width=\"500\"/>\n\n</p>\n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/3as73k26vjhjlns/QR_DQN_Atari2.png?dl=1\" class =\"center\" width=\"500\"/>\n\n</p>\n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/giantm2igkpy53t/QR_DQN_Atari1.png?dl=1\" alt=\"algorithm\" class =\"center\" width=\"500\"/>\n\n</p>\n\n위의 결과에서 볼 수 있듯이 QR-DQN을 썼을 때, 그리고 Quantile huber loss의 kappa를 1로 하였을 때 가장 성능이 좋은 것을 확인할 수 있습니다. \n\n<br>\n\n## Conclusion\n\nQR-DQN의 경우 이전 논문인 C51에 비해 다음의 부분들에서 많은 개선점을 가져온 논문이라 할 수 있습니다. \n\n- Wasserstein distance를 줄이는 방향으로 학습을 수행하므로 distributional RL의 수렴성을 수학적으로 만족함!\n- Support와 관련된 파라미터가 Quantile의 숫자 하나밖에 없음! (support의 범위 같은 것을 정할 필요 없음)\n- 귀찮은 Projection 과정 생략 가능 \n\n<br>\n\nQR-DQN의 경우 C51 및 다양한 deep reinforcement learning 알고리즘들에 비해 좋은 성능을 보였으며 확률적인 환경에서 value distribution에 대한 추정도 매우 정확했음을 알 수 있습니다. \n\n다음 게시물에서는 QR-DQN 논문의 후속 논문인 [Implicit Quantile Networks for Distributional Reinforcement Learning(IQN))](https://arxiv.org/abs/1806.06923) 논문에 대해 살펴보도록 하겠습니다!!! 😄\n\n<br>\n\n## Implementation\n\n본 논문의 코드는 다음의 Github를 참고해주세요. \n\n[Github](https://github.com/reinforcement-learning-kr/distributional_rl)\n\n<br>\n\n## Other Posts\n\n[Distributional RL 개요](https://reinforcement-learning-kr.github.io/2018/09/27/Distributional_intro/)\n\n[C51](https://reinforcement-learning-kr.github.io/2018/10/02/C51/)\n\n[IQN](https://reinforcement-learning-kr.github.io/2018/10/30/IQN/)\n\n<br>\n\n## Reference\n\n- [Distributional Reinforcement Learning with Quantile Regression](https://arxiv.org/abs/1710.10044)\n\n- [Blog: Distributional RL](https://mtomassoli.github.io/2017/12/08/distributional_rl/) \n\n- [Blog: Quantile Reinforcement Learning](https://medium.com/@fuller.evan/quantile-reinforcement-learning-56f8b3c3f134)\n\n  ​\n\n\n\n<br>\n\n## Team\n\n민규식: [Github](https://github.com/Kyushik), [Facebook](https://www.facebook.com/kyushik.min)\n\n차금강: [Github](https://github.com/chagmgang), [Facebook](https://www.facebook.com/profile.php?id=100002147815509)\n\n윤승제: [Github](https://github.com/sjYoondeltar), [Facebook](https://www.facebook.com/seungje.yoon)\n\n김하영: [Github](https://github.com/hayoung-kim), [Facebook](https://www.facebook.com/altairyoung)\n\n김정대: [Github](https://github.com/kekmodel), [Facebook](https://www.facebook.com/kekmodel)\n\n\n\n","source":"_posts/QR-DQN.md","raw":"---\ntitle: Distributional Reinforcement Learning with Quantile Regression\ndate: 2018-10-22 17:22:40\ntags: [\"논문\", \"Distributional RL\", \"QR-DQN\"]\ncategories: 논문 정리\nauthor: 민규식\nsubtitle: Distributional RL 2번째 논문\n\n\n---\n\n<center> <img src=\"https://www.dropbox.com/s/ftl2dcwz274qkh5/paper_qrdqn.png?dl=1\" width=\"800\"> </center>\n\n논문 저자 : [Will Dabney](https://arxiv.org/search/cs?searchtype=author&query=Dabney%2C+W), [Mark Rowland](https://arxiv.org/search/cs?searchtype=author&query=Rowland%2C+M), [Marc G. Bellemare](https://arxiv.org/search/cs?searchtype=author&query=Bellemare%2C+M+G), [Rémi Munos](https://arxiv.org/search/cs?searchtype=author&query=Munos%2C+R)    \n논문 링크 : [ArXiv](https://arxiv.org/abs/1710.10044)\nProceeding : The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)         \n정리 : 민규식\n\n---\n\n## Introduction\n\n본 게시물은 2017년 10월에 발표된 논문 [Distributional Reinforcement Learning with Quantile Regression(QR-DQN)](https://arxiv.org/abs/1710.10044) 의 내용에 대해 설명합니다.\n\n<p align= \"center\">\n<img src=\"https://www.dropbox.com/s/ftl2dcwz274qkh5/paper_qrdqn.png?dl=1\" alt=\"paper\" style=\"width: 800px;\"/>\n\n </p>\n\n<br>\n\n## Algorithm \n\nQR-DQN의 경우 C51과 비교했을 때 다음의 내용들에서 차이를 가집니다. \n\n- Network의 Output\n- Loss\n\n\n\n위와 같이 사실상 별로 다른 점은 없습니다. 위의 내용들에 대해 하나하나 살펴보도록 하겠습니다. \n\n<br>\n\n### 1. Network의 Output\n\n이 파트에서는 QR-DQN이 output으로 어떤 값들을 추정하는지 알아봅니다. 왜 Quantile regression을 이용하는지, 이를 이용하면 어떻게 Wasserstein distance를 줄일 수 있고 이에 따라 distributional RL의 수렴성을 증명하게 되는지, 어떤 quantile을 이용해서 Wasserstein distance를 최소화 할 수 있는지 알아보도록 하겠습니다. \n\n<br>\n\n#### C51 vs QR-DQN\n\n QR-DQN의 경우도 C51과 같이 distributional RL 알고리즘입니다. 이에 따라 QR-DQN에서도 network의 output은 각 action에 대한 value distribution 입니다. C51 게시물에서 value distribution을 구성하는 것은 아래 그림과 같이 **support**와 해당 support의 **확률**, 2가지였습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/hfy4ynt2ic9tgtx/support_and_prob.png?dl=1\" alt=\"value distribution\" width=\"800\"/>\n\n</p>\n\n\n\nQR-DQN과 C51의 경우 output을 구하는데 차이가 있습니다. 해당 차이를 그림으로 나타낸 것이 다음과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/7qkn9a4fz37th6m/c51_qrdqn.png?dl=1\" width=\"600\"/>\n\n</p>\n\n즉 다음과 같은 차이가 있습니다. \n\n- C51: support를 동일한 간격으로 고정, network의 output으로 확률을 구함 \n- QR-DQN: 확률을 동일하게 설정, network의 output으로 support를 구함 \n\n\n\nC51의 경우 support를 구하기 위해서 다음의 parameter들을 결정해줘야 했습니다. \n\n- Support의 수\n- Support의 최대값\n- Support의 최소값\n\n\n\n하지만 QR-DQN의 경우 network가 바로 supports를 구하기 때문에 support의 최대값이나 최소값은 정해줄 필요가 없습니다. 이에 따라 QR-DQN은 support의 수만 추가적인 parameter로 결정해주면 됩니다. QR-DQN에서 확률은 모두 동일하게 결정해주기 때문에 (1/support의 수) 로 단순하게 결정해주면 됩니다. \n\n\n\n#### Quantile Regression\n\n 그럼 QR-DQN은 왜 확률은 고정하고 network를 통해 supports를 선택하는 방법을 취할까요?? 단순히 support와 관련된 parameter들의 수를 줄이기 위함은 아닙니다! 바로 QR-DQN은 **Quantile Regression**이라는 기법을 사용하기 때문입니다. Quantile regression이 무엇인지, 왜 사용하는지 한번 알아보도록 하겠습니다. \n\n\n\n그럼 일단 **Quantile**이 무엇인지부터 알아보겠습니다. 우선 논문에서 사용된 Quantile은 확률분포는 몇 등분 했는가를 나타냅니다. 예를 들어 4-quantiles 라고 하면 아래와 같이 확률분포를 25%씩 4등분 하게 되는 것입니다. 그리고 이때 quantile의 값들은 [0.25, 0.5, 0.75, 1]이 됩니다. \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/dint0ee5q2kkpcm/quantile.png?dl=1\" alt=\"quantile\" width=\"400\"/>\n\n</p>\n\n Quantile regression은 [Cumulative Distribution Function (CDF)](https://en.wikipedia.org/wiki/Cumulative_distribution_function)에서 적용하는 알고리즘이므로 Quantile의 예시를 CDF로 나타낸 결과가 다음과 같습니다. \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/dzkhyy77oljdi7o/cdf.png?dl=1\" alt=\"cdf\" width=\"400\"/>\n\n</p>\n\n\n\nCDF의 함수를 F, distribution function을 Z라고 했을 때 다음과 같이 식을 표시합니다. \n\n <img src=\"https://www.dropbox.com/s/dsone4lqernj54p/cdf_function.png?dl=1\" alt=\"cdf\" width=\"200\"/>\n\n\n\nQuantile regression은 모든 quantile에 대한 CDF의 역함수입니다. 그렇기 때문에 위의 식을 다음과 같이 역함수의 형태로 나타낼 수 있습니다. \n\n<img src=\"https://www.dropbox.com/s/7kgktid1hpc7nba/quantile_regression.png?dl=1\" alt=\"quantile regression\" width=\"200\"/>\n\n즉 Quantile regression은 동일하게 나눈 확률들을 input으로 하여 각각의 support를 구하는 것입니다. 그럼 왜 본 논문에서는 quantile regression을 통해 구한 support들간의 차이를 줄이는 방향으로 학습을 수행할까요? 이것은 **Wasserstein Distance**와 관련이 있습니다. \n\n<br>\n\n#### Wasserstein Distance\n\nC51논문에서 언급하였듯이 Distributional RL은 다음의 contraction 조건을 만족할 때 알고리즘의 수렴성을 보장합니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/wb2vqtia3064ydw/contraction.png?dl=1\" alt=\"cdf\" width=\"600\"/>\n\n</p>\n\n이때 distribution간의 거리를 나타내는 d_p가 Wasserstein distance일때는 수학적으로 위의 조건을 만족하지만 C51은 cross entropy를 이용했기 때문에 수학적으로 위의 조건을 만족한다는 것을 증명할 수 없었습니다. \n\np-Wasserstein distance의 식은 아래와 같습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/jja6ahq9l0q5a78/wasserstein.png?dl=1\" alt=\"cdf\" width=\"600\"/>\n\n</p> \n\n위의 식에서 볼 수 있듯이 p-Wasserstein distance는 CDF의 역함수의 L^p metric입니다. 본 논문에서는 1-Wasserstein distance를 이용합니다. 1-Wasserstein distance는 두 확률분포에 대한 CDF의 역함수간의 차이입니다. 아래의 그래프는 [Distributional RL 블로그](https://mtomassoli.github.io/2017/12/08/distributional_rl/)에서 참고한 그래프입니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/l1zx9hgb271kzar/wasserstein_graph.png?dl=1\" alt=\"cdf\" width=\"500\"/>\n\n</p> \n\n위의 graph에서 하늘색으로 된 부분이 1-Wasserstein distance를 나타냅니다. 이 부분은 두 확률 분포의 CDF의 역함수간 차이입니다. 그런데 아까 Quantile regression에 대해 이야기할 때 quantile regression의 정의가 바로 모든 quantile에 대한 CDF의 역함수였습니다. 즉 이 quantile regression을 통해 구한 support 간의 차이를 줄어들게 되면 wasserstein distance 또한 줄어들게 되는 것입니다. 본 논문에서는 quantile regression을 통해 구한 support를 이용하여 확률분포를 추정하고 이를 target distribution과 유사해지도록 학습을 수행합니다. 그렇기 때문에 본 논문의 방식을 이용하면 distribution간의 Wasserstein distance를 줄이는 방향으로 학습할 수 있는 것이고 이에 따라 contraction 조건을 만족하게 되어 수학적으로 distributional RL의 수렴성 또한 증명하게 되는 것입니다. \n\n\n\n#### Unique Minimizer\n\n하지만 이 논문에서 quantile은 단순히 (1/quantile의 수)를 이용하지 않습니다. 바로 각 quantiles의 중간값인 **quantile midpoint**를 이용합니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/yghdhpm4d2wk94d/midpoint.png?dl=1\" alt=\"cdf\" width=\"500\"/>\n\n</p> \n\n왜 이렇게 할까요? 바로 아래의 Lemma와 같이 두 확률 사이의 중간 지점이 해당 구간에서 Wasserstein distance를 최소로 하는 **unique minimizer**이기 때문입니다.\n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/z75aewiscsojdhl/minimizer_lemma.png?dl=1\" alt=\"cdf\" width=\"700\"/>\n\n</p> \n\n해당 내용을 그래프로 표현한 결과가 다음과 같습니다. 해당 내용은 [Distributional RL 블로그](https://mtomassoli.github.io/2017/12/08/distributional_rl/)의 내용을 참고하였습니다. \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/k7bes1ujqui8vss/midpoint_graph.png?dl=1\" alt=\"cdf\" width=\"800\"/>\n\n</p> \n\n위의 그래프들을 통해 볼 수 있듯이 단순히 (1/Number of quantiles) 를 통해 구한 Wasserstein distance보다 해당 값들의 중간값들을 이용하여 구한 Wasserstein distance의 크기가 더 작은 것을 확인할 수 있습니다. 오른쪽 그래프에서 하늘색 영역이 많이 줄어든 것을 확인할 수 있습니다. 이에 따라 본 논문에서는 quantile regression을 적용할 때 **quantile midpoint**를 이용하여 그때의 support들을 추정합니다. \n\n<br>\n\n이번 파트에서는 왜 QR-DQN이 C51과는 반대로 확률을 고정하고 support들을 추정하는지 살펴보았습니다. 다음 파트에서는 quantile regression 적용에 따라 사용되는 loss인 **Quantile Huber Loss**에 대해서 살펴보도록 하겠습니다. \n\n\n\n<br>\n\n### 2. Quantile Huber Loss\n\n이번 파트에서는 Quantile regression 사용에 따른 quantile regression loss와 여기에 Huber loss를 적용한 Quantile Huber loss에 대해서 살펴보도록 하겠습니다. \n\n<br>\n\n#### Quantile Regression Loss\n\n위에서 보셨듯이 QR-DQN은 Quantile Regression이라는 기법을 이용하여 value distribution을 정의합니다. 이에 따라 **Quantile Regression Loss**라는 특별한 loss를 이용하여 학습을 수행합니다. 우선 quantile regression loss의 목적은 다음의 2가지입니다. \n\n1. Target value distribution과 네트워크를 통해 예측된 value distribution간 차이를 줄이도록 네트워크 학습 \n2. 네트워크가 낮은 quantile에 대해서는 낮은 support값을, 높은 quantile에 대해서는 높은 support를 도출하도록 학습  \n\n\n\n위의 상황에서 1의 경우 일반적인 loss의 목표입니다. Target distribution과 network를 통해 예측된 distribution간의 차이를 최소화 하도록 network를 학습시키는 것이죠. 하지만 2의 경우 quantile regression의 적용 때문에 필요한 부분입니다. 일단 2의 내용에 대해서 살펴보도록 하겠습니다. \n\nQR-DQN은 아래와 같이 CDF를 동일한 수의 quantile로 나누고 그때의 support를 찾는 기법입니다. 한번 예시를 들어보겠습니다. Quantile의 수가 4인 경우 중 tau=[0.25, 0.5, 0.75, 1]이 될 것이고 그 중앙값들은 [0.125, 0.375, 0.625, 0.875]가 될 것입니다. 중앙값들에 대해 network가 도출한 support들이 [1, 4, 5, 7]이라고 해보겠습니다. 위의 결과를 CDF로 나타낸 것이 아래의 그림과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/xk1eey4bh35w0qc/QR_cdf_normal.png?dl=1\" alt=\"cdf normal\" width=\"400\"/>\n\n</p>\n\n 위의 경우 정상적인 형태의 CDF입니다. 결과를 보면 tau의 중앙값 중 낮은 값들은 작은 값의 support를, 높은 값들은 큰 값의 support를 추정하여 cdf의 형태가 그 정의에 맞게 단조 증가하는 형태를 볼 수 있습니다. \n\n하지만 만약 위와 같은 상황에서 network가 도출한 support가 [1, **5**, **4**, 7] 이라고 생각해보겠습니다. 4와 5의 위치만 바뀌었죠? 이 결과를 CDF로 표현한 것이 아래와 같습니다. \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/68ng0senkvdjp7n/QR_cdf_abnormal.png?dl=1\" alt=\"cdf abnormal\" width=\"400\"/>\n\n</p>\n\nCDF는 확률변수 값에 따른 확률을 누적해서 더하기 때문에 확률변수 값이 커질수록 누적확률값이 커지다가 최종적으로 누적 확률이 1이 되는 **단조 증가** 특성을 가집니다. 위의 경우는 확률변수가 증가하는데 반해 누적 확률값은 오르락 내리락 하기 때문에 CDF의 기본 특성 중 하나인 단조 증가 특성을 지키지 못한 형태입니다. Quantile regression이 CDF의 역함수인데 network를 통해 구한 결과가 CDF의 기본적인 특성가지지 못한 이상한 형태로 나오면 quantile regression을 사용하는 의미가 없어지게 됩니다. \n\n위와 같은 이유로 network의 결과는 CDF가 단조증가 특성을 가질 수 있도록 낮은 값부터 높은 값의 순서로 도출되어야 합니다. Quantile regression loss의 경우 낮은 quantile이 높은 값의 support를 추정할수록, 혹은 높은 quantile이 낮은 값의 support를 추정할수록 큰 패널티를 주는 방식으로 설계되어 있습니다. 한번 Quantile regression loss의 계산 과정은 어떻게 되는지, quantile regression loss의 식을 통해 어떻게 penalty를 주는지 한번 알아보도록 하겠습니다. Quantile regression loss의 식은 아래와 같습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/md1l7vbvt7w0brq/quantile_regression_loss.png?dl=1\" alt=\"Quantile regression loss\" width=\"400\"/>\n\n</p>\n\n위의 식은 다음의 과정을 거쳐서 진행됩니다. \n\n1. Target network를 통해 구한 target support들과 network를 통해 추정한 support들의 차이를 구한다. \n   (각 target support와 추정된 support의 차이를 모두 구해야함)\n2. 차이 값이 0보다 작은 경우 (tau-1)을, 0보다 크거나 같은 경우 (tau)를 곱해준다. \n3. 해당 결과를 target에 대해서는 평균을 (E), prediction에 대해서는 sum을 해주어 최종 loss를 도출   \n\n<br>\n\n위의 과정만 봤을때는 어떻게 loss를 구해야 될지 직관적으로 이해되지 않을 수 있기 때문에 한번 예시를 들어보도록 하겠습니다. \n\nTarget supports가 [2, 4, 8, 9]이고 추정된 support가 [1, 4, 5, 8]이라고 해보겠습니다. 예시를 위한 값들을 이용하여 위의 1, 2, 3 과정을 순서대로 살펴보겠습니다. \n\n우선 **과정 1**의 경우, 먼저 target support와 추정된 support 각각 모든 값에 대해 차이를 구해야합니다. 이를 구현하기 위해 target support와 추정된 support를 각각 다른 축으로 쌓아서 matrix의 형태로 만든 다음에 빼주도록 하겠습니다. 위 내용을 아래와 같이 표현할 수 있습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/itc14ozvvglr4xc/qr_dqn_loss1.png?dl=1\" alt=\"Quantile regression loss1\" width=\"800\"/>\n\n</p>\n\n현재 quantile의 수는 4이므로 tau = [0.25, 0.5, 0.75, 1]이고 해당 tau의 중앙값들은 [0.125, 0.375, 0.625, 0.875] 입니다. \n\nQuantile regression loss 중 **과정 2**에 해당하는 부분이 다음과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/fns0p28m7utsyb0/qr_dqn_loss2.png?dl=1\" alt=\"Quantile regression loss\" width=\"300\"/>\n\n</p>\n\nError의 각 column에 해당하는 quantile의 중앙값들을 나타낸 것이 아래의 그림과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/kavn0uisgwdityo/qr_dqn_loss2_1.png?dl=1\" alt=\"Quantile regression loss\" width=\"800\"/>\n\n</p>\n\n이제 과정 2의 연산을 수행한 결과가 아래의 그림과 같습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/6whtc7t28e7dyz9/qr_dqn_loss2_2.png?dl=1\" alt=\"Quantile regression loss\" width=\"800\"/>\n\n</p>\n\n이제 **과정 3**에 해당하는 부분을 살펴보도록 하겠습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/0hlpemekpg16cgr/qr_dqn_loss3_1.png?dl=1\" alt=\"Quantile regression loss\" width=\"300\"/>\n\n</p>\n\n과정 3에서는 과정 2를 제외한 나머지 부분, 즉 **j (target)**에 대해서 평균하고 **i (prediction)**에 대해서 더해주는 부분에 대한 연산만 수행해주면 됩니다. 다음에 살펴볼 논문인 IQN에서는 위의 식을 아래와 같이 표현하기도 합니다. \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/efckt3pgrewclqn/qr_dqn_loss3_2.png?dl=1\" alt=\"Quantile regression loss\" width=\"300\"/>\n\n</p>\n\n이는 과정 2를 통해 구한 matrix의 row들에 대해서는 평균을, column들에 대해서는 sum을 해주면 됩니다. \n\n해당 연산의 결과가 아래와 같습니다. \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/wfjzmvdgx8zxsf1/qr_dqn_loss4.png?dl=1\" alt=\"Quantile regression loss\" width=\"800\"/>\n\n</p>\n\n위와 같이 최종적으로 구한 Quantile regression loss가 3.6525 입니다!! \n\n<br>\n\n그렇다면 Quantile regression loss를 이용하면 어떻게 cdf가 단조증가할 수 있게 support를 추정하게 되는 것일까요? 바로 낮은 quantile이 높은 support를 추정하거나, 높은 quantile이 낮은 support를 추정하는 경우 더 penalty를 많이 줘서 loss의 값이 커지도록 하는 것입니다. \n\n이 경우 또한 예를 들어 설명해보도록 하겠습니다. 위에서 들었던 예시와 동일하게 Target support가 [2,4,8,9]이고 predicted support가 아래와 같이 두 경우일때를 비교해보겠습니다. \n\n1. [2, 4, 8, 3]\n2. [2, 4, 8, 15]\n\n\n\n1과 2의 경우 마지막 추정된 support 이외에는 모두 target값과 동일합니다. 그리고 마지막으로 추정된 support는 target support와 비교했을 때 그 오차의 크기가 6으로 동일합니다. 하지만 **quantile regression의 입장에서는 1번의 경우 2번보다 문제가 큽니다.** 왜냐하면 큰 quantile에 대한 support값이 상대적으로 매우 작은 값을 추정했기 때문입니다. 위의 두 경우에 대한 quantile regression loss 계산 결과는 다음과 같습니다. \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/ikzy0i7k78nmr51/qr_dqn_compare.png?dl=1\" alt=\"Quantile regression loss\" width=\"800\"/>\n\n</p>\n\n위의 결과 중 왼쪽이 predicted support = [2, 4, 8, 3] 일때, 오른쪽이 predicted support = [2, 4, 8, 15] 일 때 입니다. Quantile값 [0.25, 0.5, 0.75, 1] 중 가장 큰 quantile값인 1에 해당하는 support가 작게 나온 경우 quantile huber loss의 값이 더 크게 도출되었습니다!! 이런 차이를 만들어 낸 것은 위의 비교 중에서 다음에 해당하는 부분입니다. \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/vpjidvf7a1dkqry/qr_dqn_compare2.png?dl=1\" alt=\"Quantile regression loss\" width=\"800\"/>\n\n</p>\n\n위 부분에서 볼 수 있듯이 높은 quantile에 대해서 낮은 support값을 추정하는 경우 error (u)가 대부분 양수인 것을 확인할 수 있습니다. 가장 마지막 support에 대해서 u가 0보다 크거나 같은 경우 곱해지는 tau값은 0.875입니다. 반대로 u가 0보다 작은 경우 곱해지는 값은  (tau-1)인 0.125입니다. \n\n이렇게 높은 quantile에 대한 support가 낮은 값을 추정하는 경우 위와 같은 연산 때문에 penalty가 생기게 되고 loss를 줄이는 방향으로 학습하다보면 높은 quantile에 대한 support가 다른 support들에 비해 높은 값을 추정할 수 있도록 학습되는 것입니다!! 위의 예시는 낮은 quantile에 대한 support가 높은 값을 추정하는 경우에도 유사하게 적용할 수 있습니다. \n\n위의 과정을 통해서 낮은 quantile에 대해서는 낮은 support가, 높은 quantile에 대해서는 높은 support가 추정되는 것입니다. 즉, 단조증가의 형태로 cdf를 추정할 수 있게 되는 것입니다. \n\n\n\n#### Quantile Huber Loss\n\n하지만 QR-DQN 논문에서는 quantile regression loss를 그대로 이용하지 않고 **Quantile Huber Loss** 를 사용합니다. \n\n아래의 그림은 [Distributional RL 블로그](https://mtomassoli.github.io/2017/12/08/distributional_rl/)의 자료입니다. \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/ik1v03jcxj1crhi/quantile_huber.png?dl=1\" alt=\"Huber loss\" width=\"500\"/>\n\n</p>\n\n위의 결과에서 주황색선이 일반적인 quantile regression loss를 적용하였을 때 loss의 결과이며 파란색선이 Quantile huber loss를 사용하였을 때 loss의 결과입니다. Deep learning은 일반적으로 gradient를 기반으로 최적화를 수행하며 학습하는 알고리즘입니다. 하지만 quantile regression loss를 그냥 이용하는 경우 0에서 smooth하지 않아 미분이 불가능해집니다. 이런 이유로 [Huber loss](https://en.wikipedia.org/wiki/Huber_loss) 를 추가해주어 학습의 안정성을 높여줍니다. \n\n우선 Huber Loss의 식은 아래와 같습니다. \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/dtfi0y1retnchwa/HuberLoss.png?dl=1\" alt=\"Huber loss\" width=\"500\"/>\n\n</p>\n\n이를 이용하여 Quantile regression loss의 식 중 rho 부분을 다음과 같이 변경합니다. \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/0b1dim8bafuig9p/Quantile_Huber_Loss.png?dl=1\" alt=\"Huber loss\" width=\"500\"/>\n\n</p>\n\n위의 부분에서 기존에는 u<0 일 때 (tau-1) 이었던 것이 (1-tau)로 바뀌었습니다. 이는 Huber loss가 적용되면서 L(u)가 0보다 커졌기 때문에 (1-tau)를 해줘야 최종적으로 loss값이 양수가 됩니다. \n\nRho의 부분을 위와 같이 수정하여 최종적으로 아래와 같은 Quantile Huber Loss에 적용해주면 Quantile Huber Loss에 대한 모든 설명이 마무리됩니다!! :) \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/g834dg6uq8kvtce/Quantile_Huber_Loss_final.png?dl=1\" alt=\"Huber loss\" width=\"500\"/>\n\n</p>\n\n<br>\n\n본 논문의 알고리즘은 다음과 같습니다. \n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/wt2ohbo1cifwvw4/QR-DQN_algorithm.png?dl=1\" alt=\"Algorithm\" width=\"600\"/>\n\n</p>\n\n위의 알고리즘에서 볼 수 있듯이 Q-value를 계산하는 방법은 C51과 동일합니다. \n\nTarget distribution을 계산할 때는 target network를 통해 추정한 support에 discount factor gamma를 곱하고 reward를 더하는 방식을 이용합니다. \n\n마지막으로 loss는 Quantile Huber Loss를 이용하여 이를 최소화 하는 방향으로 학습을 수행합니다. \n\n<br>\n\n## Result\n\n본 알고리즘의 성능은 다음의 두 종류의 환경을 이용하여 검증됩니다. \n\n- Two-room windy gridworld\n- Atari 2600 games\n\n각각의 결과에 대해 살펴보겠습니다. \n\n<br>\n\n### Two-room Windy Gridworld\n\n <p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/xcy1im29xjc1rel/env1.png?dl=1\" alt=\"Two room windy grid world\" width=\"300\"/>\n\n</p>\n\n해당 환경은 확률적인 환경을 구성하기 위해 gridworld에 몇가지 장치를 추가해주었습니다. Randomness를 추가하기 위해 사용한 장치들은 다음과 같습니다. \n\n- Doorway\n- Wind (바람이 agent를 위쪽 방향으로 밀어냄)\n- Transition stochasticity (0.1 확률로 random한 방향으로 움직임)\n\nAgent는 x_s에서 출발하며 x_G에 도달하면 1의 reward를 얻습니다. 해당 환경에서는 실제로 각 state에서 1000 step 만큼 Monte-Carlo rollout을 수행 후 직접적인 경험을 통해 확률변수를 만들어내고 QR-DQN이 이를 잘 추정하는지 검증합니다.  \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/xivl1qhtfmr8z7b/QR_result_value.png?dl=1\" alt=\"Two room windy grid world\" width=\"700\"/>\n\n</p>\n\n위의 결과가 Monte-Carlo rollout을 통해 구한 value distribution과 QR-DQN을 통해 추정한 value distribution간의 차이를 보여줍니다. 왼쪽의 경우 value-distribution으로, 오른쪽의 경우 CDF로 나타낸 결과입니다. 위 결과와 같이 QR-DQN은 value distribution을 실제와 유사하게 추정한다는 것을 확인할 수 있습니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/tzav16rkhtt07cz/result_value_wasserstein.png?dl=1\" alt=\"Two room windy grid world\" width=\"700\"/>\n\n</p>\n\n위의 결과는 Monte-Carlo rollout으로 구한 value 간 차이와 Wasserstein distance를 나타낸 그래프입니다. 위에서 볼 수 있듯이 학습이 진행될수록 Value간의 오차뿐 아니라 Wasserstein distance도 감소하는 것을 확인할 수 있습니다. 이렇게 Wasserstein distance가 줄어드는 것을 통해서 QR-DQN을 이용하는 경우 contraction 조건을 만족하며 distributional RL의 수렴성을 수학적으로 만족함을 확인할 수 있습니다. \n\n\n\n### Atari 2600 \n\nAtari 환경에서 성능을 검증하기 위해 본 논문에서 설정한 파라미터들은 다음과 같습니다. \n\n- Learning rate = 0.00005\n- Epsilon(adam) = 0.01/32\n- Number of Quantiles = 200\n\n본 논문에서는 다양한 deep reinforcement learning 알고리즘 (DQN, Double DQN, Prioritized DQN, C51) 및 Quantile huber loss의 kappa = 0, 1를 적용하였을때 결과를 비교합니다. \n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/of3x9s2xkx7y4yo/QR_DQN_Atari3.png?dl=1\" class =\"center\" width=\"500\"/>\n\n</p>\n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/3as73k26vjhjlns/QR_DQN_Atari2.png?dl=1\" class =\"center\" width=\"500\"/>\n\n</p>\n\n<p align=\"center\">\n\n <img src=\"https://www.dropbox.com/s/giantm2igkpy53t/QR_DQN_Atari1.png?dl=1\" alt=\"algorithm\" class =\"center\" width=\"500\"/>\n\n</p>\n\n위의 결과에서 볼 수 있듯이 QR-DQN을 썼을 때, 그리고 Quantile huber loss의 kappa를 1로 하였을 때 가장 성능이 좋은 것을 확인할 수 있습니다. \n\n<br>\n\n## Conclusion\n\nQR-DQN의 경우 이전 논문인 C51에 비해 다음의 부분들에서 많은 개선점을 가져온 논문이라 할 수 있습니다. \n\n- Wasserstein distance를 줄이는 방향으로 학습을 수행하므로 distributional RL의 수렴성을 수학적으로 만족함!\n- Support와 관련된 파라미터가 Quantile의 숫자 하나밖에 없음! (support의 범위 같은 것을 정할 필요 없음)\n- 귀찮은 Projection 과정 생략 가능 \n\n<br>\n\nQR-DQN의 경우 C51 및 다양한 deep reinforcement learning 알고리즘들에 비해 좋은 성능을 보였으며 확률적인 환경에서 value distribution에 대한 추정도 매우 정확했음을 알 수 있습니다. \n\n다음 게시물에서는 QR-DQN 논문의 후속 논문인 [Implicit Quantile Networks for Distributional Reinforcement Learning(IQN))](https://arxiv.org/abs/1806.06923) 논문에 대해 살펴보도록 하겠습니다!!! 😄\n\n<br>\n\n## Implementation\n\n본 논문의 코드는 다음의 Github를 참고해주세요. \n\n[Github](https://github.com/reinforcement-learning-kr/distributional_rl)\n\n<br>\n\n## Other Posts\n\n[Distributional RL 개요](https://reinforcement-learning-kr.github.io/2018/09/27/Distributional_intro/)\n\n[C51](https://reinforcement-learning-kr.github.io/2018/10/02/C51/)\n\n[IQN](https://reinforcement-learning-kr.github.io/2018/10/30/IQN/)\n\n<br>\n\n## Reference\n\n- [Distributional Reinforcement Learning with Quantile Regression](https://arxiv.org/abs/1710.10044)\n\n- [Blog: Distributional RL](https://mtomassoli.github.io/2017/12/08/distributional_rl/) \n\n- [Blog: Quantile Reinforcement Learning](https://medium.com/@fuller.evan/quantile-reinforcement-learning-56f8b3c3f134)\n\n  ​\n\n\n\n<br>\n\n## Team\n\n민규식: [Github](https://github.com/Kyushik), [Facebook](https://www.facebook.com/kyushik.min)\n\n차금강: [Github](https://github.com/chagmgang), [Facebook](https://www.facebook.com/profile.php?id=100002147815509)\n\n윤승제: [Github](https://github.com/sjYoondeltar), [Facebook](https://www.facebook.com/seungje.yoon)\n\n김하영: [Github](https://github.com/hayoung-kim), [Facebook](https://www.facebook.com/altairyoung)\n\n김정대: [Github](https://github.com/kekmodel), [Facebook](https://www.facebook.com/kekmodel)\n\n\n\n","slug":"QR-DQN","published":1,"updated":"2019-02-07T11:21:31.998Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjrujlu12001l5wfe9b8501z6","content":"<center> <img src=\"https://www.dropbox.com/s/ftl2dcwz274qkh5/paper_qrdqn.png?dl=1\" width=\"800\"> </center>\n\n<p>논문 저자 : <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Dabney%2C+W\" target=\"_blank\" rel=\"noopener\">Will Dabney</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Rowland%2C+M\" target=\"_blank\" rel=\"noopener\">Mark Rowland</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Bellemare%2C+M+G\" target=\"_blank\" rel=\"noopener\">Marc G. Bellemare</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Munos%2C+R\" target=\"_blank\" rel=\"noopener\">Rémi Munos</a><br>논문 링크 : <a href=\"https://arxiv.org/abs/1710.10044\" target=\"_blank\" rel=\"noopener\">ArXiv</a><br>Proceeding : The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)<br>정리 : 민규식</p>\n<hr>\n<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>본 게시물은 2017년 10월에 발표된 논문 <a href=\"https://arxiv.org/abs/1710.10044\" target=\"_blank\" rel=\"noopener\">Distributional Reinforcement Learning with Quantile Regression(QR-DQN)</a> 의 내용에 대해 설명합니다.</p>\n<p align=\"center\"><br><img src=\"https://www.dropbox.com/s/ftl2dcwz274qkh5/paper_qrdqn.png?dl=1\" alt=\"paper\" style=\"width: 800px;\"><br><br> </p>\n\n<p><br></p>\n<h2 id=\"Algorithm\"><a href=\"#Algorithm\" class=\"headerlink\" title=\"Algorithm\"></a>Algorithm</h2><p>QR-DQN의 경우 C51과 비교했을 때 다음의 내용들에서 차이를 가집니다. </p>\n<ul>\n<li>Network의 Output</li>\n<li>Loss</li>\n</ul>\n<p>위와 같이 사실상 별로 다른 점은 없습니다. 위의 내용들에 대해 하나하나 살펴보도록 하겠습니다. </p>\n<p><br></p>\n<h3 id=\"1-Network의-Output\"><a href=\"#1-Network의-Output\" class=\"headerlink\" title=\"1. Network의 Output\"></a>1. Network의 Output</h3><p>이 파트에서는 QR-DQN이 output으로 어떤 값들을 추정하는지 알아봅니다. 왜 Quantile regression을 이용하는지, 이를 이용하면 어떻게 Wasserstein distance를 줄일 수 있고 이에 따라 distributional RL의 수렴성을 증명하게 되는지, 어떤 quantile을 이용해서 Wasserstein distance를 최소화 할 수 있는지 알아보도록 하겠습니다. </p>\n<p><br></p>\n<h4 id=\"C51-vs-QR-DQN\"><a href=\"#C51-vs-QR-DQN\" class=\"headerlink\" title=\"C51 vs QR-DQN\"></a>C51 vs QR-DQN</h4><p> QR-DQN의 경우도 C51과 같이 distributional RL 알고리즘입니다. 이에 따라 QR-DQN에서도 network의 output은 각 action에 대한 value distribution 입니다. C51 게시물에서 value distribution을 구성하는 것은 아래 그림과 같이 <strong>support</strong>와 해당 support의 <strong>확률</strong>, 2가지였습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/hfy4ynt2ic9tgtx/support_and_prob.png?dl=1\" alt=\"value distribution\" width=\"800\"><br><br></p>\n\n\n\n<p>QR-DQN과 C51의 경우 output을 구하는데 차이가 있습니다. 해당 차이를 그림으로 나타낸 것이 다음과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/7qkn9a4fz37th6m/c51_qrdqn.png?dl=1\" width=\"600\"><br><br></p>\n\n<p>즉 다음과 같은 차이가 있습니다. </p>\n<ul>\n<li>C51: support를 동일한 간격으로 고정, network의 output으로 확률을 구함 </li>\n<li>QR-DQN: 확률을 동일하게 설정, network의 output으로 support를 구함 </li>\n</ul>\n<p>C51의 경우 support를 구하기 위해서 다음의 parameter들을 결정해줘야 했습니다. </p>\n<ul>\n<li>Support의 수</li>\n<li>Support의 최대값</li>\n<li>Support의 최소값</li>\n</ul>\n<p>하지만 QR-DQN의 경우 network가 바로 supports를 구하기 때문에 support의 최대값이나 최소값은 정해줄 필요가 없습니다. 이에 따라 QR-DQN은 support의 수만 추가적인 parameter로 결정해주면 됩니다. QR-DQN에서 확률은 모두 동일하게 결정해주기 때문에 (1/support의 수) 로 단순하게 결정해주면 됩니다. </p>\n<h4 id=\"Quantile-Regression\"><a href=\"#Quantile-Regression\" class=\"headerlink\" title=\"Quantile Regression\"></a>Quantile Regression</h4><p> 그럼 QR-DQN은 왜 확률은 고정하고 network를 통해 supports를 선택하는 방법을 취할까요?? 단순히 support와 관련된 parameter들의 수를 줄이기 위함은 아닙니다! 바로 QR-DQN은 <strong>Quantile Regression</strong>이라는 기법을 사용하기 때문입니다. Quantile regression이 무엇인지, 왜 사용하는지 한번 알아보도록 하겠습니다. </p>\n<p>그럼 일단 <strong>Quantile</strong>이 무엇인지부터 알아보겠습니다. 우선 논문에서 사용된 Quantile은 확률분포는 몇 등분 했는가를 나타냅니다. 예를 들어 4-quantiles 라고 하면 아래와 같이 확률분포를 25%씩 4등분 하게 되는 것입니다. 그리고 이때 quantile의 값들은 [0.25, 0.5, 0.75, 1]이 됩니다. </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/dint0ee5q2kkpcm/quantile.png?dl=1\" alt=\"quantile\" width=\"400\"><br><br></p>\n\n<p> Quantile regression은 <a href=\"https://en.wikipedia.org/wiki/Cumulative_distribution_function\" target=\"_blank\" rel=\"noopener\">Cumulative Distribution Function (CDF)</a>에서 적용하는 알고리즘이므로 Quantile의 예시를 CDF로 나타낸 결과가 다음과 같습니다. </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/dzkhyy77oljdi7o/cdf.png?dl=1\" alt=\"cdf\" width=\"400\"><br><br></p>\n\n\n\n<p>CDF의 함수를 F, distribution function을 Z라고 했을 때 다음과 같이 식을 표시합니다. </p>\n<p> <img src=\"https://www.dropbox.com/s/dsone4lqernj54p/cdf_function.png?dl=1\" alt=\"cdf\" width=\"200\"></p>\n<p>Quantile regression은 모든 quantile에 대한 CDF의 역함수입니다. 그렇기 때문에 위의 식을 다음과 같이 역함수의 형태로 나타낼 수 있습니다. </p>\n<p><img src=\"https://www.dropbox.com/s/7kgktid1hpc7nba/quantile_regression.png?dl=1\" alt=\"quantile regression\" width=\"200\"></p>\n<p>즉 Quantile regression은 동일하게 나눈 확률들을 input으로 하여 각각의 support를 구하는 것입니다. 그럼 왜 본 논문에서는 quantile regression을 통해 구한 support들간의 차이를 줄이는 방향으로 학습을 수행할까요? 이것은 <strong>Wasserstein Distance</strong>와 관련이 있습니다. </p>\n<p><br></p>\n<h4 id=\"Wasserstein-Distance\"><a href=\"#Wasserstein-Distance\" class=\"headerlink\" title=\"Wasserstein Distance\"></a>Wasserstein Distance</h4><p>C51논문에서 언급하였듯이 Distributional RL은 다음의 contraction 조건을 만족할 때 알고리즘의 수렴성을 보장합니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/wb2vqtia3064ydw/contraction.png?dl=1\" alt=\"cdf\" width=\"600\"><br><br></p>\n\n<p>이때 distribution간의 거리를 나타내는 d_p가 Wasserstein distance일때는 수학적으로 위의 조건을 만족하지만 C51은 cross entropy를 이용했기 때문에 수학적으로 위의 조건을 만족한다는 것을 증명할 수 없었습니다. </p>\n<p>p-Wasserstein distance의 식은 아래와 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/jja6ahq9l0q5a78/wasserstein.png?dl=1\" alt=\"cdf\" width=\"600\"><br><br></p> \n\n<p>위의 식에서 볼 수 있듯이 p-Wasserstein distance는 CDF의 역함수의 L^p metric입니다. 본 논문에서는 1-Wasserstein distance를 이용합니다. 1-Wasserstein distance는 두 확률분포에 대한 CDF의 역함수간의 차이입니다. 아래의 그래프는 <a href=\"https://mtomassoli.github.io/2017/12/08/distributional_rl/\" target=\"_blank\" rel=\"noopener\">Distributional RL 블로그</a>에서 참고한 그래프입니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/l1zx9hgb271kzar/wasserstein_graph.png?dl=1\" alt=\"cdf\" width=\"500\"><br><br></p> \n\n<p>위의 graph에서 하늘색으로 된 부분이 1-Wasserstein distance를 나타냅니다. 이 부분은 두 확률 분포의 CDF의 역함수간 차이입니다. 그런데 아까 Quantile regression에 대해 이야기할 때 quantile regression의 정의가 바로 모든 quantile에 대한 CDF의 역함수였습니다. 즉 이 quantile regression을 통해 구한 support 간의 차이를 줄어들게 되면 wasserstein distance 또한 줄어들게 되는 것입니다. 본 논문에서는 quantile regression을 통해 구한 support를 이용하여 확률분포를 추정하고 이를 target distribution과 유사해지도록 학습을 수행합니다. 그렇기 때문에 본 논문의 방식을 이용하면 distribution간의 Wasserstein distance를 줄이는 방향으로 학습할 수 있는 것이고 이에 따라 contraction 조건을 만족하게 되어 수학적으로 distributional RL의 수렴성 또한 증명하게 되는 것입니다. </p>\n<h4 id=\"Unique-Minimizer\"><a href=\"#Unique-Minimizer\" class=\"headerlink\" title=\"Unique Minimizer\"></a>Unique Minimizer</h4><p>하지만 이 논문에서 quantile은 단순히 (1/quantile의 수)를 이용하지 않습니다. 바로 각 quantiles의 중간값인 <strong>quantile midpoint</strong>를 이용합니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/yghdhpm4d2wk94d/midpoint.png?dl=1\" alt=\"cdf\" width=\"500\"><br><br></p> \n\n<p>왜 이렇게 할까요? 바로 아래의 Lemma와 같이 두 확률 사이의 중간 지점이 해당 구간에서 Wasserstein distance를 최소로 하는 <strong>unique minimizer</strong>이기 때문입니다.</p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/z75aewiscsojdhl/minimizer_lemma.png?dl=1\" alt=\"cdf\" width=\"700\"><br><br></p> \n\n<p>해당 내용을 그래프로 표현한 결과가 다음과 같습니다. 해당 내용은 <a href=\"https://mtomassoli.github.io/2017/12/08/distributional_rl/\" target=\"_blank\" rel=\"noopener\">Distributional RL 블로그</a>의 내용을 참고하였습니다. </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/k7bes1ujqui8vss/midpoint_graph.png?dl=1\" alt=\"cdf\" width=\"800\"><br><br></p> \n\n<p>위의 그래프들을 통해 볼 수 있듯이 단순히 (1/Number of quantiles) 를 통해 구한 Wasserstein distance보다 해당 값들의 중간값들을 이용하여 구한 Wasserstein distance의 크기가 더 작은 것을 확인할 수 있습니다. 오른쪽 그래프에서 하늘색 영역이 많이 줄어든 것을 확인할 수 있습니다. 이에 따라 본 논문에서는 quantile regression을 적용할 때 <strong>quantile midpoint</strong>를 이용하여 그때의 support들을 추정합니다. </p>\n<p><br></p>\n<p>이번 파트에서는 왜 QR-DQN이 C51과는 반대로 확률을 고정하고 support들을 추정하는지 살펴보았습니다. 다음 파트에서는 quantile regression 적용에 따라 사용되는 loss인 <strong>Quantile Huber Loss</strong>에 대해서 살펴보도록 하겠습니다. </p>\n<p><br></p>\n<h3 id=\"2-Quantile-Huber-Loss\"><a href=\"#2-Quantile-Huber-Loss\" class=\"headerlink\" title=\"2. Quantile Huber Loss\"></a>2. Quantile Huber Loss</h3><p>이번 파트에서는 Quantile regression 사용에 따른 quantile regression loss와 여기에 Huber loss를 적용한 Quantile Huber loss에 대해서 살펴보도록 하겠습니다. </p>\n<p><br></p>\n<h4 id=\"Quantile-Regression-Loss\"><a href=\"#Quantile-Regression-Loss\" class=\"headerlink\" title=\"Quantile Regression Loss\"></a>Quantile Regression Loss</h4><p>위에서 보셨듯이 QR-DQN은 Quantile Regression이라는 기법을 이용하여 value distribution을 정의합니다. 이에 따라 <strong>Quantile Regression Loss</strong>라는 특별한 loss를 이용하여 학습을 수행합니다. 우선 quantile regression loss의 목적은 다음의 2가지입니다. </p>\n<ol>\n<li>Target value distribution과 네트워크를 통해 예측된 value distribution간 차이를 줄이도록 네트워크 학습 </li>\n<li>네트워크가 낮은 quantile에 대해서는 낮은 support값을, 높은 quantile에 대해서는 높은 support를 도출하도록 학습  </li>\n</ol>\n<p>위의 상황에서 1의 경우 일반적인 loss의 목표입니다. Target distribution과 network를 통해 예측된 distribution간의 차이를 최소화 하도록 network를 학습시키는 것이죠. 하지만 2의 경우 quantile regression의 적용 때문에 필요한 부분입니다. 일단 2의 내용에 대해서 살펴보도록 하겠습니다. </p>\n<p>QR-DQN은 아래와 같이 CDF를 동일한 수의 quantile로 나누고 그때의 support를 찾는 기법입니다. 한번 예시를 들어보겠습니다. Quantile의 수가 4인 경우 중 tau=[0.25, 0.5, 0.75, 1]이 될 것이고 그 중앙값들은 [0.125, 0.375, 0.625, 0.875]가 될 것입니다. 중앙값들에 대해 network가 도출한 support들이 [1, 4, 5, 7]이라고 해보겠습니다. 위의 결과를 CDF로 나타낸 것이 아래의 그림과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/xk1eey4bh35w0qc/QR_cdf_normal.png?dl=1\" alt=\"cdf normal\" width=\"400\"><br><br></p>\n\n<p> 위의 경우 정상적인 형태의 CDF입니다. 결과를 보면 tau의 중앙값 중 낮은 값들은 작은 값의 support를, 높은 값들은 큰 값의 support를 추정하여 cdf의 형태가 그 정의에 맞게 단조 증가하는 형태를 볼 수 있습니다. </p>\n<p>하지만 만약 위와 같은 상황에서 network가 도출한 support가 [1, <strong>5</strong>, <strong>4</strong>, 7] 이라고 생각해보겠습니다. 4와 5의 위치만 바뀌었죠? 이 결과를 CDF로 표현한 것이 아래와 같습니다. </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/68ng0senkvdjp7n/QR_cdf_abnormal.png?dl=1\" alt=\"cdf abnormal\" width=\"400\"><br><br></p>\n\n<p>CDF는 확률변수 값에 따른 확률을 누적해서 더하기 때문에 확률변수 값이 커질수록 누적확률값이 커지다가 최종적으로 누적 확률이 1이 되는 <strong>단조 증가</strong> 특성을 가집니다. 위의 경우는 확률변수가 증가하는데 반해 누적 확률값은 오르락 내리락 하기 때문에 CDF의 기본 특성 중 하나인 단조 증가 특성을 지키지 못한 형태입니다. Quantile regression이 CDF의 역함수인데 network를 통해 구한 결과가 CDF의 기본적인 특성가지지 못한 이상한 형태로 나오면 quantile regression을 사용하는 의미가 없어지게 됩니다. </p>\n<p>위와 같은 이유로 network의 결과는 CDF가 단조증가 특성을 가질 수 있도록 낮은 값부터 높은 값의 순서로 도출되어야 합니다. Quantile regression loss의 경우 낮은 quantile이 높은 값의 support를 추정할수록, 혹은 높은 quantile이 낮은 값의 support를 추정할수록 큰 패널티를 주는 방식으로 설계되어 있습니다. 한번 Quantile regression loss의 계산 과정은 어떻게 되는지, quantile regression loss의 식을 통해 어떻게 penalty를 주는지 한번 알아보도록 하겠습니다. Quantile regression loss의 식은 아래와 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/md1l7vbvt7w0brq/quantile_regression_loss.png?dl=1\" alt=\"Quantile regression loss\" width=\"400\"><br><br></p>\n\n<p>위의 식은 다음의 과정을 거쳐서 진행됩니다. </p>\n<ol>\n<li>Target network를 통해 구한 target support들과 network를 통해 추정한 support들의 차이를 구한다.<br>(각 target support와 추정된 support의 차이를 모두 구해야함)</li>\n<li>차이 값이 0보다 작은 경우 (tau-1)을, 0보다 크거나 같은 경우 (tau)를 곱해준다. </li>\n<li>해당 결과를 target에 대해서는 평균을 (E), prediction에 대해서는 sum을 해주어 최종 loss를 도출   </li>\n</ol>\n<p><br></p>\n<p>위의 과정만 봤을때는 어떻게 loss를 구해야 될지 직관적으로 이해되지 않을 수 있기 때문에 한번 예시를 들어보도록 하겠습니다. </p>\n<p>Target supports가 [2, 4, 8, 9]이고 추정된 support가 [1, 4, 5, 8]이라고 해보겠습니다. 예시를 위한 값들을 이용하여 위의 1, 2, 3 과정을 순서대로 살펴보겠습니다. </p>\n<p>우선 <strong>과정 1</strong>의 경우, 먼저 target support와 추정된 support 각각 모든 값에 대해 차이를 구해야합니다. 이를 구현하기 위해 target support와 추정된 support를 각각 다른 축으로 쌓아서 matrix의 형태로 만든 다음에 빼주도록 하겠습니다. 위 내용을 아래와 같이 표현할 수 있습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/itc14ozvvglr4xc/qr_dqn_loss1.png?dl=1\" alt=\"Quantile regression loss1\" width=\"800\"><br><br></p>\n\n<p>현재 quantile의 수는 4이므로 tau = [0.25, 0.5, 0.75, 1]이고 해당 tau의 중앙값들은 [0.125, 0.375, 0.625, 0.875] 입니다. </p>\n<p>Quantile regression loss 중 <strong>과정 2</strong>에 해당하는 부분이 다음과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/fns0p28m7utsyb0/qr_dqn_loss2.png?dl=1\" alt=\"Quantile regression loss\" width=\"300\"><br><br></p>\n\n<p>Error의 각 column에 해당하는 quantile의 중앙값들을 나타낸 것이 아래의 그림과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/kavn0uisgwdityo/qr_dqn_loss2_1.png?dl=1\" alt=\"Quantile regression loss\" width=\"800\"><br><br></p>\n\n<p>이제 과정 2의 연산을 수행한 결과가 아래의 그림과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/6whtc7t28e7dyz9/qr_dqn_loss2_2.png?dl=1\" alt=\"Quantile regression loss\" width=\"800\"><br><br></p>\n\n<p>이제 <strong>과정 3</strong>에 해당하는 부분을 살펴보도록 하겠습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/0hlpemekpg16cgr/qr_dqn_loss3_1.png?dl=1\" alt=\"Quantile regression loss\" width=\"300\"><br><br></p>\n\n<p>과정 3에서는 과정 2를 제외한 나머지 부분, 즉 <strong>j (target)</strong>에 대해서 평균하고 <strong>i (prediction)</strong>에 대해서 더해주는 부분에 대한 연산만 수행해주면 됩니다. 다음에 살펴볼 논문인 IQN에서는 위의 식을 아래와 같이 표현하기도 합니다. </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/efckt3pgrewclqn/qr_dqn_loss3_2.png?dl=1\" alt=\"Quantile regression loss\" width=\"300\"><br><br></p>\n\n<p>이는 과정 2를 통해 구한 matrix의 row들에 대해서는 평균을, column들에 대해서는 sum을 해주면 됩니다. </p>\n<p>해당 연산의 결과가 아래와 같습니다. </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/wfjzmvdgx8zxsf1/qr_dqn_loss4.png?dl=1\" alt=\"Quantile regression loss\" width=\"800\"><br><br></p>\n\n<p>위와 같이 최종적으로 구한 Quantile regression loss가 3.6525 입니다!! </p>\n<p><br></p>\n<p>그렇다면 Quantile regression loss를 이용하면 어떻게 cdf가 단조증가할 수 있게 support를 추정하게 되는 것일까요? 바로 낮은 quantile이 높은 support를 추정하거나, 높은 quantile이 낮은 support를 추정하는 경우 더 penalty를 많이 줘서 loss의 값이 커지도록 하는 것입니다. </p>\n<p>이 경우 또한 예를 들어 설명해보도록 하겠습니다. 위에서 들었던 예시와 동일하게 Target support가 [2,4,8,9]이고 predicted support가 아래와 같이 두 경우일때를 비교해보겠습니다. </p>\n<ol>\n<li>[2, 4, 8, 3]</li>\n<li>[2, 4, 8, 15]</li>\n</ol>\n<p>1과 2의 경우 마지막 추정된 support 이외에는 모두 target값과 동일합니다. 그리고 마지막으로 추정된 support는 target support와 비교했을 때 그 오차의 크기가 6으로 동일합니다. 하지만 <strong>quantile regression의 입장에서는 1번의 경우 2번보다 문제가 큽니다.</strong> 왜냐하면 큰 quantile에 대한 support값이 상대적으로 매우 작은 값을 추정했기 때문입니다. 위의 두 경우에 대한 quantile regression loss 계산 결과는 다음과 같습니다. </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/ikzy0i7k78nmr51/qr_dqn_compare.png?dl=1\" alt=\"Quantile regression loss\" width=\"800\"><br><br></p>\n\n<p>위의 결과 중 왼쪽이 predicted support = [2, 4, 8, 3] 일때, 오른쪽이 predicted support = [2, 4, 8, 15] 일 때 입니다. Quantile값 [0.25, 0.5, 0.75, 1] 중 가장 큰 quantile값인 1에 해당하는 support가 작게 나온 경우 quantile huber loss의 값이 더 크게 도출되었습니다!! 이런 차이를 만들어 낸 것은 위의 비교 중에서 다음에 해당하는 부분입니다. </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/vpjidvf7a1dkqry/qr_dqn_compare2.png?dl=1\" alt=\"Quantile regression loss\" width=\"800\"><br><br></p>\n\n<p>위 부분에서 볼 수 있듯이 높은 quantile에 대해서 낮은 support값을 추정하는 경우 error (u)가 대부분 양수인 것을 확인할 수 있습니다. 가장 마지막 support에 대해서 u가 0보다 크거나 같은 경우 곱해지는 tau값은 0.875입니다. 반대로 u가 0보다 작은 경우 곱해지는 값은  (tau-1)인 0.125입니다. </p>\n<p>이렇게 높은 quantile에 대한 support가 낮은 값을 추정하는 경우 위와 같은 연산 때문에 penalty가 생기게 되고 loss를 줄이는 방향으로 학습하다보면 높은 quantile에 대한 support가 다른 support들에 비해 높은 값을 추정할 수 있도록 학습되는 것입니다!! 위의 예시는 낮은 quantile에 대한 support가 높은 값을 추정하는 경우에도 유사하게 적용할 수 있습니다. </p>\n<p>위의 과정을 통해서 낮은 quantile에 대해서는 낮은 support가, 높은 quantile에 대해서는 높은 support가 추정되는 것입니다. 즉, 단조증가의 형태로 cdf를 추정할 수 있게 되는 것입니다. </p>\n<h4 id=\"Quantile-Huber-Loss\"><a href=\"#Quantile-Huber-Loss\" class=\"headerlink\" title=\"Quantile Huber Loss\"></a>Quantile Huber Loss</h4><p>하지만 QR-DQN 논문에서는 quantile regression loss를 그대로 이용하지 않고 <strong>Quantile Huber Loss</strong> 를 사용합니다. </p>\n<p>아래의 그림은 <a href=\"https://mtomassoli.github.io/2017/12/08/distributional_rl/\" target=\"_blank\" rel=\"noopener\">Distributional RL 블로그</a>의 자료입니다. </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/ik1v03jcxj1crhi/quantile_huber.png?dl=1\" alt=\"Huber loss\" width=\"500\"><br><br></p>\n\n<p>위의 결과에서 주황색선이 일반적인 quantile regression loss를 적용하였을 때 loss의 결과이며 파란색선이 Quantile huber loss를 사용하였을 때 loss의 결과입니다. Deep learning은 일반적으로 gradient를 기반으로 최적화를 수행하며 학습하는 알고리즘입니다. 하지만 quantile regression loss를 그냥 이용하는 경우 0에서 smooth하지 않아 미분이 불가능해집니다. 이런 이유로 <a href=\"https://en.wikipedia.org/wiki/Huber_loss\" target=\"_blank\" rel=\"noopener\">Huber loss</a> 를 추가해주어 학습의 안정성을 높여줍니다. </p>\n<p>우선 Huber Loss의 식은 아래와 같습니다. </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/dtfi0y1retnchwa/HuberLoss.png?dl=1\" alt=\"Huber loss\" width=\"500\"><br><br></p>\n\n<p>이를 이용하여 Quantile regression loss의 식 중 rho 부분을 다음과 같이 변경합니다. </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/0b1dim8bafuig9p/Quantile_Huber_Loss.png?dl=1\" alt=\"Huber loss\" width=\"500\"><br><br></p>\n\n<p>위의 부분에서 기존에는 u&lt;0 일 때 (tau-1) 이었던 것이 (1-tau)로 바뀌었습니다. 이는 Huber loss가 적용되면서 L(u)가 0보다 커졌기 때문에 (1-tau)를 해줘야 최종적으로 loss값이 양수가 됩니다. </p>\n<p>Rho의 부분을 위와 같이 수정하여 최종적으로 아래와 같은 Quantile Huber Loss에 적용해주면 Quantile Huber Loss에 대한 모든 설명이 마무리됩니다!! :) </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/g834dg6uq8kvtce/Quantile_Huber_Loss_final.png?dl=1\" alt=\"Huber loss\" width=\"500\"><br><br></p>\n\n<p><br></p>\n<p>본 논문의 알고리즘은 다음과 같습니다. </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/wt2ohbo1cifwvw4/QR-DQN_algorithm.png?dl=1\" alt=\"Algorithm\" width=\"600\"><br><br></p>\n\n<p>위의 알고리즘에서 볼 수 있듯이 Q-value를 계산하는 방법은 C51과 동일합니다. </p>\n<p>Target distribution을 계산할 때는 target network를 통해 추정한 support에 discount factor gamma를 곱하고 reward를 더하는 방식을 이용합니다. </p>\n<p>마지막으로 loss는 Quantile Huber Loss를 이용하여 이를 최소화 하는 방향으로 학습을 수행합니다. </p>\n<p><br></p>\n<h2 id=\"Result\"><a href=\"#Result\" class=\"headerlink\" title=\"Result\"></a>Result</h2><p>본 알고리즘의 성능은 다음의 두 종류의 환경을 이용하여 검증됩니다. </p>\n<ul>\n<li>Two-room windy gridworld</li>\n<li>Atari 2600 games</li>\n</ul>\n<p>각각의 결과에 대해 살펴보겠습니다. </p>\n<p><br></p>\n<h3 id=\"Two-room-Windy-Gridworld\"><a href=\"#Two-room-Windy-Gridworld\" class=\"headerlink\" title=\"Two-room Windy Gridworld\"></a>Two-room Windy Gridworld</h3> <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/xcy1im29xjc1rel/env1.png?dl=1\" alt=\"Two room windy grid world\" width=\"300\"><br><br></p>\n\n<p>해당 환경은 확률적인 환경을 구성하기 위해 gridworld에 몇가지 장치를 추가해주었습니다. Randomness를 추가하기 위해 사용한 장치들은 다음과 같습니다. </p>\n<ul>\n<li>Doorway</li>\n<li>Wind (바람이 agent를 위쪽 방향으로 밀어냄)</li>\n<li>Transition stochasticity (0.1 확률로 random한 방향으로 움직임)</li>\n</ul>\n<p>Agent는 x_s에서 출발하며 x_G에 도달하면 1의 reward를 얻습니다. 해당 환경에서는 실제로 각 state에서 1000 step 만큼 Monte-Carlo rollout을 수행 후 직접적인 경험을 통해 확률변수를 만들어내고 QR-DQN이 이를 잘 추정하는지 검증합니다.  </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/xivl1qhtfmr8z7b/QR_result_value.png?dl=1\" alt=\"Two room windy grid world\" width=\"700\"><br><br></p>\n\n<p>위의 결과가 Monte-Carlo rollout을 통해 구한 value distribution과 QR-DQN을 통해 추정한 value distribution간의 차이를 보여줍니다. 왼쪽의 경우 value-distribution으로, 오른쪽의 경우 CDF로 나타낸 결과입니다. 위 결과와 같이 QR-DQN은 value distribution을 실제와 유사하게 추정한다는 것을 확인할 수 있습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/tzav16rkhtt07cz/result_value_wasserstein.png?dl=1\" alt=\"Two room windy grid world\" width=\"700\"><br><br></p>\n\n<p>위의 결과는 Monte-Carlo rollout으로 구한 value 간 차이와 Wasserstein distance를 나타낸 그래프입니다. 위에서 볼 수 있듯이 학습이 진행될수록 Value간의 오차뿐 아니라 Wasserstein distance도 감소하는 것을 확인할 수 있습니다. 이렇게 Wasserstein distance가 줄어드는 것을 통해서 QR-DQN을 이용하는 경우 contraction 조건을 만족하며 distributional RL의 수렴성을 수학적으로 만족함을 확인할 수 있습니다. </p>\n<h3 id=\"Atari-2600\"><a href=\"#Atari-2600\" class=\"headerlink\" title=\"Atari 2600\"></a>Atari 2600</h3><p>Atari 환경에서 성능을 검증하기 위해 본 논문에서 설정한 파라미터들은 다음과 같습니다. </p>\n<ul>\n<li>Learning rate = 0.00005</li>\n<li>Epsilon(adam) = 0.01/32</li>\n<li>Number of Quantiles = 200</li>\n</ul>\n<p>본 논문에서는 다양한 deep reinforcement learning 알고리즘 (DQN, Double DQN, Prioritized DQN, C51) 및 Quantile huber loss의 kappa = 0, 1를 적용하였을때 결과를 비교합니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/of3x9s2xkx7y4yo/QR_DQN_Atari3.png?dl=1\" class=\"center\" width=\"500\"><br><br></p>\n\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/3as73k26vjhjlns/QR_DQN_Atari2.png?dl=1\" class=\"center\" width=\"500\"><br><br></p>\n\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/giantm2igkpy53t/QR_DQN_Atari1.png?dl=1\" alt=\"algorithm\" class=\"center\" width=\"500\"><br><br></p>\n\n<p>위의 결과에서 볼 수 있듯이 QR-DQN을 썼을 때, 그리고 Quantile huber loss의 kappa를 1로 하였을 때 가장 성능이 좋은 것을 확인할 수 있습니다. </p>\n<p><br></p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>QR-DQN의 경우 이전 논문인 C51에 비해 다음의 부분들에서 많은 개선점을 가져온 논문이라 할 수 있습니다. </p>\n<ul>\n<li>Wasserstein distance를 줄이는 방향으로 학습을 수행하므로 distributional RL의 수렴성을 수학적으로 만족함!</li>\n<li>Support와 관련된 파라미터가 Quantile의 숫자 하나밖에 없음! (support의 범위 같은 것을 정할 필요 없음)</li>\n<li>귀찮은 Projection 과정 생략 가능 </li>\n</ul>\n<p><br></p>\n<p>QR-DQN의 경우 C51 및 다양한 deep reinforcement learning 알고리즘들에 비해 좋은 성능을 보였으며 확률적인 환경에서 value distribution에 대한 추정도 매우 정확했음을 알 수 있습니다. </p>\n<p>다음 게시물에서는 QR-DQN 논문의 후속 논문인 <a href=\"https://arxiv.org/abs/1806.06923\" target=\"_blank\" rel=\"noopener\">Implicit Quantile Networks for Distributional Reinforcement Learning(IQN))</a> 논문에 대해 살펴보도록 하겠습니다!!! 😄</p>\n<p><br></p>\n<h2 id=\"Implementation\"><a href=\"#Implementation\" class=\"headerlink\" title=\"Implementation\"></a>Implementation</h2><p>본 논문의 코드는 다음의 Github를 참고해주세요. </p>\n<p><a href=\"https://github.com/reinforcement-learning-kr/distributional_rl\" target=\"_blank\" rel=\"noopener\">Github</a></p>\n<p><br></p>\n<h2 id=\"Other-Posts\"><a href=\"#Other-Posts\" class=\"headerlink\" title=\"Other Posts\"></a>Other Posts</h2><p><a href=\"https://reinforcement-learning-kr.github.io/2018/09/27/Distributional_intro/\">Distributional RL 개요</a></p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/10/02/C51/\">C51</a></p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/10/30/IQN/\">IQN</a></p>\n<p><br></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><p><a href=\"https://arxiv.org/abs/1710.10044\" target=\"_blank\" rel=\"noopener\">Distributional Reinforcement Learning with Quantile Regression</a></p>\n</li>\n<li><p><a href=\"https://mtomassoli.github.io/2017/12/08/distributional_rl/\" target=\"_blank\" rel=\"noopener\">Blog: Distributional RL</a> </p>\n</li>\n<li><p><a href=\"https://medium.com/@fuller.evan/quantile-reinforcement-learning-56f8b3c3f134\" target=\"_blank\" rel=\"noopener\">Blog: Quantile Reinforcement Learning</a></p>\n<p>​</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"Team\"><a href=\"#Team\" class=\"headerlink\" title=\"Team\"></a>Team</h2><p>민규식: <a href=\"https://github.com/Kyushik\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/kyushik.min\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>차금강: <a href=\"https://github.com/chagmgang\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/profile.php?id=100002147815509\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>윤승제: <a href=\"https://github.com/sjYoondeltar\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/seungje.yoon\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>김하영: <a href=\"https://github.com/hayoung-kim\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/altairyoung\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>김정대: <a href=\"https://github.com/kekmodel\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/kekmodel\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"https://www.dropbox.com/s/ftl2dcwz274qkh5/paper_qrdqn.png?dl=1\" width=\"800\"> </center>\n\n<p>논문 저자 : <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Dabney%2C+W\" target=\"_blank\" rel=\"noopener\">Will Dabney</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Rowland%2C+M\" target=\"_blank\" rel=\"noopener\">Mark Rowland</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Bellemare%2C+M+G\" target=\"_blank\" rel=\"noopener\">Marc G. Bellemare</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Munos%2C+R\" target=\"_blank\" rel=\"noopener\">Rémi Munos</a><br>논문 링크 : <a href=\"https://arxiv.org/abs/1710.10044\" target=\"_blank\" rel=\"noopener\">ArXiv</a><br>Proceeding : The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)<br>정리 : 민규식</p>\n<hr>\n<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>본 게시물은 2017년 10월에 발표된 논문 <a href=\"https://arxiv.org/abs/1710.10044\" target=\"_blank\" rel=\"noopener\">Distributional Reinforcement Learning with Quantile Regression(QR-DQN)</a> 의 내용에 대해 설명합니다.</p>\n<p align=\"center\"><br><img src=\"https://www.dropbox.com/s/ftl2dcwz274qkh5/paper_qrdqn.png?dl=1\" alt=\"paper\" style=\"width: 800px;\"><br><br> </p>\n\n<p><br></p>\n<h2 id=\"Algorithm\"><a href=\"#Algorithm\" class=\"headerlink\" title=\"Algorithm\"></a>Algorithm</h2><p>QR-DQN의 경우 C51과 비교했을 때 다음의 내용들에서 차이를 가집니다. </p>\n<ul>\n<li>Network의 Output</li>\n<li>Loss</li>\n</ul>\n<p>위와 같이 사실상 별로 다른 점은 없습니다. 위의 내용들에 대해 하나하나 살펴보도록 하겠습니다. </p>\n<p><br></p>\n<h3 id=\"1-Network의-Output\"><a href=\"#1-Network의-Output\" class=\"headerlink\" title=\"1. Network의 Output\"></a>1. Network의 Output</h3><p>이 파트에서는 QR-DQN이 output으로 어떤 값들을 추정하는지 알아봅니다. 왜 Quantile regression을 이용하는지, 이를 이용하면 어떻게 Wasserstein distance를 줄일 수 있고 이에 따라 distributional RL의 수렴성을 증명하게 되는지, 어떤 quantile을 이용해서 Wasserstein distance를 최소화 할 수 있는지 알아보도록 하겠습니다. </p>\n<p><br></p>\n<h4 id=\"C51-vs-QR-DQN\"><a href=\"#C51-vs-QR-DQN\" class=\"headerlink\" title=\"C51 vs QR-DQN\"></a>C51 vs QR-DQN</h4><p> QR-DQN의 경우도 C51과 같이 distributional RL 알고리즘입니다. 이에 따라 QR-DQN에서도 network의 output은 각 action에 대한 value distribution 입니다. C51 게시물에서 value distribution을 구성하는 것은 아래 그림과 같이 <strong>support</strong>와 해당 support의 <strong>확률</strong>, 2가지였습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/hfy4ynt2ic9tgtx/support_and_prob.png?dl=1\" alt=\"value distribution\" width=\"800\"><br><br></p>\n\n\n\n<p>QR-DQN과 C51의 경우 output을 구하는데 차이가 있습니다. 해당 차이를 그림으로 나타낸 것이 다음과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/7qkn9a4fz37th6m/c51_qrdqn.png?dl=1\" width=\"600\"><br><br></p>\n\n<p>즉 다음과 같은 차이가 있습니다. </p>\n<ul>\n<li>C51: support를 동일한 간격으로 고정, network의 output으로 확률을 구함 </li>\n<li>QR-DQN: 확률을 동일하게 설정, network의 output으로 support를 구함 </li>\n</ul>\n<p>C51의 경우 support를 구하기 위해서 다음의 parameter들을 결정해줘야 했습니다. </p>\n<ul>\n<li>Support의 수</li>\n<li>Support의 최대값</li>\n<li>Support의 최소값</li>\n</ul>\n<p>하지만 QR-DQN의 경우 network가 바로 supports를 구하기 때문에 support의 최대값이나 최소값은 정해줄 필요가 없습니다. 이에 따라 QR-DQN은 support의 수만 추가적인 parameter로 결정해주면 됩니다. QR-DQN에서 확률은 모두 동일하게 결정해주기 때문에 (1/support의 수) 로 단순하게 결정해주면 됩니다. </p>\n<h4 id=\"Quantile-Regression\"><a href=\"#Quantile-Regression\" class=\"headerlink\" title=\"Quantile Regression\"></a>Quantile Regression</h4><p> 그럼 QR-DQN은 왜 확률은 고정하고 network를 통해 supports를 선택하는 방법을 취할까요?? 단순히 support와 관련된 parameter들의 수를 줄이기 위함은 아닙니다! 바로 QR-DQN은 <strong>Quantile Regression</strong>이라는 기법을 사용하기 때문입니다. Quantile regression이 무엇인지, 왜 사용하는지 한번 알아보도록 하겠습니다. </p>\n<p>그럼 일단 <strong>Quantile</strong>이 무엇인지부터 알아보겠습니다. 우선 논문에서 사용된 Quantile은 확률분포는 몇 등분 했는가를 나타냅니다. 예를 들어 4-quantiles 라고 하면 아래와 같이 확률분포를 25%씩 4등분 하게 되는 것입니다. 그리고 이때 quantile의 값들은 [0.25, 0.5, 0.75, 1]이 됩니다. </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/dint0ee5q2kkpcm/quantile.png?dl=1\" alt=\"quantile\" width=\"400\"><br><br></p>\n\n<p> Quantile regression은 <a href=\"https://en.wikipedia.org/wiki/Cumulative_distribution_function\" target=\"_blank\" rel=\"noopener\">Cumulative Distribution Function (CDF)</a>에서 적용하는 알고리즘이므로 Quantile의 예시를 CDF로 나타낸 결과가 다음과 같습니다. </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/dzkhyy77oljdi7o/cdf.png?dl=1\" alt=\"cdf\" width=\"400\"><br><br></p>\n\n\n\n<p>CDF의 함수를 F, distribution function을 Z라고 했을 때 다음과 같이 식을 표시합니다. </p>\n<p> <img src=\"https://www.dropbox.com/s/dsone4lqernj54p/cdf_function.png?dl=1\" alt=\"cdf\" width=\"200\"></p>\n<p>Quantile regression은 모든 quantile에 대한 CDF의 역함수입니다. 그렇기 때문에 위의 식을 다음과 같이 역함수의 형태로 나타낼 수 있습니다. </p>\n<p><img src=\"https://www.dropbox.com/s/7kgktid1hpc7nba/quantile_regression.png?dl=1\" alt=\"quantile regression\" width=\"200\"></p>\n<p>즉 Quantile regression은 동일하게 나눈 확률들을 input으로 하여 각각의 support를 구하는 것입니다. 그럼 왜 본 논문에서는 quantile regression을 통해 구한 support들간의 차이를 줄이는 방향으로 학습을 수행할까요? 이것은 <strong>Wasserstein Distance</strong>와 관련이 있습니다. </p>\n<p><br></p>\n<h4 id=\"Wasserstein-Distance\"><a href=\"#Wasserstein-Distance\" class=\"headerlink\" title=\"Wasserstein Distance\"></a>Wasserstein Distance</h4><p>C51논문에서 언급하였듯이 Distributional RL은 다음의 contraction 조건을 만족할 때 알고리즘의 수렴성을 보장합니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/wb2vqtia3064ydw/contraction.png?dl=1\" alt=\"cdf\" width=\"600\"><br><br></p>\n\n<p>이때 distribution간의 거리를 나타내는 d_p가 Wasserstein distance일때는 수학적으로 위의 조건을 만족하지만 C51은 cross entropy를 이용했기 때문에 수학적으로 위의 조건을 만족한다는 것을 증명할 수 없었습니다. </p>\n<p>p-Wasserstein distance의 식은 아래와 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/jja6ahq9l0q5a78/wasserstein.png?dl=1\" alt=\"cdf\" width=\"600\"><br><br></p> \n\n<p>위의 식에서 볼 수 있듯이 p-Wasserstein distance는 CDF의 역함수의 L^p metric입니다. 본 논문에서는 1-Wasserstein distance를 이용합니다. 1-Wasserstein distance는 두 확률분포에 대한 CDF의 역함수간의 차이입니다. 아래의 그래프는 <a href=\"https://mtomassoli.github.io/2017/12/08/distributional_rl/\" target=\"_blank\" rel=\"noopener\">Distributional RL 블로그</a>에서 참고한 그래프입니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/l1zx9hgb271kzar/wasserstein_graph.png?dl=1\" alt=\"cdf\" width=\"500\"><br><br></p> \n\n<p>위의 graph에서 하늘색으로 된 부분이 1-Wasserstein distance를 나타냅니다. 이 부분은 두 확률 분포의 CDF의 역함수간 차이입니다. 그런데 아까 Quantile regression에 대해 이야기할 때 quantile regression의 정의가 바로 모든 quantile에 대한 CDF의 역함수였습니다. 즉 이 quantile regression을 통해 구한 support 간의 차이를 줄어들게 되면 wasserstein distance 또한 줄어들게 되는 것입니다. 본 논문에서는 quantile regression을 통해 구한 support를 이용하여 확률분포를 추정하고 이를 target distribution과 유사해지도록 학습을 수행합니다. 그렇기 때문에 본 논문의 방식을 이용하면 distribution간의 Wasserstein distance를 줄이는 방향으로 학습할 수 있는 것이고 이에 따라 contraction 조건을 만족하게 되어 수학적으로 distributional RL의 수렴성 또한 증명하게 되는 것입니다. </p>\n<h4 id=\"Unique-Minimizer\"><a href=\"#Unique-Minimizer\" class=\"headerlink\" title=\"Unique Minimizer\"></a>Unique Minimizer</h4><p>하지만 이 논문에서 quantile은 단순히 (1/quantile의 수)를 이용하지 않습니다. 바로 각 quantiles의 중간값인 <strong>quantile midpoint</strong>를 이용합니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/yghdhpm4d2wk94d/midpoint.png?dl=1\" alt=\"cdf\" width=\"500\"><br><br></p> \n\n<p>왜 이렇게 할까요? 바로 아래의 Lemma와 같이 두 확률 사이의 중간 지점이 해당 구간에서 Wasserstein distance를 최소로 하는 <strong>unique minimizer</strong>이기 때문입니다.</p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/z75aewiscsojdhl/minimizer_lemma.png?dl=1\" alt=\"cdf\" width=\"700\"><br><br></p> \n\n<p>해당 내용을 그래프로 표현한 결과가 다음과 같습니다. 해당 내용은 <a href=\"https://mtomassoli.github.io/2017/12/08/distributional_rl/\" target=\"_blank\" rel=\"noopener\">Distributional RL 블로그</a>의 내용을 참고하였습니다. </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/k7bes1ujqui8vss/midpoint_graph.png?dl=1\" alt=\"cdf\" width=\"800\"><br><br></p> \n\n<p>위의 그래프들을 통해 볼 수 있듯이 단순히 (1/Number of quantiles) 를 통해 구한 Wasserstein distance보다 해당 값들의 중간값들을 이용하여 구한 Wasserstein distance의 크기가 더 작은 것을 확인할 수 있습니다. 오른쪽 그래프에서 하늘색 영역이 많이 줄어든 것을 확인할 수 있습니다. 이에 따라 본 논문에서는 quantile regression을 적용할 때 <strong>quantile midpoint</strong>를 이용하여 그때의 support들을 추정합니다. </p>\n<p><br></p>\n<p>이번 파트에서는 왜 QR-DQN이 C51과는 반대로 확률을 고정하고 support들을 추정하는지 살펴보았습니다. 다음 파트에서는 quantile regression 적용에 따라 사용되는 loss인 <strong>Quantile Huber Loss</strong>에 대해서 살펴보도록 하겠습니다. </p>\n<p><br></p>\n<h3 id=\"2-Quantile-Huber-Loss\"><a href=\"#2-Quantile-Huber-Loss\" class=\"headerlink\" title=\"2. Quantile Huber Loss\"></a>2. Quantile Huber Loss</h3><p>이번 파트에서는 Quantile regression 사용에 따른 quantile regression loss와 여기에 Huber loss를 적용한 Quantile Huber loss에 대해서 살펴보도록 하겠습니다. </p>\n<p><br></p>\n<h4 id=\"Quantile-Regression-Loss\"><a href=\"#Quantile-Regression-Loss\" class=\"headerlink\" title=\"Quantile Regression Loss\"></a>Quantile Regression Loss</h4><p>위에서 보셨듯이 QR-DQN은 Quantile Regression이라는 기법을 이용하여 value distribution을 정의합니다. 이에 따라 <strong>Quantile Regression Loss</strong>라는 특별한 loss를 이용하여 학습을 수행합니다. 우선 quantile regression loss의 목적은 다음의 2가지입니다. </p>\n<ol>\n<li>Target value distribution과 네트워크를 통해 예측된 value distribution간 차이를 줄이도록 네트워크 학습 </li>\n<li>네트워크가 낮은 quantile에 대해서는 낮은 support값을, 높은 quantile에 대해서는 높은 support를 도출하도록 학습  </li>\n</ol>\n<p>위의 상황에서 1의 경우 일반적인 loss의 목표입니다. Target distribution과 network를 통해 예측된 distribution간의 차이를 최소화 하도록 network를 학습시키는 것이죠. 하지만 2의 경우 quantile regression의 적용 때문에 필요한 부분입니다. 일단 2의 내용에 대해서 살펴보도록 하겠습니다. </p>\n<p>QR-DQN은 아래와 같이 CDF를 동일한 수의 quantile로 나누고 그때의 support를 찾는 기법입니다. 한번 예시를 들어보겠습니다. Quantile의 수가 4인 경우 중 tau=[0.25, 0.5, 0.75, 1]이 될 것이고 그 중앙값들은 [0.125, 0.375, 0.625, 0.875]가 될 것입니다. 중앙값들에 대해 network가 도출한 support들이 [1, 4, 5, 7]이라고 해보겠습니다. 위의 결과를 CDF로 나타낸 것이 아래의 그림과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/xk1eey4bh35w0qc/QR_cdf_normal.png?dl=1\" alt=\"cdf normal\" width=\"400\"><br><br></p>\n\n<p> 위의 경우 정상적인 형태의 CDF입니다. 결과를 보면 tau의 중앙값 중 낮은 값들은 작은 값의 support를, 높은 값들은 큰 값의 support를 추정하여 cdf의 형태가 그 정의에 맞게 단조 증가하는 형태를 볼 수 있습니다. </p>\n<p>하지만 만약 위와 같은 상황에서 network가 도출한 support가 [1, <strong>5</strong>, <strong>4</strong>, 7] 이라고 생각해보겠습니다. 4와 5의 위치만 바뀌었죠? 이 결과를 CDF로 표현한 것이 아래와 같습니다. </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/68ng0senkvdjp7n/QR_cdf_abnormal.png?dl=1\" alt=\"cdf abnormal\" width=\"400\"><br><br></p>\n\n<p>CDF는 확률변수 값에 따른 확률을 누적해서 더하기 때문에 확률변수 값이 커질수록 누적확률값이 커지다가 최종적으로 누적 확률이 1이 되는 <strong>단조 증가</strong> 특성을 가집니다. 위의 경우는 확률변수가 증가하는데 반해 누적 확률값은 오르락 내리락 하기 때문에 CDF의 기본 특성 중 하나인 단조 증가 특성을 지키지 못한 형태입니다. Quantile regression이 CDF의 역함수인데 network를 통해 구한 결과가 CDF의 기본적인 특성가지지 못한 이상한 형태로 나오면 quantile regression을 사용하는 의미가 없어지게 됩니다. </p>\n<p>위와 같은 이유로 network의 결과는 CDF가 단조증가 특성을 가질 수 있도록 낮은 값부터 높은 값의 순서로 도출되어야 합니다. Quantile regression loss의 경우 낮은 quantile이 높은 값의 support를 추정할수록, 혹은 높은 quantile이 낮은 값의 support를 추정할수록 큰 패널티를 주는 방식으로 설계되어 있습니다. 한번 Quantile regression loss의 계산 과정은 어떻게 되는지, quantile regression loss의 식을 통해 어떻게 penalty를 주는지 한번 알아보도록 하겠습니다. Quantile regression loss의 식은 아래와 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/md1l7vbvt7w0brq/quantile_regression_loss.png?dl=1\" alt=\"Quantile regression loss\" width=\"400\"><br><br></p>\n\n<p>위의 식은 다음의 과정을 거쳐서 진행됩니다. </p>\n<ol>\n<li>Target network를 통해 구한 target support들과 network를 통해 추정한 support들의 차이를 구한다.<br>(각 target support와 추정된 support의 차이를 모두 구해야함)</li>\n<li>차이 값이 0보다 작은 경우 (tau-1)을, 0보다 크거나 같은 경우 (tau)를 곱해준다. </li>\n<li>해당 결과를 target에 대해서는 평균을 (E), prediction에 대해서는 sum을 해주어 최종 loss를 도출   </li>\n</ol>\n<p><br></p>\n<p>위의 과정만 봤을때는 어떻게 loss를 구해야 될지 직관적으로 이해되지 않을 수 있기 때문에 한번 예시를 들어보도록 하겠습니다. </p>\n<p>Target supports가 [2, 4, 8, 9]이고 추정된 support가 [1, 4, 5, 8]이라고 해보겠습니다. 예시를 위한 값들을 이용하여 위의 1, 2, 3 과정을 순서대로 살펴보겠습니다. </p>\n<p>우선 <strong>과정 1</strong>의 경우, 먼저 target support와 추정된 support 각각 모든 값에 대해 차이를 구해야합니다. 이를 구현하기 위해 target support와 추정된 support를 각각 다른 축으로 쌓아서 matrix의 형태로 만든 다음에 빼주도록 하겠습니다. 위 내용을 아래와 같이 표현할 수 있습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/itc14ozvvglr4xc/qr_dqn_loss1.png?dl=1\" alt=\"Quantile regression loss1\" width=\"800\"><br><br></p>\n\n<p>현재 quantile의 수는 4이므로 tau = [0.25, 0.5, 0.75, 1]이고 해당 tau의 중앙값들은 [0.125, 0.375, 0.625, 0.875] 입니다. </p>\n<p>Quantile regression loss 중 <strong>과정 2</strong>에 해당하는 부분이 다음과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/fns0p28m7utsyb0/qr_dqn_loss2.png?dl=1\" alt=\"Quantile regression loss\" width=\"300\"><br><br></p>\n\n<p>Error의 각 column에 해당하는 quantile의 중앙값들을 나타낸 것이 아래의 그림과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/kavn0uisgwdityo/qr_dqn_loss2_1.png?dl=1\" alt=\"Quantile regression loss\" width=\"800\"><br><br></p>\n\n<p>이제 과정 2의 연산을 수행한 결과가 아래의 그림과 같습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/6whtc7t28e7dyz9/qr_dqn_loss2_2.png?dl=1\" alt=\"Quantile regression loss\" width=\"800\"><br><br></p>\n\n<p>이제 <strong>과정 3</strong>에 해당하는 부분을 살펴보도록 하겠습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/0hlpemekpg16cgr/qr_dqn_loss3_1.png?dl=1\" alt=\"Quantile regression loss\" width=\"300\"><br><br></p>\n\n<p>과정 3에서는 과정 2를 제외한 나머지 부분, 즉 <strong>j (target)</strong>에 대해서 평균하고 <strong>i (prediction)</strong>에 대해서 더해주는 부분에 대한 연산만 수행해주면 됩니다. 다음에 살펴볼 논문인 IQN에서는 위의 식을 아래와 같이 표현하기도 합니다. </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/efckt3pgrewclqn/qr_dqn_loss3_2.png?dl=1\" alt=\"Quantile regression loss\" width=\"300\"><br><br></p>\n\n<p>이는 과정 2를 통해 구한 matrix의 row들에 대해서는 평균을, column들에 대해서는 sum을 해주면 됩니다. </p>\n<p>해당 연산의 결과가 아래와 같습니다. </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/wfjzmvdgx8zxsf1/qr_dqn_loss4.png?dl=1\" alt=\"Quantile regression loss\" width=\"800\"><br><br></p>\n\n<p>위와 같이 최종적으로 구한 Quantile regression loss가 3.6525 입니다!! </p>\n<p><br></p>\n<p>그렇다면 Quantile regression loss를 이용하면 어떻게 cdf가 단조증가할 수 있게 support를 추정하게 되는 것일까요? 바로 낮은 quantile이 높은 support를 추정하거나, 높은 quantile이 낮은 support를 추정하는 경우 더 penalty를 많이 줘서 loss의 값이 커지도록 하는 것입니다. </p>\n<p>이 경우 또한 예를 들어 설명해보도록 하겠습니다. 위에서 들었던 예시와 동일하게 Target support가 [2,4,8,9]이고 predicted support가 아래와 같이 두 경우일때를 비교해보겠습니다. </p>\n<ol>\n<li>[2, 4, 8, 3]</li>\n<li>[2, 4, 8, 15]</li>\n</ol>\n<p>1과 2의 경우 마지막 추정된 support 이외에는 모두 target값과 동일합니다. 그리고 마지막으로 추정된 support는 target support와 비교했을 때 그 오차의 크기가 6으로 동일합니다. 하지만 <strong>quantile regression의 입장에서는 1번의 경우 2번보다 문제가 큽니다.</strong> 왜냐하면 큰 quantile에 대한 support값이 상대적으로 매우 작은 값을 추정했기 때문입니다. 위의 두 경우에 대한 quantile regression loss 계산 결과는 다음과 같습니다. </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/ikzy0i7k78nmr51/qr_dqn_compare.png?dl=1\" alt=\"Quantile regression loss\" width=\"800\"><br><br></p>\n\n<p>위의 결과 중 왼쪽이 predicted support = [2, 4, 8, 3] 일때, 오른쪽이 predicted support = [2, 4, 8, 15] 일 때 입니다. Quantile값 [0.25, 0.5, 0.75, 1] 중 가장 큰 quantile값인 1에 해당하는 support가 작게 나온 경우 quantile huber loss의 값이 더 크게 도출되었습니다!! 이런 차이를 만들어 낸 것은 위의 비교 중에서 다음에 해당하는 부분입니다. </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/vpjidvf7a1dkqry/qr_dqn_compare2.png?dl=1\" alt=\"Quantile regression loss\" width=\"800\"><br><br></p>\n\n<p>위 부분에서 볼 수 있듯이 높은 quantile에 대해서 낮은 support값을 추정하는 경우 error (u)가 대부분 양수인 것을 확인할 수 있습니다. 가장 마지막 support에 대해서 u가 0보다 크거나 같은 경우 곱해지는 tau값은 0.875입니다. 반대로 u가 0보다 작은 경우 곱해지는 값은  (tau-1)인 0.125입니다. </p>\n<p>이렇게 높은 quantile에 대한 support가 낮은 값을 추정하는 경우 위와 같은 연산 때문에 penalty가 생기게 되고 loss를 줄이는 방향으로 학습하다보면 높은 quantile에 대한 support가 다른 support들에 비해 높은 값을 추정할 수 있도록 학습되는 것입니다!! 위의 예시는 낮은 quantile에 대한 support가 높은 값을 추정하는 경우에도 유사하게 적용할 수 있습니다. </p>\n<p>위의 과정을 통해서 낮은 quantile에 대해서는 낮은 support가, 높은 quantile에 대해서는 높은 support가 추정되는 것입니다. 즉, 단조증가의 형태로 cdf를 추정할 수 있게 되는 것입니다. </p>\n<h4 id=\"Quantile-Huber-Loss\"><a href=\"#Quantile-Huber-Loss\" class=\"headerlink\" title=\"Quantile Huber Loss\"></a>Quantile Huber Loss</h4><p>하지만 QR-DQN 논문에서는 quantile regression loss를 그대로 이용하지 않고 <strong>Quantile Huber Loss</strong> 를 사용합니다. </p>\n<p>아래의 그림은 <a href=\"https://mtomassoli.github.io/2017/12/08/distributional_rl/\" target=\"_blank\" rel=\"noopener\">Distributional RL 블로그</a>의 자료입니다. </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/ik1v03jcxj1crhi/quantile_huber.png?dl=1\" alt=\"Huber loss\" width=\"500\"><br><br></p>\n\n<p>위의 결과에서 주황색선이 일반적인 quantile regression loss를 적용하였을 때 loss의 결과이며 파란색선이 Quantile huber loss를 사용하였을 때 loss의 결과입니다. Deep learning은 일반적으로 gradient를 기반으로 최적화를 수행하며 학습하는 알고리즘입니다. 하지만 quantile regression loss를 그냥 이용하는 경우 0에서 smooth하지 않아 미분이 불가능해집니다. 이런 이유로 <a href=\"https://en.wikipedia.org/wiki/Huber_loss\" target=\"_blank\" rel=\"noopener\">Huber loss</a> 를 추가해주어 학습의 안정성을 높여줍니다. </p>\n<p>우선 Huber Loss의 식은 아래와 같습니다. </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/dtfi0y1retnchwa/HuberLoss.png?dl=1\" alt=\"Huber loss\" width=\"500\"><br><br></p>\n\n<p>이를 이용하여 Quantile regression loss의 식 중 rho 부분을 다음과 같이 변경합니다. </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/0b1dim8bafuig9p/Quantile_Huber_Loss.png?dl=1\" alt=\"Huber loss\" width=\"500\"><br><br></p>\n\n<p>위의 부분에서 기존에는 u&lt;0 일 때 (tau-1) 이었던 것이 (1-tau)로 바뀌었습니다. 이는 Huber loss가 적용되면서 L(u)가 0보다 커졌기 때문에 (1-tau)를 해줘야 최종적으로 loss값이 양수가 됩니다. </p>\n<p>Rho의 부분을 위와 같이 수정하여 최종적으로 아래와 같은 Quantile Huber Loss에 적용해주면 Quantile Huber Loss에 대한 모든 설명이 마무리됩니다!! :) </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/g834dg6uq8kvtce/Quantile_Huber_Loss_final.png?dl=1\" alt=\"Huber loss\" width=\"500\"><br><br></p>\n\n<p><br></p>\n<p>본 논문의 알고리즘은 다음과 같습니다. </p>\n <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/wt2ohbo1cifwvw4/QR-DQN_algorithm.png?dl=1\" alt=\"Algorithm\" width=\"600\"><br><br></p>\n\n<p>위의 알고리즘에서 볼 수 있듯이 Q-value를 계산하는 방법은 C51과 동일합니다. </p>\n<p>Target distribution을 계산할 때는 target network를 통해 추정한 support에 discount factor gamma를 곱하고 reward를 더하는 방식을 이용합니다. </p>\n<p>마지막으로 loss는 Quantile Huber Loss를 이용하여 이를 최소화 하는 방향으로 학습을 수행합니다. </p>\n<p><br></p>\n<h2 id=\"Result\"><a href=\"#Result\" class=\"headerlink\" title=\"Result\"></a>Result</h2><p>본 알고리즘의 성능은 다음의 두 종류의 환경을 이용하여 검증됩니다. </p>\n<ul>\n<li>Two-room windy gridworld</li>\n<li>Atari 2600 games</li>\n</ul>\n<p>각각의 결과에 대해 살펴보겠습니다. </p>\n<p><br></p>\n<h3 id=\"Two-room-Windy-Gridworld\"><a href=\"#Two-room-Windy-Gridworld\" class=\"headerlink\" title=\"Two-room Windy Gridworld\"></a>Two-room Windy Gridworld</h3> <p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/xcy1im29xjc1rel/env1.png?dl=1\" alt=\"Two room windy grid world\" width=\"300\"><br><br></p>\n\n<p>해당 환경은 확률적인 환경을 구성하기 위해 gridworld에 몇가지 장치를 추가해주었습니다. Randomness를 추가하기 위해 사용한 장치들은 다음과 같습니다. </p>\n<ul>\n<li>Doorway</li>\n<li>Wind (바람이 agent를 위쪽 방향으로 밀어냄)</li>\n<li>Transition stochasticity (0.1 확률로 random한 방향으로 움직임)</li>\n</ul>\n<p>Agent는 x_s에서 출발하며 x_G에 도달하면 1의 reward를 얻습니다. 해당 환경에서는 실제로 각 state에서 1000 step 만큼 Monte-Carlo rollout을 수행 후 직접적인 경험을 통해 확률변수를 만들어내고 QR-DQN이 이를 잘 추정하는지 검증합니다.  </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/xivl1qhtfmr8z7b/QR_result_value.png?dl=1\" alt=\"Two room windy grid world\" width=\"700\"><br><br></p>\n\n<p>위의 결과가 Monte-Carlo rollout을 통해 구한 value distribution과 QR-DQN을 통해 추정한 value distribution간의 차이를 보여줍니다. 왼쪽의 경우 value-distribution으로, 오른쪽의 경우 CDF로 나타낸 결과입니다. 위 결과와 같이 QR-DQN은 value distribution을 실제와 유사하게 추정한다는 것을 확인할 수 있습니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/tzav16rkhtt07cz/result_value_wasserstein.png?dl=1\" alt=\"Two room windy grid world\" width=\"700\"><br><br></p>\n\n<p>위의 결과는 Monte-Carlo rollout으로 구한 value 간 차이와 Wasserstein distance를 나타낸 그래프입니다. 위에서 볼 수 있듯이 학습이 진행될수록 Value간의 오차뿐 아니라 Wasserstein distance도 감소하는 것을 확인할 수 있습니다. 이렇게 Wasserstein distance가 줄어드는 것을 통해서 QR-DQN을 이용하는 경우 contraction 조건을 만족하며 distributional RL의 수렴성을 수학적으로 만족함을 확인할 수 있습니다. </p>\n<h3 id=\"Atari-2600\"><a href=\"#Atari-2600\" class=\"headerlink\" title=\"Atari 2600\"></a>Atari 2600</h3><p>Atari 환경에서 성능을 검증하기 위해 본 논문에서 설정한 파라미터들은 다음과 같습니다. </p>\n<ul>\n<li>Learning rate = 0.00005</li>\n<li>Epsilon(adam) = 0.01/32</li>\n<li>Number of Quantiles = 200</li>\n</ul>\n<p>본 논문에서는 다양한 deep reinforcement learning 알고리즘 (DQN, Double DQN, Prioritized DQN, C51) 및 Quantile huber loss의 kappa = 0, 1를 적용하였을때 결과를 비교합니다. </p>\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/of3x9s2xkx7y4yo/QR_DQN_Atari3.png?dl=1\" class=\"center\" width=\"500\"><br><br></p>\n\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/3as73k26vjhjlns/QR_DQN_Atari2.png?dl=1\" class=\"center\" width=\"500\"><br><br></p>\n\n<p align=\"center\"><br><br> <img src=\"https://www.dropbox.com/s/giantm2igkpy53t/QR_DQN_Atari1.png?dl=1\" alt=\"algorithm\" class=\"center\" width=\"500\"><br><br></p>\n\n<p>위의 결과에서 볼 수 있듯이 QR-DQN을 썼을 때, 그리고 Quantile huber loss의 kappa를 1로 하였을 때 가장 성능이 좋은 것을 확인할 수 있습니다. </p>\n<p><br></p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>QR-DQN의 경우 이전 논문인 C51에 비해 다음의 부분들에서 많은 개선점을 가져온 논문이라 할 수 있습니다. </p>\n<ul>\n<li>Wasserstein distance를 줄이는 방향으로 학습을 수행하므로 distributional RL의 수렴성을 수학적으로 만족함!</li>\n<li>Support와 관련된 파라미터가 Quantile의 숫자 하나밖에 없음! (support의 범위 같은 것을 정할 필요 없음)</li>\n<li>귀찮은 Projection 과정 생략 가능 </li>\n</ul>\n<p><br></p>\n<p>QR-DQN의 경우 C51 및 다양한 deep reinforcement learning 알고리즘들에 비해 좋은 성능을 보였으며 확률적인 환경에서 value distribution에 대한 추정도 매우 정확했음을 알 수 있습니다. </p>\n<p>다음 게시물에서는 QR-DQN 논문의 후속 논문인 <a href=\"https://arxiv.org/abs/1806.06923\" target=\"_blank\" rel=\"noopener\">Implicit Quantile Networks for Distributional Reinforcement Learning(IQN))</a> 논문에 대해 살펴보도록 하겠습니다!!! 😄</p>\n<p><br></p>\n<h2 id=\"Implementation\"><a href=\"#Implementation\" class=\"headerlink\" title=\"Implementation\"></a>Implementation</h2><p>본 논문의 코드는 다음의 Github를 참고해주세요. </p>\n<p><a href=\"https://github.com/reinforcement-learning-kr/distributional_rl\" target=\"_blank\" rel=\"noopener\">Github</a></p>\n<p><br></p>\n<h2 id=\"Other-Posts\"><a href=\"#Other-Posts\" class=\"headerlink\" title=\"Other Posts\"></a>Other Posts</h2><p><a href=\"https://reinforcement-learning-kr.github.io/2018/09/27/Distributional_intro/\">Distributional RL 개요</a></p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/10/02/C51/\">C51</a></p>\n<p><a href=\"https://reinforcement-learning-kr.github.io/2018/10/30/IQN/\">IQN</a></p>\n<p><br></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><p><a href=\"https://arxiv.org/abs/1710.10044\" target=\"_blank\" rel=\"noopener\">Distributional Reinforcement Learning with Quantile Regression</a></p>\n</li>\n<li><p><a href=\"https://mtomassoli.github.io/2017/12/08/distributional_rl/\" target=\"_blank\" rel=\"noopener\">Blog: Distributional RL</a> </p>\n</li>\n<li><p><a href=\"https://medium.com/@fuller.evan/quantile-reinforcement-learning-56f8b3c3f134\" target=\"_blank\" rel=\"noopener\">Blog: Quantile Reinforcement Learning</a></p>\n<p>​</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"Team\"><a href=\"#Team\" class=\"headerlink\" title=\"Team\"></a>Team</h2><p>민규식: <a href=\"https://github.com/Kyushik\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/kyushik.min\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>차금강: <a href=\"https://github.com/chagmgang\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/profile.php?id=100002147815509\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>윤승제: <a href=\"https://github.com/sjYoondeltar\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/seungje.yoon\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>김하영: <a href=\"https://github.com/hayoung-kim\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/altairyoung\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n<p>김정대: <a href=\"https://github.com/kekmodel\" target=\"_blank\" rel=\"noopener\">Github</a>, <a href=\"https://www.facebook.com/kekmodel\" target=\"_blank\" rel=\"noopener\">Facebook</a></p>\n"},{"title":"Algorithms for Inverse Reinforcement Learning","date":"2019-01-27T15:00:00.000Z","author":"이동민","subtitle":"Inverse RL 1번째 논문","_content":"\n<center> <img src=\"../../../../img/irl/linear_irl_1.png\" width=\"850\"> </center>\n\nAuthor : Andrew Y. Ng, Stuart Russell\nPaper Link : http://ai.stanford.edu/~ang/papers/icml00-irl.pdf\nProceeding : International Conference on Machine Learning (ICML) 2000\n\n---\n\n# 0. Abstract\n\n이 논문은 Markov Decision Processes에서의 **Inverse Reinforcement Learning(IRL)**을 다룹니다. 여기서 IRL이란, observed, optimal behavior이 주어질 때 reward function을 찾는 것입니다.\n\nIRL은 두 가지 장점이 존재합니다.\n1) 숙련된 행동을 얻기 위한 apprenticeship Learning\n2) 최적화된 reward function을 알아내는 것\n\n이 논문에서는 IRL의 reward function에 대해 세 가지 알고리즘을 제시합니다.\n1) 첫 번째와 두 번째는 전체의 policy를 알고 있는 case를 다루는 것입니다. finite state space에서의 tabulated reward function과 potentially infinite state space에 대한 reward function의 linear functional approximation을 다룹니다.\n2) 세 번째로는 observed trajectories의 finite set을 통해서만 policy를 알고 있을 때 더 다양한 realistic case에 대한 알고리즘입니다.\n\n이 논문은 2000년에 나온 논문으로서 그 당시에 어떻게 하면 reward function을 역으로 찾을 수 있는지를 증명을 통해서 다루고 있고, 왜 reward function이 중요한 지를 중요하게 말하고 있습니다. 그러니까 IRL을 통해 reward를 얻어 RL을 하는 실질적인 학습을 말하는 논문보다는 reward function에 대해서 말하고 있고 이에 따른 알고리즘들을 말하는 논문입니다. \"reward function은 이렇게 생겨야 돼! 그리고 우리는 이러한 알고리즘들을 통해 reward function을 찾아낼 수 있어!\"라는 듯이 말하고 있습니다.\n\n위에서 말한 세 가지 알고리즘의 중요한 문제는 degeneracy입니다. 여기서 degeneracy란, 어느 observed policy가 optimal한 지에 대하여 reward function의 large set이 존재하는 지에 관한 degeneracy를 말합니다. 다시 말해 이러한 reward function의 large set을 찾기 위한 알고리즘이 존재하는 지, degeneracy는 없는 지를 말하는 것입니다. 이 논문에서는 degeneracy를 없애기 위해서 natural heuristics를 제시합니다. 그리고 natural heuristics를 통해 IRL에 대해서 효과적으로 해결가능한 **linear programming** formulation을 나타냅니다.\n\n추가적으로 이러한 문제를 하나의 용어로 말하면 **\"ill-posed problem\"** 이라고 합니다. ill-posed problem을 검색하면 위키피디아에 있는 well-posed problem이 먼저 나오는데 여기서 말하는 well-posed problem이란 특정한 solution이 존재하고, 그 solution이 unique하다고 나와있습니다. 반대로, 역강화학습에서는 reward가 정해진 것이 아니라 여러가지 형태의 값으로 나타날 수 있기 때문에(정해진 값이 아니기 때문에, not unique) ill-posed problem이라고 볼 수 있습니다.\n\n실험에서는 이 논문에서의 알고리즘들을 통해 간단한 discrete/finite and continuous/infinite state problem들을 해결합니다.\n\n<br><br>\n\n# 1. Introduction\n\nIRL은 Russell이 쓴 논문에서 비공식적으로 정의합니다.\n\nGiven : 1) 시간에 따른 agent의 행동들, 2) 필요하다면, agent의 sensory input들, 3) 가능하다면, 환경의 모델  이 주어질 때\nDetermine : reward function을 최적화하는 것\n\n뒤이어 나오는 두 가지 언급을 통해서 IRL이 왜 중요한 지를 알아봅시다.\n\n첫 번째로 animal and human learning에 대한 computational model로서 RL의 잠재적인 사용입니다. 예를 들어 벌이 먹이를 찾는 모델이 있다고 했을 때 여러 꽃들 사이에서의 reward가 꿀에 대한 간단한 saturating function이라고 가정해봅시다. reward function은 보통 고정되어 있고 + 우리가 정하고 + 이미 알고 있습니다. 하지만 animal and human behavior을 조사할 때 우리는 추가적으로 알지 못하는 reward function까지도 생각해야합니다. 다시 말해 reward function의 multiattribute(다속성)도 생각을 해야한다는 것입니다. 벌이 바람과 벌을 먹이로 하는 포식자들로 부터 비행 거리, 시간, 위험 요소들을 고려하여 꿀 섭취를 할 수도 있습니다. 사람의 경제적 행동 속에서도 이러한 경우가 많습니다. 따라서 IRL은 이론적으로 생물학, 경제학, 또는 다른 분야들까지 포함하는 근본적인 문제에서 나타낼 수 있습니다.\n\n두 번째로는 특정한 도메인에서 잘 행동할 수 있는 intelligent agent를 구성할 수 있다는 것입니다. 보통 agent designer들은 그들이 정하는 reward function의 optimization이 \"desirable\" 행동을 만들 것이라는 굉장히 rough한 생각을 가질 수 있습니다. 그렇지만 아주 간단한 RL 문제라도 이것은 agent designer들을 괴롭힐 수 있습니다. 여기서 한 가지 사용할 수 있는 것이 바로 다른 \"expert\" agent의 행동입니다.\n\n가장 넓은 set인 **\"Imitation Learning\"** 안에는 일반적으로 expert agent를 통한 학습이 두 가지 방법이 있습니다.\n1) 첫 번째로는 **\"IRL\"** 이라는 것이 있고, IRL을 이용한 하나의 방법으로서 Pieter Abbeel이 주로 쓰는 알고리즘의 이름인 Apprenticeship Learning이라는 것이 존재합니다. 그래서 IRL이란 teacher의 demonstation에서의 optimal policy를 통해 reward를 찾는 것을 말합니다.\n2) 다음으로 **\"Behavioral Cloning\"** 이라는 것이 있습니다. 아예 supervised learning처럼 행동을 복제한다고 생각하면 됩니다.\n\n일반적으로 IRL을 통해서 cost를 얻고, RL을 통해서 policy를 찾는 것이 모든 IRL의 목적입니다. 하지만 이렇게 2 step을 해야한다는 점에서 많은 complexity가 생기고, 그렇다고 IRL을 통해서 얻은 cost만 가지고는 별로 할 수 있는게 없습니다. 따라서 앞서 말한 과정을 2 step만에 하는 것이 아니라 1 step만에 풀어버리는 논문이 바로 Generative Adversarial Imitation Learning(GAIL)이라는 논문입니다. 이후의 논문들은 다 GAIL을 응용한 것이기 때문에 GAIL만 이해하면 그 뒤로는 필요할 때 찾아서 보면 될 것 같고, GAIL이라는 것을 이용하여 여러 가지 연구를 해볼 수도 있을 것 같습니다.\n\n이 논문에 나오는 section들은 다음과 같습니다.\n\n- Section 2 : finite Markov Decision Processes(MDPs)의 formal definition들과 IRL의 문제를 다룹니다.\n- Section 3 : finite state spaces에서 주어진 policy 중의 어느 policy가 optimal한 지에 대해 모든 reward function의 set을 다룹니다.\n- Section 4 : reward function의 explicit, tabular representation이 가능하지 않을 수 있기 때문에 large or infinite state spaces의 case를 다룹니다.\n- Section 5 : observed trajectories의 finite set을 통해서만 policy를 안다고 했을 때, 더 realistic case에 대해서 다룹니다.\n- Section 6 : 앞서 언급했던 세 가지 알고리즘을 적용하여 discrete and continuous stochastic navigation problems와 mountain-car problem에 대한 실험부분이 나옵니다.\n- Section 7 : 결론과 더 나아가 연구되어야할 방향에 대해서 다룹니다.\n\n<br><br>\n\n# 2. Notation and Problem Formulation\n\n먼저 IRL version의 notation, definitions, basic theorems for Markov decision processes(MDPs)에 대해서 알아봅시다.\n\n<br>\n## 2.1 Markov Decision Processes\n\nA finite MDP is a tuple $(S, A, \\\\{P_{sa}\\\\}, \\gamma, R)$\n\n$S$ is a finite set on $N$ states.\n\n$A = \\\\{a_1, ... , a_k\\\\}$ is a set of $k$ actions.\n\n$P_{sa} (\\cdot)$ are the state transition probabilities upon taking action $a$ in state $s$.\n\n$\\gamma \\in [0,1)$ is the discount factor.\n\n$R : S \\rightarrow \\mathbb{R}$ is the reinforcement function (reward function) bounded absolute value by $R_{max}$.\n\n$R$에 대해 간단하게 말하기 위해서 $R(s,a)$보다 $R(s)$로서 reward $R$을 정의했습니다. $R$같은 경우, 사실 정의하기 나름인 것 같습니다. 이 논문에서는 $R(s,a)$로 생각하는 것보다 $R(s)$로 생각하는 것이 편하고 간단하기 때문에 이렇게 notation을 적은 것 같습니다. 아무래도 역강화학습에서 action $a$까지 생각해주면 추가적인 notation이 더 나오기 때문에 복잡해집니다. 또한 사실 우리가 최적의 행동만 안다면 보상을 행동까지 생각해줄 필요는 없기 때문에 $R(s)$라고 적었다고 봐도 될 것 같습니다. 역강화학습 논문들이 다 $R(s)$을 사용하는 것은 아닙니다. 논문마다 case by case로 쓸 수 있는 것 같습니다.\n\nPolicy is defined as any map $\\pi : S \\rightarrow A$.\n\nValue function evaluated at any state $s_1$ for a policy $\\pi$ is given by\n\n$$V^\\pi (s_1) = \\mathbb{E} [R(s_1) + \\gamma R(s_2) + \\gamma^2 R(s_3) + \\cdots | \\pi]$$\n\nQ-function is\n\n$$Q^\\pi (s, a) = R(s) + \\gamma \\mathbb{E}_{s' \\sim P} [V^\\pi (s')]$$\n\n- 여기서 notation $s' \\sim P$는 $s'$를 $P_{sa}$에 따라 sampling한 것입니다.\n\nOptimal value function is $V^* (s) = sup_\\pi V^\\pi (s)$. (sup is supremum, 상한, 최소 상계)\n\nOptimal Q-function is $Q^* (s,a) = sup_\\pi Q^\\pi (s,a)$.\n\n위의 모든 function들은 discrete, finite spaces에서 정의합니다.\n\n마지막으로 Symbol $\\prec$ and $\\preceq$ denote strict and non-strict vectorial inequality - i.e., $x \\, \\prec \\, y$ if and only if $\\, \\forall i \\, \\, x_i < y_i$\n\n<br>\n## 2.2 Basic Properties of MDPs\n\nIRL 문제의 solution에 대해 MDPs와 관련된 두 가지 classical result가 필요합니다.\n\nTheorem 1 (Bellman Equations) : MDP $M = (S, A, \\\\{P_{sa}\\\\}, \\gamma, R)$ and policy $\\pi : S \\rightarrow A$가 주어질 때, 모든 $s \\in S$, $a \\in A$에 대해서 $V^\\pi$ 와 $Q^\\pi$는 다음을 만족합니다.\n\n$$V^\\pi (s) = R(s) + \\gamma \\sum_{s'} P_{s\\pi(s)} (s') \\, V^\\pi (s') \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (1)$$\n\n$$Q^\\pi (s,a) = R(s) + \\gamma \\sum_{s'} P_{sa} (s') \\, V^\\pi (s') \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (2)$$\n\nTheorem 2 (Bellman Optimality) : MDP $M = (S, A, \\\\{P_{sa}\\\\}, \\gamma, R)$ and policy $\\pi : S \\rightarrow A$가 주어질 때, $\\pi$는 $M$에 대해 optimal policy라는 것은 $\\equiv$ 모든 $s \\in S$에 대해 다음과 수식과 같습니다.\n\n$$\\pi (s) \\in arg\\max_{a \\in A} Q^\\pi (s,a) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (3)$$\n\nTheorem 1, 2의 증명은 [서튼책](http://incompleteideas.net/book/RLbook2018.pdf)에 있고, 여기서는 추가적으로 다루지 않겠습니다.\n\n<br>\n## 2.3 Inverse Reinforcement Learning\n\nIRL의 문제는 observed 행동을 설명할 수 있는 reward function을 찾는 것입니다. 그래서 가장 먼저 무엇을 하고 싶은 것이냐면, state space가 finite이고, model은 이미 알고 있고, complete policy가 observe된 simple case부터 시작하고자 합니다. 다시 말해 $\\pi$가 optimal policy일 때, 가능한 한 reward function의 set을 찾고자 하는 것입니다.\n\n추가적으로 action들을 renaming함으로써 $\\pi (s) \\equiv a_1$ (여기서 $a_1$는 임의의 하나의 행동)라는 것을 가정할 것입니다. 이 trick은 notation을 단순화하기 위해 사용될 것입니다.\n\n<br><br>\n\n# 3. IRL in Finite State Spaces\n\n이번 section에서는 주어진 policy 중의 어느 policy가 optimal한 지에 대해서 모든 reward functions의 set에 대한 간단한 정의를 할 것입니다. 그리고 모든 reward functions의 set에 degeneracy를 없애기 위해서 간단한 heuristic 방법인 **\"Linear Programming\"** ([Wikipedia](https://en.wikipedia.org/wiki/Linear_programming), [위키백과](https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%EA%B3%84%ED%9A%8D%EB%B2%95))을 제안합니다.\n\nLinear Programming에 대해서 간략하게만 알아봅시다. 대부분의 사람들은 Linear Programming(선형 계획법)보다 더 친숙한 Dynamic Programming(동적 계획법)을 알고 있는 경우가 많습니다. 여기서 \"Dynamic\"이란 동적, 즉 시간에 따라 변하며 다단계적인 특성을 말합니다. 그리고 \"Programming\"이란 컴퓨터 프로그래밍이 아니라 무언가를 계획하여 최적의 program을 찾는 방법을 말합니다. 한마디로 \"시간에 따라서 큰 문제들 안에 작은 문제들이 중첩된 경우에 전체 큰 문제를 작은 문제로 쪼개서 최적의(optimal) program을 찾는 방법으로 풀겠다\" 라는 것입니다.\n\n이와 비슷하게 Linear Programming, 선형 계획법도 Programming(계획법)이 들어갑니다. 그러니까 선형 계획법 또한 무언가를 계획하는 것으로서 \"최적의 program을 찾는 방법으로 풀겠다.\"라는 것인데, 앞에 Linear만 추가적으로 붙었습니다. 정리하자면, **\"최적화 문제 일종으로 주어진 선형 조건들을 만족시키면서 선형인 목적 함수를 최적화하여 풀겠다.\"** 라는 것입니다. 이 Linear Programming(LP)은 주로 Operations Research(OR)에서 가장 일반적인 기법으로 뽑힙니다.\n\n관련하여 예를 한번 들어보겠습니다. 아래의 예는 위키백과에 있는 예입니다. 홍길동 씨가 두 가지 종류의 빵을 판매하는데, 초코빵을 만들기 위해서는 밀가루 100g과 초콜릿 10g이 필요하고, 밀빵을 만들기 위해서는 밀가루 50g이 필요합니다. 재료비를 제하고 초코빵을 팔면 100원이 남고, 밀빵을 팔면 40원이 남습니다. 오늘 홍길동 씨는 밀가루 3000g과 초콜릿 100g을 재료로 갖고 있습니다. 만든 빵을 전부 팔 수 있고 더 이상 재료 공급을 받지 않는다고 가정한다면, 홍길동 씨는 이익을 극대화 하기 위해서 어떤 종류의 빵을 얼마나 만들어야 할까요? 선형 계획법을 통해서 알아봅니다!\n\n<center> <img src=\"../../../../img/irl/linear_irl_2.png\" width=\"350\"> </center>\n\n여기서 $x_1$은 초코빵을 $x_2$는 밀빵의 개수를 의미하는 변수입니다. 그림으로 나타내면 아래와 같이 가장 많은 이익을 남기는 방법은 초코빵 10개와 밀빵 40개를 만드는 것이고, 그렇게 해서 얻을 수 있는 최대 이익은 2600원입니다.\n\n<center> <img src=\"../../../../img/irl/linear_irl_3.png\" width=\"600\"> </center>\n\n이익이 최대가 될 때는 이익을 나타내는 직선이 해가 존재할 수 있는 영역 중 원점에서 가장 떨어진 점 (10, 40)에 접할 때입니다. $100 x_1 + 40 x_2 = 2600$\n\n<br>\n## 3.1 Characterization of the Solution Set\n\n**Theorem 3** : Let a finite state space $S$, a set of actions $A = \\{a_1, ... , a_k\\}$, transition probability matrices $\\\\{P_a\\\\}$, a discount factor $\\gamma \\in (0, 1)$이 주어질 때, $\\pi (s) \\equiv a_1$에 의해 주어진 policy $\\pi$가 optimal인 것은 $\\equiv$ 모든 $a = a_2, ... , a_k$에 대해서 reward $R$이 아래의 수식을 만족하는 것과 같습니다.\n\n$$(P_{a_1} - P_a)(I - \\gamma P_{a_1})^{-1} R \\succeq 0 \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (4)$$\n\n**Proof**. $\\pi (s) \\equiv a_1$로 인하여, Equation (1)은 $V^\\pi = R + \\gamma P_{a_1} V^\\pi$이라고 쓸 수 있다. 따라서 아래의 수식으로 쓸 수 있습니다.\n\n$$V^\\pi = (I - \\gamma P_{a_1})^{-1} R$$\n\n위의 수식에서 주석처리가 되어있는 것을 보니 저자가 할 말이 더 있는 것 같습니다.\n\n$I - \\gamma P_{a_1}$(여기서 $I$는 단위행렬)은 항상 invertible(역으로 되는)합니다. 실제로 정말 invertible한 지를 보기 위해 transition matrix인 $P_{a_1}$가 복잡한 공간에 unit circle(단위원)에서 모든 eigenvalue들을 가진다는 것을 먼저 언급합니다. 다시 말해 $\\gamma < 1$일 때, matrix $\\gamma P_{a_1}$가 unit circle 내에 모든 eigenvalues를 가진다는 것을 말합니다. (특히 여기서 1은 eigenvalue가 될 수 없습니다.) transition matrix의 특성상 이렇게 되는 것이기 때문에 기억하는 것이 좋습니다.\n\n뒤이어 위의 특성은 $I - \\gamma P_{a_1}$이 zero eigenvalue를 가지고 있지 않고, ($\\gamma$ 때문에 $I - \\gamma P_{a_1}$가 0~1 사이에 있게 됩니다.) 이것과 동치로 말할 수 있는 것이 singular가 아니라는 것을 의미합니다. 참고로 singular하다는 것은 해당되는 행렬이 역행렬이 존재하지 않는다는 것을 말합니다.\n\n정리하자면, $I - \\gamma P_{a_1}$ -> zero eigenvalue가 없다. $\\equiv$ sigular하지 않다. $\\equiv$ 역행렬이 존재한다.\n\n추가적인 자료\n1) [Stationary distribution relation to eigenvectors and samplices in Markov chain](https://en.wikipedia.org/wiki/Markov_chain#Stationary_distribution_relation_to_eigenvectors_and_simplices)\n2) [특이값 분해(Singular Value Decomposition, SVD)의 활용)](https://darkpgmr.tistory.com/106)\n\n다시 Proof를 따라가봅니다. Equation (2)를 (3)으로 대체하면, $\\pi \\equiv a_1$가 optimal하는 것은 $\\equiv$ 아래의 수식과 같습니다.\n\n$$a_1 \\equiv \\pi (s) \\in arg\\max_{a \\in A} \\sum_{s'} P_{sa} (s') V^\\pi (s') \\,\\,\\,\\,\\, \\forall s \\in S$$ \n\n$$\\Leftrightarrow \\sum_{s'} P_{sa_1} (s') V^\\pi (s') \\geq \\sum_{s'} P_{sa} (s') V^\\pi (s') \\,\\,\\,\\,\\, \\forall s \\in S, a \\in A$$\n\n$$\\Leftrightarrow P_{a_1} V^\\pi \\succeq P_a V^\\pi \\,\\,\\,\\,\\, \\forall a \\in A \\setminus a_1$$\n\n$$\\Leftrightarrow P_{a_1} (I - \\gamma P_{a_1})^{-1} R \\succeq P_a (I - \\gamma P_{a_1})^{-1} R \\,\\,\\,\\,\\, \\forall a \\in A \\setminus a_1$$\n\n여기서 $a \\in A \\setminus a_1$이란 $a_1$을 제외한 set of actions $A$에 있는 $a$들을 말한다.\n\n**Remark**. 매우 비슷한 argument를 사용하여 추가적인 언급을 합니다. $(P_{a_1} - P_a)(I - \\gamma P_{a_1})^{-1} R \\succ 0$라는 조건이 $\\pi \\equiv a_1$가 unique optimal policy가 되는 것에 필요하고, 충분하다고 볼 수 있습니다. (또한 추가적으로 위에 증명에 모든 inequalities를 strict inequalities로 대체함으로써)\n\nfinite-state MDPs에 대해, 이러한 결과는 IRL의 solution인 모든 reward function들의 set을 정의합니다. 그러나 여기에는 두 가지 문제점이 있습니다.\n1) $R = 0$ (and indeed any other constant vector)은 항상 solution입니다. 다시 말해 만약 우리가 어떠한 행동을 취했을 지라도 reward가 항상 같다면, $\\pi \\equiv a_1$을 포함하여 어떠한 policy들은 항상 optimal하다는 것입니다.  $\\pi$가 unique optimal policy라는 점에서는 이 문제를 완화시키지만, 전체적으로 만족시키진 않습니다. 왜냐하면 보통 0에 임의적으로 가까운 일부 reward vector들이 여전히 solution이 될 수 있기 때문입니다.\n2) 대부분의 MDPs에서, criteria (4)를 만족시키는 R에 대한 많은 choice들이 있습니다.\n\n그렇다면 우리는 어떻게 많은 reward function들 중의 하나를 결정할 수 있을까요? 다음 section을 통해서 이러한 문제점들을 해결할 수 있는 natural criteria를 알아봅시다!\n\n## 3.2 LP Formulation and Penalty Terms\n\n위의 질문에 대한 답변으로 명확하게 말하자면, Linear Programming(LP)은 Equation (4)로 인한 문제점들에 대해 실행 가능한 point로서 사용될 수 있습니다.\n\n그래서 R을 고를수 있는 한 가지 natural한 방법은 가장 먼저 $\\pi$를 optimal하도록 만드는 것입니다. 또한 $\\pi$로부터 어떠한 single-step deviation(편차)을 가능한 한 costly하게 만드는 것입니다. **쉽게 말해 최적의 행동이 있다면 최적의 정책을 찾을 수 있고, 최적의 정책을 찾을 수 있다면 R을 고를 수 있다는 것입니다.**\n\n수식으로 표현해보면, (4)를 만족시키는 모든 function R 중에서, 다음의 수식을 maximize하도록 하는 $a_1$을 고를 수 있습니다.\n\n$$\\sum_{s \\in S} (Q^\\pi (s, a_1) - \\max_{a \\in A \\setminus a_1} Q^\\pi (s, a)) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (6)$$\n\n수식을 설명해보면 quality of the optimal action과 quality of the next-best action 사이의 differences의 sum을 maximize하는 것을 말합니다.\n\n추가적으로 다른 모든 Q값들이 동등할 때, 대부분의 small rewards에 대한 solution은 \"simpler\"하도록 optional하게 objective function이 $-\\lambda ||R||_1$ ($\\ell_1$-penalty)와 같은 weight decay-like penalty term을 추가할 것입니다. (여기서 $\\lambda$는 small rewards를 가지는 것과 (6)을 maximizing하는 것 두 목표 사이를 balancing할 수 있는 adjustable penalty coefficient입니다.)\n\n최대한 쉽게 풀어서 다시 설명해보겠습니다. 위의 수식 (6)으로 maximizing을 한다고 했을 때, Q값들에 대해서 보다 더 R를 simple하게 정하고, 수식 (6)을 더 maximizing이 잘 되도록 penalty term을 추가하는 것입니다. 쉽게 말해 우리가 Loss function에서 regularization term을 두는 것처럼 더 효과적이고 쉽게 사용하기 위해서 하는 작업이라고 생각하면 편합니다. $Q^\\pi (s, a_1)$과 Q값들 사이가 더 극명하도록 L1 regularization term을 두어서 R를 쉽게 정하자! 라는 것입니다.\n\n이렇게 함으로써 \"simplest\" R (largest penalty coefficient)을 찾을 수 있고, R은 왜 $\\pi$가 optimal한 지를 \"explain\"할 수 있습니다.\n\n정리하여 다시 optimization problem을 수식으로 말하자면 다음과 같습니다.\n**First objective function & algorithm**\n\n$$maximize \\,\\,\\,\\,\\, \\sum_{i=1}^N \\min_{a \\in \\{a_2, ... , a_k\\}} \\\\{(P_{a_1} (i) - P_a (i))(I - \\gamma P_{a_1})^{-1} R\\\\} - \\lambda ||R||_1$$\n\n$$s.t. \\,\\,\\,\\,\\, (P_{a_1} - P_a)(I - \\gamma P_{a_1})^{-1} R \\succeq 0 \\,\\,\\,\\,\\, \\forall a \\in A \\setminus a_1$$\n\n$$|R_i| \\leq R_{max}, i = 1, ... , N$$\n\n이러한 수식들을 통해 linear program으로 표현될 수 있고, 효과적으로 해결될 수 있습니다.\n\n<br><br>\n\n# 4. Linear Function Approximation in Large State Spaces\n\n이번 section에서는 infinite state spaces의 case를 다룹니다. infinite-state MDPs는 section 2에서의 finite-state와 같은 방식으로 정의될 수 있습니다. 추가적으로 state는 $S = \\mathbb{R}^n$의 case에 대해서만 다루고자 합니다. 따라서 reward function R은 $S = \\mathbb{R}^n$로부터의 function이라고 할 수 있습니다.\n\nCalculus of variations는 infinite state spaces에서 optimizing하는 데에 있어 좋은 tool이지만, 종종 알고리즘적으로 어렵게 만듭니다. 따라서 reward function에 대해 **\"Linear Functional Approximation\"** 을 사용합니다. 수식으로 R을 표현하자면 다음과 같습니다.\n\n$$R(s) = \\alpha_1 \\phi_1 (s) + \\alpha_2 \\phi_2 (s) + \\cdots + \\alpha_d \\phi_d (s) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (8)$$\n\n여기서 $\\phi_1, ... , \\phi_d$는 $S$로부터 mapping된 고정되어 있고, 우리가 알고 있고, bound되어 있는 basis function입니다. 그리고 $\\alpha_i s$는 우리가 fit해야하는 알고있지 않은 parameter입니다.\n\n다음으로 $V^\\pi$에 대해서도 linearity of expectation을 함으로써, reward function R이 Equation (8)로 주어질 때 value function을 다음과 같이 표현할 수 있습니다.\n\n$$V^\\pi = \\alpha_1 V_1^\\pi + \\cdots + \\alpha_d V_d^\\pi \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (9)$$\n\n위의 수식과 Theorem 2 (3번 수식)을 사용하면서, policy $\\pi (s) \\equiv a_1$를 optimal하도록 만드는 R에 대해서 (4)의 appropriate generalization은 다음의 조건입니다.\n\n<center> <img src=\"../../../../img/irl/linear_irl_4.png\" width=\"370\"> </center>\n\n, for all states $s$ and all actions $a \\in A \\setminus a_1$\n\n하지만 위의 formulation들에는 두 가지 문제가 있습니다.\n1) infinite state spaces에서, Equation (10)의 형태에는 infinitely 많은 제약이 있습니다. infinite state spaces이기 때문에 모든 state를 check하기가 불가능하고 어렵습니다. 따라서 알고리즘적으로, states 중의 finite subset $S_0$만 sampling함으로써 이러한 문제를 피하고자 합니다.\n2) R을 표현하기 위해 Equation (8)에서 linear function approximator를 사용한다고 제한했기 때문에, 어느 $\\pi$가 optimal한 지에 대해 더 이상 어떠한 reward function도 표현할 수 없습니다. 그럼에도 불구하고, linear function approximator를 사용할 것입니다.\n\n최종적으로 linear programming formulation은 다음과 같습니다.\n**Second objective function & algorithm**\n\n<center> <img src=\"../../../../img/irl/linear_irl_5.png\" width=\"460\"> </center>\n\n$$s.t. \\,\\,\\, |\\alpha_i| \\leq 1, \\,\\,\\, i = 1, ... , d$$\n\n<br><br>\n\n# 5. IRL from Sampled Trajectories\n\n이번 section에서는 오직 state space에서의 actual trajectories의 set을 통해서만 policy $\\pi$를 접근하는 좀 더 realistic case에 대해서 IRL 문제를 다룹니다. 그래서 MDP의 explicit model을 필요로 하지 않습니다.\n\ninitial state distribution $D$를 고정하고, (unknown) policy $\\pi$에 대해 우리의 목표는 $\\pi$가 $\\mathbb{E}_{s_0 \\sim D} [V^\\pi (s_0)]$를 maximize하는 R를 찾는 것입니다. (기억합시다!) 추가적으로 notation을 단순화하기 위해 고정된 start state $s_0$를 가정합니다.\n\n먼저 $\\alpha_i$의 setting을 통해 $V^\\pi (s_0)$를 estimating하는 방법이 필요합니다. 이것을 하기 위해, 첫 번째로 $m$ Monte Carlo trajectories를 만들어냅니다. 그리고 나서 $i = 1, ... , d$에 대해 만약 reward가 $R = \\phi_i$라면, $V_i^\\pi (s_0)$를 얼마나 average empirical return이 $m$ trajectories에 있었는 지로 정의합니다.\n\n예를 들어, 만약 $m = 1$ trajectories이고, 이 trajectory가 states ($s_0, s_1, ...$)의 sequence라면, 다음과 같이 나타낼 수 있습니다.\n\n$$\\hat{V}_i^\\pi (s_0) = \\phi_i (s_0) + \\gamma \\phi_i (s_1) + \\gamma^2 \\phi_i (s_2) + \\cdots$$\n\n일반적으로, $\\hat{V}_i^\\pi (s_0)$은 어떠한 $m$ trajectories의 empirical returns에 대하여 average합니다. (여기서 말하는 어떠한 $m$ trajectories는 임의의 finite number에 의해 truncate된 trajectories를 말합니다.) 그리고 그 때 $\\alpha_i$의 어떠한 setting에 대해서, $V^\\pi (s_0)$의 natural estimate는 다음과 같습니다.\n\n$$\\hat{V}^\\pi (s_0) = \\alpha_1 \\hat{V}_1^\\pi (s_0) + \\cdots + \\alpha_d \\hat{V}_d^\\pi (s_0) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (11)$$\n\n위의 수식의 \"inductive(귀납적인) step\"은 다음을 뒤따릅니다 : set of policies $\\{ \\pi_1, ... , \\pi_k \\}$이 있고, resulting reward function은 아래의 수식을 만족하기 때문에 $\\alpha_i$의 setting을 찾을 수 있습니다.\n\n$$V^{\\pi^*} (s_0) \\geq V^{\\pi_i} (s_0), \\,\\,\\, i = 1, ... , k \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (12)$$\n\n그리고 section 4에서 마지막에 있던 수식에서, objective function을 약간 바꿀 수 있습니다. 따라서 optimization의 식은 다음과 같이 될 수 있다.\n**Final objective function & algorithm**\n\n$$maximize \\,\\,\\,\\,\\, \\sum_{i=1}^k p(\\hat{V}^{\\pi^*} (s_0) - \\hat{V}^{\\pi_i} (s_0))$$\n\n$$s.t. \\,\\,\\,\\,\\, |\\alpha_i| \\leq 1, \\,\\,\\, i = 1, ... , d$$\n\n위의 수식에서 $\\hat{V}^{\\pi_i} (s_0)$과 $\\hat{V}^{\\pi^*} (s_0)$은 Equation (11)에서 주어진 $\\alpha_i$의 (implicit) linear function이다.\n\n그러므로, 위의 $\\hat{V}^{\\pi_i} (s_0)$과 $\\hat{V}^{\\pi^*} (s_0)$은 쉽게 linear programming으로 해결할 수 있습니다.\n\n위의 optimization 식은 $\\alpha_i$의 새로운 setting으로 설정할 수 있고, 그러므로 새로운 reward function $R = \\alpha_1 \\phi_1 + \\cdots + \\alpha_d \\phi_d$을 가질 수 있습니다.\n\n그리고 그 때 $R$로 인한 $V^\\pi (s_0)$를 maximize하는 policy $\\pi_{k+1}$을 찾을 수 있고, $\\pi_{k+1}$을 current set of policies에 추가할 수 있습니다. 그리고 이것을 계속할 수 있습니다. (많은 수의 iteration을 통해 우리가 \"satisfied\"하는 $R$를 찾을 수 있습니다.)\n\n<br><br>\n\n# 6. Experiments\n\n<br>\n## 6.1 First experiment : 5 x 5 grid world\n\n<center> <img src=\"../../../../img/irl/linear_irl_6.png\" width=\"500\"> </center>\n\n첫 번째 실험은 agent가 왼쪽 아래에서 시작하여 오른쪽 위로 가면 보상 1을 받는 5x5 grid world를 사용했습니다. action은 상하좌우지만 noise가 있으며 30%의 확률로 랜덤하게 움직입니다.\n\n위에 있는 Figure 1은 5x5 grid world에서의 optimal policy와 true reward function을 나타낸 것입니다.\n\n<center> <img src=\"../../../../img/irl/linear_irl_7.png\" width=\"500\"> </center>\n\n앞에서 설명한 penalty term인 $\\lambda$를 주지 않은 Section 3.2의 discrete/finite state problem에서 사용할 수 있는 algorithm을 쓰면 Figure 2 윗 부분의 그림과 같은 울퉁불퉁한 reward function의 모습을 볼 수 있습니다. 위에서 말했던 section 3.2에 있는 algorithm은 다음과 같습니다.\n\n$$maximize \\,\\,\\,\\,\\, \\sum_{i=1}^N \\min_{a \\in \\{a_2, ... , a_k\\}} \\\\{(P_{a_1} (i) - P_a (i))(I - \\gamma P_{a_1})^{-1} R\\\\} - \\lambda ||R||_1$$\n\n$$s.t. \\,\\,\\,\\,\\, (P_{a_1} - P_a)(I - \\gamma P_{a_1})^{-1} R \\succeq 0 \\,\\,\\,\\,\\, \\forall a \\in A \\setminus a_1$$\n\n$$|R_i| \\leq R_{max}, i = 1, ... , N$$\n\n그러나 Figure 2 아래에 쓰여져 있는 $\\lambda$ = 1.05 로 설정을 하면 Figure 2 아래에 있는 그림과 같은 true reward에 밀접한 reward function을 얻을 수 있습니다.\n\n<br>\n## 6.2 Second experiment : Mountain-car\n\n<center> <img src=\"../../../../img/irl/linear_irl_8.png\" width=\"500\"> </center>\n\n다음 실험으로는 보통 잘 알려져있는 mountain-car 환경입니다. ture, undiscounted reward는 언덕에 있는 goal지점에 도달하기 전까지 step마다 -1을 받는 것입니다. 그리고 state는 자동차의 x-위치와 속도입니다.\n\n<center> <img src=\"../../../../img/irl/linear_irl_5.png\" width=\"460\"> </center>\n\n$$s.t. \\,\\,\\, |\\alpha_i| \\leq 1, \\,\\,\\, i = 1, ... , d$$\n\nstate가 연속적이기 때문에 위의 수식과 같이 section 4의 continuous/finite state problem에서 사용할 수 있는 algorithm을 사용했습니다. 또한 reward에 대해서 오직 자동차의 x-위치에 대한 functions가 되도록 Gaussian 모양으로 된 26개의 basis functions의 linear combinations을 가지는 function approximator class를 구성하였습니다.\n\n<center> <img src=\"../../../../img/irl/linear_irl_9.png\" width=\"500\"> </center>\n\nalgorithm으로 optimal policy를 고려하여 전형적인 reward function은 Figure 4의 윗 부분에 있는 그래프로 나타납니다. (Note the scale on the $y$ axis.) 명확하게도, solution은 reward의 $R = -c$ structure를 거의 완벽하게 나타내었습니다.\n\n좀 더 challenging한 문제로서, 이 논문에서는 언덕 아래 주변을 중심으로 [-0.72, -0.32] 사이에 있으면 reward 1 아니면 0을 주도록 changed ture reward에 대한 실험을 동일하게 하였습니다. 그리고 여기서 $\\gamma$는 0.99를 주었다고 합니다.\n\n이 문제에서의 optimal policy는 가능한 한 빨리 언덕 아래로 가서 주차를 하는 것입니다. 새로운 문제에 대해 알고리즘을 적용해봤을 때 전형적인 solution은 Figure 4 아래에 있는 그래프로 나타낼 수 있습니다.\n\n대체로 reward의 중요한 structure를 정해진 [-0.72, -0.32]에 대해 성공적으로 찾았다고 합니다. 또한 오른쪽에는 artifact가 있어 오른쪽 끝을 피할 수 없도록 \"shooting out\"하는 효과가 있다고 합니다. 그럼에도 불구하고, solution이 이 문제에 꽤 좋았다고 합니다.\n\n<br>\n## 6.3 Final experiment : Continuous version of the 5 x 5 grid world\n\n마지막 실험은 5x5 grid world의 continuous version에서 적용했습니다.\n\nstate는 [0, 1] x [0, 1]이고, action은 상하좌우 방향으로 0.2만큼 움직입니다. 또한 [-0.1, 0.1]의 uniform noise가 각각 좌표에 추가되고, 만약 unit square 내에 uniform noise가 필요한 경우, state는 truncate됩니다.\n\nreward는 [0.8, 1] x [0.8, 1]에서는 1을 받고 나머지는 0을 받습니다. 그리고 $\\gamma$는 0.9를 사용했습니다. 또한 function approximator class는 2-dimensional Gaussian basis functions에 대한 15x15 array의 linear combinations로 구성하였습니다.\n\ninitial state distribution $D$는 state space에 대해  uniform하였고, algorithm은 policy를 평가하기 위해 각각의 30 steps 마다 $m=5000$ trajectories를 사용하여 적용하였습니다.\n\n$$maximize \\,\\,\\,\\,\\, \\sum_{i=1}^k p(\\hat{V}^{\\pi^*} (s_0) - \\hat{V}^{\\pi_i} (s_0))$$\n\n$$s.t. \\,\\,\\,\\,\\, |\\alpha_i| \\leq 1, \\,\\,\\, i = 1, ... , d$$\n\n위에서 다뤘던 section 5에서의 algorithm을 사용하여 찾아진 solution은 단지 1 iteration만 했는데도 꽤나 reasonable했습니다. 그래서 약 15 iterations에 대해서도 해본 결과, algorithm은 동일하게 좋은 performance로 해결했습니다.\n\n<center> <img src=\"../../../../img/irl/linear_irl_10.png\" width=\"500\"> </center>\n\n또한 action choice가 다른 state sapce의 부분을 계산하면서 true optimal policy와 fitted reward’s optimal policy를 비교하였고 (Figure 5, 위), 대체적으로 3% ~ 10% 사이로 불일치했습니다. 아마도 algorithm's performance의 좀 더 적절한 측정은 true optimal policy의 quality와 fitted reward’s optimal policy의 quality를 비교하는 것입니다. (Quality는 물론 true reward function을 사용하여 측정됩니다.)\n\n결과적으로 algorithm의 약 15 iterations에 대해서, evaluations은 (which used 50000 Monte Carlo trials of 50 steps each) true \"optimal policy\"의 value와 fitted reward’s optimal policy사이에 statistically한 중요한 차이를 detect할 수 없다는 것을 볼 수 있었습니다. 그만큼 true optimal policy와 reward's optimal policy의 차이가 없다는 것을 뜻합니다. (Figure 5, 아래)\n\n<br><br>\n\n# 7. Conclusions and Future work\n\n이 논문은 moderate-sized discrete and continuous domain에서 Inverse Reinforcement Learning 문제가 해결될 수 있다는 것을 보였습니다.\n\n하지만 많은 open question들이 아래와 같이 남아있습니다.\n1) Potential-based shaping rewards는 MDP에서 학습시키기 위한 하나의 solution으로서 reward function을 더 쉽게 만들 수 있습니다. 그렇다면 우리는 더 \"easy\" reward function을 만들기 위한 IRL 알고리즘들을 만들 수 있을까요?\n2) IRL를 real-world empirical application측면에서 보면, sensor inputs and actions에 대해서 observer의 측정에 상당한 noise가 있을지도 모릅니다. 여기에 더하여 많은 optimal policy들이 존재할 지도 모릅니다. 어떠한 data를 noise없이 fit하도록 하는 적절한 metric은 무엇일까요?\n3) 만약 행동이 절대로 optimality와 일치하지 않는다면, state space에 specific region에 대한 \"locally consistent\" reward function을 어떻게 알 수 있을까요?\n4) 어떻게 reward function의 identifiability를 maximize하기 위한 실험을 고안해낼 수 있을까요?\n5) 이 논문에 적힌 알고리즘적인 접근이 partially observable environment의 case를 얼마나 잘 실행할 수 있을까요?\n\n<br><br>\n\n# 처음으로\n\n## [Let's do Inverse RL Guide](https://reinforcement-learning-kr.github.io/2019/01/22/0_lets-do-irl-guide/)\n\n<br>\n\n# 다음으로\n\n## [APP 여행하기](https://reinforcement-learning-kr.github.io/2019/02/01/2_app/)","source":"_posts/1_linear-irl.md","raw":"---\ntitle: Algorithms for Inverse Reinforcement Learning\ndate: 2019-01-28\ntags: [\"프로젝트\", \"GAIL하자!\"]\ncategories: 프로젝트\nauthor: 이동민 \nsubtitle: Inverse RL 1번째 논문\n---\n\n<center> <img src=\"../../../../img/irl/linear_irl_1.png\" width=\"850\"> </center>\n\nAuthor : Andrew Y. Ng, Stuart Russell\nPaper Link : http://ai.stanford.edu/~ang/papers/icml00-irl.pdf\nProceeding : International Conference on Machine Learning (ICML) 2000\n\n---\n\n# 0. Abstract\n\n이 논문은 Markov Decision Processes에서의 **Inverse Reinforcement Learning(IRL)**을 다룹니다. 여기서 IRL이란, observed, optimal behavior이 주어질 때 reward function을 찾는 것입니다.\n\nIRL은 두 가지 장점이 존재합니다.\n1) 숙련된 행동을 얻기 위한 apprenticeship Learning\n2) 최적화된 reward function을 알아내는 것\n\n이 논문에서는 IRL의 reward function에 대해 세 가지 알고리즘을 제시합니다.\n1) 첫 번째와 두 번째는 전체의 policy를 알고 있는 case를 다루는 것입니다. finite state space에서의 tabulated reward function과 potentially infinite state space에 대한 reward function의 linear functional approximation을 다룹니다.\n2) 세 번째로는 observed trajectories의 finite set을 통해서만 policy를 알고 있을 때 더 다양한 realistic case에 대한 알고리즘입니다.\n\n이 논문은 2000년에 나온 논문으로서 그 당시에 어떻게 하면 reward function을 역으로 찾을 수 있는지를 증명을 통해서 다루고 있고, 왜 reward function이 중요한 지를 중요하게 말하고 있습니다. 그러니까 IRL을 통해 reward를 얻어 RL을 하는 실질적인 학습을 말하는 논문보다는 reward function에 대해서 말하고 있고 이에 따른 알고리즘들을 말하는 논문입니다. \"reward function은 이렇게 생겨야 돼! 그리고 우리는 이러한 알고리즘들을 통해 reward function을 찾아낼 수 있어!\"라는 듯이 말하고 있습니다.\n\n위에서 말한 세 가지 알고리즘의 중요한 문제는 degeneracy입니다. 여기서 degeneracy란, 어느 observed policy가 optimal한 지에 대하여 reward function의 large set이 존재하는 지에 관한 degeneracy를 말합니다. 다시 말해 이러한 reward function의 large set을 찾기 위한 알고리즘이 존재하는 지, degeneracy는 없는 지를 말하는 것입니다. 이 논문에서는 degeneracy를 없애기 위해서 natural heuristics를 제시합니다. 그리고 natural heuristics를 통해 IRL에 대해서 효과적으로 해결가능한 **linear programming** formulation을 나타냅니다.\n\n추가적으로 이러한 문제를 하나의 용어로 말하면 **\"ill-posed problem\"** 이라고 합니다. ill-posed problem을 검색하면 위키피디아에 있는 well-posed problem이 먼저 나오는데 여기서 말하는 well-posed problem이란 특정한 solution이 존재하고, 그 solution이 unique하다고 나와있습니다. 반대로, 역강화학습에서는 reward가 정해진 것이 아니라 여러가지 형태의 값으로 나타날 수 있기 때문에(정해진 값이 아니기 때문에, not unique) ill-posed problem이라고 볼 수 있습니다.\n\n실험에서는 이 논문에서의 알고리즘들을 통해 간단한 discrete/finite and continuous/infinite state problem들을 해결합니다.\n\n<br><br>\n\n# 1. Introduction\n\nIRL은 Russell이 쓴 논문에서 비공식적으로 정의합니다.\n\nGiven : 1) 시간에 따른 agent의 행동들, 2) 필요하다면, agent의 sensory input들, 3) 가능하다면, 환경의 모델  이 주어질 때\nDetermine : reward function을 최적화하는 것\n\n뒤이어 나오는 두 가지 언급을 통해서 IRL이 왜 중요한 지를 알아봅시다.\n\n첫 번째로 animal and human learning에 대한 computational model로서 RL의 잠재적인 사용입니다. 예를 들어 벌이 먹이를 찾는 모델이 있다고 했을 때 여러 꽃들 사이에서의 reward가 꿀에 대한 간단한 saturating function이라고 가정해봅시다. reward function은 보통 고정되어 있고 + 우리가 정하고 + 이미 알고 있습니다. 하지만 animal and human behavior을 조사할 때 우리는 추가적으로 알지 못하는 reward function까지도 생각해야합니다. 다시 말해 reward function의 multiattribute(다속성)도 생각을 해야한다는 것입니다. 벌이 바람과 벌을 먹이로 하는 포식자들로 부터 비행 거리, 시간, 위험 요소들을 고려하여 꿀 섭취를 할 수도 있습니다. 사람의 경제적 행동 속에서도 이러한 경우가 많습니다. 따라서 IRL은 이론적으로 생물학, 경제학, 또는 다른 분야들까지 포함하는 근본적인 문제에서 나타낼 수 있습니다.\n\n두 번째로는 특정한 도메인에서 잘 행동할 수 있는 intelligent agent를 구성할 수 있다는 것입니다. 보통 agent designer들은 그들이 정하는 reward function의 optimization이 \"desirable\" 행동을 만들 것이라는 굉장히 rough한 생각을 가질 수 있습니다. 그렇지만 아주 간단한 RL 문제라도 이것은 agent designer들을 괴롭힐 수 있습니다. 여기서 한 가지 사용할 수 있는 것이 바로 다른 \"expert\" agent의 행동입니다.\n\n가장 넓은 set인 **\"Imitation Learning\"** 안에는 일반적으로 expert agent를 통한 학습이 두 가지 방법이 있습니다.\n1) 첫 번째로는 **\"IRL\"** 이라는 것이 있고, IRL을 이용한 하나의 방법으로서 Pieter Abbeel이 주로 쓰는 알고리즘의 이름인 Apprenticeship Learning이라는 것이 존재합니다. 그래서 IRL이란 teacher의 demonstation에서의 optimal policy를 통해 reward를 찾는 것을 말합니다.\n2) 다음으로 **\"Behavioral Cloning\"** 이라는 것이 있습니다. 아예 supervised learning처럼 행동을 복제한다고 생각하면 됩니다.\n\n일반적으로 IRL을 통해서 cost를 얻고, RL을 통해서 policy를 찾는 것이 모든 IRL의 목적입니다. 하지만 이렇게 2 step을 해야한다는 점에서 많은 complexity가 생기고, 그렇다고 IRL을 통해서 얻은 cost만 가지고는 별로 할 수 있는게 없습니다. 따라서 앞서 말한 과정을 2 step만에 하는 것이 아니라 1 step만에 풀어버리는 논문이 바로 Generative Adversarial Imitation Learning(GAIL)이라는 논문입니다. 이후의 논문들은 다 GAIL을 응용한 것이기 때문에 GAIL만 이해하면 그 뒤로는 필요할 때 찾아서 보면 될 것 같고, GAIL이라는 것을 이용하여 여러 가지 연구를 해볼 수도 있을 것 같습니다.\n\n이 논문에 나오는 section들은 다음과 같습니다.\n\n- Section 2 : finite Markov Decision Processes(MDPs)의 formal definition들과 IRL의 문제를 다룹니다.\n- Section 3 : finite state spaces에서 주어진 policy 중의 어느 policy가 optimal한 지에 대해 모든 reward function의 set을 다룹니다.\n- Section 4 : reward function의 explicit, tabular representation이 가능하지 않을 수 있기 때문에 large or infinite state spaces의 case를 다룹니다.\n- Section 5 : observed trajectories의 finite set을 통해서만 policy를 안다고 했을 때, 더 realistic case에 대해서 다룹니다.\n- Section 6 : 앞서 언급했던 세 가지 알고리즘을 적용하여 discrete and continuous stochastic navigation problems와 mountain-car problem에 대한 실험부분이 나옵니다.\n- Section 7 : 결론과 더 나아가 연구되어야할 방향에 대해서 다룹니다.\n\n<br><br>\n\n# 2. Notation and Problem Formulation\n\n먼저 IRL version의 notation, definitions, basic theorems for Markov decision processes(MDPs)에 대해서 알아봅시다.\n\n<br>\n## 2.1 Markov Decision Processes\n\nA finite MDP is a tuple $(S, A, \\\\{P_{sa}\\\\}, \\gamma, R)$\n\n$S$ is a finite set on $N$ states.\n\n$A = \\\\{a_1, ... , a_k\\\\}$ is a set of $k$ actions.\n\n$P_{sa} (\\cdot)$ are the state transition probabilities upon taking action $a$ in state $s$.\n\n$\\gamma \\in [0,1)$ is the discount factor.\n\n$R : S \\rightarrow \\mathbb{R}$ is the reinforcement function (reward function) bounded absolute value by $R_{max}$.\n\n$R$에 대해 간단하게 말하기 위해서 $R(s,a)$보다 $R(s)$로서 reward $R$을 정의했습니다. $R$같은 경우, 사실 정의하기 나름인 것 같습니다. 이 논문에서는 $R(s,a)$로 생각하는 것보다 $R(s)$로 생각하는 것이 편하고 간단하기 때문에 이렇게 notation을 적은 것 같습니다. 아무래도 역강화학습에서 action $a$까지 생각해주면 추가적인 notation이 더 나오기 때문에 복잡해집니다. 또한 사실 우리가 최적의 행동만 안다면 보상을 행동까지 생각해줄 필요는 없기 때문에 $R(s)$라고 적었다고 봐도 될 것 같습니다. 역강화학습 논문들이 다 $R(s)$을 사용하는 것은 아닙니다. 논문마다 case by case로 쓸 수 있는 것 같습니다.\n\nPolicy is defined as any map $\\pi : S \\rightarrow A$.\n\nValue function evaluated at any state $s_1$ for a policy $\\pi$ is given by\n\n$$V^\\pi (s_1) = \\mathbb{E} [R(s_1) + \\gamma R(s_2) + \\gamma^2 R(s_3) + \\cdots | \\pi]$$\n\nQ-function is\n\n$$Q^\\pi (s, a) = R(s) + \\gamma \\mathbb{E}_{s' \\sim P} [V^\\pi (s')]$$\n\n- 여기서 notation $s' \\sim P$는 $s'$를 $P_{sa}$에 따라 sampling한 것입니다.\n\nOptimal value function is $V^* (s) = sup_\\pi V^\\pi (s)$. (sup is supremum, 상한, 최소 상계)\n\nOptimal Q-function is $Q^* (s,a) = sup_\\pi Q^\\pi (s,a)$.\n\n위의 모든 function들은 discrete, finite spaces에서 정의합니다.\n\n마지막으로 Symbol $\\prec$ and $\\preceq$ denote strict and non-strict vectorial inequality - i.e., $x \\, \\prec \\, y$ if and only if $\\, \\forall i \\, \\, x_i < y_i$\n\n<br>\n## 2.2 Basic Properties of MDPs\n\nIRL 문제의 solution에 대해 MDPs와 관련된 두 가지 classical result가 필요합니다.\n\nTheorem 1 (Bellman Equations) : MDP $M = (S, A, \\\\{P_{sa}\\\\}, \\gamma, R)$ and policy $\\pi : S \\rightarrow A$가 주어질 때, 모든 $s \\in S$, $a \\in A$에 대해서 $V^\\pi$ 와 $Q^\\pi$는 다음을 만족합니다.\n\n$$V^\\pi (s) = R(s) + \\gamma \\sum_{s'} P_{s\\pi(s)} (s') \\, V^\\pi (s') \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (1)$$\n\n$$Q^\\pi (s,a) = R(s) + \\gamma \\sum_{s'} P_{sa} (s') \\, V^\\pi (s') \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (2)$$\n\nTheorem 2 (Bellman Optimality) : MDP $M = (S, A, \\\\{P_{sa}\\\\}, \\gamma, R)$ and policy $\\pi : S \\rightarrow A$가 주어질 때, $\\pi$는 $M$에 대해 optimal policy라는 것은 $\\equiv$ 모든 $s \\in S$에 대해 다음과 수식과 같습니다.\n\n$$\\pi (s) \\in arg\\max_{a \\in A} Q^\\pi (s,a) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (3)$$\n\nTheorem 1, 2의 증명은 [서튼책](http://incompleteideas.net/book/RLbook2018.pdf)에 있고, 여기서는 추가적으로 다루지 않겠습니다.\n\n<br>\n## 2.3 Inverse Reinforcement Learning\n\nIRL의 문제는 observed 행동을 설명할 수 있는 reward function을 찾는 것입니다. 그래서 가장 먼저 무엇을 하고 싶은 것이냐면, state space가 finite이고, model은 이미 알고 있고, complete policy가 observe된 simple case부터 시작하고자 합니다. 다시 말해 $\\pi$가 optimal policy일 때, 가능한 한 reward function의 set을 찾고자 하는 것입니다.\n\n추가적으로 action들을 renaming함으로써 $\\pi (s) \\equiv a_1$ (여기서 $a_1$는 임의의 하나의 행동)라는 것을 가정할 것입니다. 이 trick은 notation을 단순화하기 위해 사용될 것입니다.\n\n<br><br>\n\n# 3. IRL in Finite State Spaces\n\n이번 section에서는 주어진 policy 중의 어느 policy가 optimal한 지에 대해서 모든 reward functions의 set에 대한 간단한 정의를 할 것입니다. 그리고 모든 reward functions의 set에 degeneracy를 없애기 위해서 간단한 heuristic 방법인 **\"Linear Programming\"** ([Wikipedia](https://en.wikipedia.org/wiki/Linear_programming), [위키백과](https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%EA%B3%84%ED%9A%8D%EB%B2%95))을 제안합니다.\n\nLinear Programming에 대해서 간략하게만 알아봅시다. 대부분의 사람들은 Linear Programming(선형 계획법)보다 더 친숙한 Dynamic Programming(동적 계획법)을 알고 있는 경우가 많습니다. 여기서 \"Dynamic\"이란 동적, 즉 시간에 따라 변하며 다단계적인 특성을 말합니다. 그리고 \"Programming\"이란 컴퓨터 프로그래밍이 아니라 무언가를 계획하여 최적의 program을 찾는 방법을 말합니다. 한마디로 \"시간에 따라서 큰 문제들 안에 작은 문제들이 중첩된 경우에 전체 큰 문제를 작은 문제로 쪼개서 최적의(optimal) program을 찾는 방법으로 풀겠다\" 라는 것입니다.\n\n이와 비슷하게 Linear Programming, 선형 계획법도 Programming(계획법)이 들어갑니다. 그러니까 선형 계획법 또한 무언가를 계획하는 것으로서 \"최적의 program을 찾는 방법으로 풀겠다.\"라는 것인데, 앞에 Linear만 추가적으로 붙었습니다. 정리하자면, **\"최적화 문제 일종으로 주어진 선형 조건들을 만족시키면서 선형인 목적 함수를 최적화하여 풀겠다.\"** 라는 것입니다. 이 Linear Programming(LP)은 주로 Operations Research(OR)에서 가장 일반적인 기법으로 뽑힙니다.\n\n관련하여 예를 한번 들어보겠습니다. 아래의 예는 위키백과에 있는 예입니다. 홍길동 씨가 두 가지 종류의 빵을 판매하는데, 초코빵을 만들기 위해서는 밀가루 100g과 초콜릿 10g이 필요하고, 밀빵을 만들기 위해서는 밀가루 50g이 필요합니다. 재료비를 제하고 초코빵을 팔면 100원이 남고, 밀빵을 팔면 40원이 남습니다. 오늘 홍길동 씨는 밀가루 3000g과 초콜릿 100g을 재료로 갖고 있습니다. 만든 빵을 전부 팔 수 있고 더 이상 재료 공급을 받지 않는다고 가정한다면, 홍길동 씨는 이익을 극대화 하기 위해서 어떤 종류의 빵을 얼마나 만들어야 할까요? 선형 계획법을 통해서 알아봅니다!\n\n<center> <img src=\"../../../../img/irl/linear_irl_2.png\" width=\"350\"> </center>\n\n여기서 $x_1$은 초코빵을 $x_2$는 밀빵의 개수를 의미하는 변수입니다. 그림으로 나타내면 아래와 같이 가장 많은 이익을 남기는 방법은 초코빵 10개와 밀빵 40개를 만드는 것이고, 그렇게 해서 얻을 수 있는 최대 이익은 2600원입니다.\n\n<center> <img src=\"../../../../img/irl/linear_irl_3.png\" width=\"600\"> </center>\n\n이익이 최대가 될 때는 이익을 나타내는 직선이 해가 존재할 수 있는 영역 중 원점에서 가장 떨어진 점 (10, 40)에 접할 때입니다. $100 x_1 + 40 x_2 = 2600$\n\n<br>\n## 3.1 Characterization of the Solution Set\n\n**Theorem 3** : Let a finite state space $S$, a set of actions $A = \\{a_1, ... , a_k\\}$, transition probability matrices $\\\\{P_a\\\\}$, a discount factor $\\gamma \\in (0, 1)$이 주어질 때, $\\pi (s) \\equiv a_1$에 의해 주어진 policy $\\pi$가 optimal인 것은 $\\equiv$ 모든 $a = a_2, ... , a_k$에 대해서 reward $R$이 아래의 수식을 만족하는 것과 같습니다.\n\n$$(P_{a_1} - P_a)(I - \\gamma P_{a_1})^{-1} R \\succeq 0 \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (4)$$\n\n**Proof**. $\\pi (s) \\equiv a_1$로 인하여, Equation (1)은 $V^\\pi = R + \\gamma P_{a_1} V^\\pi$이라고 쓸 수 있다. 따라서 아래의 수식으로 쓸 수 있습니다.\n\n$$V^\\pi = (I - \\gamma P_{a_1})^{-1} R$$\n\n위의 수식에서 주석처리가 되어있는 것을 보니 저자가 할 말이 더 있는 것 같습니다.\n\n$I - \\gamma P_{a_1}$(여기서 $I$는 단위행렬)은 항상 invertible(역으로 되는)합니다. 실제로 정말 invertible한 지를 보기 위해 transition matrix인 $P_{a_1}$가 복잡한 공간에 unit circle(단위원)에서 모든 eigenvalue들을 가진다는 것을 먼저 언급합니다. 다시 말해 $\\gamma < 1$일 때, matrix $\\gamma P_{a_1}$가 unit circle 내에 모든 eigenvalues를 가진다는 것을 말합니다. (특히 여기서 1은 eigenvalue가 될 수 없습니다.) transition matrix의 특성상 이렇게 되는 것이기 때문에 기억하는 것이 좋습니다.\n\n뒤이어 위의 특성은 $I - \\gamma P_{a_1}$이 zero eigenvalue를 가지고 있지 않고, ($\\gamma$ 때문에 $I - \\gamma P_{a_1}$가 0~1 사이에 있게 됩니다.) 이것과 동치로 말할 수 있는 것이 singular가 아니라는 것을 의미합니다. 참고로 singular하다는 것은 해당되는 행렬이 역행렬이 존재하지 않는다는 것을 말합니다.\n\n정리하자면, $I - \\gamma P_{a_1}$ -> zero eigenvalue가 없다. $\\equiv$ sigular하지 않다. $\\equiv$ 역행렬이 존재한다.\n\n추가적인 자료\n1) [Stationary distribution relation to eigenvectors and samplices in Markov chain](https://en.wikipedia.org/wiki/Markov_chain#Stationary_distribution_relation_to_eigenvectors_and_simplices)\n2) [특이값 분해(Singular Value Decomposition, SVD)의 활용)](https://darkpgmr.tistory.com/106)\n\n다시 Proof를 따라가봅니다. Equation (2)를 (3)으로 대체하면, $\\pi \\equiv a_1$가 optimal하는 것은 $\\equiv$ 아래의 수식과 같습니다.\n\n$$a_1 \\equiv \\pi (s) \\in arg\\max_{a \\in A} \\sum_{s'} P_{sa} (s') V^\\pi (s') \\,\\,\\,\\,\\, \\forall s \\in S$$ \n\n$$\\Leftrightarrow \\sum_{s'} P_{sa_1} (s') V^\\pi (s') \\geq \\sum_{s'} P_{sa} (s') V^\\pi (s') \\,\\,\\,\\,\\, \\forall s \\in S, a \\in A$$\n\n$$\\Leftrightarrow P_{a_1} V^\\pi \\succeq P_a V^\\pi \\,\\,\\,\\,\\, \\forall a \\in A \\setminus a_1$$\n\n$$\\Leftrightarrow P_{a_1} (I - \\gamma P_{a_1})^{-1} R \\succeq P_a (I - \\gamma P_{a_1})^{-1} R \\,\\,\\,\\,\\, \\forall a \\in A \\setminus a_1$$\n\n여기서 $a \\in A \\setminus a_1$이란 $a_1$을 제외한 set of actions $A$에 있는 $a$들을 말한다.\n\n**Remark**. 매우 비슷한 argument를 사용하여 추가적인 언급을 합니다. $(P_{a_1} - P_a)(I - \\gamma P_{a_1})^{-1} R \\succ 0$라는 조건이 $\\pi \\equiv a_1$가 unique optimal policy가 되는 것에 필요하고, 충분하다고 볼 수 있습니다. (또한 추가적으로 위에 증명에 모든 inequalities를 strict inequalities로 대체함으로써)\n\nfinite-state MDPs에 대해, 이러한 결과는 IRL의 solution인 모든 reward function들의 set을 정의합니다. 그러나 여기에는 두 가지 문제점이 있습니다.\n1) $R = 0$ (and indeed any other constant vector)은 항상 solution입니다. 다시 말해 만약 우리가 어떠한 행동을 취했을 지라도 reward가 항상 같다면, $\\pi \\equiv a_1$을 포함하여 어떠한 policy들은 항상 optimal하다는 것입니다.  $\\pi$가 unique optimal policy라는 점에서는 이 문제를 완화시키지만, 전체적으로 만족시키진 않습니다. 왜냐하면 보통 0에 임의적으로 가까운 일부 reward vector들이 여전히 solution이 될 수 있기 때문입니다.\n2) 대부분의 MDPs에서, criteria (4)를 만족시키는 R에 대한 많은 choice들이 있습니다.\n\n그렇다면 우리는 어떻게 많은 reward function들 중의 하나를 결정할 수 있을까요? 다음 section을 통해서 이러한 문제점들을 해결할 수 있는 natural criteria를 알아봅시다!\n\n## 3.2 LP Formulation and Penalty Terms\n\n위의 질문에 대한 답변으로 명확하게 말하자면, Linear Programming(LP)은 Equation (4)로 인한 문제점들에 대해 실행 가능한 point로서 사용될 수 있습니다.\n\n그래서 R을 고를수 있는 한 가지 natural한 방법은 가장 먼저 $\\pi$를 optimal하도록 만드는 것입니다. 또한 $\\pi$로부터 어떠한 single-step deviation(편차)을 가능한 한 costly하게 만드는 것입니다. **쉽게 말해 최적의 행동이 있다면 최적의 정책을 찾을 수 있고, 최적의 정책을 찾을 수 있다면 R을 고를 수 있다는 것입니다.**\n\n수식으로 표현해보면, (4)를 만족시키는 모든 function R 중에서, 다음의 수식을 maximize하도록 하는 $a_1$을 고를 수 있습니다.\n\n$$\\sum_{s \\in S} (Q^\\pi (s, a_1) - \\max_{a \\in A \\setminus a_1} Q^\\pi (s, a)) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (6)$$\n\n수식을 설명해보면 quality of the optimal action과 quality of the next-best action 사이의 differences의 sum을 maximize하는 것을 말합니다.\n\n추가적으로 다른 모든 Q값들이 동등할 때, 대부분의 small rewards에 대한 solution은 \"simpler\"하도록 optional하게 objective function이 $-\\lambda ||R||_1$ ($\\ell_1$-penalty)와 같은 weight decay-like penalty term을 추가할 것입니다. (여기서 $\\lambda$는 small rewards를 가지는 것과 (6)을 maximizing하는 것 두 목표 사이를 balancing할 수 있는 adjustable penalty coefficient입니다.)\n\n최대한 쉽게 풀어서 다시 설명해보겠습니다. 위의 수식 (6)으로 maximizing을 한다고 했을 때, Q값들에 대해서 보다 더 R를 simple하게 정하고, 수식 (6)을 더 maximizing이 잘 되도록 penalty term을 추가하는 것입니다. 쉽게 말해 우리가 Loss function에서 regularization term을 두는 것처럼 더 효과적이고 쉽게 사용하기 위해서 하는 작업이라고 생각하면 편합니다. $Q^\\pi (s, a_1)$과 Q값들 사이가 더 극명하도록 L1 regularization term을 두어서 R를 쉽게 정하자! 라는 것입니다.\n\n이렇게 함으로써 \"simplest\" R (largest penalty coefficient)을 찾을 수 있고, R은 왜 $\\pi$가 optimal한 지를 \"explain\"할 수 있습니다.\n\n정리하여 다시 optimization problem을 수식으로 말하자면 다음과 같습니다.\n**First objective function & algorithm**\n\n$$maximize \\,\\,\\,\\,\\, \\sum_{i=1}^N \\min_{a \\in \\{a_2, ... , a_k\\}} \\\\{(P_{a_1} (i) - P_a (i))(I - \\gamma P_{a_1})^{-1} R\\\\} - \\lambda ||R||_1$$\n\n$$s.t. \\,\\,\\,\\,\\, (P_{a_1} - P_a)(I - \\gamma P_{a_1})^{-1} R \\succeq 0 \\,\\,\\,\\,\\, \\forall a \\in A \\setminus a_1$$\n\n$$|R_i| \\leq R_{max}, i = 1, ... , N$$\n\n이러한 수식들을 통해 linear program으로 표현될 수 있고, 효과적으로 해결될 수 있습니다.\n\n<br><br>\n\n# 4. Linear Function Approximation in Large State Spaces\n\n이번 section에서는 infinite state spaces의 case를 다룹니다. infinite-state MDPs는 section 2에서의 finite-state와 같은 방식으로 정의될 수 있습니다. 추가적으로 state는 $S = \\mathbb{R}^n$의 case에 대해서만 다루고자 합니다. 따라서 reward function R은 $S = \\mathbb{R}^n$로부터의 function이라고 할 수 있습니다.\n\nCalculus of variations는 infinite state spaces에서 optimizing하는 데에 있어 좋은 tool이지만, 종종 알고리즘적으로 어렵게 만듭니다. 따라서 reward function에 대해 **\"Linear Functional Approximation\"** 을 사용합니다. 수식으로 R을 표현하자면 다음과 같습니다.\n\n$$R(s) = \\alpha_1 \\phi_1 (s) + \\alpha_2 \\phi_2 (s) + \\cdots + \\alpha_d \\phi_d (s) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (8)$$\n\n여기서 $\\phi_1, ... , \\phi_d$는 $S$로부터 mapping된 고정되어 있고, 우리가 알고 있고, bound되어 있는 basis function입니다. 그리고 $\\alpha_i s$는 우리가 fit해야하는 알고있지 않은 parameter입니다.\n\n다음으로 $V^\\pi$에 대해서도 linearity of expectation을 함으로써, reward function R이 Equation (8)로 주어질 때 value function을 다음과 같이 표현할 수 있습니다.\n\n$$V^\\pi = \\alpha_1 V_1^\\pi + \\cdots + \\alpha_d V_d^\\pi \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (9)$$\n\n위의 수식과 Theorem 2 (3번 수식)을 사용하면서, policy $\\pi (s) \\equiv a_1$를 optimal하도록 만드는 R에 대해서 (4)의 appropriate generalization은 다음의 조건입니다.\n\n<center> <img src=\"../../../../img/irl/linear_irl_4.png\" width=\"370\"> </center>\n\n, for all states $s$ and all actions $a \\in A \\setminus a_1$\n\n하지만 위의 formulation들에는 두 가지 문제가 있습니다.\n1) infinite state spaces에서, Equation (10)의 형태에는 infinitely 많은 제약이 있습니다. infinite state spaces이기 때문에 모든 state를 check하기가 불가능하고 어렵습니다. 따라서 알고리즘적으로, states 중의 finite subset $S_0$만 sampling함으로써 이러한 문제를 피하고자 합니다.\n2) R을 표현하기 위해 Equation (8)에서 linear function approximator를 사용한다고 제한했기 때문에, 어느 $\\pi$가 optimal한 지에 대해 더 이상 어떠한 reward function도 표현할 수 없습니다. 그럼에도 불구하고, linear function approximator를 사용할 것입니다.\n\n최종적으로 linear programming formulation은 다음과 같습니다.\n**Second objective function & algorithm**\n\n<center> <img src=\"../../../../img/irl/linear_irl_5.png\" width=\"460\"> </center>\n\n$$s.t. \\,\\,\\, |\\alpha_i| \\leq 1, \\,\\,\\, i = 1, ... , d$$\n\n<br><br>\n\n# 5. IRL from Sampled Trajectories\n\n이번 section에서는 오직 state space에서의 actual trajectories의 set을 통해서만 policy $\\pi$를 접근하는 좀 더 realistic case에 대해서 IRL 문제를 다룹니다. 그래서 MDP의 explicit model을 필요로 하지 않습니다.\n\ninitial state distribution $D$를 고정하고, (unknown) policy $\\pi$에 대해 우리의 목표는 $\\pi$가 $\\mathbb{E}_{s_0 \\sim D} [V^\\pi (s_0)]$를 maximize하는 R를 찾는 것입니다. (기억합시다!) 추가적으로 notation을 단순화하기 위해 고정된 start state $s_0$를 가정합니다.\n\n먼저 $\\alpha_i$의 setting을 통해 $V^\\pi (s_0)$를 estimating하는 방법이 필요합니다. 이것을 하기 위해, 첫 번째로 $m$ Monte Carlo trajectories를 만들어냅니다. 그리고 나서 $i = 1, ... , d$에 대해 만약 reward가 $R = \\phi_i$라면, $V_i^\\pi (s_0)$를 얼마나 average empirical return이 $m$ trajectories에 있었는 지로 정의합니다.\n\n예를 들어, 만약 $m = 1$ trajectories이고, 이 trajectory가 states ($s_0, s_1, ...$)의 sequence라면, 다음과 같이 나타낼 수 있습니다.\n\n$$\\hat{V}_i^\\pi (s_0) = \\phi_i (s_0) + \\gamma \\phi_i (s_1) + \\gamma^2 \\phi_i (s_2) + \\cdots$$\n\n일반적으로, $\\hat{V}_i^\\pi (s_0)$은 어떠한 $m$ trajectories의 empirical returns에 대하여 average합니다. (여기서 말하는 어떠한 $m$ trajectories는 임의의 finite number에 의해 truncate된 trajectories를 말합니다.) 그리고 그 때 $\\alpha_i$의 어떠한 setting에 대해서, $V^\\pi (s_0)$의 natural estimate는 다음과 같습니다.\n\n$$\\hat{V}^\\pi (s_0) = \\alpha_1 \\hat{V}_1^\\pi (s_0) + \\cdots + \\alpha_d \\hat{V}_d^\\pi (s_0) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (11)$$\n\n위의 수식의 \"inductive(귀납적인) step\"은 다음을 뒤따릅니다 : set of policies $\\{ \\pi_1, ... , \\pi_k \\}$이 있고, resulting reward function은 아래의 수식을 만족하기 때문에 $\\alpha_i$의 setting을 찾을 수 있습니다.\n\n$$V^{\\pi^*} (s_0) \\geq V^{\\pi_i} (s_0), \\,\\,\\, i = 1, ... , k \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (12)$$\n\n그리고 section 4에서 마지막에 있던 수식에서, objective function을 약간 바꿀 수 있습니다. 따라서 optimization의 식은 다음과 같이 될 수 있다.\n**Final objective function & algorithm**\n\n$$maximize \\,\\,\\,\\,\\, \\sum_{i=1}^k p(\\hat{V}^{\\pi^*} (s_0) - \\hat{V}^{\\pi_i} (s_0))$$\n\n$$s.t. \\,\\,\\,\\,\\, |\\alpha_i| \\leq 1, \\,\\,\\, i = 1, ... , d$$\n\n위의 수식에서 $\\hat{V}^{\\pi_i} (s_0)$과 $\\hat{V}^{\\pi^*} (s_0)$은 Equation (11)에서 주어진 $\\alpha_i$의 (implicit) linear function이다.\n\n그러므로, 위의 $\\hat{V}^{\\pi_i} (s_0)$과 $\\hat{V}^{\\pi^*} (s_0)$은 쉽게 linear programming으로 해결할 수 있습니다.\n\n위의 optimization 식은 $\\alpha_i$의 새로운 setting으로 설정할 수 있고, 그러므로 새로운 reward function $R = \\alpha_1 \\phi_1 + \\cdots + \\alpha_d \\phi_d$을 가질 수 있습니다.\n\n그리고 그 때 $R$로 인한 $V^\\pi (s_0)$를 maximize하는 policy $\\pi_{k+1}$을 찾을 수 있고, $\\pi_{k+1}$을 current set of policies에 추가할 수 있습니다. 그리고 이것을 계속할 수 있습니다. (많은 수의 iteration을 통해 우리가 \"satisfied\"하는 $R$를 찾을 수 있습니다.)\n\n<br><br>\n\n# 6. Experiments\n\n<br>\n## 6.1 First experiment : 5 x 5 grid world\n\n<center> <img src=\"../../../../img/irl/linear_irl_6.png\" width=\"500\"> </center>\n\n첫 번째 실험은 agent가 왼쪽 아래에서 시작하여 오른쪽 위로 가면 보상 1을 받는 5x5 grid world를 사용했습니다. action은 상하좌우지만 noise가 있으며 30%의 확률로 랜덤하게 움직입니다.\n\n위에 있는 Figure 1은 5x5 grid world에서의 optimal policy와 true reward function을 나타낸 것입니다.\n\n<center> <img src=\"../../../../img/irl/linear_irl_7.png\" width=\"500\"> </center>\n\n앞에서 설명한 penalty term인 $\\lambda$를 주지 않은 Section 3.2의 discrete/finite state problem에서 사용할 수 있는 algorithm을 쓰면 Figure 2 윗 부분의 그림과 같은 울퉁불퉁한 reward function의 모습을 볼 수 있습니다. 위에서 말했던 section 3.2에 있는 algorithm은 다음과 같습니다.\n\n$$maximize \\,\\,\\,\\,\\, \\sum_{i=1}^N \\min_{a \\in \\{a_2, ... , a_k\\}} \\\\{(P_{a_1} (i) - P_a (i))(I - \\gamma P_{a_1})^{-1} R\\\\} - \\lambda ||R||_1$$\n\n$$s.t. \\,\\,\\,\\,\\, (P_{a_1} - P_a)(I - \\gamma P_{a_1})^{-1} R \\succeq 0 \\,\\,\\,\\,\\, \\forall a \\in A \\setminus a_1$$\n\n$$|R_i| \\leq R_{max}, i = 1, ... , N$$\n\n그러나 Figure 2 아래에 쓰여져 있는 $\\lambda$ = 1.05 로 설정을 하면 Figure 2 아래에 있는 그림과 같은 true reward에 밀접한 reward function을 얻을 수 있습니다.\n\n<br>\n## 6.2 Second experiment : Mountain-car\n\n<center> <img src=\"../../../../img/irl/linear_irl_8.png\" width=\"500\"> </center>\n\n다음 실험으로는 보통 잘 알려져있는 mountain-car 환경입니다. ture, undiscounted reward는 언덕에 있는 goal지점에 도달하기 전까지 step마다 -1을 받는 것입니다. 그리고 state는 자동차의 x-위치와 속도입니다.\n\n<center> <img src=\"../../../../img/irl/linear_irl_5.png\" width=\"460\"> </center>\n\n$$s.t. \\,\\,\\, |\\alpha_i| \\leq 1, \\,\\,\\, i = 1, ... , d$$\n\nstate가 연속적이기 때문에 위의 수식과 같이 section 4의 continuous/finite state problem에서 사용할 수 있는 algorithm을 사용했습니다. 또한 reward에 대해서 오직 자동차의 x-위치에 대한 functions가 되도록 Gaussian 모양으로 된 26개의 basis functions의 linear combinations을 가지는 function approximator class를 구성하였습니다.\n\n<center> <img src=\"../../../../img/irl/linear_irl_9.png\" width=\"500\"> </center>\n\nalgorithm으로 optimal policy를 고려하여 전형적인 reward function은 Figure 4의 윗 부분에 있는 그래프로 나타납니다. (Note the scale on the $y$ axis.) 명확하게도, solution은 reward의 $R = -c$ structure를 거의 완벽하게 나타내었습니다.\n\n좀 더 challenging한 문제로서, 이 논문에서는 언덕 아래 주변을 중심으로 [-0.72, -0.32] 사이에 있으면 reward 1 아니면 0을 주도록 changed ture reward에 대한 실험을 동일하게 하였습니다. 그리고 여기서 $\\gamma$는 0.99를 주었다고 합니다.\n\n이 문제에서의 optimal policy는 가능한 한 빨리 언덕 아래로 가서 주차를 하는 것입니다. 새로운 문제에 대해 알고리즘을 적용해봤을 때 전형적인 solution은 Figure 4 아래에 있는 그래프로 나타낼 수 있습니다.\n\n대체로 reward의 중요한 structure를 정해진 [-0.72, -0.32]에 대해 성공적으로 찾았다고 합니다. 또한 오른쪽에는 artifact가 있어 오른쪽 끝을 피할 수 없도록 \"shooting out\"하는 효과가 있다고 합니다. 그럼에도 불구하고, solution이 이 문제에 꽤 좋았다고 합니다.\n\n<br>\n## 6.3 Final experiment : Continuous version of the 5 x 5 grid world\n\n마지막 실험은 5x5 grid world의 continuous version에서 적용했습니다.\n\nstate는 [0, 1] x [0, 1]이고, action은 상하좌우 방향으로 0.2만큼 움직입니다. 또한 [-0.1, 0.1]의 uniform noise가 각각 좌표에 추가되고, 만약 unit square 내에 uniform noise가 필요한 경우, state는 truncate됩니다.\n\nreward는 [0.8, 1] x [0.8, 1]에서는 1을 받고 나머지는 0을 받습니다. 그리고 $\\gamma$는 0.9를 사용했습니다. 또한 function approximator class는 2-dimensional Gaussian basis functions에 대한 15x15 array의 linear combinations로 구성하였습니다.\n\ninitial state distribution $D$는 state space에 대해  uniform하였고, algorithm은 policy를 평가하기 위해 각각의 30 steps 마다 $m=5000$ trajectories를 사용하여 적용하였습니다.\n\n$$maximize \\,\\,\\,\\,\\, \\sum_{i=1}^k p(\\hat{V}^{\\pi^*} (s_0) - \\hat{V}^{\\pi_i} (s_0))$$\n\n$$s.t. \\,\\,\\,\\,\\, |\\alpha_i| \\leq 1, \\,\\,\\, i = 1, ... , d$$\n\n위에서 다뤘던 section 5에서의 algorithm을 사용하여 찾아진 solution은 단지 1 iteration만 했는데도 꽤나 reasonable했습니다. 그래서 약 15 iterations에 대해서도 해본 결과, algorithm은 동일하게 좋은 performance로 해결했습니다.\n\n<center> <img src=\"../../../../img/irl/linear_irl_10.png\" width=\"500\"> </center>\n\n또한 action choice가 다른 state sapce의 부분을 계산하면서 true optimal policy와 fitted reward’s optimal policy를 비교하였고 (Figure 5, 위), 대체적으로 3% ~ 10% 사이로 불일치했습니다. 아마도 algorithm's performance의 좀 더 적절한 측정은 true optimal policy의 quality와 fitted reward’s optimal policy의 quality를 비교하는 것입니다. (Quality는 물론 true reward function을 사용하여 측정됩니다.)\n\n결과적으로 algorithm의 약 15 iterations에 대해서, evaluations은 (which used 50000 Monte Carlo trials of 50 steps each) true \"optimal policy\"의 value와 fitted reward’s optimal policy사이에 statistically한 중요한 차이를 detect할 수 없다는 것을 볼 수 있었습니다. 그만큼 true optimal policy와 reward's optimal policy의 차이가 없다는 것을 뜻합니다. (Figure 5, 아래)\n\n<br><br>\n\n# 7. Conclusions and Future work\n\n이 논문은 moderate-sized discrete and continuous domain에서 Inverse Reinforcement Learning 문제가 해결될 수 있다는 것을 보였습니다.\n\n하지만 많은 open question들이 아래와 같이 남아있습니다.\n1) Potential-based shaping rewards는 MDP에서 학습시키기 위한 하나의 solution으로서 reward function을 더 쉽게 만들 수 있습니다. 그렇다면 우리는 더 \"easy\" reward function을 만들기 위한 IRL 알고리즘들을 만들 수 있을까요?\n2) IRL를 real-world empirical application측면에서 보면, sensor inputs and actions에 대해서 observer의 측정에 상당한 noise가 있을지도 모릅니다. 여기에 더하여 많은 optimal policy들이 존재할 지도 모릅니다. 어떠한 data를 noise없이 fit하도록 하는 적절한 metric은 무엇일까요?\n3) 만약 행동이 절대로 optimality와 일치하지 않는다면, state space에 specific region에 대한 \"locally consistent\" reward function을 어떻게 알 수 있을까요?\n4) 어떻게 reward function의 identifiability를 maximize하기 위한 실험을 고안해낼 수 있을까요?\n5) 이 논문에 적힌 알고리즘적인 접근이 partially observable environment의 case를 얼마나 잘 실행할 수 있을까요?\n\n<br><br>\n\n# 처음으로\n\n## [Let's do Inverse RL Guide](https://reinforcement-learning-kr.github.io/2019/01/22/0_lets-do-irl-guide/)\n\n<br>\n\n# 다음으로\n\n## [APP 여행하기](https://reinforcement-learning-kr.github.io/2019/02/01/2_app/)","slug":"1_linear-irl","published":1,"updated":"2019-02-07T11:21:31.916Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjrujlu1f001z5wfeybf5vjkx","content":"<center> <img src=\"../../../../img/irl/linear_irl_1.png\" width=\"850\"> </center>\n\n<p>Author : Andrew Y. Ng, Stuart Russell<br>Paper Link : <a href=\"http://ai.stanford.edu/~ang/papers/icml00-irl.pdf\" target=\"_blank\" rel=\"noopener\">http://ai.stanford.edu/~ang/papers/icml00-irl.pdf</a><br>Proceeding : International Conference on Machine Learning (ICML) 2000</p>\n<hr>\n<h1 id=\"0-Abstract\"><a href=\"#0-Abstract\" class=\"headerlink\" title=\"0. Abstract\"></a>0. Abstract</h1><p>이 논문은 Markov Decision Processes에서의 <strong>Inverse Reinforcement Learning(IRL)</strong>을 다룹니다. 여기서 IRL이란, observed, optimal behavior이 주어질 때 reward function을 찾는 것입니다.</p>\n<p>IRL은 두 가지 장점이 존재합니다.<br>1) 숙련된 행동을 얻기 위한 apprenticeship Learning<br>2) 최적화된 reward function을 알아내는 것</p>\n<p>이 논문에서는 IRL의 reward function에 대해 세 가지 알고리즘을 제시합니다.<br>1) 첫 번째와 두 번째는 전체의 policy를 알고 있는 case를 다루는 것입니다. finite state space에서의 tabulated reward function과 potentially infinite state space에 대한 reward function의 linear functional approximation을 다룹니다.<br>2) 세 번째로는 observed trajectories의 finite set을 통해서만 policy를 알고 있을 때 더 다양한 realistic case에 대한 알고리즘입니다.</p>\n<p>이 논문은 2000년에 나온 논문으로서 그 당시에 어떻게 하면 reward function을 역으로 찾을 수 있는지를 증명을 통해서 다루고 있고, 왜 reward function이 중요한 지를 중요하게 말하고 있습니다. 그러니까 IRL을 통해 reward를 얻어 RL을 하는 실질적인 학습을 말하는 논문보다는 reward function에 대해서 말하고 있고 이에 따른 알고리즘들을 말하는 논문입니다. “reward function은 이렇게 생겨야 돼! 그리고 우리는 이러한 알고리즘들을 통해 reward function을 찾아낼 수 있어!”라는 듯이 말하고 있습니다.</p>\n<p>위에서 말한 세 가지 알고리즘의 중요한 문제는 degeneracy입니다. 여기서 degeneracy란, 어느 observed policy가 optimal한 지에 대하여 reward function의 large set이 존재하는 지에 관한 degeneracy를 말합니다. 다시 말해 이러한 reward function의 large set을 찾기 위한 알고리즘이 존재하는 지, degeneracy는 없는 지를 말하는 것입니다. 이 논문에서는 degeneracy를 없애기 위해서 natural heuristics를 제시합니다. 그리고 natural heuristics를 통해 IRL에 대해서 효과적으로 해결가능한 <strong>linear programming</strong> formulation을 나타냅니다.</p>\n<p>추가적으로 이러한 문제를 하나의 용어로 말하면 <strong>“ill-posed problem”</strong> 이라고 합니다. ill-posed problem을 검색하면 위키피디아에 있는 well-posed problem이 먼저 나오는데 여기서 말하는 well-posed problem이란 특정한 solution이 존재하고, 그 solution이 unique하다고 나와있습니다. 반대로, 역강화학습에서는 reward가 정해진 것이 아니라 여러가지 형태의 값으로 나타날 수 있기 때문에(정해진 값이 아니기 때문에, not unique) ill-posed problem이라고 볼 수 있습니다.</p>\n<p>실험에서는 이 논문에서의 알고리즘들을 통해 간단한 discrete/finite and continuous/infinite state problem들을 해결합니다.</p>\n<p><br><br></p>\n<h1 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h1><p>IRL은 Russell이 쓴 논문에서 비공식적으로 정의합니다.</p>\n<p>Given : 1) 시간에 따른 agent의 행동들, 2) 필요하다면, agent의 sensory input들, 3) 가능하다면, 환경의 모델  이 주어질 때<br>Determine : reward function을 최적화하는 것</p>\n<p>뒤이어 나오는 두 가지 언급을 통해서 IRL이 왜 중요한 지를 알아봅시다.</p>\n<p>첫 번째로 animal and human learning에 대한 computational model로서 RL의 잠재적인 사용입니다. 예를 들어 벌이 먹이를 찾는 모델이 있다고 했을 때 여러 꽃들 사이에서의 reward가 꿀에 대한 간단한 saturating function이라고 가정해봅시다. reward function은 보통 고정되어 있고 + 우리가 정하고 + 이미 알고 있습니다. 하지만 animal and human behavior을 조사할 때 우리는 추가적으로 알지 못하는 reward function까지도 생각해야합니다. 다시 말해 reward function의 multiattribute(다속성)도 생각을 해야한다는 것입니다. 벌이 바람과 벌을 먹이로 하는 포식자들로 부터 비행 거리, 시간, 위험 요소들을 고려하여 꿀 섭취를 할 수도 있습니다. 사람의 경제적 행동 속에서도 이러한 경우가 많습니다. 따라서 IRL은 이론적으로 생물학, 경제학, 또는 다른 분야들까지 포함하는 근본적인 문제에서 나타낼 수 있습니다.</p>\n<p>두 번째로는 특정한 도메인에서 잘 행동할 수 있는 intelligent agent를 구성할 수 있다는 것입니다. 보통 agent designer들은 그들이 정하는 reward function의 optimization이 “desirable” 행동을 만들 것이라는 굉장히 rough한 생각을 가질 수 있습니다. 그렇지만 아주 간단한 RL 문제라도 이것은 agent designer들을 괴롭힐 수 있습니다. 여기서 한 가지 사용할 수 있는 것이 바로 다른 “expert” agent의 행동입니다.</p>\n<p>가장 넓은 set인 <strong>“Imitation Learning”</strong> 안에는 일반적으로 expert agent를 통한 학습이 두 가지 방법이 있습니다.<br>1) 첫 번째로는 <strong>“IRL”</strong> 이라는 것이 있고, IRL을 이용한 하나의 방법으로서 Pieter Abbeel이 주로 쓰는 알고리즘의 이름인 Apprenticeship Learning이라는 것이 존재합니다. 그래서 IRL이란 teacher의 demonstation에서의 optimal policy를 통해 reward를 찾는 것을 말합니다.<br>2) 다음으로 <strong>“Behavioral Cloning”</strong> 이라는 것이 있습니다. 아예 supervised learning처럼 행동을 복제한다고 생각하면 됩니다.</p>\n<p>일반적으로 IRL을 통해서 cost를 얻고, RL을 통해서 policy를 찾는 것이 모든 IRL의 목적입니다. 하지만 이렇게 2 step을 해야한다는 점에서 많은 complexity가 생기고, 그렇다고 IRL을 통해서 얻은 cost만 가지고는 별로 할 수 있는게 없습니다. 따라서 앞서 말한 과정을 2 step만에 하는 것이 아니라 1 step만에 풀어버리는 논문이 바로 Generative Adversarial Imitation Learning(GAIL)이라는 논문입니다. 이후의 논문들은 다 GAIL을 응용한 것이기 때문에 GAIL만 이해하면 그 뒤로는 필요할 때 찾아서 보면 될 것 같고, GAIL이라는 것을 이용하여 여러 가지 연구를 해볼 수도 있을 것 같습니다.</p>\n<p>이 논문에 나오는 section들은 다음과 같습니다.</p>\n<ul>\n<li>Section 2 : finite Markov Decision Processes(MDPs)의 formal definition들과 IRL의 문제를 다룹니다.</li>\n<li>Section 3 : finite state spaces에서 주어진 policy 중의 어느 policy가 optimal한 지에 대해 모든 reward function의 set을 다룹니다.</li>\n<li>Section 4 : reward function의 explicit, tabular representation이 가능하지 않을 수 있기 때문에 large or infinite state spaces의 case를 다룹니다.</li>\n<li>Section 5 : observed trajectories의 finite set을 통해서만 policy를 안다고 했을 때, 더 realistic case에 대해서 다룹니다.</li>\n<li>Section 6 : 앞서 언급했던 세 가지 알고리즘을 적용하여 discrete and continuous stochastic navigation problems와 mountain-car problem에 대한 실험부분이 나옵니다.</li>\n<li>Section 7 : 결론과 더 나아가 연구되어야할 방향에 대해서 다룹니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"2-Notation-and-Problem-Formulation\"><a href=\"#2-Notation-and-Problem-Formulation\" class=\"headerlink\" title=\"2. Notation and Problem Formulation\"></a>2. Notation and Problem Formulation</h1><p>먼저 IRL version의 notation, definitions, basic theorems for Markov decision processes(MDPs)에 대해서 알아봅시다.</p>\n<p><br></p>\n<h2 id=\"2-1-Markov-Decision-Processes\"><a href=\"#2-1-Markov-Decision-Processes\" class=\"headerlink\" title=\"2.1 Markov Decision Processes\"></a>2.1 Markov Decision Processes</h2><p>A finite MDP is a tuple $(S, A, \\{P_{sa}\\}, \\gamma, R)$</p>\n<p>$S$ is a finite set on $N$ states.</p>\n<p>$A = \\{a_1, … , a_k\\}$ is a set of $k$ actions.</p>\n<p>$P_{sa} (\\cdot)$ are the state transition probabilities upon taking action $a$ in state $s$.</p>\n<p>$\\gamma \\in [0,1)$ is the discount factor.</p>\n<p>$R : S \\rightarrow \\mathbb{R}$ is the reinforcement function (reward function) bounded absolute value by $R_{max}$.</p>\n<p>$R$에 대해 간단하게 말하기 위해서 $R(s,a)$보다 $R(s)$로서 reward $R$을 정의했습니다. $R$같은 경우, 사실 정의하기 나름인 것 같습니다. 이 논문에서는 $R(s,a)$로 생각하는 것보다 $R(s)$로 생각하는 것이 편하고 간단하기 때문에 이렇게 notation을 적은 것 같습니다. 아무래도 역강화학습에서 action $a$까지 생각해주면 추가적인 notation이 더 나오기 때문에 복잡해집니다. 또한 사실 우리가 최적의 행동만 안다면 보상을 행동까지 생각해줄 필요는 없기 때문에 $R(s)$라고 적었다고 봐도 될 것 같습니다. 역강화학습 논문들이 다 $R(s)$을 사용하는 것은 아닙니다. 논문마다 case by case로 쓸 수 있는 것 같습니다.</p>\n<p>Policy is defined as any map $\\pi : S \\rightarrow A$.</p>\n<p>Value function evaluated at any state $s_1$ for a policy $\\pi$ is given by</p>\n<p>$$V^\\pi (s_1) = \\mathbb{E} [R(s_1) + \\gamma R(s_2) + \\gamma^2 R(s_3) + \\cdots | \\pi]$$</p>\n<p>Q-function is</p>\n<p>$$Q^\\pi (s, a) = R(s) + \\gamma \\mathbb{E}_{s’ \\sim P} [V^\\pi (s’)]$$</p>\n<ul>\n<li>여기서 notation $s’ \\sim P$는 $s’$를 $P_{sa}$에 따라 sampling한 것입니다.</li>\n</ul>\n<p>Optimal value function is $V^* (s) = sup_\\pi V^\\pi (s)$. (sup is supremum, 상한, 최소 상계)</p>\n<p>Optimal Q-function is $Q^* (s,a) = sup_\\pi Q^\\pi (s,a)$.</p>\n<p>위의 모든 function들은 discrete, finite spaces에서 정의합니다.</p>\n<p>마지막으로 Symbol $\\prec$ and $\\preceq$ denote strict and non-strict vectorial inequality - i.e., $x \\, \\prec \\, y$ if and only if $\\, \\forall i \\, \\, x_i &lt; y_i$</p>\n<p><br></p>\n<h2 id=\"2-2-Basic-Properties-of-MDPs\"><a href=\"#2-2-Basic-Properties-of-MDPs\" class=\"headerlink\" title=\"2.2 Basic Properties of MDPs\"></a>2.2 Basic Properties of MDPs</h2><p>IRL 문제의 solution에 대해 MDPs와 관련된 두 가지 classical result가 필요합니다.</p>\n<p>Theorem 1 (Bellman Equations) : MDP $M = (S, A, \\{P_{sa}\\}, \\gamma, R)$ and policy $\\pi : S \\rightarrow A$가 주어질 때, 모든 $s \\in S$, $a \\in A$에 대해서 $V^\\pi$ 와 $Q^\\pi$는 다음을 만족합니다.</p>\n<p>$$V^\\pi (s) = R(s) + \\gamma \\sum_{s’} P_{s\\pi(s)} (s’) \\, V^\\pi (s’) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (1)$$</p>\n<p>$$Q^\\pi (s,a) = R(s) + \\gamma \\sum_{s’} P_{sa} (s’) \\, V^\\pi (s’) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (2)$$</p>\n<p>Theorem 2 (Bellman Optimality) : MDP $M = (S, A, \\{P_{sa}\\}, \\gamma, R)$ and policy $\\pi : S \\rightarrow A$가 주어질 때, $\\pi$는 $M$에 대해 optimal policy라는 것은 $\\equiv$ 모든 $s \\in S$에 대해 다음과 수식과 같습니다.</p>\n<p>$$\\pi (s) \\in arg\\max_{a \\in A} Q^\\pi (s,a) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (3)$$</p>\n<p>Theorem 1, 2의 증명은 <a href=\"http://incompleteideas.net/book/RLbook2018.pdf\" target=\"_blank\" rel=\"noopener\">서튼책</a>에 있고, 여기서는 추가적으로 다루지 않겠습니다.</p>\n<p><br></p>\n<h2 id=\"2-3-Inverse-Reinforcement-Learning\"><a href=\"#2-3-Inverse-Reinforcement-Learning\" class=\"headerlink\" title=\"2.3 Inverse Reinforcement Learning\"></a>2.3 Inverse Reinforcement Learning</h2><p>IRL의 문제는 observed 행동을 설명할 수 있는 reward function을 찾는 것입니다. 그래서 가장 먼저 무엇을 하고 싶은 것이냐면, state space가 finite이고, model은 이미 알고 있고, complete policy가 observe된 simple case부터 시작하고자 합니다. 다시 말해 $\\pi$가 optimal policy일 때, 가능한 한 reward function의 set을 찾고자 하는 것입니다.</p>\n<p>추가적으로 action들을 renaming함으로써 $\\pi (s) \\equiv a_1$ (여기서 $a_1$는 임의의 하나의 행동)라는 것을 가정할 것입니다. 이 trick은 notation을 단순화하기 위해 사용될 것입니다.</p>\n<p><br><br></p>\n<h1 id=\"3-IRL-in-Finite-State-Spaces\"><a href=\"#3-IRL-in-Finite-State-Spaces\" class=\"headerlink\" title=\"3. IRL in Finite State Spaces\"></a>3. IRL in Finite State Spaces</h1><p>이번 section에서는 주어진 policy 중의 어느 policy가 optimal한 지에 대해서 모든 reward functions의 set에 대한 간단한 정의를 할 것입니다. 그리고 모든 reward functions의 set에 degeneracy를 없애기 위해서 간단한 heuristic 방법인 <strong>“Linear Programming”</strong> (<a href=\"https://en.wikipedia.org/wiki/Linear_programming\" target=\"_blank\" rel=\"noopener\">Wikipedia</a>, <a href=\"https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%EA%B3%84%ED%9A%8D%EB%B2%95\" target=\"_blank\" rel=\"noopener\">위키백과</a>)을 제안합니다.</p>\n<p>Linear Programming에 대해서 간략하게만 알아봅시다. 대부분의 사람들은 Linear Programming(선형 계획법)보다 더 친숙한 Dynamic Programming(동적 계획법)을 알고 있는 경우가 많습니다. 여기서 “Dynamic”이란 동적, 즉 시간에 따라 변하며 다단계적인 특성을 말합니다. 그리고 “Programming”이란 컴퓨터 프로그래밍이 아니라 무언가를 계획하여 최적의 program을 찾는 방법을 말합니다. 한마디로 “시간에 따라서 큰 문제들 안에 작은 문제들이 중첩된 경우에 전체 큰 문제를 작은 문제로 쪼개서 최적의(optimal) program을 찾는 방법으로 풀겠다” 라는 것입니다.</p>\n<p>이와 비슷하게 Linear Programming, 선형 계획법도 Programming(계획법)이 들어갑니다. 그러니까 선형 계획법 또한 무언가를 계획하는 것으로서 “최적의 program을 찾는 방법으로 풀겠다.”라는 것인데, 앞에 Linear만 추가적으로 붙었습니다. 정리하자면, <strong>“최적화 문제 일종으로 주어진 선형 조건들을 만족시키면서 선형인 목적 함수를 최적화하여 풀겠다.”</strong> 라는 것입니다. 이 Linear Programming(LP)은 주로 Operations Research(OR)에서 가장 일반적인 기법으로 뽑힙니다.</p>\n<p>관련하여 예를 한번 들어보겠습니다. 아래의 예는 위키백과에 있는 예입니다. 홍길동 씨가 두 가지 종류의 빵을 판매하는데, 초코빵을 만들기 위해서는 밀가루 100g과 초콜릿 10g이 필요하고, 밀빵을 만들기 위해서는 밀가루 50g이 필요합니다. 재료비를 제하고 초코빵을 팔면 100원이 남고, 밀빵을 팔면 40원이 남습니다. 오늘 홍길동 씨는 밀가루 3000g과 초콜릿 100g을 재료로 갖고 있습니다. 만든 빵을 전부 팔 수 있고 더 이상 재료 공급을 받지 않는다고 가정한다면, 홍길동 씨는 이익을 극대화 하기 위해서 어떤 종류의 빵을 얼마나 만들어야 할까요? 선형 계획법을 통해서 알아봅니다!</p>\n<center> <img src=\"../../../../img/irl/linear_irl_2.png\" width=\"350\"> </center>\n\n<p>여기서 $x_1$은 초코빵을 $x_2$는 밀빵의 개수를 의미하는 변수입니다. 그림으로 나타내면 아래와 같이 가장 많은 이익을 남기는 방법은 초코빵 10개와 밀빵 40개를 만드는 것이고, 그렇게 해서 얻을 수 있는 최대 이익은 2600원입니다.</p>\n<center> <img src=\"../../../../img/irl/linear_irl_3.png\" width=\"600\"> </center>\n\n<p>이익이 최대가 될 때는 이익을 나타내는 직선이 해가 존재할 수 있는 영역 중 원점에서 가장 떨어진 점 (10, 40)에 접할 때입니다. $100 x_1 + 40 x_2 = 2600$</p>\n<p><br></p>\n<h2 id=\"3-1-Characterization-of-the-Solution-Set\"><a href=\"#3-1-Characterization-of-the-Solution-Set\" class=\"headerlink\" title=\"3.1 Characterization of the Solution Set\"></a>3.1 Characterization of the Solution Set</h2><p><strong>Theorem 3</strong> : Let a finite state space $S$, a set of actions $A = {a_1, … , a_k}$, transition probability matrices $\\{P_a\\}$, a discount factor $\\gamma \\in (0, 1)$이 주어질 때, $\\pi (s) \\equiv a_1$에 의해 주어진 policy $\\pi$가 optimal인 것은 $\\equiv$ 모든 $a = a_2, … , a_k$에 대해서 reward $R$이 아래의 수식을 만족하는 것과 같습니다.</p>\n<p>$$(P_{a_1} - P_a)(I - \\gamma P_{a_1})^{-1} R \\succeq 0 \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (4)$$</p>\n<p><strong>Proof</strong>. $\\pi (s) \\equiv a_1$로 인하여, Equation (1)은 $V^\\pi = R + \\gamma P_{a_1} V^\\pi$이라고 쓸 수 있다. 따라서 아래의 수식으로 쓸 수 있습니다.</p>\n<p>$$V^\\pi = (I - \\gamma P_{a_1})^{-1} R$$</p>\n<p>위의 수식에서 주석처리가 되어있는 것을 보니 저자가 할 말이 더 있는 것 같습니다.</p>\n<p>$I - \\gamma P_{a_1}$(여기서 $I$는 단위행렬)은 항상 invertible(역으로 되는)합니다. 실제로 정말 invertible한 지를 보기 위해 transition matrix인 $P_{a_1}$가 복잡한 공간에 unit circle(단위원)에서 모든 eigenvalue들을 가진다는 것을 먼저 언급합니다. 다시 말해 $\\gamma &lt; 1$일 때, matrix $\\gamma P_{a_1}$가 unit circle 내에 모든 eigenvalues를 가진다는 것을 말합니다. (특히 여기서 1은 eigenvalue가 될 수 없습니다.) transition matrix의 특성상 이렇게 되는 것이기 때문에 기억하는 것이 좋습니다.</p>\n<p>뒤이어 위의 특성은 $I - \\gamma P_{a_1}$이 zero eigenvalue를 가지고 있지 않고, ($\\gamma$ 때문에 $I - \\gamma P_{a_1}$가 0~1 사이에 있게 됩니다.) 이것과 동치로 말할 수 있는 것이 singular가 아니라는 것을 의미합니다. 참고로 singular하다는 것은 해당되는 행렬이 역행렬이 존재하지 않는다는 것을 말합니다.</p>\n<p>정리하자면, $I - \\gamma P_{a_1}$ -&gt; zero eigenvalue가 없다. $\\equiv$ sigular하지 않다. $\\equiv$ 역행렬이 존재한다.</p>\n<p>추가적인 자료<br>1) <a href=\"https://en.wikipedia.org/wiki/Markov_chain#Stationary_distribution_relation_to_eigenvectors_and_simplices\" target=\"_blank\" rel=\"noopener\">Stationary distribution relation to eigenvectors and samplices in Markov chain</a><br>2) <a href=\"https://darkpgmr.tistory.com/106\" target=\"_blank\" rel=\"noopener\">특이값 분해(Singular Value Decomposition, SVD)의 활용)</a></p>\n<p>다시 Proof를 따라가봅니다. Equation (2)를 (3)으로 대체하면, $\\pi \\equiv a_1$가 optimal하는 것은 $\\equiv$ 아래의 수식과 같습니다.</p>\n<p>$$a_1 \\equiv \\pi (s) \\in arg\\max_{a \\in A} \\sum_{s’} P_{sa} (s’) V^\\pi (s’) \\,\\,\\,\\,\\, \\forall s \\in S$$ </p>\n<p>$$\\Leftrightarrow \\sum_{s’} P_{sa_1} (s’) V^\\pi (s’) \\geq \\sum_{s’} P_{sa} (s’) V^\\pi (s’) \\,\\,\\,\\,\\, \\forall s \\in S, a \\in A$$</p>\n<p>$$\\Leftrightarrow P_{a_1} V^\\pi \\succeq P_a V^\\pi \\,\\,\\,\\,\\, \\forall a \\in A \\setminus a_1$$</p>\n<p>$$\\Leftrightarrow P_{a_1} (I - \\gamma P_{a_1})^{-1} R \\succeq P_a (I - \\gamma P_{a_1})^{-1} R \\,\\,\\,\\,\\, \\forall a \\in A \\setminus a_1$$</p>\n<p>여기서 $a \\in A \\setminus a_1$이란 $a_1$을 제외한 set of actions $A$에 있는 $a$들을 말한다.</p>\n<p><strong>Remark</strong>. 매우 비슷한 argument를 사용하여 추가적인 언급을 합니다. $(P_{a_1} - P_a)(I - \\gamma P_{a_1})^{-1} R \\succ 0$라는 조건이 $\\pi \\equiv a_1$가 unique optimal policy가 되는 것에 필요하고, 충분하다고 볼 수 있습니다. (또한 추가적으로 위에 증명에 모든 inequalities를 strict inequalities로 대체함으로써)</p>\n<p>finite-state MDPs에 대해, 이러한 결과는 IRL의 solution인 모든 reward function들의 set을 정의합니다. 그러나 여기에는 두 가지 문제점이 있습니다.<br>1) $R = 0$ (and indeed any other constant vector)은 항상 solution입니다. 다시 말해 만약 우리가 어떠한 행동을 취했을 지라도 reward가 항상 같다면, $\\pi \\equiv a_1$을 포함하여 어떠한 policy들은 항상 optimal하다는 것입니다.  $\\pi$가 unique optimal policy라는 점에서는 이 문제를 완화시키지만, 전체적으로 만족시키진 않습니다. 왜냐하면 보통 0에 임의적으로 가까운 일부 reward vector들이 여전히 solution이 될 수 있기 때문입니다.<br>2) 대부분의 MDPs에서, criteria (4)를 만족시키는 R에 대한 많은 choice들이 있습니다.</p>\n<p>그렇다면 우리는 어떻게 많은 reward function들 중의 하나를 결정할 수 있을까요? 다음 section을 통해서 이러한 문제점들을 해결할 수 있는 natural criteria를 알아봅시다!</p>\n<h2 id=\"3-2-LP-Formulation-and-Penalty-Terms\"><a href=\"#3-2-LP-Formulation-and-Penalty-Terms\" class=\"headerlink\" title=\"3.2 LP Formulation and Penalty Terms\"></a>3.2 LP Formulation and Penalty Terms</h2><p>위의 질문에 대한 답변으로 명확하게 말하자면, Linear Programming(LP)은 Equation (4)로 인한 문제점들에 대해 실행 가능한 point로서 사용될 수 있습니다.</p>\n<p>그래서 R을 고를수 있는 한 가지 natural한 방법은 가장 먼저 $\\pi$를 optimal하도록 만드는 것입니다. 또한 $\\pi$로부터 어떠한 single-step deviation(편차)을 가능한 한 costly하게 만드는 것입니다. <strong>쉽게 말해 최적의 행동이 있다면 최적의 정책을 찾을 수 있고, 최적의 정책을 찾을 수 있다면 R을 고를 수 있다는 것입니다.</strong></p>\n<p>수식으로 표현해보면, (4)를 만족시키는 모든 function R 중에서, 다음의 수식을 maximize하도록 하는 $a_1$을 고를 수 있습니다.</p>\n<p>$$\\sum_{s \\in S} (Q^\\pi (s, a_1) - \\max_{a \\in A \\setminus a_1} Q^\\pi (s, a)) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (6)$$</p>\n<p>수식을 설명해보면 quality of the optimal action과 quality of the next-best action 사이의 differences의 sum을 maximize하는 것을 말합니다.</p>\n<p>추가적으로 다른 모든 Q값들이 동등할 때, 대부분의 small rewards에 대한 solution은 “simpler”하도록 optional하게 objective function이 $-\\lambda ||R||_1$ ($\\ell_1$-penalty)와 같은 weight decay-like penalty term을 추가할 것입니다. (여기서 $\\lambda$는 small rewards를 가지는 것과 (6)을 maximizing하는 것 두 목표 사이를 balancing할 수 있는 adjustable penalty coefficient입니다.)</p>\n<p>최대한 쉽게 풀어서 다시 설명해보겠습니다. 위의 수식 (6)으로 maximizing을 한다고 했을 때, Q값들에 대해서 보다 더 R를 simple하게 정하고, 수식 (6)을 더 maximizing이 잘 되도록 penalty term을 추가하는 것입니다. 쉽게 말해 우리가 Loss function에서 regularization term을 두는 것처럼 더 효과적이고 쉽게 사용하기 위해서 하는 작업이라고 생각하면 편합니다. $Q^\\pi (s, a_1)$과 Q값들 사이가 더 극명하도록 L1 regularization term을 두어서 R를 쉽게 정하자! 라는 것입니다.</p>\n<p>이렇게 함으로써 “simplest” R (largest penalty coefficient)을 찾을 수 있고, R은 왜 $\\pi$가 optimal한 지를 “explain”할 수 있습니다.</p>\n<p>정리하여 다시 optimization problem을 수식으로 말하자면 다음과 같습니다.<br><strong>First objective function &amp; algorithm</strong></p>\n<p>$$maximize \\,\\,\\,\\,\\, \\sum_{i=1}^N \\min_{a \\in {a_2, … , a_k}} \\{(P_{a_1} (i) - P_a (i))(I - \\gamma P_{a_1})^{-1} R\\} - \\lambda ||R||_1$$</p>\n<p>$$s.t. \\,\\,\\,\\,\\, (P_{a_1} - P_a)(I - \\gamma P_{a_1})^{-1} R \\succeq 0 \\,\\,\\,\\,\\, \\forall a \\in A \\setminus a_1$$</p>\n<p>$$|R_i| \\leq R_{max}, i = 1, … , N$$</p>\n<p>이러한 수식들을 통해 linear program으로 표현될 수 있고, 효과적으로 해결될 수 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"4-Linear-Function-Approximation-in-Large-State-Spaces\"><a href=\"#4-Linear-Function-Approximation-in-Large-State-Spaces\" class=\"headerlink\" title=\"4. Linear Function Approximation in Large State Spaces\"></a>4. Linear Function Approximation in Large State Spaces</h1><p>이번 section에서는 infinite state spaces의 case를 다룹니다. infinite-state MDPs는 section 2에서의 finite-state와 같은 방식으로 정의될 수 있습니다. 추가적으로 state는 $S = \\mathbb{R}^n$의 case에 대해서만 다루고자 합니다. 따라서 reward function R은 $S = \\mathbb{R}^n$로부터의 function이라고 할 수 있습니다.</p>\n<p>Calculus of variations는 infinite state spaces에서 optimizing하는 데에 있어 좋은 tool이지만, 종종 알고리즘적으로 어렵게 만듭니다. 따라서 reward function에 대해 <strong>“Linear Functional Approximation”</strong> 을 사용합니다. 수식으로 R을 표현하자면 다음과 같습니다.</p>\n<p>$$R(s) = \\alpha_1 \\phi_1 (s) + \\alpha_2 \\phi_2 (s) + \\cdots + \\alpha_d \\phi_d (s) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (8)$$</p>\n<p>여기서 $\\phi_1, … , \\phi_d$는 $S$로부터 mapping된 고정되어 있고, 우리가 알고 있고, bound되어 있는 basis function입니다. 그리고 $\\alpha_i s$는 우리가 fit해야하는 알고있지 않은 parameter입니다.</p>\n<p>다음으로 $V^\\pi$에 대해서도 linearity of expectation을 함으로써, reward function R이 Equation (8)로 주어질 때 value function을 다음과 같이 표현할 수 있습니다.</p>\n<p>$$V^\\pi = \\alpha_1 V_1^\\pi + \\cdots + \\alpha_d V_d^\\pi \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (9)$$</p>\n<p>위의 수식과 Theorem 2 (3번 수식)을 사용하면서, policy $\\pi (s) \\equiv a_1$를 optimal하도록 만드는 R에 대해서 (4)의 appropriate generalization은 다음의 조건입니다.</p>\n<center> <img src=\"../../../../img/irl/linear_irl_4.png\" width=\"370\"> </center>\n\n<p>, for all states $s$ and all actions $a \\in A \\setminus a_1$</p>\n<p>하지만 위의 formulation들에는 두 가지 문제가 있습니다.<br>1) infinite state spaces에서, Equation (10)의 형태에는 infinitely 많은 제약이 있습니다. infinite state spaces이기 때문에 모든 state를 check하기가 불가능하고 어렵습니다. 따라서 알고리즘적으로, states 중의 finite subset $S_0$만 sampling함으로써 이러한 문제를 피하고자 합니다.<br>2) R을 표현하기 위해 Equation (8)에서 linear function approximator를 사용한다고 제한했기 때문에, 어느 $\\pi$가 optimal한 지에 대해 더 이상 어떠한 reward function도 표현할 수 없습니다. 그럼에도 불구하고, linear function approximator를 사용할 것입니다.</p>\n<p>최종적으로 linear programming formulation은 다음과 같습니다.<br><strong>Second objective function &amp; algorithm</strong></p>\n<center> <img src=\"../../../../img/irl/linear_irl_5.png\" width=\"460\"> </center>\n\n<p>$$s.t. \\,\\,\\, |\\alpha_i| \\leq 1, \\,\\,\\, i = 1, … , d$$</p>\n<p><br><br></p>\n<h1 id=\"5-IRL-from-Sampled-Trajectories\"><a href=\"#5-IRL-from-Sampled-Trajectories\" class=\"headerlink\" title=\"5. IRL from Sampled Trajectories\"></a>5. IRL from Sampled Trajectories</h1><p>이번 section에서는 오직 state space에서의 actual trajectories의 set을 통해서만 policy $\\pi$를 접근하는 좀 더 realistic case에 대해서 IRL 문제를 다룹니다. 그래서 MDP의 explicit model을 필요로 하지 않습니다.</p>\n<p>initial state distribution $D$를 고정하고, (unknown) policy $\\pi$에 대해 우리의 목표는 $\\pi$가 $\\mathbb{E}_{s_0 \\sim D} [V^\\pi (s_0)]$를 maximize하는 R를 찾는 것입니다. (기억합시다!) 추가적으로 notation을 단순화하기 위해 고정된 start state $s_0$를 가정합니다.</p>\n<p>먼저 $\\alpha_i$의 setting을 통해 $V^\\pi (s_0)$를 estimating하는 방법이 필요합니다. 이것을 하기 위해, 첫 번째로 $m$ Monte Carlo trajectories를 만들어냅니다. 그리고 나서 $i = 1, … , d$에 대해 만약 reward가 $R = \\phi_i$라면, $V_i^\\pi (s_0)$를 얼마나 average empirical return이 $m$ trajectories에 있었는 지로 정의합니다.</p>\n<p>예를 들어, 만약 $m = 1$ trajectories이고, 이 trajectory가 states ($s_0, s_1, …$)의 sequence라면, 다음과 같이 나타낼 수 있습니다.</p>\n<p>$$\\hat{V}_i^\\pi (s_0) = \\phi_i (s_0) + \\gamma \\phi_i (s_1) + \\gamma^2 \\phi_i (s_2) + \\cdots$$</p>\n<p>일반적으로, $\\hat{V}_i^\\pi (s_0)$은 어떠한 $m$ trajectories의 empirical returns에 대하여 average합니다. (여기서 말하는 어떠한 $m$ trajectories는 임의의 finite number에 의해 truncate된 trajectories를 말합니다.) 그리고 그 때 $\\alpha_i$의 어떠한 setting에 대해서, $V^\\pi (s_0)$의 natural estimate는 다음과 같습니다.</p>\n<p>$$\\hat{V}^\\pi (s_0) = \\alpha_1 \\hat{V}_1^\\pi (s_0) + \\cdots + \\alpha_d \\hat{V}_d^\\pi (s_0) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (11)$$</p>\n<p>위의 수식의 “inductive(귀납적인) step”은 다음을 뒤따릅니다 : set of policies ${ \\pi_1, … , \\pi_k }$이 있고, resulting reward function은 아래의 수식을 만족하기 때문에 $\\alpha_i$의 setting을 찾을 수 있습니다.</p>\n<p>$$V^{\\pi^*} (s_0) \\geq V^{\\pi_i} (s_0), \\,\\,\\, i = 1, … , k \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (12)$$</p>\n<p>그리고 section 4에서 마지막에 있던 수식에서, objective function을 약간 바꿀 수 있습니다. 따라서 optimization의 식은 다음과 같이 될 수 있다.<br><strong>Final objective function &amp; algorithm</strong></p>\n<p>$$maximize \\,\\,\\,\\,\\, \\sum_{i=1}^k p(\\hat{V}^{\\pi^*} (s_0) - \\hat{V}^{\\pi_i} (s_0))$$</p>\n<p>$$s.t. \\,\\,\\,\\,\\, |\\alpha_i| \\leq 1, \\,\\,\\, i = 1, … , d$$</p>\n<p>위의 수식에서 $\\hat{V}^{\\pi_i} (s_0)$과 $\\hat{V}^{\\pi^*} (s_0)$은 Equation (11)에서 주어진 $\\alpha_i$의 (implicit) linear function이다.</p>\n<p>그러므로, 위의 $\\hat{V}^{\\pi_i} (s_0)$과 $\\hat{V}^{\\pi^*} (s_0)$은 쉽게 linear programming으로 해결할 수 있습니다.</p>\n<p>위의 optimization 식은 $\\alpha_i$의 새로운 setting으로 설정할 수 있고, 그러므로 새로운 reward function $R = \\alpha_1 \\phi_1 + \\cdots + \\alpha_d \\phi_d$을 가질 수 있습니다.</p>\n<p>그리고 그 때 $R$로 인한 $V^\\pi (s_0)$를 maximize하는 policy $\\pi_{k+1}$을 찾을 수 있고, $\\pi_{k+1}$을 current set of policies에 추가할 수 있습니다. 그리고 이것을 계속할 수 있습니다. (많은 수의 iteration을 통해 우리가 “satisfied”하는 $R$를 찾을 수 있습니다.)</p>\n<p><br><br></p>\n<h1 id=\"6-Experiments\"><a href=\"#6-Experiments\" class=\"headerlink\" title=\"6. Experiments\"></a>6. Experiments</h1><p><br></p>\n<h2 id=\"6-1-First-experiment-5-x-5-grid-world\"><a href=\"#6-1-First-experiment-5-x-5-grid-world\" class=\"headerlink\" title=\"6.1 First experiment : 5 x 5 grid world\"></a>6.1 First experiment : 5 x 5 grid world</h2><center> <img src=\"../../../../img/irl/linear_irl_6.png\" width=\"500\"> </center>\n\n<p>첫 번째 실험은 agent가 왼쪽 아래에서 시작하여 오른쪽 위로 가면 보상 1을 받는 5x5 grid world를 사용했습니다. action은 상하좌우지만 noise가 있으며 30%의 확률로 랜덤하게 움직입니다.</p>\n<p>위에 있는 Figure 1은 5x5 grid world에서의 optimal policy와 true reward function을 나타낸 것입니다.</p>\n<center> <img src=\"../../../../img/irl/linear_irl_7.png\" width=\"500\"> </center>\n\n<p>앞에서 설명한 penalty term인 $\\lambda$를 주지 않은 Section 3.2의 discrete/finite state problem에서 사용할 수 있는 algorithm을 쓰면 Figure 2 윗 부분의 그림과 같은 울퉁불퉁한 reward function의 모습을 볼 수 있습니다. 위에서 말했던 section 3.2에 있는 algorithm은 다음과 같습니다.</p>\n<p>$$maximize \\,\\,\\,\\,\\, \\sum_{i=1}^N \\min_{a \\in {a_2, … , a_k}} \\{(P_{a_1} (i) - P_a (i))(I - \\gamma P_{a_1})^{-1} R\\} - \\lambda ||R||_1$$</p>\n<p>$$s.t. \\,\\,\\,\\,\\, (P_{a_1} - P_a)(I - \\gamma P_{a_1})^{-1} R \\succeq 0 \\,\\,\\,\\,\\, \\forall a \\in A \\setminus a_1$$</p>\n<p>$$|R_i| \\leq R_{max}, i = 1, … , N$$</p>\n<p>그러나 Figure 2 아래에 쓰여져 있는 $\\lambda$ = 1.05 로 설정을 하면 Figure 2 아래에 있는 그림과 같은 true reward에 밀접한 reward function을 얻을 수 있습니다.</p>\n<p><br></p>\n<h2 id=\"6-2-Second-experiment-Mountain-car\"><a href=\"#6-2-Second-experiment-Mountain-car\" class=\"headerlink\" title=\"6.2 Second experiment : Mountain-car\"></a>6.2 Second experiment : Mountain-car</h2><center> <img src=\"../../../../img/irl/linear_irl_8.png\" width=\"500\"> </center>\n\n<p>다음 실험으로는 보통 잘 알려져있는 mountain-car 환경입니다. ture, undiscounted reward는 언덕에 있는 goal지점에 도달하기 전까지 step마다 -1을 받는 것입니다. 그리고 state는 자동차의 x-위치와 속도입니다.</p>\n<center> <img src=\"../../../../img/irl/linear_irl_5.png\" width=\"460\"> </center>\n\n<p>$$s.t. \\,\\,\\, |\\alpha_i| \\leq 1, \\,\\,\\, i = 1, … , d$$</p>\n<p>state가 연속적이기 때문에 위의 수식과 같이 section 4의 continuous/finite state problem에서 사용할 수 있는 algorithm을 사용했습니다. 또한 reward에 대해서 오직 자동차의 x-위치에 대한 functions가 되도록 Gaussian 모양으로 된 26개의 basis functions의 linear combinations을 가지는 function approximator class를 구성하였습니다.</p>\n<center> <img src=\"../../../../img/irl/linear_irl_9.png\" width=\"500\"> </center>\n\n<p>algorithm으로 optimal policy를 고려하여 전형적인 reward function은 Figure 4의 윗 부분에 있는 그래프로 나타납니다. (Note the scale on the $y$ axis.) 명확하게도, solution은 reward의 $R = -c$ structure를 거의 완벽하게 나타내었습니다.</p>\n<p>좀 더 challenging한 문제로서, 이 논문에서는 언덕 아래 주변을 중심으로 [-0.72, -0.32] 사이에 있으면 reward 1 아니면 0을 주도록 changed ture reward에 대한 실험을 동일하게 하였습니다. 그리고 여기서 $\\gamma$는 0.99를 주었다고 합니다.</p>\n<p>이 문제에서의 optimal policy는 가능한 한 빨리 언덕 아래로 가서 주차를 하는 것입니다. 새로운 문제에 대해 알고리즘을 적용해봤을 때 전형적인 solution은 Figure 4 아래에 있는 그래프로 나타낼 수 있습니다.</p>\n<p>대체로 reward의 중요한 structure를 정해진 [-0.72, -0.32]에 대해 성공적으로 찾았다고 합니다. 또한 오른쪽에는 artifact가 있어 오른쪽 끝을 피할 수 없도록 “shooting out”하는 효과가 있다고 합니다. 그럼에도 불구하고, solution이 이 문제에 꽤 좋았다고 합니다.</p>\n<p><br></p>\n<h2 id=\"6-3-Final-experiment-Continuous-version-of-the-5-x-5-grid-world\"><a href=\"#6-3-Final-experiment-Continuous-version-of-the-5-x-5-grid-world\" class=\"headerlink\" title=\"6.3 Final experiment : Continuous version of the 5 x 5 grid world\"></a>6.3 Final experiment : Continuous version of the 5 x 5 grid world</h2><p>마지막 실험은 5x5 grid world의 continuous version에서 적용했습니다.</p>\n<p>state는 [0, 1] x [0, 1]이고, action은 상하좌우 방향으로 0.2만큼 움직입니다. 또한 [-0.1, 0.1]의 uniform noise가 각각 좌표에 추가되고, 만약 unit square 내에 uniform noise가 필요한 경우, state는 truncate됩니다.</p>\n<p>reward는 [0.8, 1] x [0.8, 1]에서는 1을 받고 나머지는 0을 받습니다. 그리고 $\\gamma$는 0.9를 사용했습니다. 또한 function approximator class는 2-dimensional Gaussian basis functions에 대한 15x15 array의 linear combinations로 구성하였습니다.</p>\n<p>initial state distribution $D$는 state space에 대해  uniform하였고, algorithm은 policy를 평가하기 위해 각각의 30 steps 마다 $m=5000$ trajectories를 사용하여 적용하였습니다.</p>\n<p>$$maximize \\,\\,\\,\\,\\, \\sum_{i=1}^k p(\\hat{V}^{\\pi^*} (s_0) - \\hat{V}^{\\pi_i} (s_0))$$</p>\n<p>$$s.t. \\,\\,\\,\\,\\, |\\alpha_i| \\leq 1, \\,\\,\\, i = 1, … , d$$</p>\n<p>위에서 다뤘던 section 5에서의 algorithm을 사용하여 찾아진 solution은 단지 1 iteration만 했는데도 꽤나 reasonable했습니다. 그래서 약 15 iterations에 대해서도 해본 결과, algorithm은 동일하게 좋은 performance로 해결했습니다.</p>\n<center> <img src=\"../../../../img/irl/linear_irl_10.png\" width=\"500\"> </center>\n\n<p>또한 action choice가 다른 state sapce의 부분을 계산하면서 true optimal policy와 fitted reward’s optimal policy를 비교하였고 (Figure 5, 위), 대체적으로 3% ~ 10% 사이로 불일치했습니다. 아마도 algorithm’s performance의 좀 더 적절한 측정은 true optimal policy의 quality와 fitted reward’s optimal policy의 quality를 비교하는 것입니다. (Quality는 물론 true reward function을 사용하여 측정됩니다.)</p>\n<p>결과적으로 algorithm의 약 15 iterations에 대해서, evaluations은 (which used 50000 Monte Carlo trials of 50 steps each) true “optimal policy”의 value와 fitted reward’s optimal policy사이에 statistically한 중요한 차이를 detect할 수 없다는 것을 볼 수 있었습니다. 그만큼 true optimal policy와 reward’s optimal policy의 차이가 없다는 것을 뜻합니다. (Figure 5, 아래)</p>\n<p><br><br></p>\n<h1 id=\"7-Conclusions-and-Future-work\"><a href=\"#7-Conclusions-and-Future-work\" class=\"headerlink\" title=\"7. Conclusions and Future work\"></a>7. Conclusions and Future work</h1><p>이 논문은 moderate-sized discrete and continuous domain에서 Inverse Reinforcement Learning 문제가 해결될 수 있다는 것을 보였습니다.</p>\n<p>하지만 많은 open question들이 아래와 같이 남아있습니다.<br>1) Potential-based shaping rewards는 MDP에서 학습시키기 위한 하나의 solution으로서 reward function을 더 쉽게 만들 수 있습니다. 그렇다면 우리는 더 “easy” reward function을 만들기 위한 IRL 알고리즘들을 만들 수 있을까요?<br>2) IRL를 real-world empirical application측면에서 보면, sensor inputs and actions에 대해서 observer의 측정에 상당한 noise가 있을지도 모릅니다. 여기에 더하여 많은 optimal policy들이 존재할 지도 모릅니다. 어떠한 data를 noise없이 fit하도록 하는 적절한 metric은 무엇일까요?<br>3) 만약 행동이 절대로 optimality와 일치하지 않는다면, state space에 specific region에 대한 “locally consistent” reward function을 어떻게 알 수 있을까요?<br>4) 어떻게 reward function의 identifiability를 maximize하기 위한 실험을 고안해낼 수 있을까요?<br>5) 이 논문에 적힌 알고리즘적인 접근이 partially observable environment의 case를 얼마나 잘 실행할 수 있을까요?</p>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"Let’s-do-Inverse-RL-Guide\"><a href=\"#Let’s-do-Inverse-RL-Guide\" class=\"headerlink\" title=\"Let’s do Inverse RL Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2019/01/22/0_lets-do-irl-guide/\">Let’s do Inverse RL Guide</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"APP-여행하기\"><a href=\"#APP-여행하기\" class=\"headerlink\" title=\"APP 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2019/02/01/2_app/\">APP 여행하기</a></h2>","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"../../../../img/irl/linear_irl_1.png\" width=\"850\"> </center>\n\n<p>Author : Andrew Y. Ng, Stuart Russell<br>Paper Link : <a href=\"http://ai.stanford.edu/~ang/papers/icml00-irl.pdf\" target=\"_blank\" rel=\"noopener\">http://ai.stanford.edu/~ang/papers/icml00-irl.pdf</a><br>Proceeding : International Conference on Machine Learning (ICML) 2000</p>\n<hr>\n<h1 id=\"0-Abstract\"><a href=\"#0-Abstract\" class=\"headerlink\" title=\"0. Abstract\"></a>0. Abstract</h1><p>이 논문은 Markov Decision Processes에서의 <strong>Inverse Reinforcement Learning(IRL)</strong>을 다룹니다. 여기서 IRL이란, observed, optimal behavior이 주어질 때 reward function을 찾는 것입니다.</p>\n<p>IRL은 두 가지 장점이 존재합니다.<br>1) 숙련된 행동을 얻기 위한 apprenticeship Learning<br>2) 최적화된 reward function을 알아내는 것</p>\n<p>이 논문에서는 IRL의 reward function에 대해 세 가지 알고리즘을 제시합니다.<br>1) 첫 번째와 두 번째는 전체의 policy를 알고 있는 case를 다루는 것입니다. finite state space에서의 tabulated reward function과 potentially infinite state space에 대한 reward function의 linear functional approximation을 다룹니다.<br>2) 세 번째로는 observed trajectories의 finite set을 통해서만 policy를 알고 있을 때 더 다양한 realistic case에 대한 알고리즘입니다.</p>\n<p>이 논문은 2000년에 나온 논문으로서 그 당시에 어떻게 하면 reward function을 역으로 찾을 수 있는지를 증명을 통해서 다루고 있고, 왜 reward function이 중요한 지를 중요하게 말하고 있습니다. 그러니까 IRL을 통해 reward를 얻어 RL을 하는 실질적인 학습을 말하는 논문보다는 reward function에 대해서 말하고 있고 이에 따른 알고리즘들을 말하는 논문입니다. “reward function은 이렇게 생겨야 돼! 그리고 우리는 이러한 알고리즘들을 통해 reward function을 찾아낼 수 있어!”라는 듯이 말하고 있습니다.</p>\n<p>위에서 말한 세 가지 알고리즘의 중요한 문제는 degeneracy입니다. 여기서 degeneracy란, 어느 observed policy가 optimal한 지에 대하여 reward function의 large set이 존재하는 지에 관한 degeneracy를 말합니다. 다시 말해 이러한 reward function의 large set을 찾기 위한 알고리즘이 존재하는 지, degeneracy는 없는 지를 말하는 것입니다. 이 논문에서는 degeneracy를 없애기 위해서 natural heuristics를 제시합니다. 그리고 natural heuristics를 통해 IRL에 대해서 효과적으로 해결가능한 <strong>linear programming</strong> formulation을 나타냅니다.</p>\n<p>추가적으로 이러한 문제를 하나의 용어로 말하면 <strong>“ill-posed problem”</strong> 이라고 합니다. ill-posed problem을 검색하면 위키피디아에 있는 well-posed problem이 먼저 나오는데 여기서 말하는 well-posed problem이란 특정한 solution이 존재하고, 그 solution이 unique하다고 나와있습니다. 반대로, 역강화학습에서는 reward가 정해진 것이 아니라 여러가지 형태의 값으로 나타날 수 있기 때문에(정해진 값이 아니기 때문에, not unique) ill-posed problem이라고 볼 수 있습니다.</p>\n<p>실험에서는 이 논문에서의 알고리즘들을 통해 간단한 discrete/finite and continuous/infinite state problem들을 해결합니다.</p>\n<p><br><br></p>\n<h1 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h1><p>IRL은 Russell이 쓴 논문에서 비공식적으로 정의합니다.</p>\n<p>Given : 1) 시간에 따른 agent의 행동들, 2) 필요하다면, agent의 sensory input들, 3) 가능하다면, 환경의 모델  이 주어질 때<br>Determine : reward function을 최적화하는 것</p>\n<p>뒤이어 나오는 두 가지 언급을 통해서 IRL이 왜 중요한 지를 알아봅시다.</p>\n<p>첫 번째로 animal and human learning에 대한 computational model로서 RL의 잠재적인 사용입니다. 예를 들어 벌이 먹이를 찾는 모델이 있다고 했을 때 여러 꽃들 사이에서의 reward가 꿀에 대한 간단한 saturating function이라고 가정해봅시다. reward function은 보통 고정되어 있고 + 우리가 정하고 + 이미 알고 있습니다. 하지만 animal and human behavior을 조사할 때 우리는 추가적으로 알지 못하는 reward function까지도 생각해야합니다. 다시 말해 reward function의 multiattribute(다속성)도 생각을 해야한다는 것입니다. 벌이 바람과 벌을 먹이로 하는 포식자들로 부터 비행 거리, 시간, 위험 요소들을 고려하여 꿀 섭취를 할 수도 있습니다. 사람의 경제적 행동 속에서도 이러한 경우가 많습니다. 따라서 IRL은 이론적으로 생물학, 경제학, 또는 다른 분야들까지 포함하는 근본적인 문제에서 나타낼 수 있습니다.</p>\n<p>두 번째로는 특정한 도메인에서 잘 행동할 수 있는 intelligent agent를 구성할 수 있다는 것입니다. 보통 agent designer들은 그들이 정하는 reward function의 optimization이 “desirable” 행동을 만들 것이라는 굉장히 rough한 생각을 가질 수 있습니다. 그렇지만 아주 간단한 RL 문제라도 이것은 agent designer들을 괴롭힐 수 있습니다. 여기서 한 가지 사용할 수 있는 것이 바로 다른 “expert” agent의 행동입니다.</p>\n<p>가장 넓은 set인 <strong>“Imitation Learning”</strong> 안에는 일반적으로 expert agent를 통한 학습이 두 가지 방법이 있습니다.<br>1) 첫 번째로는 <strong>“IRL”</strong> 이라는 것이 있고, IRL을 이용한 하나의 방법으로서 Pieter Abbeel이 주로 쓰는 알고리즘의 이름인 Apprenticeship Learning이라는 것이 존재합니다. 그래서 IRL이란 teacher의 demonstation에서의 optimal policy를 통해 reward를 찾는 것을 말합니다.<br>2) 다음으로 <strong>“Behavioral Cloning”</strong> 이라는 것이 있습니다. 아예 supervised learning처럼 행동을 복제한다고 생각하면 됩니다.</p>\n<p>일반적으로 IRL을 통해서 cost를 얻고, RL을 통해서 policy를 찾는 것이 모든 IRL의 목적입니다. 하지만 이렇게 2 step을 해야한다는 점에서 많은 complexity가 생기고, 그렇다고 IRL을 통해서 얻은 cost만 가지고는 별로 할 수 있는게 없습니다. 따라서 앞서 말한 과정을 2 step만에 하는 것이 아니라 1 step만에 풀어버리는 논문이 바로 Generative Adversarial Imitation Learning(GAIL)이라는 논문입니다. 이후의 논문들은 다 GAIL을 응용한 것이기 때문에 GAIL만 이해하면 그 뒤로는 필요할 때 찾아서 보면 될 것 같고, GAIL이라는 것을 이용하여 여러 가지 연구를 해볼 수도 있을 것 같습니다.</p>\n<p>이 논문에 나오는 section들은 다음과 같습니다.</p>\n<ul>\n<li>Section 2 : finite Markov Decision Processes(MDPs)의 formal definition들과 IRL의 문제를 다룹니다.</li>\n<li>Section 3 : finite state spaces에서 주어진 policy 중의 어느 policy가 optimal한 지에 대해 모든 reward function의 set을 다룹니다.</li>\n<li>Section 4 : reward function의 explicit, tabular representation이 가능하지 않을 수 있기 때문에 large or infinite state spaces의 case를 다룹니다.</li>\n<li>Section 5 : observed trajectories의 finite set을 통해서만 policy를 안다고 했을 때, 더 realistic case에 대해서 다룹니다.</li>\n<li>Section 6 : 앞서 언급했던 세 가지 알고리즘을 적용하여 discrete and continuous stochastic navigation problems와 mountain-car problem에 대한 실험부분이 나옵니다.</li>\n<li>Section 7 : 결론과 더 나아가 연구되어야할 방향에 대해서 다룹니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"2-Notation-and-Problem-Formulation\"><a href=\"#2-Notation-and-Problem-Formulation\" class=\"headerlink\" title=\"2. Notation and Problem Formulation\"></a>2. Notation and Problem Formulation</h1><p>먼저 IRL version의 notation, definitions, basic theorems for Markov decision processes(MDPs)에 대해서 알아봅시다.</p>\n<p><br></p>\n<h2 id=\"2-1-Markov-Decision-Processes\"><a href=\"#2-1-Markov-Decision-Processes\" class=\"headerlink\" title=\"2.1 Markov Decision Processes\"></a>2.1 Markov Decision Processes</h2><p>A finite MDP is a tuple $(S, A, \\{P_{sa}\\}, \\gamma, R)$</p>\n<p>$S$ is a finite set on $N$ states.</p>\n<p>$A = \\{a_1, … , a_k\\}$ is a set of $k$ actions.</p>\n<p>$P_{sa} (\\cdot)$ are the state transition probabilities upon taking action $a$ in state $s$.</p>\n<p>$\\gamma \\in [0,1)$ is the discount factor.</p>\n<p>$R : S \\rightarrow \\mathbb{R}$ is the reinforcement function (reward function) bounded absolute value by $R_{max}$.</p>\n<p>$R$에 대해 간단하게 말하기 위해서 $R(s,a)$보다 $R(s)$로서 reward $R$을 정의했습니다. $R$같은 경우, 사실 정의하기 나름인 것 같습니다. 이 논문에서는 $R(s,a)$로 생각하는 것보다 $R(s)$로 생각하는 것이 편하고 간단하기 때문에 이렇게 notation을 적은 것 같습니다. 아무래도 역강화학습에서 action $a$까지 생각해주면 추가적인 notation이 더 나오기 때문에 복잡해집니다. 또한 사실 우리가 최적의 행동만 안다면 보상을 행동까지 생각해줄 필요는 없기 때문에 $R(s)$라고 적었다고 봐도 될 것 같습니다. 역강화학습 논문들이 다 $R(s)$을 사용하는 것은 아닙니다. 논문마다 case by case로 쓸 수 있는 것 같습니다.</p>\n<p>Policy is defined as any map $\\pi : S \\rightarrow A$.</p>\n<p>Value function evaluated at any state $s_1$ for a policy $\\pi$ is given by</p>\n<p>$$V^\\pi (s_1) = \\mathbb{E} [R(s_1) + \\gamma R(s_2) + \\gamma^2 R(s_3) + \\cdots | \\pi]$$</p>\n<p>Q-function is</p>\n<p>$$Q^\\pi (s, a) = R(s) + \\gamma \\mathbb{E}_{s’ \\sim P} [V^\\pi (s’)]$$</p>\n<ul>\n<li>여기서 notation $s’ \\sim P$는 $s’$를 $P_{sa}$에 따라 sampling한 것입니다.</li>\n</ul>\n<p>Optimal value function is $V^* (s) = sup_\\pi V^\\pi (s)$. (sup is supremum, 상한, 최소 상계)</p>\n<p>Optimal Q-function is $Q^* (s,a) = sup_\\pi Q^\\pi (s,a)$.</p>\n<p>위의 모든 function들은 discrete, finite spaces에서 정의합니다.</p>\n<p>마지막으로 Symbol $\\prec$ and $\\preceq$ denote strict and non-strict vectorial inequality - i.e., $x \\, \\prec \\, y$ if and only if $\\, \\forall i \\, \\, x_i &lt; y_i$</p>\n<p><br></p>\n<h2 id=\"2-2-Basic-Properties-of-MDPs\"><a href=\"#2-2-Basic-Properties-of-MDPs\" class=\"headerlink\" title=\"2.2 Basic Properties of MDPs\"></a>2.2 Basic Properties of MDPs</h2><p>IRL 문제의 solution에 대해 MDPs와 관련된 두 가지 classical result가 필요합니다.</p>\n<p>Theorem 1 (Bellman Equations) : MDP $M = (S, A, \\{P_{sa}\\}, \\gamma, R)$ and policy $\\pi : S \\rightarrow A$가 주어질 때, 모든 $s \\in S$, $a \\in A$에 대해서 $V^\\pi$ 와 $Q^\\pi$는 다음을 만족합니다.</p>\n<p>$$V^\\pi (s) = R(s) + \\gamma \\sum_{s’} P_{s\\pi(s)} (s’) \\, V^\\pi (s’) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (1)$$</p>\n<p>$$Q^\\pi (s,a) = R(s) + \\gamma \\sum_{s’} P_{sa} (s’) \\, V^\\pi (s’) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (2)$$</p>\n<p>Theorem 2 (Bellman Optimality) : MDP $M = (S, A, \\{P_{sa}\\}, \\gamma, R)$ and policy $\\pi : S \\rightarrow A$가 주어질 때, $\\pi$는 $M$에 대해 optimal policy라는 것은 $\\equiv$ 모든 $s \\in S$에 대해 다음과 수식과 같습니다.</p>\n<p>$$\\pi (s) \\in arg\\max_{a \\in A} Q^\\pi (s,a) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (3)$$</p>\n<p>Theorem 1, 2의 증명은 <a href=\"http://incompleteideas.net/book/RLbook2018.pdf\" target=\"_blank\" rel=\"noopener\">서튼책</a>에 있고, 여기서는 추가적으로 다루지 않겠습니다.</p>\n<p><br></p>\n<h2 id=\"2-3-Inverse-Reinforcement-Learning\"><a href=\"#2-3-Inverse-Reinforcement-Learning\" class=\"headerlink\" title=\"2.3 Inverse Reinforcement Learning\"></a>2.3 Inverse Reinforcement Learning</h2><p>IRL의 문제는 observed 행동을 설명할 수 있는 reward function을 찾는 것입니다. 그래서 가장 먼저 무엇을 하고 싶은 것이냐면, state space가 finite이고, model은 이미 알고 있고, complete policy가 observe된 simple case부터 시작하고자 합니다. 다시 말해 $\\pi$가 optimal policy일 때, 가능한 한 reward function의 set을 찾고자 하는 것입니다.</p>\n<p>추가적으로 action들을 renaming함으로써 $\\pi (s) \\equiv a_1$ (여기서 $a_1$는 임의의 하나의 행동)라는 것을 가정할 것입니다. 이 trick은 notation을 단순화하기 위해 사용될 것입니다.</p>\n<p><br><br></p>\n<h1 id=\"3-IRL-in-Finite-State-Spaces\"><a href=\"#3-IRL-in-Finite-State-Spaces\" class=\"headerlink\" title=\"3. IRL in Finite State Spaces\"></a>3. IRL in Finite State Spaces</h1><p>이번 section에서는 주어진 policy 중의 어느 policy가 optimal한 지에 대해서 모든 reward functions의 set에 대한 간단한 정의를 할 것입니다. 그리고 모든 reward functions의 set에 degeneracy를 없애기 위해서 간단한 heuristic 방법인 <strong>“Linear Programming”</strong> (<a href=\"https://en.wikipedia.org/wiki/Linear_programming\" target=\"_blank\" rel=\"noopener\">Wikipedia</a>, <a href=\"https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%EA%B3%84%ED%9A%8D%EB%B2%95\" target=\"_blank\" rel=\"noopener\">위키백과</a>)을 제안합니다.</p>\n<p>Linear Programming에 대해서 간략하게만 알아봅시다. 대부분의 사람들은 Linear Programming(선형 계획법)보다 더 친숙한 Dynamic Programming(동적 계획법)을 알고 있는 경우가 많습니다. 여기서 “Dynamic”이란 동적, 즉 시간에 따라 변하며 다단계적인 특성을 말합니다. 그리고 “Programming”이란 컴퓨터 프로그래밍이 아니라 무언가를 계획하여 최적의 program을 찾는 방법을 말합니다. 한마디로 “시간에 따라서 큰 문제들 안에 작은 문제들이 중첩된 경우에 전체 큰 문제를 작은 문제로 쪼개서 최적의(optimal) program을 찾는 방법으로 풀겠다” 라는 것입니다.</p>\n<p>이와 비슷하게 Linear Programming, 선형 계획법도 Programming(계획법)이 들어갑니다. 그러니까 선형 계획법 또한 무언가를 계획하는 것으로서 “최적의 program을 찾는 방법으로 풀겠다.”라는 것인데, 앞에 Linear만 추가적으로 붙었습니다. 정리하자면, <strong>“최적화 문제 일종으로 주어진 선형 조건들을 만족시키면서 선형인 목적 함수를 최적화하여 풀겠다.”</strong> 라는 것입니다. 이 Linear Programming(LP)은 주로 Operations Research(OR)에서 가장 일반적인 기법으로 뽑힙니다.</p>\n<p>관련하여 예를 한번 들어보겠습니다. 아래의 예는 위키백과에 있는 예입니다. 홍길동 씨가 두 가지 종류의 빵을 판매하는데, 초코빵을 만들기 위해서는 밀가루 100g과 초콜릿 10g이 필요하고, 밀빵을 만들기 위해서는 밀가루 50g이 필요합니다. 재료비를 제하고 초코빵을 팔면 100원이 남고, 밀빵을 팔면 40원이 남습니다. 오늘 홍길동 씨는 밀가루 3000g과 초콜릿 100g을 재료로 갖고 있습니다. 만든 빵을 전부 팔 수 있고 더 이상 재료 공급을 받지 않는다고 가정한다면, 홍길동 씨는 이익을 극대화 하기 위해서 어떤 종류의 빵을 얼마나 만들어야 할까요? 선형 계획법을 통해서 알아봅니다!</p>\n<center> <img src=\"../../../../img/irl/linear_irl_2.png\" width=\"350\"> </center>\n\n<p>여기서 $x_1$은 초코빵을 $x_2$는 밀빵의 개수를 의미하는 변수입니다. 그림으로 나타내면 아래와 같이 가장 많은 이익을 남기는 방법은 초코빵 10개와 밀빵 40개를 만드는 것이고, 그렇게 해서 얻을 수 있는 최대 이익은 2600원입니다.</p>\n<center> <img src=\"../../../../img/irl/linear_irl_3.png\" width=\"600\"> </center>\n\n<p>이익이 최대가 될 때는 이익을 나타내는 직선이 해가 존재할 수 있는 영역 중 원점에서 가장 떨어진 점 (10, 40)에 접할 때입니다. $100 x_1 + 40 x_2 = 2600$</p>\n<p><br></p>\n<h2 id=\"3-1-Characterization-of-the-Solution-Set\"><a href=\"#3-1-Characterization-of-the-Solution-Set\" class=\"headerlink\" title=\"3.1 Characterization of the Solution Set\"></a>3.1 Characterization of the Solution Set</h2><p><strong>Theorem 3</strong> : Let a finite state space $S$, a set of actions $A = {a_1, … , a_k}$, transition probability matrices $\\{P_a\\}$, a discount factor $\\gamma \\in (0, 1)$이 주어질 때, $\\pi (s) \\equiv a_1$에 의해 주어진 policy $\\pi$가 optimal인 것은 $\\equiv$ 모든 $a = a_2, … , a_k$에 대해서 reward $R$이 아래의 수식을 만족하는 것과 같습니다.</p>\n<p>$$(P_{a_1} - P_a)(I - \\gamma P_{a_1})^{-1} R \\succeq 0 \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (4)$$</p>\n<p><strong>Proof</strong>. $\\pi (s) \\equiv a_1$로 인하여, Equation (1)은 $V^\\pi = R + \\gamma P_{a_1} V^\\pi$이라고 쓸 수 있다. 따라서 아래의 수식으로 쓸 수 있습니다.</p>\n<p>$$V^\\pi = (I - \\gamma P_{a_1})^{-1} R$$</p>\n<p>위의 수식에서 주석처리가 되어있는 것을 보니 저자가 할 말이 더 있는 것 같습니다.</p>\n<p>$I - \\gamma P_{a_1}$(여기서 $I$는 단위행렬)은 항상 invertible(역으로 되는)합니다. 실제로 정말 invertible한 지를 보기 위해 transition matrix인 $P_{a_1}$가 복잡한 공간에 unit circle(단위원)에서 모든 eigenvalue들을 가진다는 것을 먼저 언급합니다. 다시 말해 $\\gamma &lt; 1$일 때, matrix $\\gamma P_{a_1}$가 unit circle 내에 모든 eigenvalues를 가진다는 것을 말합니다. (특히 여기서 1은 eigenvalue가 될 수 없습니다.) transition matrix의 특성상 이렇게 되는 것이기 때문에 기억하는 것이 좋습니다.</p>\n<p>뒤이어 위의 특성은 $I - \\gamma P_{a_1}$이 zero eigenvalue를 가지고 있지 않고, ($\\gamma$ 때문에 $I - \\gamma P_{a_1}$가 0~1 사이에 있게 됩니다.) 이것과 동치로 말할 수 있는 것이 singular가 아니라는 것을 의미합니다. 참고로 singular하다는 것은 해당되는 행렬이 역행렬이 존재하지 않는다는 것을 말합니다.</p>\n<p>정리하자면, $I - \\gamma P_{a_1}$ -&gt; zero eigenvalue가 없다. $\\equiv$ sigular하지 않다. $\\equiv$ 역행렬이 존재한다.</p>\n<p>추가적인 자료<br>1) <a href=\"https://en.wikipedia.org/wiki/Markov_chain#Stationary_distribution_relation_to_eigenvectors_and_simplices\" target=\"_blank\" rel=\"noopener\">Stationary distribution relation to eigenvectors and samplices in Markov chain</a><br>2) <a href=\"https://darkpgmr.tistory.com/106\" target=\"_blank\" rel=\"noopener\">특이값 분해(Singular Value Decomposition, SVD)의 활용)</a></p>\n<p>다시 Proof를 따라가봅니다. Equation (2)를 (3)으로 대체하면, $\\pi \\equiv a_1$가 optimal하는 것은 $\\equiv$ 아래의 수식과 같습니다.</p>\n<p>$$a_1 \\equiv \\pi (s) \\in arg\\max_{a \\in A} \\sum_{s’} P_{sa} (s’) V^\\pi (s’) \\,\\,\\,\\,\\, \\forall s \\in S$$ </p>\n<p>$$\\Leftrightarrow \\sum_{s’} P_{sa_1} (s’) V^\\pi (s’) \\geq \\sum_{s’} P_{sa} (s’) V^\\pi (s’) \\,\\,\\,\\,\\, \\forall s \\in S, a \\in A$$</p>\n<p>$$\\Leftrightarrow P_{a_1} V^\\pi \\succeq P_a V^\\pi \\,\\,\\,\\,\\, \\forall a \\in A \\setminus a_1$$</p>\n<p>$$\\Leftrightarrow P_{a_1} (I - \\gamma P_{a_1})^{-1} R \\succeq P_a (I - \\gamma P_{a_1})^{-1} R \\,\\,\\,\\,\\, \\forall a \\in A \\setminus a_1$$</p>\n<p>여기서 $a \\in A \\setminus a_1$이란 $a_1$을 제외한 set of actions $A$에 있는 $a$들을 말한다.</p>\n<p><strong>Remark</strong>. 매우 비슷한 argument를 사용하여 추가적인 언급을 합니다. $(P_{a_1} - P_a)(I - \\gamma P_{a_1})^{-1} R \\succ 0$라는 조건이 $\\pi \\equiv a_1$가 unique optimal policy가 되는 것에 필요하고, 충분하다고 볼 수 있습니다. (또한 추가적으로 위에 증명에 모든 inequalities를 strict inequalities로 대체함으로써)</p>\n<p>finite-state MDPs에 대해, 이러한 결과는 IRL의 solution인 모든 reward function들의 set을 정의합니다. 그러나 여기에는 두 가지 문제점이 있습니다.<br>1) $R = 0$ (and indeed any other constant vector)은 항상 solution입니다. 다시 말해 만약 우리가 어떠한 행동을 취했을 지라도 reward가 항상 같다면, $\\pi \\equiv a_1$을 포함하여 어떠한 policy들은 항상 optimal하다는 것입니다.  $\\pi$가 unique optimal policy라는 점에서는 이 문제를 완화시키지만, 전체적으로 만족시키진 않습니다. 왜냐하면 보통 0에 임의적으로 가까운 일부 reward vector들이 여전히 solution이 될 수 있기 때문입니다.<br>2) 대부분의 MDPs에서, criteria (4)를 만족시키는 R에 대한 많은 choice들이 있습니다.</p>\n<p>그렇다면 우리는 어떻게 많은 reward function들 중의 하나를 결정할 수 있을까요? 다음 section을 통해서 이러한 문제점들을 해결할 수 있는 natural criteria를 알아봅시다!</p>\n<h2 id=\"3-2-LP-Formulation-and-Penalty-Terms\"><a href=\"#3-2-LP-Formulation-and-Penalty-Terms\" class=\"headerlink\" title=\"3.2 LP Formulation and Penalty Terms\"></a>3.2 LP Formulation and Penalty Terms</h2><p>위의 질문에 대한 답변으로 명확하게 말하자면, Linear Programming(LP)은 Equation (4)로 인한 문제점들에 대해 실행 가능한 point로서 사용될 수 있습니다.</p>\n<p>그래서 R을 고를수 있는 한 가지 natural한 방법은 가장 먼저 $\\pi$를 optimal하도록 만드는 것입니다. 또한 $\\pi$로부터 어떠한 single-step deviation(편차)을 가능한 한 costly하게 만드는 것입니다. <strong>쉽게 말해 최적의 행동이 있다면 최적의 정책을 찾을 수 있고, 최적의 정책을 찾을 수 있다면 R을 고를 수 있다는 것입니다.</strong></p>\n<p>수식으로 표현해보면, (4)를 만족시키는 모든 function R 중에서, 다음의 수식을 maximize하도록 하는 $a_1$을 고를 수 있습니다.</p>\n<p>$$\\sum_{s \\in S} (Q^\\pi (s, a_1) - \\max_{a \\in A \\setminus a_1} Q^\\pi (s, a)) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (6)$$</p>\n<p>수식을 설명해보면 quality of the optimal action과 quality of the next-best action 사이의 differences의 sum을 maximize하는 것을 말합니다.</p>\n<p>추가적으로 다른 모든 Q값들이 동등할 때, 대부분의 small rewards에 대한 solution은 “simpler”하도록 optional하게 objective function이 $-\\lambda ||R||_1$ ($\\ell_1$-penalty)와 같은 weight decay-like penalty term을 추가할 것입니다. (여기서 $\\lambda$는 small rewards를 가지는 것과 (6)을 maximizing하는 것 두 목표 사이를 balancing할 수 있는 adjustable penalty coefficient입니다.)</p>\n<p>최대한 쉽게 풀어서 다시 설명해보겠습니다. 위의 수식 (6)으로 maximizing을 한다고 했을 때, Q값들에 대해서 보다 더 R를 simple하게 정하고, 수식 (6)을 더 maximizing이 잘 되도록 penalty term을 추가하는 것입니다. 쉽게 말해 우리가 Loss function에서 regularization term을 두는 것처럼 더 효과적이고 쉽게 사용하기 위해서 하는 작업이라고 생각하면 편합니다. $Q^\\pi (s, a_1)$과 Q값들 사이가 더 극명하도록 L1 regularization term을 두어서 R를 쉽게 정하자! 라는 것입니다.</p>\n<p>이렇게 함으로써 “simplest” R (largest penalty coefficient)을 찾을 수 있고, R은 왜 $\\pi$가 optimal한 지를 “explain”할 수 있습니다.</p>\n<p>정리하여 다시 optimization problem을 수식으로 말하자면 다음과 같습니다.<br><strong>First objective function &amp; algorithm</strong></p>\n<p>$$maximize \\,\\,\\,\\,\\, \\sum_{i=1}^N \\min_{a \\in {a_2, … , a_k}} \\{(P_{a_1} (i) - P_a (i))(I - \\gamma P_{a_1})^{-1} R\\} - \\lambda ||R||_1$$</p>\n<p>$$s.t. \\,\\,\\,\\,\\, (P_{a_1} - P_a)(I - \\gamma P_{a_1})^{-1} R \\succeq 0 \\,\\,\\,\\,\\, \\forall a \\in A \\setminus a_1$$</p>\n<p>$$|R_i| \\leq R_{max}, i = 1, … , N$$</p>\n<p>이러한 수식들을 통해 linear program으로 표현될 수 있고, 효과적으로 해결될 수 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"4-Linear-Function-Approximation-in-Large-State-Spaces\"><a href=\"#4-Linear-Function-Approximation-in-Large-State-Spaces\" class=\"headerlink\" title=\"4. Linear Function Approximation in Large State Spaces\"></a>4. Linear Function Approximation in Large State Spaces</h1><p>이번 section에서는 infinite state spaces의 case를 다룹니다. infinite-state MDPs는 section 2에서의 finite-state와 같은 방식으로 정의될 수 있습니다. 추가적으로 state는 $S = \\mathbb{R}^n$의 case에 대해서만 다루고자 합니다. 따라서 reward function R은 $S = \\mathbb{R}^n$로부터의 function이라고 할 수 있습니다.</p>\n<p>Calculus of variations는 infinite state spaces에서 optimizing하는 데에 있어 좋은 tool이지만, 종종 알고리즘적으로 어렵게 만듭니다. 따라서 reward function에 대해 <strong>“Linear Functional Approximation”</strong> 을 사용합니다. 수식으로 R을 표현하자면 다음과 같습니다.</p>\n<p>$$R(s) = \\alpha_1 \\phi_1 (s) + \\alpha_2 \\phi_2 (s) + \\cdots + \\alpha_d \\phi_d (s) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (8)$$</p>\n<p>여기서 $\\phi_1, … , \\phi_d$는 $S$로부터 mapping된 고정되어 있고, 우리가 알고 있고, bound되어 있는 basis function입니다. 그리고 $\\alpha_i s$는 우리가 fit해야하는 알고있지 않은 parameter입니다.</p>\n<p>다음으로 $V^\\pi$에 대해서도 linearity of expectation을 함으로써, reward function R이 Equation (8)로 주어질 때 value function을 다음과 같이 표현할 수 있습니다.</p>\n<p>$$V^\\pi = \\alpha_1 V_1^\\pi + \\cdots + \\alpha_d V_d^\\pi \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (9)$$</p>\n<p>위의 수식과 Theorem 2 (3번 수식)을 사용하면서, policy $\\pi (s) \\equiv a_1$를 optimal하도록 만드는 R에 대해서 (4)의 appropriate generalization은 다음의 조건입니다.</p>\n<center> <img src=\"../../../../img/irl/linear_irl_4.png\" width=\"370\"> </center>\n\n<p>, for all states $s$ and all actions $a \\in A \\setminus a_1$</p>\n<p>하지만 위의 formulation들에는 두 가지 문제가 있습니다.<br>1) infinite state spaces에서, Equation (10)의 형태에는 infinitely 많은 제약이 있습니다. infinite state spaces이기 때문에 모든 state를 check하기가 불가능하고 어렵습니다. 따라서 알고리즘적으로, states 중의 finite subset $S_0$만 sampling함으로써 이러한 문제를 피하고자 합니다.<br>2) R을 표현하기 위해 Equation (8)에서 linear function approximator를 사용한다고 제한했기 때문에, 어느 $\\pi$가 optimal한 지에 대해 더 이상 어떠한 reward function도 표현할 수 없습니다. 그럼에도 불구하고, linear function approximator를 사용할 것입니다.</p>\n<p>최종적으로 linear programming formulation은 다음과 같습니다.<br><strong>Second objective function &amp; algorithm</strong></p>\n<center> <img src=\"../../../../img/irl/linear_irl_5.png\" width=\"460\"> </center>\n\n<p>$$s.t. \\,\\,\\, |\\alpha_i| \\leq 1, \\,\\,\\, i = 1, … , d$$</p>\n<p><br><br></p>\n<h1 id=\"5-IRL-from-Sampled-Trajectories\"><a href=\"#5-IRL-from-Sampled-Trajectories\" class=\"headerlink\" title=\"5. IRL from Sampled Trajectories\"></a>5. IRL from Sampled Trajectories</h1><p>이번 section에서는 오직 state space에서의 actual trajectories의 set을 통해서만 policy $\\pi$를 접근하는 좀 더 realistic case에 대해서 IRL 문제를 다룹니다. 그래서 MDP의 explicit model을 필요로 하지 않습니다.</p>\n<p>initial state distribution $D$를 고정하고, (unknown) policy $\\pi$에 대해 우리의 목표는 $\\pi$가 $\\mathbb{E}_{s_0 \\sim D} [V^\\pi (s_0)]$를 maximize하는 R를 찾는 것입니다. (기억합시다!) 추가적으로 notation을 단순화하기 위해 고정된 start state $s_0$를 가정합니다.</p>\n<p>먼저 $\\alpha_i$의 setting을 통해 $V^\\pi (s_0)$를 estimating하는 방법이 필요합니다. 이것을 하기 위해, 첫 번째로 $m$ Monte Carlo trajectories를 만들어냅니다. 그리고 나서 $i = 1, … , d$에 대해 만약 reward가 $R = \\phi_i$라면, $V_i^\\pi (s_0)$를 얼마나 average empirical return이 $m$ trajectories에 있었는 지로 정의합니다.</p>\n<p>예를 들어, 만약 $m = 1$ trajectories이고, 이 trajectory가 states ($s_0, s_1, …$)의 sequence라면, 다음과 같이 나타낼 수 있습니다.</p>\n<p>$$\\hat{V}_i^\\pi (s_0) = \\phi_i (s_0) + \\gamma \\phi_i (s_1) + \\gamma^2 \\phi_i (s_2) + \\cdots$$</p>\n<p>일반적으로, $\\hat{V}_i^\\pi (s_0)$은 어떠한 $m$ trajectories의 empirical returns에 대하여 average합니다. (여기서 말하는 어떠한 $m$ trajectories는 임의의 finite number에 의해 truncate된 trajectories를 말합니다.) 그리고 그 때 $\\alpha_i$의 어떠한 setting에 대해서, $V^\\pi (s_0)$의 natural estimate는 다음과 같습니다.</p>\n<p>$$\\hat{V}^\\pi (s_0) = \\alpha_1 \\hat{V}_1^\\pi (s_0) + \\cdots + \\alpha_d \\hat{V}_d^\\pi (s_0) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (11)$$</p>\n<p>위의 수식의 “inductive(귀납적인) step”은 다음을 뒤따릅니다 : set of policies ${ \\pi_1, … , \\pi_k }$이 있고, resulting reward function은 아래의 수식을 만족하기 때문에 $\\alpha_i$의 setting을 찾을 수 있습니다.</p>\n<p>$$V^{\\pi^*} (s_0) \\geq V^{\\pi_i} (s_0), \\,\\,\\, i = 1, … , k \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, (12)$$</p>\n<p>그리고 section 4에서 마지막에 있던 수식에서, objective function을 약간 바꿀 수 있습니다. 따라서 optimization의 식은 다음과 같이 될 수 있다.<br><strong>Final objective function &amp; algorithm</strong></p>\n<p>$$maximize \\,\\,\\,\\,\\, \\sum_{i=1}^k p(\\hat{V}^{\\pi^*} (s_0) - \\hat{V}^{\\pi_i} (s_0))$$</p>\n<p>$$s.t. \\,\\,\\,\\,\\, |\\alpha_i| \\leq 1, \\,\\,\\, i = 1, … , d$$</p>\n<p>위의 수식에서 $\\hat{V}^{\\pi_i} (s_0)$과 $\\hat{V}^{\\pi^*} (s_0)$은 Equation (11)에서 주어진 $\\alpha_i$의 (implicit) linear function이다.</p>\n<p>그러므로, 위의 $\\hat{V}^{\\pi_i} (s_0)$과 $\\hat{V}^{\\pi^*} (s_0)$은 쉽게 linear programming으로 해결할 수 있습니다.</p>\n<p>위의 optimization 식은 $\\alpha_i$의 새로운 setting으로 설정할 수 있고, 그러므로 새로운 reward function $R = \\alpha_1 \\phi_1 + \\cdots + \\alpha_d \\phi_d$을 가질 수 있습니다.</p>\n<p>그리고 그 때 $R$로 인한 $V^\\pi (s_0)$를 maximize하는 policy $\\pi_{k+1}$을 찾을 수 있고, $\\pi_{k+1}$을 current set of policies에 추가할 수 있습니다. 그리고 이것을 계속할 수 있습니다. (많은 수의 iteration을 통해 우리가 “satisfied”하는 $R$를 찾을 수 있습니다.)</p>\n<p><br><br></p>\n<h1 id=\"6-Experiments\"><a href=\"#6-Experiments\" class=\"headerlink\" title=\"6. Experiments\"></a>6. Experiments</h1><p><br></p>\n<h2 id=\"6-1-First-experiment-5-x-5-grid-world\"><a href=\"#6-1-First-experiment-5-x-5-grid-world\" class=\"headerlink\" title=\"6.1 First experiment : 5 x 5 grid world\"></a>6.1 First experiment : 5 x 5 grid world</h2><center> <img src=\"../../../../img/irl/linear_irl_6.png\" width=\"500\"> </center>\n\n<p>첫 번째 실험은 agent가 왼쪽 아래에서 시작하여 오른쪽 위로 가면 보상 1을 받는 5x5 grid world를 사용했습니다. action은 상하좌우지만 noise가 있으며 30%의 확률로 랜덤하게 움직입니다.</p>\n<p>위에 있는 Figure 1은 5x5 grid world에서의 optimal policy와 true reward function을 나타낸 것입니다.</p>\n<center> <img src=\"../../../../img/irl/linear_irl_7.png\" width=\"500\"> </center>\n\n<p>앞에서 설명한 penalty term인 $\\lambda$를 주지 않은 Section 3.2의 discrete/finite state problem에서 사용할 수 있는 algorithm을 쓰면 Figure 2 윗 부분의 그림과 같은 울퉁불퉁한 reward function의 모습을 볼 수 있습니다. 위에서 말했던 section 3.2에 있는 algorithm은 다음과 같습니다.</p>\n<p>$$maximize \\,\\,\\,\\,\\, \\sum_{i=1}^N \\min_{a \\in {a_2, … , a_k}} \\{(P_{a_1} (i) - P_a (i))(I - \\gamma P_{a_1})^{-1} R\\} - \\lambda ||R||_1$$</p>\n<p>$$s.t. \\,\\,\\,\\,\\, (P_{a_1} - P_a)(I - \\gamma P_{a_1})^{-1} R \\succeq 0 \\,\\,\\,\\,\\, \\forall a \\in A \\setminus a_1$$</p>\n<p>$$|R_i| \\leq R_{max}, i = 1, … , N$$</p>\n<p>그러나 Figure 2 아래에 쓰여져 있는 $\\lambda$ = 1.05 로 설정을 하면 Figure 2 아래에 있는 그림과 같은 true reward에 밀접한 reward function을 얻을 수 있습니다.</p>\n<p><br></p>\n<h2 id=\"6-2-Second-experiment-Mountain-car\"><a href=\"#6-2-Second-experiment-Mountain-car\" class=\"headerlink\" title=\"6.2 Second experiment : Mountain-car\"></a>6.2 Second experiment : Mountain-car</h2><center> <img src=\"../../../../img/irl/linear_irl_8.png\" width=\"500\"> </center>\n\n<p>다음 실험으로는 보통 잘 알려져있는 mountain-car 환경입니다. ture, undiscounted reward는 언덕에 있는 goal지점에 도달하기 전까지 step마다 -1을 받는 것입니다. 그리고 state는 자동차의 x-위치와 속도입니다.</p>\n<center> <img src=\"../../../../img/irl/linear_irl_5.png\" width=\"460\"> </center>\n\n<p>$$s.t. \\,\\,\\, |\\alpha_i| \\leq 1, \\,\\,\\, i = 1, … , d$$</p>\n<p>state가 연속적이기 때문에 위의 수식과 같이 section 4의 continuous/finite state problem에서 사용할 수 있는 algorithm을 사용했습니다. 또한 reward에 대해서 오직 자동차의 x-위치에 대한 functions가 되도록 Gaussian 모양으로 된 26개의 basis functions의 linear combinations을 가지는 function approximator class를 구성하였습니다.</p>\n<center> <img src=\"../../../../img/irl/linear_irl_9.png\" width=\"500\"> </center>\n\n<p>algorithm으로 optimal policy를 고려하여 전형적인 reward function은 Figure 4의 윗 부분에 있는 그래프로 나타납니다. (Note the scale on the $y$ axis.) 명확하게도, solution은 reward의 $R = -c$ structure를 거의 완벽하게 나타내었습니다.</p>\n<p>좀 더 challenging한 문제로서, 이 논문에서는 언덕 아래 주변을 중심으로 [-0.72, -0.32] 사이에 있으면 reward 1 아니면 0을 주도록 changed ture reward에 대한 실험을 동일하게 하였습니다. 그리고 여기서 $\\gamma$는 0.99를 주었다고 합니다.</p>\n<p>이 문제에서의 optimal policy는 가능한 한 빨리 언덕 아래로 가서 주차를 하는 것입니다. 새로운 문제에 대해 알고리즘을 적용해봤을 때 전형적인 solution은 Figure 4 아래에 있는 그래프로 나타낼 수 있습니다.</p>\n<p>대체로 reward의 중요한 structure를 정해진 [-0.72, -0.32]에 대해 성공적으로 찾았다고 합니다. 또한 오른쪽에는 artifact가 있어 오른쪽 끝을 피할 수 없도록 “shooting out”하는 효과가 있다고 합니다. 그럼에도 불구하고, solution이 이 문제에 꽤 좋았다고 합니다.</p>\n<p><br></p>\n<h2 id=\"6-3-Final-experiment-Continuous-version-of-the-5-x-5-grid-world\"><a href=\"#6-3-Final-experiment-Continuous-version-of-the-5-x-5-grid-world\" class=\"headerlink\" title=\"6.3 Final experiment : Continuous version of the 5 x 5 grid world\"></a>6.3 Final experiment : Continuous version of the 5 x 5 grid world</h2><p>마지막 실험은 5x5 grid world의 continuous version에서 적용했습니다.</p>\n<p>state는 [0, 1] x [0, 1]이고, action은 상하좌우 방향으로 0.2만큼 움직입니다. 또한 [-0.1, 0.1]의 uniform noise가 각각 좌표에 추가되고, 만약 unit square 내에 uniform noise가 필요한 경우, state는 truncate됩니다.</p>\n<p>reward는 [0.8, 1] x [0.8, 1]에서는 1을 받고 나머지는 0을 받습니다. 그리고 $\\gamma$는 0.9를 사용했습니다. 또한 function approximator class는 2-dimensional Gaussian basis functions에 대한 15x15 array의 linear combinations로 구성하였습니다.</p>\n<p>initial state distribution $D$는 state space에 대해  uniform하였고, algorithm은 policy를 평가하기 위해 각각의 30 steps 마다 $m=5000$ trajectories를 사용하여 적용하였습니다.</p>\n<p>$$maximize \\,\\,\\,\\,\\, \\sum_{i=1}^k p(\\hat{V}^{\\pi^*} (s_0) - \\hat{V}^{\\pi_i} (s_0))$$</p>\n<p>$$s.t. \\,\\,\\,\\,\\, |\\alpha_i| \\leq 1, \\,\\,\\, i = 1, … , d$$</p>\n<p>위에서 다뤘던 section 5에서의 algorithm을 사용하여 찾아진 solution은 단지 1 iteration만 했는데도 꽤나 reasonable했습니다. 그래서 약 15 iterations에 대해서도 해본 결과, algorithm은 동일하게 좋은 performance로 해결했습니다.</p>\n<center> <img src=\"../../../../img/irl/linear_irl_10.png\" width=\"500\"> </center>\n\n<p>또한 action choice가 다른 state sapce의 부분을 계산하면서 true optimal policy와 fitted reward’s optimal policy를 비교하였고 (Figure 5, 위), 대체적으로 3% ~ 10% 사이로 불일치했습니다. 아마도 algorithm’s performance의 좀 더 적절한 측정은 true optimal policy의 quality와 fitted reward’s optimal policy의 quality를 비교하는 것입니다. (Quality는 물론 true reward function을 사용하여 측정됩니다.)</p>\n<p>결과적으로 algorithm의 약 15 iterations에 대해서, evaluations은 (which used 50000 Monte Carlo trials of 50 steps each) true “optimal policy”의 value와 fitted reward’s optimal policy사이에 statistically한 중요한 차이를 detect할 수 없다는 것을 볼 수 있었습니다. 그만큼 true optimal policy와 reward’s optimal policy의 차이가 없다는 것을 뜻합니다. (Figure 5, 아래)</p>\n<p><br><br></p>\n<h1 id=\"7-Conclusions-and-Future-work\"><a href=\"#7-Conclusions-and-Future-work\" class=\"headerlink\" title=\"7. Conclusions and Future work\"></a>7. Conclusions and Future work</h1><p>이 논문은 moderate-sized discrete and continuous domain에서 Inverse Reinforcement Learning 문제가 해결될 수 있다는 것을 보였습니다.</p>\n<p>하지만 많은 open question들이 아래와 같이 남아있습니다.<br>1) Potential-based shaping rewards는 MDP에서 학습시키기 위한 하나의 solution으로서 reward function을 더 쉽게 만들 수 있습니다. 그렇다면 우리는 더 “easy” reward function을 만들기 위한 IRL 알고리즘들을 만들 수 있을까요?<br>2) IRL를 real-world empirical application측면에서 보면, sensor inputs and actions에 대해서 observer의 측정에 상당한 noise가 있을지도 모릅니다. 여기에 더하여 많은 optimal policy들이 존재할 지도 모릅니다. 어떠한 data를 noise없이 fit하도록 하는 적절한 metric은 무엇일까요?<br>3) 만약 행동이 절대로 optimality와 일치하지 않는다면, state space에 specific region에 대한 “locally consistent” reward function을 어떻게 알 수 있을까요?<br>4) 어떻게 reward function의 identifiability를 maximize하기 위한 실험을 고안해낼 수 있을까요?<br>5) 이 논문에 적힌 알고리즘적인 접근이 partially observable environment의 case를 얼마나 잘 실행할 수 있을까요?</p>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"Let’s-do-Inverse-RL-Guide\"><a href=\"#Let’s-do-Inverse-RL-Guide\" class=\"headerlink\" title=\"Let’s do Inverse RL Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2019/01/22/0_lets-do-irl-guide/\">Let’s do Inverse RL Guide</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"APP-여행하기\"><a href=\"#APP-여행하기\" class=\"headerlink\" title=\"APP 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2019/02/01/2_app/\">APP 여행하기</a></h2>"},{"title":"Generative Adversarial Imitation Learning","date":"2019-02-12T15:00:00.000Z","author":"이승현","subtitle":"Inverse RL 5번째 논문","_content":"\n<center> <img src=\"../../../../img/irl/gail_1.png\" width=\"850\"> </center>\n\nAuthor : Jonathan Ho, Stefano Ermon\nPaper Link : https://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf\nProceeding : Advances in Neural Information Processing Systems (NIPS) 2016\n\n---\n\n# 0. Abstract\n\nTrust region policy optimization (TRPO)는 상당히 우수한 성능을 보여주는 policy gradient 기법으로 알려져 있습니다. 높은 차원의 action space를 가진 robot locomotion부터 action은 적지만 화면을 그대로 처리하여 플레이하기 때문에 control parameter가 매우 많은 Atari game까지 각 application에 세부적으로 hyperparameter들을 특화시키지 않아도 두루두루 좋은 성능을 나타내기 때문에 일반화성능이 매우 좋은 기법입니다. 이 TRPO에 대해서 알아보겠습니다.\n\n※TRPO를 매우 재밌게 설명한 Crazymuse AI의 [Youtube video](https://www.youtube.com/watch?v=CKaN5PgkSBc&t=90s)에서 일부 그림을 차용했습니다. 이 비디오를 시청하시는 것을 강력하게 추천합니다!\n\n<br>\n## 1.1 TRPO 흐름 잡기\n\nTRPO 논문은 많은 수식이 등장하여 이 수식들을 따라가다보면 큰 그림을 놓칠 수도 있습니다. 세부적인 내용을 살펴보기 전에 기존 연구에서 출발해서 TRPO로 어떻게 발전해나가는지 간략하게 살펴보겠습니다.\n\n### 1.1.1 Original Problem\n\n$$\\max\\_\\pi \\eta(\\pi)$$\n\n모든 강화학습이 그렇듯이 expected discounted reward를 최대화하는 policy를 찾는 문제로부터 출발합니다.\n\n### 1.1.2 Conservative policy iteration\n\n$$\\max L\\_\\pi(\\tilde\\pi) = \\eta(\\pi) + \\sum\\_s \\rho_\\pi(s)\\sum\\_a\\tilde\\pi(a\\vert s)A\\_\\pi(s,a)$$\n\n$\\eta(\\pi)$를 바로 최대화하는 것은 많은 경우 어렵습니다. $\\eta(\\pi)$의 성능향상을 보장하면서 policy를 update하는 conservative policy iteration 기법이 [Kakade와 Langford](http://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf)에 의하여 제안되었습니다. 이 기법을 이용하면 policy update가 성능을 향상시키는지는 못하더라도 최소한 성능을 악화시키지는 않는다는 것이 이론적으로 보장됩니다.\n\n### 1.1.3 Theorem 1 of TRPO\n\n$$\\max L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\quad\\left(\\alpha=D\\_\\mathrm{TV}^\\max\\left(\\pi\\_\\mathrm{old},\\pi\\_\\mathrm{new}\\right)\\right)$$\n\n기존의 conservative policy iteration은 과거 policy와 새로운 policy를 섞어서 사용해서 실용적이지 않다는 단점이 있었는데 이것을 보완하여 온전히 새로운 policy만으로 update할 수 있는 기법을 제안합니다. \n\n### 1.1.4 KL divergence version of Theorem 1\n\n$$\\max L\\_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\pi,\\tilde\\pi\\right),\\quad\\left(C = \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\right)$$\n\ndistance metric을 KL divergence로 바꿀 수 있습니다.\n\n### 1.1.5 Using parameterized policy\n\n$$\\max\\_\\theta L\\_{\\theta\\_\\mathrm{old} }(\\theta) - C\\cdot D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)$$\n\n최적화문제를 더욱 편리하게 풀 수 있도록 낮은 dimension을 가지는 parameter들로 parameterized된 policy를 사용할 수 있습니다.\n\n### 1.1.6 Trust region constraint\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n1.1.5까지는 아직 conservative policy iteration을 약간 변형시킨 것입니다. policy를 update할 때 지나치게 많이 변하는 것을 방지하기 위하여 [trust region](https://en.wikipedia.org/wiki/Trust_region)을 constraint로 설정할 수 있습니다. 이 아이디어로 인해서 TRPO라는 명칭을 가지게 됩니다.\n\n### 1.1.7 Heuristic approximation\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n사실 1.1.6의 constraint는 모든 state에 대해서 성립해야 하기 때문에 문제를 매우 어렵게 만듭니다. 이것을 좀 더 다루기 쉽게 state distribution에 대한 평균을 취한 것으로 변형합니다.\n\n### 1.1.8 Monte Carlo simulation\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}$$\n\nSampling을 통한 계산이 가능하도록 식을 다시 표현할 수 있습니다. \n\n### 1.1.9 Efficiently solving TRPO\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &\\nabla\\_\\theta \\left. L\\_{\\theta\\_\\mathrm{old} }(\\theta)\\right\\vert \\_{\\theta=\\theta\\_\\mathrm{old} } \\left(\\theta - \\theta\\_\\mathrm{old}\\right) \\\\\\\\\n\\mathrm{s.t.\\ }&\\frac{1}{2}\\left(\\theta\\_\\mathrm{old} - \\theta \\right)^T A\\left(\\theta\\_\\mathrm{old}\\right)\\left(\\theta\\_\\mathrm{old} - \\theta \\right) \\leq \\delta \\\\\\\\\n&A\\_{ij} = \\frac{\\partial}{\\partial \\theta\\_i}\\frac{\\partial}{\\partial \\theta\\_j}\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\n\\end{align}$$\n\n문제를 효율적으로 풀기 위하여 approximation을 적용할 수 있습니다. objective function은 first order approximation, constraint는 quadratic approximation을 취하면 효율적으로 문제를 풀 수 있는 형태로 바뀌는데 이것을 natural gradient로 풀 수도 있고 conjugate gradient로 풀 수도 있습니다.\n\nTRPO는 이렇게 다양한 방법으로 문제를 변형한 것입니다! 이제 좀 더 자세히 살펴보겠습니다.\n\n<br><br>\n\n# 2. Preliminaries\n\n다음과 같은 파라미터를 가지는 infinite-horizon discounted Markov decision process (MDP)를 고려합니다. \n\n* $\\mathcal{S}$: finite set of states\n* $\\\\mathcal{A}$: finite set of actions\n* $P: \\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$: transition probability distribution\n* $r: \\mathcal{S}\\rightarrow\\mathbb{R}$: reward function\n* $\\rho\\_0:\\mathcal{S}\\rightarrow\\mathbb{R}$: distribution of the initial state $s\\_0$\n* $\\gamma\\in(0,1)$: discount factor\n* $\\eta(\\pi)=E\\_{s\\_0,a_0,\\ldots}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r\\left(s\\_t\\right)\\right]$, where $s\\_0\\sim\\rho\\_0\\left(s\\_0\\right), a\\_t\\sim\\pi\\left(\\left.a\\_t \\right\\vert s\\_t\\right), s\\_{t+1}\\sim P\\left(\\left.s\\_{t+1}\\right\\vert s\\_t,a\\_t\\right)$\n* $Q\\_\\pi\\left(s\\_t,a\\_t\\right)=E\\_{ { \\color{red}{s\\_{t+1}} },a\\_{t+1},\\ldots}\\left[\\sum\\_{l=0}^\\infty \\gamma^l r\\left(s\\_{t+l}\\right)\\right]$: action value function\n* $V\\_\\pi\\left(s\\_t\\right)=E\\_{ { \\color{red}{a\\_t} }, s\\_{t+1},a\\_{t+1},\\ldots}\\left[\\sum\\_{l=0}^\\infty \\gamma^l r\\left(s\\_{t+l}\\right)\\right]$: value function (action value function과 expectation의 첨자가 다른 부분을 유의하세요.)\n* $A\\_\\pi(s,a) = Q\\_\\pi(s,a) - V\\_\\pi(s)$: advantage function\n\n<br>\n## 2.1 Useful identity [Kakade & Langford 2002]\n\n우리의 목표는 $\\eta\\(\\pi\\)$가 최대화되도록 만드는 것입니다. 하지만 $\\pi$의 변화에 따라 $\\eta$가 어떻게 변하는지 알아내는 것도 쉽지 않습니다. $\\pi$는 기존의 policy이고 $\\tilde\\pi$는 새로운 policy를 나타낸다고 할 때 $\\eta$와 policy update 사이에 다음과 같은 관계가 있다는 것이 밝혀졌습니다.\n\n**Lemma 1.** $\\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E\\_{s\\_{0},a\\_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right]$\n\n*Proof.* $A\\_\\pi(s,a)=E\\_{s'\\sim P\\left(s'\\vert s,a\\right)}\\left[r(s)+\\gamma V\\_\\pi\\left(s'\\right)-V\\_\\pi(s)\\right]$로 다시 표현할 수 있습니다. 표기의 편의를 위하여 $\\tau:=\\left(s\\_0,a\\_0,s\\_1,a\\_1,\\ldots\\right)$를 정의하겠습니다. 다음과 같은 수식 전개가 가능합니다.\n$$\n\\begin{align}\n&E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right] \\\\\\\\\n &= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t \\left(r(s\\_t)+\\gamma V\\_\\pi\\left(s\\_{t+1}\\right)-V\\_\\pi(s\\_t)\\right)\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\left(\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right)+\\gamma V\\_\\pi\\left(s\\_{1}\\right)-V\\_\\pi(s\\_0)+\\gamma^2 V\\_\\pi\\left(s\\_{2}\\right)-\\gamma V\\_\\pi(s\\_1)+\\gamma^3 V\\_\\pi\\left(s\\_{3}\\right)-\\gamma^2 V\\_\\pi(s\\_2)+\\cdots\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)+\\color{red}{\\gamma V\\_\\pi\\left(s\\_{1}\\right)}-V\\_\\pi(s\\_0)+\\color{red}{\\gamma^2 V\\_\\pi\\left(s\\_{2}\\right)}-\\color{red}{\\gamma V\\_\\pi(s\\_1)}+\\cdots\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[-V\\_\\pi(s\\_0)+\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right] \\\\\\\\\n&\\overset{\\underset{\\mathrm{(a)} }{} }{=} -E\\_{\\color{red}{s\\_0}}\\left[V\\_\\pi(s\\_0)\\right]+E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right] \\\\\\\\\n&=-\\eta(\\pi) + \\eta\\left(\\tilde\\pi\\right)\\\\\\\\\n&\\quad\\therefore \\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E\\_{s\\_{0},a\\_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right]\n\\end{align}\n$$\n위의 전개의 (a) 부분은 $V\\_\\pi\\left(s\\_0\\right)=E\\_{a\\_0,a\\_1,s\\_1,\\ldots}\\left[\\sum\\_{t=0}^\\infty\\gamma^l r\\left(s\\_l\\right)\\right]$이므로 $\\tau$ 중 많은 부분들이 이미 expectation이 취해진 값이므로 무시됩니다. 오직 $s\\_0$만 expectation이 취해지지 않았기 때문에 남아있습니다.\n\n**Lemma 1**은 새로운 policy와 기존의 policy 사이의 관계를 규정합니다. 다음과 같은 식을 정의하고 이것을 이용해서 **Lemma 1**을 변형시켜 봅시다.\n\n* $\\rho\\_\\pi(s) = P\\left(s\\_0=s\\right) + \\gamma P\\left(s\\_1=s\\right)\\ + \\gamma^2 P\\left(s\\_2=s\\right) + \\cdots $: (unnormalized) discounted visitation frequencies\n\n\n\n$$\n\\begin{align}\n\\eta\\left(\\tilde\\pi\\right) &= \\eta(\\pi) + \\sum\\_{t=0}^\\infty\\sum\\_{s}P\\left(\\left.s\\_t=s\\right\\vert \\tilde\\pi\\right)\\sum\\_a\\tilde\\pi(a\\vert s)\\gamma^t A\\_\\pi(s,a) \\\\\\\\\n&= \\eta(\\pi) + \\sum\\_{s}{\\color{red}{\\sum\\_{t=0}^\\infty \\gamma^t P\\left(\\left.s\\_t=s\\right\\vert \\tilde\\pi\\right)} } \\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\\\\\\\\n&= \\eta(\\pi) + \\sum\\_{s}{\\color{red}{\\rho\\_\\tilde\\pi(s)} }\\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\\\\\\\\n\\end{align}\n$$\n\n이 수식의 의미가 무엇일까요? 만약 $\\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\geq 0$이라면 $\\eta(\\tilde\\pi)$는 항상 $\\eta(\\pi)$보다 큽니다. 즉, policy를 업데이트함으로써 항상 개선됩니다. 즉, 이 수식을 통해서 항상 더 좋은 성능을 내는 policy update가 가져야 할 특징을 알 수 있습니다. 다음과 같은 deterministic policy가 있다고 합시다.\n\n$$\n\\tilde\\pi(s) = \\arg\\max_a A\\_\\pi(s,a)\n$$\n\n이 policy는 적어도 하나의 state-action pair에서 0보다 큰 값을 가지는 advantage가 있고 그 때의 확률이 0이 아니라면 항상 성능을 개선시킵니다. 어려운 점은 policy를 바꾸면 $\\rho$도 바뀐다는 점입니다. \n\n다음 그림을 봅시다. Starting Point에서 여러가지 경로를 거쳐서 Destination으로 갈 수 있습니다. 다른 policy를 이용하는 것은 다른 경로를 이용한 것입니다.\n\n[![policy_change](../../../../img/policy_change.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=16s)\n\n이 때 다른 policy를 이용함에 따라 state visitation frequency도 변하게 됩니다.\n\n[![state_visitation_change](../../../../img/state_visitation_change.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=36s)\n\n이로 인해서 policy를 최적화하는 것은 어려워집니다. 그래서 이러한 변화를 무시하는 다음과 같은 approximation을 취합니다.\n\n$$\n\\begin{align}\nL_\\pi\\left(\\tilde\\pi\\right) &= \\eta(\\pi) + \\sum\\_{s}{\\color{red}{\\rho\\_\\pi(s)} } \\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a)\n\\end{align}\n$$\n\npolicy가 바뀌었음에도 이전의 state distribution을 계속 이용하는 것입니다. 이것은 policy의 변화가 크지 않다면 어느 정도 허용될 수 있을 것입니다. 그렇지만 얼마나 많은 변화가 허용될까요? 이것을 정하기 위해서 이용하는 것이 trust region입니다.\n\n<br>\n## 2.2 Conservative Policy Iteration\n\npolicy의 변화를 다루기 용이하게 하기 위해서 policy를 다음과 같이 파라미터를 이용해서 표현합시다.\n\n* $\\pi\\_\\theta(a\\vert s)$: parameterized policy\n\n$\\pi\\_\\theta$는 $\\theta$에 대하여 미분가능한 함수입니다. $L\\_\\pi\\left(\\tilde\\pi\\right)$을 $\\theta\\_0$에서 $\\eta(\\pi)$에 대한 first order approximation이라고 하면 다음 식이 성립합니다.\n\n$$\n\\begin{align}\nL\\_{\\pi\\_{\\theta\\_0} }\\left(\\pi\\_{\\theta_0}\\right) &= \\eta\\left(\\pi\\_{\\theta\\_0}\\right) \\\\\\\\\n\\nabla\\_\\theta \\left.L\\_{\\pi\\_{\\theta\\_0} }\\left(\\pi\\_{\\theta\\_0}\\right)\\right\\vert \\_{\\theta=\\theta\\_0} &= \\nabla\\_\\theta\\left.\\eta(\\pi\\_{\\theta\\_0})\\right\\vert \\_{\\theta=\\theta\\_0}\n\\end{align}\n$$\n\n이것의 의미는 $\\pi\\_{\\theta\\_0}$가 매우 작게 변한다면 $L\\_{\\pi\\_{\\theta\\_0} }$를 개선시키는 것이 $\\eta$를 개선시키는 것이라는 것입니다. 그러나 지금까지의 설명만으로는 $\\pi\\_{\\theta\\_0}$를 얼마나 작게 변화시켜야 할지에 대해서는 알 수 없습니다.\n\nKakade & Langford가 2002년 발표한 논문에서도 이것에 대해서 고민했습니다. 그 논문에서 *conservative policy iteration*이라는 기법을 제안합니다. 그 논문의 contribution은 다음과 같습니다.\n\n* $\\eta$ 개선의 lower bound를 제공\n* 기존의 policy를 $\\pi\\_\\mathrm{old}$라고 하고 $\\pi'$를 $\\pi'=\\arg\\max\\_\\{\\pi'}L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi'\\right)$과 같이 정의할 때, 새로운 mixture policy $\\pi\\_\\mathrm{new}$를 다음과 같이 제안\n$$\n\\pi\\_\\mathrm{new}(a\\vert s) = (1-\\alpha)\\pi\\_\\mathrm{old}(a\\vert s) + \\alpha \\pi'(a\\vert s)\n$$\n\n그림으로 표현하면 다음과 같습니다.\n\n[![mixure_policy](../../../../img/mixure_policy.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=2m46s)\n\n* 다음과 같은 lower bound를 정의\n$$\n\\eta\\left(\\pi\\_\\mathrm{new}\\right) \\geq L\\_{\\pi\\_{\\theta\\_\\mathrm{old} }}\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{2\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\\\\\\\\n\\mathrm{where\\ }\\epsilon = \\max_s\\left\\vert E\\_{a\\sim\\pi'(a\\vert s)}\\left[A\\_\\pi(s,a)\\right]\\right\\vert \n$$\n\n하지만 mixture된 policy라는 것은 실용적이지 않습니다. \n\n<br><br>\n\n# 3. Monotonic Improvement Guarantee for General Stochastic Policies\n\n이전 장에서 설명한 lower bound는 오직 mixture policy에 대해서만 성립하고 더 많이 사용되는 stochastic policy에는 적용되지 않습니다. 따라서 stochastic policy를 이용할 수 있도록 개선할 필요가 있습니다. 아래와 같이 기존 수식에서 두 가지를 바꿈으로써 이것이 가능합니다.\n\n* $\\alpha\\rightarrow$ distance measure between $\\pi$ and $\\tilde\\pi$\n* constant $\\epsilon\\rightarrow\\max\\_{s,a}\\left\\vert A\\_\\pi(s,a)\\right\\vert $\n\n여기서 distance measure로 total variation divergence를 이용합니다. discrete porbability distribution $p$와 $q$에 대하여 다음과 같이 정의됩니다.\n$$\nD\\_\\mathrm{TV}(p\\parallel q) = \\frac{1}{2}\\sum\\_i\\left\\vert p\\_i - q\\_i\\right\\vert \n$$\n\n이것을 이용하여 $D\\_\\mathrm{TV}^\\max$를 다음과 같이 정의합니다.\n$$\nD\\_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D\\_\\mathrm{TV}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)\n$$\n\n그림으로 표현하면 다음과 같습니다.\n\n[![tvd](../../../../img/tvd.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=3m15s)\n\n이것을 이용하여 다음과 같은 관계식을 얻을 수 있습니다.\n\n**Theorem 1.** Let $\\alpha=D\\_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi)$. Then the following bound holds:\n$$\n\\begin{align}\n\\eta\\left(\\pi\\_\\mathrm{new}\\right) &\\geq L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2, \\\\\\\\\n\\mathrm{where\\ } \\epsilon&= \\max\\_{s,a}\\left\\vert A\\_\\pi(s,a)\\right\\vert \n\\end{align}\n$$\n<!--*Proof.* TBD.-->\n\n또다른 distance metric으로 아래 그림과 같은 KL divergence가 있습니다.\n(그런데 왜 하필 KL divergence로 바꿀까요? 논문의 뒤쪽에서 계산효율을 위해서 conjugate gradient method를 이용하는데 이를 위해서 바꾼게 아닐까 싶습니다. Wasserstein distance 같은 다른 방향으로 발전시킬 수도 있을 것 같습니다. Schulmann이 아마 해봤겠죠?^^a)\n\n[![kld](../../../../img/kld.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=3m34s)\n\ntotal variation divergence와 KL divergence 사이에는 다음과 같은 관계가 있습니다.\n$$\nD\\_\\mathrm{TV}(p\\parallel q)^2 \\leq D\\_\\mathrm{KL}(p\\parallel q)\n$$\n\n다음 수식을 정의합니다.\n$$\nD\\_\\mathrm{KL}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D\\_\\mathrm{KL}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)\n$$\n\n**Theorem 1**을 이용하여 다음과 같은 수식이 성립함을 알 수 있습니다.\n$$\n\\begin{align}\n\\eta\\left(\\tilde\\pi\\right) &\\geq L\\_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D\\_\\mathrm{KL}^\\max(\\pi, \\tilde\\pi), \\\\\\\\\n\\mathrm{where\\ } C&= \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\n\\end{align}\n$$\n\n이러한 policy imporvement bound를 기반으로 다음과 같은 approximate policy iteration 알고리듬을 고안해낼 수 있습니다.\n\n**Algorithm 1** Policy iteration algorithm guaranteeing non-decreasing expected return $\\eta$\n> Initialize $\\pi\\_0$   \n> **for** $i=0,1,2,\\ldots$ until convergence **do**   \n> $\\quad$Compute all advantage values $A\\_{\\pi\\_i}(s,a)$    \n> $\\quad$Solve the contstrained optimization problem    \n> $\\quad\\pi\\_{i+1}=\\arg\\max\\_\\pi\\left[L\\_{\\pi\\_i} - CD\\_\\mathrm{KL}^\\max\\left(\\pi\\_i,\\pi\\right)\\right]$     \n> $\\quad\\quad$where $C = 4\\epsilon\\gamma / (1-\\gamma)^2$    \n> $\\quad\\quad$and $L\\_{\\pi\\_i}\\left(\\pi\\right) = \\eta(\\pi\\_i) + \\sum\\_{s}\\rho\\_{\\pi\\_i}(s)\\sum\\_a\\pi(a\\vert s) A\\_{\\pi\\_i}(s,a)$    \n> **end for**\n\n**Algorithm 1**은 advantage를 정확하게 계산할 수 있다고 가정하고 있습니다. 이 알고리듬은  monotonical한 성능 증가($\\eta(\\pi\\_0)\\leq\\eta(\\pi\\_1)\\leq\\cdots$)를 한다는 것을 다음과 같이 보일 수 있습니다. $M\\_i(\\pi)=L\\_{\\pi\\_i}(\\pi) - CD\\_\\mathrm{KL}^\\max\\left(\\pi\\_i,\\pi\\right)$라고 합시다.\n\n$$\n\\begin{align}\n\\eta \\left(\\pi\\_{i+1}\\right) &\\geq M\\_i\\left(\\pi\\_{i+1}\\right)\\\\\\\\\n\\eta \\left(\\pi\\_{i}\\right) &= M\\_i\\left(\\pi\\_{i}\\right) \\\\\\\\\n\\eta \\left(\\pi\\_{i+1}\\right) - \\eta \\left(\\pi\\_{i}\\right) &\\geq M\\_i\\left(\\pi\\_{i+1}\\right) - M\\_i\\left(\\pi\\_{i}\\right)\n\\end{align}\n$$\n\n위 수식과 같이 매 iteration 마다 $M\\_i$를 최대화함으로써, $\\eta$가 감소하지 않는다는 것을 보장할 수 있습니다. 이와 같은 타입의 알고리듬을 [minorization-maximization (MM) algorithm](https://www.jstor.org/stable/pdf/27643496.pdf?casa_token=0qHamcl60WoAAAAA:-fkZ9JcA_nrY3-zbCUpqvPOgcAMgw7Gr96MajCZg2byHf8m5GU1KTSxyJJcBy1lPZbBTZVCjHHUXilh4k-iuwF91Wka4B5qdltC1IR2qMWk8q1FoV6__)이라고 합니다. EM 알고리듬도 이 타입의 알고리듬 중 하나입니다.\n\n**Algorithm 1**은 아래 그림과 같이 동작합니다.\n\n[![surrogate](../../../../img/surrogate.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=3m52s)\n\n\n$M\\_i$는 $\\pi\\_i$일 때 equality가 되는 $\\eta$에 대한  surrogate function입니다. TRPO는 이 surrogate function을 최대화하고 KL divergence를 penalty가 아닌 constraint로 두는 알고리듬입니다.\n\n<br><br>\n\n# 4. Optimization of Parameterized Policies\n\n표기의 편의를 위해 다음과 같이 notation들을 더 간략하게 정의합니다.\n\n* $\\eta(\\theta):=\\eta\\left(\\pi\\_\\theta\\right)$\n* $L\\_\\theta\\left(\\tilde\\theta\\right):=L\\_{\\pi\\_\\theta}\\left(\\pi\\_{\\tilde\\theta}\\right)$\n* $D\\_\\mathrm{KL}\\left(\\theta\\parallel\\tilde\\theta\\right):=D\\_\\mathrm{KL}\\left(\\pi\\_\\theta\\parallel\\pi_{\\tilde\\theta}\\right)$\n* $\\theta\\_\\mathrm{old}$: previous policy parameter\n\n<br>\n## 4.1 Trust Region Policy Optimization\n\n이전 장의 중요 결과를 위의 notation으로 다시 표기하면 아래와 같습니다.\n\n$$\n\\eta\\left(\\theta\\right) \\geq L\\_{\\theta\\_\\mathrm{old} }\\left(\\theta\\right) - C\\cdot D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\n$$\n\n$\\eta$의 성능 향상을 보장하기 위해서 그것의 lower bound를 최대화할 수 있습니다.\n\n$$\n\\max\\_\\theta \\left[L\\_{\\theta\\_\\mathrm{old} }\\left(\\theta\\right) - C D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\\right]\n$$\n\n이 최적화 문제는 step size를 매우 작게 해야 올바른 동작을 합니다. 위에서 살펴봤듯이 first order approximation이기 때문입니다. 좀 더 큰 step size를 가질 수 있도록 이 최적화 문제를 trust region constraint를 도입하여 다음과 같이 바꿉니다.\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n이 최적화문제의 constraint는 모든 state space에 대해서 성립해야 합니다. 또한 maximum값을 매번 찾아야 합니다. state가 많은 경우 constraint의 수가 매우 많아져서 문제를 풀기 어렵게 만듭니다. constraint의 수를 줄이기 위하여 다음과 같은 avergae KL divergence를 이용하는 heuristic approximation을 취합니다. 이것이 최선의 방법은 아닐 수 있지만 실용적인 방법입니다.\n\n$$\n\\overline D\\_\\mathrm{KL}^{\\rho}\\left(\\theta\\_1, \\theta\\_2\\right):=E\\_{s\\sim\\rho}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta_1}(\\cdot\\vert s)\\parallel\\pi\\_{\\theta\\_2}(\\cdot\\vert s)\\right)\\right]\n$$\n\n이것을 기반으로 다음과 같은 최적화 문제를 풀 수 있습니다.\n\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n\n아래 그림처럼 step size에 대해서 고려할 수 있습니다.\n\n[![heuristic_approx](../../../../img/heuristic_approx.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=4m35s)\n\n<br><br>\n\n# 5. Sample-Based Estimation of the Objective and Constraint\n\n실용적인 알고리듬을 만들려고 하는 노력은 아직 끝나지 않았습니다. 이제 앞의 알고리듬을 sample-based estimation 즉, Monte Carlo estimation을 할 수 있도록 바꿔보겠습니다. sampling을 편하게 할 수 있도록 아래와 같이 바꿔줍니다.\n\n* $\\sum\\_s \\rho\\_{\\theta\\_\\mathrm{old} }(s)[\\ldots]\\rightarrow\\frac{1}{1-\\gamma}E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}[\\ldots]$\n* $A\\_{\\theta\\_\\mathrm{old} }\\rightarrow Q\\_{\\theta\\_\\mathrm{old} }$\n* $\\sum\\_a\\pi\\_{\\theta\\_\\mathrm{old} }(a\\vert s)A\\_{\\theta\\_\\mathrm{old} } \\rightarrow E\\_{a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}A\\_{\\theta\\_\\mathrm{old} }\\right]$\n\n이러한 변화를 그림처럼 도식화할 수 있습니다.\n\n[![sample-based](../../../../img/sample-based.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=5m31s)\n\n한 가지 짚고 넘어가야 할 점은 action sampling을 할 때 importance sampling을 사용한다는 것입니다.\n\n[![importance_sampling](../../../../img/importance_sampling.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=5m53s)\n\n바뀐 최적화문제는 아래와 같습니다.\n\n- **Equation (14).**\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}\n$$\n\n이 때 sampling하는 두 가지 방법이 있습니다.\n\n<br>\n## 5.1 Single Path\n\n*single path*는 개별 trajectory들을 이용하는 방법입니다.\n\n[![single](../../../../img/single.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=6m15s)\n\n<br>\n## 5.2 Vine\n\n*vine*은 한 state에서 rollout을 이용하여 여러 action을 수행하는 방법입니다.\n\n[![vine1](../../../../img/vine1.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=6m32s)\n\n[![vine2](../../../../img/vine2.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=6m49s)\n\nestimation의 variance를 낮출 수 있지만 계산량이 많고 한 state에서 여러 action을 수행할 수 있어야 하기 때문에 현실적인 문제에 적용하기에는 어려움이 있습니다.\n\n\n<br><br>\n\n# 6. Practical Algorithm\n\n앞서 single-path, vine 샘플링을 사용하는 두가지 방식의 policy optimization 알고리즘을 살펴봤습니다. 실용적인 알고리듬은 아래의 과정을 반복해서 수행합니다.\n\n1) Q-values의 몬테카를로 추정을 통해 state-action 쌍 집합을 single path 또는 vine 과정을 통해 수집함.\n\n2) 샘플 평균으로, (14)식의 목적함수와 제약식을 추정함\n + **Equation (14).**\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}\n$$\n\n3) policy parameter vector인 $\\theta$를 업데이트 하면서 제약조건이 있는 최적화 문제를 근사적으로 풂. 본 논문에서는 gradient를 직접 계산하는 것보다는 약간 더 계산량이 있는 line search와 conjugate gradient algorithm을 사용했습니다.\n\n3)에 대해서,  gradient의 covariance matrix를 사용하지 않고 KL divergence의 Hessian을 해석적으로 계산하여 Fisher Information Matrix를 구성했습니다.\n\n다시말해서\n\n$\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial}{\\partial \\theta_i} \\log \\pi_{\\theta}(a_n\\vert s_n) \\frac{\\partial}{\\partial \\theta_j} \\log \\pi_{\\theta}(a_n\\vert s_n)$ 대신 $\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} D_{KL}(\\pi_{\\theta_{old} }(\\cdot \\vert  s_n) \\vert \\vert  \\pi_{\\theta}(\\cdot \\vert  s_n))$를 계산한 것입니다.\n\n이 analytic estimator는 Hessian이나 trajectories의 모든 gradient를 저장하지 않아도 되기 때문에 대규모 환경을 고려할 경우 계산 상 이점이 있습니다. 좀 더 자세하게 설명한 Appendix C 부분을 살펴보겠습니다.\n\n\n<br>\n## Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem\n\n우리가 풀고자 하는 식은 다음과 같습니다.\n\n$$\n\\begin{align}\n\\max\\quad &L(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ } &\\overline{D}\\_{KL}(\\theta\\_\\mathrm{old}, \\theta) \\leq \\delta\n\\end{align}\n$$\n\n이 최적화 문제를 풀기 위하여 우리는 gradient 기법을 이용할 것입니다. gradient 기법은 결국 update할 방향과 크기를 결정하는 문제로 생각할 수 있습니다. 이에 따라 아래와 같은 두가지 과정으로 진행합니다.\n\n1) 탐색 방향을 계산, 목적함수의 first-order approximation(선형근사)와 제약식의 quadratic approximation(2차 근사).\n\n2) 이동거리 계산을 위해 해당 방향으로 line search 수행.\n\n탐색 방향은 $Ax = g$ 수식을 근사적으로 풀어서 구합니다. 여기서 $A$ 는 Fisher Information Matrix이고, 이것은 KL divergence 제약식의 2차 근사 $\\overline{D}\\_\\mathrm{KL} (\\theta\\_\\mathrm{old}, \\theta) \\approx \\frac{1}{2}(\\theta - \\theta\\_\\mathrm{old})^T A(\\theta - \\theta\\_\\mathrm{old})$를 푸는 것입니다. 여기서 $A\\_{ij} = \\frac{\\partial}{\\partial \\theta\\_i} \\frac{\\partial}{\\partial \\theta\\_j} \\overline{D}\\_\\mathrm{KL}(\\theta\\_\\mathrm{old}, \\theta)$입니다.\n\n논문에서 이 부분에 대한 설명이 약간 부족해서 좀 더 내용을 추가하자면, $\\frac{1}{2}x^T A x - x^T g + b = 0$과 같은 quadratic equation의 해는 1차 derivative를 0으로 만드는 $x$를 찾음으로써 알 수 있습니다. 위에서 우리는 목적함수를 1차 근사시키고 제약조건은 2차 근사를 시킴으로써 quadratic equation 형태로 변형했기 때문에 같은 방식으로 풀 수 있습니다. 따라서 $g$는 1차근사시킨 목적함수의 계수가 될 것입니다.\n\n대규모 문제에서 $A$ 혹은 $A^{-1}$ 을 계산하는 것은 엄청난 계산량이 필요합니다. 하지만 conjugate gradient algorithm을 잘 이용하면 전체 $A$행렬(FIM)을 계산하지 않아도 $Ax=g$ 를 근사적으로 해결할 수 있게 해줍니다.\n\n탐색 방향 $s \\approx A^{-1}g$과 더불어 최대스텝길이(step size) $\\beta$도 계산할 필요가 있습니다.\n\n즉, $\\delta = \\overline{D}\\_\\mathrm{KL} \\approx \\frac{1}{2}(\\beta s)^T A(\\beta s) = \\frac{1}{2}\\beta^2s^TAs$ 이고, \n\n$\\beta = \\sqrt{2\\delta / s^T As}$가 됩니다. 이때 $\\delta$는 KL divergence의 boundary term 입니다.\n\n$s^T As$ 항은 Hessian-vector product로 계산할 수 있고, conjugate gradient 과정에서 계산됩니다.\n\n마지막으로, surrogate objective와 KL divergence 제약식을 위해 다음과 같은 함수를 정의하고 line search를 사용합니다.\n\n$$\nL\\_{\\theta\\_\\mathrm{old} }(\\theta) - \\chi [\\overline{D}\\_{KL}(\\theta\\_\\mathrm{old}, \\theta) \\leq \\delta]\n$$\n\n$\\chi[\\cdot]$ 함수는 []안의 조건이 맞으면 0이고 틀리면 무한으로 발산하는 함수입니다.\n\n위에서 계산한 $\\beta$ 의 최대 값부터 시작해서 목적함수가 개선될때까지 exponential하게 줄여 나갑니다. line search가 없었다면 이 알고리즘은 매우 큰 스텝으로 계산될 것이고 심각한 성능 저하가 발생할 것입니다.\n\n(여기서 살짝 예고를 하자면, 나중에 이 KL 기반 기법을 개선한 더 단순한 방법이 제안됩니다. PPO라고...)\n\n<br><br>\n\n# 7. Connections with Prior Work\n\nNatural Policy Gradient는 $L$의 선형 근사와 $\\overline D\\_\\mathrm{KL}$ 제약식을 2차근사 하는 아래의 식의 special case입니다.\n\n- Equation (17).\n\n$L\\_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho\\_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A\\_\\pi(s,a)$\n\n$\\underset{\\theta}{\\max}\\quad \\[\\nabla\\_{\\theta}L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\ \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} } \\cdot (\\theta - \\theta\\_\\mathrm{old})]$\n\n$\\mathrm{s.t.}\\quad\\frac{1}{2}(\\theta\\_\\mathrm{old} - \\theta)^{T} A(\\theta\\_\\mathrm{old})(\\theta\\_\\mathrm{old}-\\theta) \\leq \\delta$\n\n$\\mathrm{where\\ }A(\\theta_{old})_{ij} = $\n\n$\\Large \\frac{\\partial}{\\partial \\theta\\_{i} } \\frac{\\partial}{\\partial \\theta\\_{j} }\nE\\_{s \\sim \\rho\\_{\\pi} }[D\\_\\mathrm{KL}(\\pi(\\cdot\\rvert s, \\theta\\_\\mathrm{old})  \\rvert\\rvert \\pi(\\cdot \\rvert s, \\theta))] \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} }$\n\n업데이트 식은 다음과 같습니다.\n\n$\\theta\\_\\mathrm{new} = \\theta\\_\\mathrm{old} +  \\color{Red}{\\frac{1}{\\lambda} } A(\\theta\\_\\mathrm{old})^{-1}\\nabla\\_{\\theta}L(\\theta) \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} }$\n\n여기서 step size인 $\\frac{1}{\\lambda}$ 일반적으로 알고리즘의 파라미터로 취급되지만 TRPO는 각 업데이트 마다 제약식으로 사용한다는 점이 다릅니다. 사소한 차이처럼 보이지만, 큰 차원을 다루는 실험에서 성능을 크게 향상시켰습니다.\n\n또한 $L^2$ 제약식(혹은 페널티) 를 사용하면 표준 Policy Gradient 업데이트 식을 얻었습니다.\n\n- Equation (18).\n\n$\\underset{\\theta}{\\max}\\quad[\\nabla\\_{\\theta} L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} } \\cdot (\\theta - \\theta\\_\\mathrm{old})]$\n\n$\\mathrm{s.t.}\\quad \\frac{1}{2} \\vert\\vert \\theta - \\theta\\_\\mathrm{old} \\vert\\vert^2 \\leq \\delta$\n\n\n\n$L\\_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho\\_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A\\_\\pi(s,a) $를 이용해서 제약조건없이 $\\underset{\\pi}{\\max}\\quad L\\_{\\pi\\_\\mathrm{old} }(\\pi)$를 풀면\nPolicy Iteration update를 하는 것과 같습니다.\n\n<br><br>\n\n# 8. Experiments\n\n- 다음과 같은 3 가지 궁금증을 해결하기 위하여 실험을 설계하였습니다.\n\n    1. Single-path 와 vine 의 성능면에서 특징을 알고싶음.\n\n    2. Fixed penalty coefficient(NPG) 보다 fixed KL divergence를 사용하는 것(TRPO)으로    \n    변경한 것이 얼마나 뚜렷한 차이를 보일지, 성능 면에서 어떤 영향을 미치는지 알고싶음.\n\n    3. TRPO가 큰 스케일의 문제를 해결할 수 있는지, 기존에 연구/적용되어왔던 방법들과\n    성능, 계산시간, 샘플 복잡도 면에서 비교하고 싶음.\n\n\n- 1, 2의 궁금증에 대해서 실험 환경에 single path, vine을 비롯한 여러 사전방법들을 함께 실험하였습니다.\n\n- 3에 대해선, robotic locomotion과 Atari game에 TRPO를 실험하였습니다.\n\n\n<br>\n## 8.1 Simulated Robotic Locomotion\n\n- MuJoCo 시뮬레이터를 활용해 로봇 locomotion 실험 진행하였습니다.\n- Robot State: positions and velocities, joint torques\n\n![](https://i.imgur.com/bs5ATS3.png)\n\n- 수영, 점프, 걷기 행동을 학습시키고자 합니다.\n\n    - 수영: 10 dimensional state space , reward: $r(x,u)=v_x - 10^{-5}\\vert \\vert u\\vert \\vert ^2$, quadratic penalty.\n\n    - 점프: 12 dimensional state space, episode의 끝은 높이와 각도 threshol로 점프를 판단, non-terminal state에 대해 +1 보상 추가.\n\n    - 걷기: 18 dimensional state space, 뜀뛰는 듯한 보법(점프에이전트)보다 부드러운 보법을 유도하기위해 페널티를 추가함.\n\n**Detailed Experiment Setup and used parameters, used network model**\n![](https://i.imgur.com/zgnsbw6.png)\n\n![](https://i.imgur.com/FqdWC53.png)\n\nequation (12) : $\\max\\quad L\\_{\\theta\\_\\mathrm{old}(\\theta)}, \\ \\ \\mathrm{s.t.\\ }\\overline{D}\\_\\mathrm{KL}^{\\rho\\_{\\theta\\_\\mathrm{old} }}(\\theta\\_\\mathrm{old}, \\theta) \\leq \\delta$\n\n이 실험에서 $\\delta = 0.01$입니다.\n\n- 비교에 사용된 모델들\n\n    - TRPO Single-path\n    - TRPO vine\n    - CEM(cross-entropy method)\n    - CMA(covariance matrix adaptation)\n    - NPG (Lagrange Multiplier penalty coefficien를 사용하는 것이 차이점)\n    - Empirical FIM\n    - maximum KL(cartpole)\n\n![](https://i.imgur.com/wSqolvS.png)\n\n- 제안한 single path, vine 모두 task를 잘 학습하는 것을 확인할 수 있습니다.\n\n- 페널티를 고정하는 것보다, KL divergenc를 제약하는 것이 step size를 선택하는 부분에서 Robust합니다.\n\n- CEM/CMA는 derivative free 알고리즘으로, 샘플 복잡도가 네트워크 파라미터수 만큼 확장되어 high dimension 문제에서는 속도와 성능이 약한 모습을 보입니다.\n\n- maxKL은 제안방법보다 느린 학습성능을 보입니다.\n\nTRPO video link: https://www.youtube.com/watch?v=jeid0wIrSn4\n\n- 사전지식이 적은 상태에서 여러 액션을 학습하는 실험으로 TRPO의 성능을 보입니다.\n\n- 지금까지의 균형이나 걸음 같은 개념을 명시적으로 인코딩한 robotic locomotion에서의 사전 연구들과 차이를 보였습니다.\n\n<br>\n## 8.2 Playing Games from Images\n\n- 부분관찰가능하며 복잡한 observation인 환경에서 TRPO를 평가하기위해 raw image를 입력으로 하는 아타리게임 환경에서 실험하였습니다.\n\n* Challenging point :\n    - High dimensional space\n    - delayed reward\n    - non-stationary image (Enduro는 배경 이미지가 바뀌고, 반전처리가 됨)\n    - complex action sequence (Q*bert는 21개의 다른 발판에서 점프 해야함)\n\n- ALE(Arcade Learning Environment) 에서 테스트\n- DQN과 전처리 방식은 동일\n    210 x 160 pixel -> gray-scaling & down-sampling -> 110 x 84  -> crop -> 84 x 84\n\n- 2 convolution layer ( 16 channels) , stride 2, FC layer with 20 units\n\n![](https://i.imgur.com/NJBC69d.png) \n\n![](https://i.imgur.com/wTe1OEW.png)\n\n![](https://i.imgur.com/j22VtNQ.png)\n\n- 30시간 동안 500 iteration 학습을 하였습니다.\n\n- DQN과 MCST를 사용한 모델(UCC-I), 사람이 플레이한 것(Human) 과의 비교하였습니다.\n\n- 이전 방법들에 비해 압도적인 점수를 기록하진 못했으나, 다양한 게임에서 적절한 게임 기록하였습니다.\n\n- 이전 방법들과 다른 점은, 특정 task에 초점을 맞춰 설계하지않았는데 robotics나 game playing Task에도 적절한 성능을 내는 점에서 **TRPO의 일반화 성능**을 확인하였습니다.\n\n<br><br>\n\n# 9. Discussion\n\n- Trust Region Policy Optimization 을 제안하였습니다.\n\n- KL Divergence 페널티로 $\\eta(\\pi)$를 최적화하는 알고리즘이 monotonically improve 함을 증명하였습니다.\n\n- Locomotion 도메인에서 여러 행동(수영, 걷기, 점프)을 제어하는 controller를 학습하였습니다.\n\n- Robotics와 게임 실험을 결합해, 시각정보와 센서데이터를 사용하는 로봇제어 정책을 학습시킬 수 있는 가능성을 보았습니다.\n\n- 샘플의 복잡도를 상당히 줄일 수 있어 실제상황에 적용가능성을 보았습니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [NPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/)\n\n## [NPG Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py)\n\n<br>\n\n# 다음으로\n\n## [TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py)\n\n## [GAE 여행하기](https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/)\n","source":"_posts/5_gail.md","raw":"---\ntitle: Generative Adversarial Imitation Learning\ndate: 2019-02-13\ntags: [\"프로젝트\", \"GAIL하자!\"]\ncategories: 프로젝트\nauthor: 이승현\nsubtitle: Inverse RL 5번째 논문\n---\n\n<center> <img src=\"../../../../img/irl/gail_1.png\" width=\"850\"> </center>\n\nAuthor : Jonathan Ho, Stefano Ermon\nPaper Link : https://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf\nProceeding : Advances in Neural Information Processing Systems (NIPS) 2016\n\n---\n\n# 0. Abstract\n\nTrust region policy optimization (TRPO)는 상당히 우수한 성능을 보여주는 policy gradient 기법으로 알려져 있습니다. 높은 차원의 action space를 가진 robot locomotion부터 action은 적지만 화면을 그대로 처리하여 플레이하기 때문에 control parameter가 매우 많은 Atari game까지 각 application에 세부적으로 hyperparameter들을 특화시키지 않아도 두루두루 좋은 성능을 나타내기 때문에 일반화성능이 매우 좋은 기법입니다. 이 TRPO에 대해서 알아보겠습니다.\n\n※TRPO를 매우 재밌게 설명한 Crazymuse AI의 [Youtube video](https://www.youtube.com/watch?v=CKaN5PgkSBc&t=90s)에서 일부 그림을 차용했습니다. 이 비디오를 시청하시는 것을 강력하게 추천합니다!\n\n<br>\n## 1.1 TRPO 흐름 잡기\n\nTRPO 논문은 많은 수식이 등장하여 이 수식들을 따라가다보면 큰 그림을 놓칠 수도 있습니다. 세부적인 내용을 살펴보기 전에 기존 연구에서 출발해서 TRPO로 어떻게 발전해나가는지 간략하게 살펴보겠습니다.\n\n### 1.1.1 Original Problem\n\n$$\\max\\_\\pi \\eta(\\pi)$$\n\n모든 강화학습이 그렇듯이 expected discounted reward를 최대화하는 policy를 찾는 문제로부터 출발합니다.\n\n### 1.1.2 Conservative policy iteration\n\n$$\\max L\\_\\pi(\\tilde\\pi) = \\eta(\\pi) + \\sum\\_s \\rho_\\pi(s)\\sum\\_a\\tilde\\pi(a\\vert s)A\\_\\pi(s,a)$$\n\n$\\eta(\\pi)$를 바로 최대화하는 것은 많은 경우 어렵습니다. $\\eta(\\pi)$의 성능향상을 보장하면서 policy를 update하는 conservative policy iteration 기법이 [Kakade와 Langford](http://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf)에 의하여 제안되었습니다. 이 기법을 이용하면 policy update가 성능을 향상시키는지는 못하더라도 최소한 성능을 악화시키지는 않는다는 것이 이론적으로 보장됩니다.\n\n### 1.1.3 Theorem 1 of TRPO\n\n$$\\max L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\quad\\left(\\alpha=D\\_\\mathrm{TV}^\\max\\left(\\pi\\_\\mathrm{old},\\pi\\_\\mathrm{new}\\right)\\right)$$\n\n기존의 conservative policy iteration은 과거 policy와 새로운 policy를 섞어서 사용해서 실용적이지 않다는 단점이 있었는데 이것을 보완하여 온전히 새로운 policy만으로 update할 수 있는 기법을 제안합니다. \n\n### 1.1.4 KL divergence version of Theorem 1\n\n$$\\max L\\_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\pi,\\tilde\\pi\\right),\\quad\\left(C = \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\right)$$\n\ndistance metric을 KL divergence로 바꿀 수 있습니다.\n\n### 1.1.5 Using parameterized policy\n\n$$\\max\\_\\theta L\\_{\\theta\\_\\mathrm{old} }(\\theta) - C\\cdot D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)$$\n\n최적화문제를 더욱 편리하게 풀 수 있도록 낮은 dimension을 가지는 parameter들로 parameterized된 policy를 사용할 수 있습니다.\n\n### 1.1.6 Trust region constraint\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n1.1.5까지는 아직 conservative policy iteration을 약간 변형시킨 것입니다. policy를 update할 때 지나치게 많이 변하는 것을 방지하기 위하여 [trust region](https://en.wikipedia.org/wiki/Trust_region)을 constraint로 설정할 수 있습니다. 이 아이디어로 인해서 TRPO라는 명칭을 가지게 됩니다.\n\n### 1.1.7 Heuristic approximation\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n사실 1.1.6의 constraint는 모든 state에 대해서 성립해야 하기 때문에 문제를 매우 어렵게 만듭니다. 이것을 좀 더 다루기 쉽게 state distribution에 대한 평균을 취한 것으로 변형합니다.\n\n### 1.1.8 Monte Carlo simulation\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}$$\n\nSampling을 통한 계산이 가능하도록 식을 다시 표현할 수 있습니다. \n\n### 1.1.9 Efficiently solving TRPO\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &\\nabla\\_\\theta \\left. L\\_{\\theta\\_\\mathrm{old} }(\\theta)\\right\\vert \\_{\\theta=\\theta\\_\\mathrm{old} } \\left(\\theta - \\theta\\_\\mathrm{old}\\right) \\\\\\\\\n\\mathrm{s.t.\\ }&\\frac{1}{2}\\left(\\theta\\_\\mathrm{old} - \\theta \\right)^T A\\left(\\theta\\_\\mathrm{old}\\right)\\left(\\theta\\_\\mathrm{old} - \\theta \\right) \\leq \\delta \\\\\\\\\n&A\\_{ij} = \\frac{\\partial}{\\partial \\theta\\_i}\\frac{\\partial}{\\partial \\theta\\_j}\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\n\\end{align}$$\n\n문제를 효율적으로 풀기 위하여 approximation을 적용할 수 있습니다. objective function은 first order approximation, constraint는 quadratic approximation을 취하면 효율적으로 문제를 풀 수 있는 형태로 바뀌는데 이것을 natural gradient로 풀 수도 있고 conjugate gradient로 풀 수도 있습니다.\n\nTRPO는 이렇게 다양한 방법으로 문제를 변형한 것입니다! 이제 좀 더 자세히 살펴보겠습니다.\n\n<br><br>\n\n# 2. Preliminaries\n\n다음과 같은 파라미터를 가지는 infinite-horizon discounted Markov decision process (MDP)를 고려합니다. \n\n* $\\mathcal{S}$: finite set of states\n* $\\\\mathcal{A}$: finite set of actions\n* $P: \\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$: transition probability distribution\n* $r: \\mathcal{S}\\rightarrow\\mathbb{R}$: reward function\n* $\\rho\\_0:\\mathcal{S}\\rightarrow\\mathbb{R}$: distribution of the initial state $s\\_0$\n* $\\gamma\\in(0,1)$: discount factor\n* $\\eta(\\pi)=E\\_{s\\_0,a_0,\\ldots}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r\\left(s\\_t\\right)\\right]$, where $s\\_0\\sim\\rho\\_0\\left(s\\_0\\right), a\\_t\\sim\\pi\\left(\\left.a\\_t \\right\\vert s\\_t\\right), s\\_{t+1}\\sim P\\left(\\left.s\\_{t+1}\\right\\vert s\\_t,a\\_t\\right)$\n* $Q\\_\\pi\\left(s\\_t,a\\_t\\right)=E\\_{ { \\color{red}{s\\_{t+1}} },a\\_{t+1},\\ldots}\\left[\\sum\\_{l=0}^\\infty \\gamma^l r\\left(s\\_{t+l}\\right)\\right]$: action value function\n* $V\\_\\pi\\left(s\\_t\\right)=E\\_{ { \\color{red}{a\\_t} }, s\\_{t+1},a\\_{t+1},\\ldots}\\left[\\sum\\_{l=0}^\\infty \\gamma^l r\\left(s\\_{t+l}\\right)\\right]$: value function (action value function과 expectation의 첨자가 다른 부분을 유의하세요.)\n* $A\\_\\pi(s,a) = Q\\_\\pi(s,a) - V\\_\\pi(s)$: advantage function\n\n<br>\n## 2.1 Useful identity [Kakade & Langford 2002]\n\n우리의 목표는 $\\eta\\(\\pi\\)$가 최대화되도록 만드는 것입니다. 하지만 $\\pi$의 변화에 따라 $\\eta$가 어떻게 변하는지 알아내는 것도 쉽지 않습니다. $\\pi$는 기존의 policy이고 $\\tilde\\pi$는 새로운 policy를 나타낸다고 할 때 $\\eta$와 policy update 사이에 다음과 같은 관계가 있다는 것이 밝혀졌습니다.\n\n**Lemma 1.** $\\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E\\_{s\\_{0},a\\_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right]$\n\n*Proof.* $A\\_\\pi(s,a)=E\\_{s'\\sim P\\left(s'\\vert s,a\\right)}\\left[r(s)+\\gamma V\\_\\pi\\left(s'\\right)-V\\_\\pi(s)\\right]$로 다시 표현할 수 있습니다. 표기의 편의를 위하여 $\\tau:=\\left(s\\_0,a\\_0,s\\_1,a\\_1,\\ldots\\right)$를 정의하겠습니다. 다음과 같은 수식 전개가 가능합니다.\n$$\n\\begin{align}\n&E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right] \\\\\\\\\n &= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t \\left(r(s\\_t)+\\gamma V\\_\\pi\\left(s\\_{t+1}\\right)-V\\_\\pi(s\\_t)\\right)\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\left(\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right)+\\gamma V\\_\\pi\\left(s\\_{1}\\right)-V\\_\\pi(s\\_0)+\\gamma^2 V\\_\\pi\\left(s\\_{2}\\right)-\\gamma V\\_\\pi(s\\_1)+\\gamma^3 V\\_\\pi\\left(s\\_{3}\\right)-\\gamma^2 V\\_\\pi(s\\_2)+\\cdots\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)+\\color{red}{\\gamma V\\_\\pi\\left(s\\_{1}\\right)}-V\\_\\pi(s\\_0)+\\color{red}{\\gamma^2 V\\_\\pi\\left(s\\_{2}\\right)}-\\color{red}{\\gamma V\\_\\pi(s\\_1)}+\\cdots\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[-V\\_\\pi(s\\_0)+\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right] \\\\\\\\\n&\\overset{\\underset{\\mathrm{(a)} }{} }{=} -E\\_{\\color{red}{s\\_0}}\\left[V\\_\\pi(s\\_0)\\right]+E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right] \\\\\\\\\n&=-\\eta(\\pi) + \\eta\\left(\\tilde\\pi\\right)\\\\\\\\\n&\\quad\\therefore \\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E\\_{s\\_{0},a\\_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right]\n\\end{align}\n$$\n위의 전개의 (a) 부분은 $V\\_\\pi\\left(s\\_0\\right)=E\\_{a\\_0,a\\_1,s\\_1,\\ldots}\\left[\\sum\\_{t=0}^\\infty\\gamma^l r\\left(s\\_l\\right)\\right]$이므로 $\\tau$ 중 많은 부분들이 이미 expectation이 취해진 값이므로 무시됩니다. 오직 $s\\_0$만 expectation이 취해지지 않았기 때문에 남아있습니다.\n\n**Lemma 1**은 새로운 policy와 기존의 policy 사이의 관계를 규정합니다. 다음과 같은 식을 정의하고 이것을 이용해서 **Lemma 1**을 변형시켜 봅시다.\n\n* $\\rho\\_\\pi(s) = P\\left(s\\_0=s\\right) + \\gamma P\\left(s\\_1=s\\right)\\ + \\gamma^2 P\\left(s\\_2=s\\right) + \\cdots $: (unnormalized) discounted visitation frequencies\n\n\n\n$$\n\\begin{align}\n\\eta\\left(\\tilde\\pi\\right) &= \\eta(\\pi) + \\sum\\_{t=0}^\\infty\\sum\\_{s}P\\left(\\left.s\\_t=s\\right\\vert \\tilde\\pi\\right)\\sum\\_a\\tilde\\pi(a\\vert s)\\gamma^t A\\_\\pi(s,a) \\\\\\\\\n&= \\eta(\\pi) + \\sum\\_{s}{\\color{red}{\\sum\\_{t=0}^\\infty \\gamma^t P\\left(\\left.s\\_t=s\\right\\vert \\tilde\\pi\\right)} } \\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\\\\\\\\n&= \\eta(\\pi) + \\sum\\_{s}{\\color{red}{\\rho\\_\\tilde\\pi(s)} }\\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\\\\\\\\n\\end{align}\n$$\n\n이 수식의 의미가 무엇일까요? 만약 $\\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\geq 0$이라면 $\\eta(\\tilde\\pi)$는 항상 $\\eta(\\pi)$보다 큽니다. 즉, policy를 업데이트함으로써 항상 개선됩니다. 즉, 이 수식을 통해서 항상 더 좋은 성능을 내는 policy update가 가져야 할 특징을 알 수 있습니다. 다음과 같은 deterministic policy가 있다고 합시다.\n\n$$\n\\tilde\\pi(s) = \\arg\\max_a A\\_\\pi(s,a)\n$$\n\n이 policy는 적어도 하나의 state-action pair에서 0보다 큰 값을 가지는 advantage가 있고 그 때의 확률이 0이 아니라면 항상 성능을 개선시킵니다. 어려운 점은 policy를 바꾸면 $\\rho$도 바뀐다는 점입니다. \n\n다음 그림을 봅시다. Starting Point에서 여러가지 경로를 거쳐서 Destination으로 갈 수 있습니다. 다른 policy를 이용하는 것은 다른 경로를 이용한 것입니다.\n\n[![policy_change](../../../../img/policy_change.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=16s)\n\n이 때 다른 policy를 이용함에 따라 state visitation frequency도 변하게 됩니다.\n\n[![state_visitation_change](../../../../img/state_visitation_change.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=36s)\n\n이로 인해서 policy를 최적화하는 것은 어려워집니다. 그래서 이러한 변화를 무시하는 다음과 같은 approximation을 취합니다.\n\n$$\n\\begin{align}\nL_\\pi\\left(\\tilde\\pi\\right) &= \\eta(\\pi) + \\sum\\_{s}{\\color{red}{\\rho\\_\\pi(s)} } \\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a)\n\\end{align}\n$$\n\npolicy가 바뀌었음에도 이전의 state distribution을 계속 이용하는 것입니다. 이것은 policy의 변화가 크지 않다면 어느 정도 허용될 수 있을 것입니다. 그렇지만 얼마나 많은 변화가 허용될까요? 이것을 정하기 위해서 이용하는 것이 trust region입니다.\n\n<br>\n## 2.2 Conservative Policy Iteration\n\npolicy의 변화를 다루기 용이하게 하기 위해서 policy를 다음과 같이 파라미터를 이용해서 표현합시다.\n\n* $\\pi\\_\\theta(a\\vert s)$: parameterized policy\n\n$\\pi\\_\\theta$는 $\\theta$에 대하여 미분가능한 함수입니다. $L\\_\\pi\\left(\\tilde\\pi\\right)$을 $\\theta\\_0$에서 $\\eta(\\pi)$에 대한 first order approximation이라고 하면 다음 식이 성립합니다.\n\n$$\n\\begin{align}\nL\\_{\\pi\\_{\\theta\\_0} }\\left(\\pi\\_{\\theta_0}\\right) &= \\eta\\left(\\pi\\_{\\theta\\_0}\\right) \\\\\\\\\n\\nabla\\_\\theta \\left.L\\_{\\pi\\_{\\theta\\_0} }\\left(\\pi\\_{\\theta\\_0}\\right)\\right\\vert \\_{\\theta=\\theta\\_0} &= \\nabla\\_\\theta\\left.\\eta(\\pi\\_{\\theta\\_0})\\right\\vert \\_{\\theta=\\theta\\_0}\n\\end{align}\n$$\n\n이것의 의미는 $\\pi\\_{\\theta\\_0}$가 매우 작게 변한다면 $L\\_{\\pi\\_{\\theta\\_0} }$를 개선시키는 것이 $\\eta$를 개선시키는 것이라는 것입니다. 그러나 지금까지의 설명만으로는 $\\pi\\_{\\theta\\_0}$를 얼마나 작게 변화시켜야 할지에 대해서는 알 수 없습니다.\n\nKakade & Langford가 2002년 발표한 논문에서도 이것에 대해서 고민했습니다. 그 논문에서 *conservative policy iteration*이라는 기법을 제안합니다. 그 논문의 contribution은 다음과 같습니다.\n\n* $\\eta$ 개선의 lower bound를 제공\n* 기존의 policy를 $\\pi\\_\\mathrm{old}$라고 하고 $\\pi'$를 $\\pi'=\\arg\\max\\_\\{\\pi'}L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi'\\right)$과 같이 정의할 때, 새로운 mixture policy $\\pi\\_\\mathrm{new}$를 다음과 같이 제안\n$$\n\\pi\\_\\mathrm{new}(a\\vert s) = (1-\\alpha)\\pi\\_\\mathrm{old}(a\\vert s) + \\alpha \\pi'(a\\vert s)\n$$\n\n그림으로 표현하면 다음과 같습니다.\n\n[![mixure_policy](../../../../img/mixure_policy.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=2m46s)\n\n* 다음과 같은 lower bound를 정의\n$$\n\\eta\\left(\\pi\\_\\mathrm{new}\\right) \\geq L\\_{\\pi\\_{\\theta\\_\\mathrm{old} }}\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{2\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\\\\\\\\n\\mathrm{where\\ }\\epsilon = \\max_s\\left\\vert E\\_{a\\sim\\pi'(a\\vert s)}\\left[A\\_\\pi(s,a)\\right]\\right\\vert \n$$\n\n하지만 mixture된 policy라는 것은 실용적이지 않습니다. \n\n<br><br>\n\n# 3. Monotonic Improvement Guarantee for General Stochastic Policies\n\n이전 장에서 설명한 lower bound는 오직 mixture policy에 대해서만 성립하고 더 많이 사용되는 stochastic policy에는 적용되지 않습니다. 따라서 stochastic policy를 이용할 수 있도록 개선할 필요가 있습니다. 아래와 같이 기존 수식에서 두 가지를 바꿈으로써 이것이 가능합니다.\n\n* $\\alpha\\rightarrow$ distance measure between $\\pi$ and $\\tilde\\pi$\n* constant $\\epsilon\\rightarrow\\max\\_{s,a}\\left\\vert A\\_\\pi(s,a)\\right\\vert $\n\n여기서 distance measure로 total variation divergence를 이용합니다. discrete porbability distribution $p$와 $q$에 대하여 다음과 같이 정의됩니다.\n$$\nD\\_\\mathrm{TV}(p\\parallel q) = \\frac{1}{2}\\sum\\_i\\left\\vert p\\_i - q\\_i\\right\\vert \n$$\n\n이것을 이용하여 $D\\_\\mathrm{TV}^\\max$를 다음과 같이 정의합니다.\n$$\nD\\_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D\\_\\mathrm{TV}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)\n$$\n\n그림으로 표현하면 다음과 같습니다.\n\n[![tvd](../../../../img/tvd.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=3m15s)\n\n이것을 이용하여 다음과 같은 관계식을 얻을 수 있습니다.\n\n**Theorem 1.** Let $\\alpha=D\\_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi)$. Then the following bound holds:\n$$\n\\begin{align}\n\\eta\\left(\\pi\\_\\mathrm{new}\\right) &\\geq L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2, \\\\\\\\\n\\mathrm{where\\ } \\epsilon&= \\max\\_{s,a}\\left\\vert A\\_\\pi(s,a)\\right\\vert \n\\end{align}\n$$\n<!--*Proof.* TBD.-->\n\n또다른 distance metric으로 아래 그림과 같은 KL divergence가 있습니다.\n(그런데 왜 하필 KL divergence로 바꿀까요? 논문의 뒤쪽에서 계산효율을 위해서 conjugate gradient method를 이용하는데 이를 위해서 바꾼게 아닐까 싶습니다. Wasserstein distance 같은 다른 방향으로 발전시킬 수도 있을 것 같습니다. Schulmann이 아마 해봤겠죠?^^a)\n\n[![kld](../../../../img/kld.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=3m34s)\n\ntotal variation divergence와 KL divergence 사이에는 다음과 같은 관계가 있습니다.\n$$\nD\\_\\mathrm{TV}(p\\parallel q)^2 \\leq D\\_\\mathrm{KL}(p\\parallel q)\n$$\n\n다음 수식을 정의합니다.\n$$\nD\\_\\mathrm{KL}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D\\_\\mathrm{KL}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)\n$$\n\n**Theorem 1**을 이용하여 다음과 같은 수식이 성립함을 알 수 있습니다.\n$$\n\\begin{align}\n\\eta\\left(\\tilde\\pi\\right) &\\geq L\\_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D\\_\\mathrm{KL}^\\max(\\pi, \\tilde\\pi), \\\\\\\\\n\\mathrm{where\\ } C&= \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\n\\end{align}\n$$\n\n이러한 policy imporvement bound를 기반으로 다음과 같은 approximate policy iteration 알고리듬을 고안해낼 수 있습니다.\n\n**Algorithm 1** Policy iteration algorithm guaranteeing non-decreasing expected return $\\eta$\n> Initialize $\\pi\\_0$   \n> **for** $i=0,1,2,\\ldots$ until convergence **do**   \n> $\\quad$Compute all advantage values $A\\_{\\pi\\_i}(s,a)$    \n> $\\quad$Solve the contstrained optimization problem    \n> $\\quad\\pi\\_{i+1}=\\arg\\max\\_\\pi\\left[L\\_{\\pi\\_i} - CD\\_\\mathrm{KL}^\\max\\left(\\pi\\_i,\\pi\\right)\\right]$     \n> $\\quad\\quad$where $C = 4\\epsilon\\gamma / (1-\\gamma)^2$    \n> $\\quad\\quad$and $L\\_{\\pi\\_i}\\left(\\pi\\right) = \\eta(\\pi\\_i) + \\sum\\_{s}\\rho\\_{\\pi\\_i}(s)\\sum\\_a\\pi(a\\vert s) A\\_{\\pi\\_i}(s,a)$    \n> **end for**\n\n**Algorithm 1**은 advantage를 정확하게 계산할 수 있다고 가정하고 있습니다. 이 알고리듬은  monotonical한 성능 증가($\\eta(\\pi\\_0)\\leq\\eta(\\pi\\_1)\\leq\\cdots$)를 한다는 것을 다음과 같이 보일 수 있습니다. $M\\_i(\\pi)=L\\_{\\pi\\_i}(\\pi) - CD\\_\\mathrm{KL}^\\max\\left(\\pi\\_i,\\pi\\right)$라고 합시다.\n\n$$\n\\begin{align}\n\\eta \\left(\\pi\\_{i+1}\\right) &\\geq M\\_i\\left(\\pi\\_{i+1}\\right)\\\\\\\\\n\\eta \\left(\\pi\\_{i}\\right) &= M\\_i\\left(\\pi\\_{i}\\right) \\\\\\\\\n\\eta \\left(\\pi\\_{i+1}\\right) - \\eta \\left(\\pi\\_{i}\\right) &\\geq M\\_i\\left(\\pi\\_{i+1}\\right) - M\\_i\\left(\\pi\\_{i}\\right)\n\\end{align}\n$$\n\n위 수식과 같이 매 iteration 마다 $M\\_i$를 최대화함으로써, $\\eta$가 감소하지 않는다는 것을 보장할 수 있습니다. 이와 같은 타입의 알고리듬을 [minorization-maximization (MM) algorithm](https://www.jstor.org/stable/pdf/27643496.pdf?casa_token=0qHamcl60WoAAAAA:-fkZ9JcA_nrY3-zbCUpqvPOgcAMgw7Gr96MajCZg2byHf8m5GU1KTSxyJJcBy1lPZbBTZVCjHHUXilh4k-iuwF91Wka4B5qdltC1IR2qMWk8q1FoV6__)이라고 합니다. EM 알고리듬도 이 타입의 알고리듬 중 하나입니다.\n\n**Algorithm 1**은 아래 그림과 같이 동작합니다.\n\n[![surrogate](../../../../img/surrogate.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=3m52s)\n\n\n$M\\_i$는 $\\pi\\_i$일 때 equality가 되는 $\\eta$에 대한  surrogate function입니다. TRPO는 이 surrogate function을 최대화하고 KL divergence를 penalty가 아닌 constraint로 두는 알고리듬입니다.\n\n<br><br>\n\n# 4. Optimization of Parameterized Policies\n\n표기의 편의를 위해 다음과 같이 notation들을 더 간략하게 정의합니다.\n\n* $\\eta(\\theta):=\\eta\\left(\\pi\\_\\theta\\right)$\n* $L\\_\\theta\\left(\\tilde\\theta\\right):=L\\_{\\pi\\_\\theta}\\left(\\pi\\_{\\tilde\\theta}\\right)$\n* $D\\_\\mathrm{KL}\\left(\\theta\\parallel\\tilde\\theta\\right):=D\\_\\mathrm{KL}\\left(\\pi\\_\\theta\\parallel\\pi_{\\tilde\\theta}\\right)$\n* $\\theta\\_\\mathrm{old}$: previous policy parameter\n\n<br>\n## 4.1 Trust Region Policy Optimization\n\n이전 장의 중요 결과를 위의 notation으로 다시 표기하면 아래와 같습니다.\n\n$$\n\\eta\\left(\\theta\\right) \\geq L\\_{\\theta\\_\\mathrm{old} }\\left(\\theta\\right) - C\\cdot D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\n$$\n\n$\\eta$의 성능 향상을 보장하기 위해서 그것의 lower bound를 최대화할 수 있습니다.\n\n$$\n\\max\\_\\theta \\left[L\\_{\\theta\\_\\mathrm{old} }\\left(\\theta\\right) - C D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\\right]\n$$\n\n이 최적화 문제는 step size를 매우 작게 해야 올바른 동작을 합니다. 위에서 살펴봤듯이 first order approximation이기 때문입니다. 좀 더 큰 step size를 가질 수 있도록 이 최적화 문제를 trust region constraint를 도입하여 다음과 같이 바꿉니다.\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n이 최적화문제의 constraint는 모든 state space에 대해서 성립해야 합니다. 또한 maximum값을 매번 찾아야 합니다. state가 많은 경우 constraint의 수가 매우 많아져서 문제를 풀기 어렵게 만듭니다. constraint의 수를 줄이기 위하여 다음과 같은 avergae KL divergence를 이용하는 heuristic approximation을 취합니다. 이것이 최선의 방법은 아닐 수 있지만 실용적인 방법입니다.\n\n$$\n\\overline D\\_\\mathrm{KL}^{\\rho}\\left(\\theta\\_1, \\theta\\_2\\right):=E\\_{s\\sim\\rho}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta_1}(\\cdot\\vert s)\\parallel\\pi\\_{\\theta\\_2}(\\cdot\\vert s)\\right)\\right]\n$$\n\n이것을 기반으로 다음과 같은 최적화 문제를 풀 수 있습니다.\n\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n\n아래 그림처럼 step size에 대해서 고려할 수 있습니다.\n\n[![heuristic_approx](../../../../img/heuristic_approx.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=4m35s)\n\n<br><br>\n\n# 5. Sample-Based Estimation of the Objective and Constraint\n\n실용적인 알고리듬을 만들려고 하는 노력은 아직 끝나지 않았습니다. 이제 앞의 알고리듬을 sample-based estimation 즉, Monte Carlo estimation을 할 수 있도록 바꿔보겠습니다. sampling을 편하게 할 수 있도록 아래와 같이 바꿔줍니다.\n\n* $\\sum\\_s \\rho\\_{\\theta\\_\\mathrm{old} }(s)[\\ldots]\\rightarrow\\frac{1}{1-\\gamma}E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}[\\ldots]$\n* $A\\_{\\theta\\_\\mathrm{old} }\\rightarrow Q\\_{\\theta\\_\\mathrm{old} }$\n* $\\sum\\_a\\pi\\_{\\theta\\_\\mathrm{old} }(a\\vert s)A\\_{\\theta\\_\\mathrm{old} } \\rightarrow E\\_{a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}A\\_{\\theta\\_\\mathrm{old} }\\right]$\n\n이러한 변화를 그림처럼 도식화할 수 있습니다.\n\n[![sample-based](../../../../img/sample-based.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=5m31s)\n\n한 가지 짚고 넘어가야 할 점은 action sampling을 할 때 importance sampling을 사용한다는 것입니다.\n\n[![importance_sampling](../../../../img/importance_sampling.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=5m53s)\n\n바뀐 최적화문제는 아래와 같습니다.\n\n- **Equation (14).**\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}\n$$\n\n이 때 sampling하는 두 가지 방법이 있습니다.\n\n<br>\n## 5.1 Single Path\n\n*single path*는 개별 trajectory들을 이용하는 방법입니다.\n\n[![single](../../../../img/single.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=6m15s)\n\n<br>\n## 5.2 Vine\n\n*vine*은 한 state에서 rollout을 이용하여 여러 action을 수행하는 방법입니다.\n\n[![vine1](../../../../img/vine1.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=6m32s)\n\n[![vine2](../../../../img/vine2.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=6m49s)\n\nestimation의 variance를 낮출 수 있지만 계산량이 많고 한 state에서 여러 action을 수행할 수 있어야 하기 때문에 현실적인 문제에 적용하기에는 어려움이 있습니다.\n\n\n<br><br>\n\n# 6. Practical Algorithm\n\n앞서 single-path, vine 샘플링을 사용하는 두가지 방식의 policy optimization 알고리즘을 살펴봤습니다. 실용적인 알고리듬은 아래의 과정을 반복해서 수행합니다.\n\n1) Q-values의 몬테카를로 추정을 통해 state-action 쌍 집합을 single path 또는 vine 과정을 통해 수집함.\n\n2) 샘플 평균으로, (14)식의 목적함수와 제약식을 추정함\n + **Equation (14).**\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}\n$$\n\n3) policy parameter vector인 $\\theta$를 업데이트 하면서 제약조건이 있는 최적화 문제를 근사적으로 풂. 본 논문에서는 gradient를 직접 계산하는 것보다는 약간 더 계산량이 있는 line search와 conjugate gradient algorithm을 사용했습니다.\n\n3)에 대해서,  gradient의 covariance matrix를 사용하지 않고 KL divergence의 Hessian을 해석적으로 계산하여 Fisher Information Matrix를 구성했습니다.\n\n다시말해서\n\n$\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial}{\\partial \\theta_i} \\log \\pi_{\\theta}(a_n\\vert s_n) \\frac{\\partial}{\\partial \\theta_j} \\log \\pi_{\\theta}(a_n\\vert s_n)$ 대신 $\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} D_{KL}(\\pi_{\\theta_{old} }(\\cdot \\vert  s_n) \\vert \\vert  \\pi_{\\theta}(\\cdot \\vert  s_n))$를 계산한 것입니다.\n\n이 analytic estimator는 Hessian이나 trajectories의 모든 gradient를 저장하지 않아도 되기 때문에 대규모 환경을 고려할 경우 계산 상 이점이 있습니다. 좀 더 자세하게 설명한 Appendix C 부분을 살펴보겠습니다.\n\n\n<br>\n## Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem\n\n우리가 풀고자 하는 식은 다음과 같습니다.\n\n$$\n\\begin{align}\n\\max\\quad &L(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ } &\\overline{D}\\_{KL}(\\theta\\_\\mathrm{old}, \\theta) \\leq \\delta\n\\end{align}\n$$\n\n이 최적화 문제를 풀기 위하여 우리는 gradient 기법을 이용할 것입니다. gradient 기법은 결국 update할 방향과 크기를 결정하는 문제로 생각할 수 있습니다. 이에 따라 아래와 같은 두가지 과정으로 진행합니다.\n\n1) 탐색 방향을 계산, 목적함수의 first-order approximation(선형근사)와 제약식의 quadratic approximation(2차 근사).\n\n2) 이동거리 계산을 위해 해당 방향으로 line search 수행.\n\n탐색 방향은 $Ax = g$ 수식을 근사적으로 풀어서 구합니다. 여기서 $A$ 는 Fisher Information Matrix이고, 이것은 KL divergence 제약식의 2차 근사 $\\overline{D}\\_\\mathrm{KL} (\\theta\\_\\mathrm{old}, \\theta) \\approx \\frac{1}{2}(\\theta - \\theta\\_\\mathrm{old})^T A(\\theta - \\theta\\_\\mathrm{old})$를 푸는 것입니다. 여기서 $A\\_{ij} = \\frac{\\partial}{\\partial \\theta\\_i} \\frac{\\partial}{\\partial \\theta\\_j} \\overline{D}\\_\\mathrm{KL}(\\theta\\_\\mathrm{old}, \\theta)$입니다.\n\n논문에서 이 부분에 대한 설명이 약간 부족해서 좀 더 내용을 추가하자면, $\\frac{1}{2}x^T A x - x^T g + b = 0$과 같은 quadratic equation의 해는 1차 derivative를 0으로 만드는 $x$를 찾음으로써 알 수 있습니다. 위에서 우리는 목적함수를 1차 근사시키고 제약조건은 2차 근사를 시킴으로써 quadratic equation 형태로 변형했기 때문에 같은 방식으로 풀 수 있습니다. 따라서 $g$는 1차근사시킨 목적함수의 계수가 될 것입니다.\n\n대규모 문제에서 $A$ 혹은 $A^{-1}$ 을 계산하는 것은 엄청난 계산량이 필요합니다. 하지만 conjugate gradient algorithm을 잘 이용하면 전체 $A$행렬(FIM)을 계산하지 않아도 $Ax=g$ 를 근사적으로 해결할 수 있게 해줍니다.\n\n탐색 방향 $s \\approx A^{-1}g$과 더불어 최대스텝길이(step size) $\\beta$도 계산할 필요가 있습니다.\n\n즉, $\\delta = \\overline{D}\\_\\mathrm{KL} \\approx \\frac{1}{2}(\\beta s)^T A(\\beta s) = \\frac{1}{2}\\beta^2s^TAs$ 이고, \n\n$\\beta = \\sqrt{2\\delta / s^T As}$가 됩니다. 이때 $\\delta$는 KL divergence의 boundary term 입니다.\n\n$s^T As$ 항은 Hessian-vector product로 계산할 수 있고, conjugate gradient 과정에서 계산됩니다.\n\n마지막으로, surrogate objective와 KL divergence 제약식을 위해 다음과 같은 함수를 정의하고 line search를 사용합니다.\n\n$$\nL\\_{\\theta\\_\\mathrm{old} }(\\theta) - \\chi [\\overline{D}\\_{KL}(\\theta\\_\\mathrm{old}, \\theta) \\leq \\delta]\n$$\n\n$\\chi[\\cdot]$ 함수는 []안의 조건이 맞으면 0이고 틀리면 무한으로 발산하는 함수입니다.\n\n위에서 계산한 $\\beta$ 의 최대 값부터 시작해서 목적함수가 개선될때까지 exponential하게 줄여 나갑니다. line search가 없었다면 이 알고리즘은 매우 큰 스텝으로 계산될 것이고 심각한 성능 저하가 발생할 것입니다.\n\n(여기서 살짝 예고를 하자면, 나중에 이 KL 기반 기법을 개선한 더 단순한 방법이 제안됩니다. PPO라고...)\n\n<br><br>\n\n# 7. Connections with Prior Work\n\nNatural Policy Gradient는 $L$의 선형 근사와 $\\overline D\\_\\mathrm{KL}$ 제약식을 2차근사 하는 아래의 식의 special case입니다.\n\n- Equation (17).\n\n$L\\_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho\\_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A\\_\\pi(s,a)$\n\n$\\underset{\\theta}{\\max}\\quad \\[\\nabla\\_{\\theta}L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\ \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} } \\cdot (\\theta - \\theta\\_\\mathrm{old})]$\n\n$\\mathrm{s.t.}\\quad\\frac{1}{2}(\\theta\\_\\mathrm{old} - \\theta)^{T} A(\\theta\\_\\mathrm{old})(\\theta\\_\\mathrm{old}-\\theta) \\leq \\delta$\n\n$\\mathrm{where\\ }A(\\theta_{old})_{ij} = $\n\n$\\Large \\frac{\\partial}{\\partial \\theta\\_{i} } \\frac{\\partial}{\\partial \\theta\\_{j} }\nE\\_{s \\sim \\rho\\_{\\pi} }[D\\_\\mathrm{KL}(\\pi(\\cdot\\rvert s, \\theta\\_\\mathrm{old})  \\rvert\\rvert \\pi(\\cdot \\rvert s, \\theta))] \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} }$\n\n업데이트 식은 다음과 같습니다.\n\n$\\theta\\_\\mathrm{new} = \\theta\\_\\mathrm{old} +  \\color{Red}{\\frac{1}{\\lambda} } A(\\theta\\_\\mathrm{old})^{-1}\\nabla\\_{\\theta}L(\\theta) \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} }$\n\n여기서 step size인 $\\frac{1}{\\lambda}$ 일반적으로 알고리즘의 파라미터로 취급되지만 TRPO는 각 업데이트 마다 제약식으로 사용한다는 점이 다릅니다. 사소한 차이처럼 보이지만, 큰 차원을 다루는 실험에서 성능을 크게 향상시켰습니다.\n\n또한 $L^2$ 제약식(혹은 페널티) 를 사용하면 표준 Policy Gradient 업데이트 식을 얻었습니다.\n\n- Equation (18).\n\n$\\underset{\\theta}{\\max}\\quad[\\nabla\\_{\\theta} L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} } \\cdot (\\theta - \\theta\\_\\mathrm{old})]$\n\n$\\mathrm{s.t.}\\quad \\frac{1}{2} \\vert\\vert \\theta - \\theta\\_\\mathrm{old} \\vert\\vert^2 \\leq \\delta$\n\n\n\n$L\\_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho\\_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A\\_\\pi(s,a) $를 이용해서 제약조건없이 $\\underset{\\pi}{\\max}\\quad L\\_{\\pi\\_\\mathrm{old} }(\\pi)$를 풀면\nPolicy Iteration update를 하는 것과 같습니다.\n\n<br><br>\n\n# 8. Experiments\n\n- 다음과 같은 3 가지 궁금증을 해결하기 위하여 실험을 설계하였습니다.\n\n    1. Single-path 와 vine 의 성능면에서 특징을 알고싶음.\n\n    2. Fixed penalty coefficient(NPG) 보다 fixed KL divergence를 사용하는 것(TRPO)으로    \n    변경한 것이 얼마나 뚜렷한 차이를 보일지, 성능 면에서 어떤 영향을 미치는지 알고싶음.\n\n    3. TRPO가 큰 스케일의 문제를 해결할 수 있는지, 기존에 연구/적용되어왔던 방법들과\n    성능, 계산시간, 샘플 복잡도 면에서 비교하고 싶음.\n\n\n- 1, 2의 궁금증에 대해서 실험 환경에 single path, vine을 비롯한 여러 사전방법들을 함께 실험하였습니다.\n\n- 3에 대해선, robotic locomotion과 Atari game에 TRPO를 실험하였습니다.\n\n\n<br>\n## 8.1 Simulated Robotic Locomotion\n\n- MuJoCo 시뮬레이터를 활용해 로봇 locomotion 실험 진행하였습니다.\n- Robot State: positions and velocities, joint torques\n\n![](https://i.imgur.com/bs5ATS3.png)\n\n- 수영, 점프, 걷기 행동을 학습시키고자 합니다.\n\n    - 수영: 10 dimensional state space , reward: $r(x,u)=v_x - 10^{-5}\\vert \\vert u\\vert \\vert ^2$, quadratic penalty.\n\n    - 점프: 12 dimensional state space, episode의 끝은 높이와 각도 threshol로 점프를 판단, non-terminal state에 대해 +1 보상 추가.\n\n    - 걷기: 18 dimensional state space, 뜀뛰는 듯한 보법(점프에이전트)보다 부드러운 보법을 유도하기위해 페널티를 추가함.\n\n**Detailed Experiment Setup and used parameters, used network model**\n![](https://i.imgur.com/zgnsbw6.png)\n\n![](https://i.imgur.com/FqdWC53.png)\n\nequation (12) : $\\max\\quad L\\_{\\theta\\_\\mathrm{old}(\\theta)}, \\ \\ \\mathrm{s.t.\\ }\\overline{D}\\_\\mathrm{KL}^{\\rho\\_{\\theta\\_\\mathrm{old} }}(\\theta\\_\\mathrm{old}, \\theta) \\leq \\delta$\n\n이 실험에서 $\\delta = 0.01$입니다.\n\n- 비교에 사용된 모델들\n\n    - TRPO Single-path\n    - TRPO vine\n    - CEM(cross-entropy method)\n    - CMA(covariance matrix adaptation)\n    - NPG (Lagrange Multiplier penalty coefficien를 사용하는 것이 차이점)\n    - Empirical FIM\n    - maximum KL(cartpole)\n\n![](https://i.imgur.com/wSqolvS.png)\n\n- 제안한 single path, vine 모두 task를 잘 학습하는 것을 확인할 수 있습니다.\n\n- 페널티를 고정하는 것보다, KL divergenc를 제약하는 것이 step size를 선택하는 부분에서 Robust합니다.\n\n- CEM/CMA는 derivative free 알고리즘으로, 샘플 복잡도가 네트워크 파라미터수 만큼 확장되어 high dimension 문제에서는 속도와 성능이 약한 모습을 보입니다.\n\n- maxKL은 제안방법보다 느린 학습성능을 보입니다.\n\nTRPO video link: https://www.youtube.com/watch?v=jeid0wIrSn4\n\n- 사전지식이 적은 상태에서 여러 액션을 학습하는 실험으로 TRPO의 성능을 보입니다.\n\n- 지금까지의 균형이나 걸음 같은 개념을 명시적으로 인코딩한 robotic locomotion에서의 사전 연구들과 차이를 보였습니다.\n\n<br>\n## 8.2 Playing Games from Images\n\n- 부분관찰가능하며 복잡한 observation인 환경에서 TRPO를 평가하기위해 raw image를 입력으로 하는 아타리게임 환경에서 실험하였습니다.\n\n* Challenging point :\n    - High dimensional space\n    - delayed reward\n    - non-stationary image (Enduro는 배경 이미지가 바뀌고, 반전처리가 됨)\n    - complex action sequence (Q*bert는 21개의 다른 발판에서 점프 해야함)\n\n- ALE(Arcade Learning Environment) 에서 테스트\n- DQN과 전처리 방식은 동일\n    210 x 160 pixel -> gray-scaling & down-sampling -> 110 x 84  -> crop -> 84 x 84\n\n- 2 convolution layer ( 16 channels) , stride 2, FC layer with 20 units\n\n![](https://i.imgur.com/NJBC69d.png) \n\n![](https://i.imgur.com/wTe1OEW.png)\n\n![](https://i.imgur.com/j22VtNQ.png)\n\n- 30시간 동안 500 iteration 학습을 하였습니다.\n\n- DQN과 MCST를 사용한 모델(UCC-I), 사람이 플레이한 것(Human) 과의 비교하였습니다.\n\n- 이전 방법들에 비해 압도적인 점수를 기록하진 못했으나, 다양한 게임에서 적절한 게임 기록하였습니다.\n\n- 이전 방법들과 다른 점은, 특정 task에 초점을 맞춰 설계하지않았는데 robotics나 game playing Task에도 적절한 성능을 내는 점에서 **TRPO의 일반화 성능**을 확인하였습니다.\n\n<br><br>\n\n# 9. Discussion\n\n- Trust Region Policy Optimization 을 제안하였습니다.\n\n- KL Divergence 페널티로 $\\eta(\\pi)$를 최적화하는 알고리즘이 monotonically improve 함을 증명하였습니다.\n\n- Locomotion 도메인에서 여러 행동(수영, 걷기, 점프)을 제어하는 controller를 학습하였습니다.\n\n- Robotics와 게임 실험을 결합해, 시각정보와 센서데이터를 사용하는 로봇제어 정책을 학습시킬 수 있는 가능성을 보았습니다.\n\n- 샘플의 복잡도를 상당히 줄일 수 있어 실제상황에 적용가능성을 보았습니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [NPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/)\n\n## [NPG Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py)\n\n<br>\n\n# 다음으로\n\n## [TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py)\n\n## [GAE 여행하기](https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/)\n","slug":"5_gail","published":1,"updated":"2019-02-07T11:21:31.950Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjrujlu1g00205wfezjgshuze","content":"<center> <img src=\"../../../../img/irl/gail_1.png\" width=\"850\"> </center>\n\n<p>Author : Jonathan Ho, Stefano Ermon<br>Paper Link : <a href=\"https://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf\" target=\"_blank\" rel=\"noopener\">https://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf</a><br>Proceeding : Advances in Neural Information Processing Systems (NIPS) 2016</p>\n<hr>\n<h1 id=\"0-Abstract\"><a href=\"#0-Abstract\" class=\"headerlink\" title=\"0. Abstract\"></a>0. Abstract</h1><p>Trust region policy optimization (TRPO)는 상당히 우수한 성능을 보여주는 policy gradient 기법으로 알려져 있습니다. 높은 차원의 action space를 가진 robot locomotion부터 action은 적지만 화면을 그대로 처리하여 플레이하기 때문에 control parameter가 매우 많은 Atari game까지 각 application에 세부적으로 hyperparameter들을 특화시키지 않아도 두루두루 좋은 성능을 나타내기 때문에 일반화성능이 매우 좋은 기법입니다. 이 TRPO에 대해서 알아보겠습니다.</p>\n<p>※TRPO를 매우 재밌게 설명한 Crazymuse AI의 <a href=\"https://www.youtube.com/watch?v=CKaN5PgkSBc&amp;t=90s\" target=\"_blank\" rel=\"noopener\">Youtube video</a>에서 일부 그림을 차용했습니다. 이 비디오를 시청하시는 것을 강력하게 추천합니다!</p>\n<p><br></p>\n<h2 id=\"1-1-TRPO-흐름-잡기\"><a href=\"#1-1-TRPO-흐름-잡기\" class=\"headerlink\" title=\"1.1 TRPO 흐름 잡기\"></a>1.1 TRPO 흐름 잡기</h2><p>TRPO 논문은 많은 수식이 등장하여 이 수식들을 따라가다보면 큰 그림을 놓칠 수도 있습니다. 세부적인 내용을 살펴보기 전에 기존 연구에서 출발해서 TRPO로 어떻게 발전해나가는지 간략하게 살펴보겠습니다.</p>\n<h3 id=\"1-1-1-Original-Problem\"><a href=\"#1-1-1-Original-Problem\" class=\"headerlink\" title=\"1.1.1 Original Problem\"></a>1.1.1 Original Problem</h3><p>$$\\max_\\pi \\eta(\\pi)$$</p>\n<p>모든 강화학습이 그렇듯이 expected discounted reward를 최대화하는 policy를 찾는 문제로부터 출발합니다.</p>\n<h3 id=\"1-1-2-Conservative-policy-iteration\"><a href=\"#1-1-2-Conservative-policy-iteration\" class=\"headerlink\" title=\"1.1.2 Conservative policy iteration\"></a>1.1.2 Conservative policy iteration</h3><p>$$\\max L_\\pi(\\tilde\\pi) = \\eta(\\pi) + \\sum_s \\rho_\\pi(s)\\sum_a\\tilde\\pi(a\\vert s)A_\\pi(s,a)$$</p>\n<p>$\\eta(\\pi)$를 바로 최대화하는 것은 많은 경우 어렵습니다. $\\eta(\\pi)$의 성능향상을 보장하면서 policy를 update하는 conservative policy iteration 기법이 <a href=\"http://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf\" target=\"_blank\" rel=\"noopener\">Kakade와 Langford</a>에 의하여 제안되었습니다. 이 기법을 이용하면 policy update가 성능을 향상시키는지는 못하더라도 최소한 성능을 악화시키지는 않는다는 것이 이론적으로 보장됩니다.</p>\n<h3 id=\"1-1-3-Theorem-1-of-TRPO\"><a href=\"#1-1-3-Theorem-1-of-TRPO\" class=\"headerlink\" title=\"1.1.3 Theorem 1 of TRPO\"></a>1.1.3 Theorem 1 of TRPO</h3><p>$$\\max L_{\\pi_\\mathrm{old} }\\left(\\pi_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\quad\\left(\\alpha=D_\\mathrm{TV}^\\max\\left(\\pi_\\mathrm{old},\\pi_\\mathrm{new}\\right)\\right)$$</p>\n<p>기존의 conservative policy iteration은 과거 policy와 새로운 policy를 섞어서 사용해서 실용적이지 않다는 단점이 있었는데 이것을 보완하여 온전히 새로운 policy만으로 update할 수 있는 기법을 제안합니다. </p>\n<h3 id=\"1-1-4-KL-divergence-version-of-Theorem-1\"><a href=\"#1-1-4-KL-divergence-version-of-Theorem-1\" class=\"headerlink\" title=\"1.1.4 KL divergence version of Theorem 1\"></a>1.1.4 KL divergence version of Theorem 1</h3><p>$$\\max L_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\pi,\\tilde\\pi\\right),\\quad\\left(C = \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\right)$$</p>\n<p>distance metric을 KL divergence로 바꿀 수 있습니다.</p>\n<h3 id=\"1-1-5-Using-parameterized-policy\"><a href=\"#1-1-5-Using-parameterized-policy\" class=\"headerlink\" title=\"1.1.5 Using parameterized policy\"></a>1.1.5 Using parameterized policy</h3><p>$$\\max_\\theta L_{\\theta_\\mathrm{old} }(\\theta) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)$$</p>\n<p>최적화문제를 더욱 편리하게 풀 수 있도록 낮은 dimension을 가지는 parameter들로 parameterized된 policy를 사용할 수 있습니다.</p>\n<h3 id=\"1-1-6-Trust-region-constraint\"><a href=\"#1-1-6-Trust-region-constraint\" class=\"headerlink\" title=\"1.1.6 Trust region constraint\"></a>1.1.6 Trust region constraint</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>1.1.5까지는 아직 conservative policy iteration을 약간 변형시킨 것입니다. policy를 update할 때 지나치게 많이 변하는 것을 방지하기 위하여 <a href=\"https://en.wikipedia.org/wiki/Trust_region\" target=\"_blank\" rel=\"noopener\">trust region</a>을 constraint로 설정할 수 있습니다. 이 아이디어로 인해서 TRPO라는 명칭을 가지게 됩니다.</p>\n<h3 id=\"1-1-7-Heuristic-approximation\"><a href=\"#1-1-7-Heuristic-approximation\" class=\"headerlink\" title=\"1.1.7 Heuristic approximation\"></a>1.1.7 Heuristic approximation</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>사실 1.1.6의 constraint는 모든 state에 대해서 성립해야 하기 때문에 문제를 매우 어렵게 만듭니다. 이것을 좀 더 다루기 쉽게 state distribution에 대한 평균을 취한 것으로 변형합니다.</p>\n<h3 id=\"1-1-8-Monte-Carlo-simulation\"><a href=\"#1-1-8-Monte-Carlo-simulation\" class=\"headerlink\" title=\"1.1.8 Monte Carlo simulation\"></a>1.1.8 Monte Carlo simulation</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}$$</p>\n<p>Sampling을 통한 계산이 가능하도록 식을 다시 표현할 수 있습니다. </p>\n<h3 id=\"1-1-9-Efficiently-solving-TRPO\"><a href=\"#1-1-9-Efficiently-solving-TRPO\" class=\"headerlink\" title=\"1.1.9 Efficiently solving TRPO\"></a>1.1.9 Efficiently solving TRPO</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;\\nabla_\\theta \\left. L_{\\theta_\\mathrm{old} }(\\theta)\\right\\vert _{\\theta=\\theta_\\mathrm{old} } \\left(\\theta - \\theta_\\mathrm{old}\\right) \\\\<br>\\mathrm{s.t.\\ }&amp;\\frac{1}{2}\\left(\\theta_\\mathrm{old} - \\theta \\right)^T A\\left(\\theta_\\mathrm{old}\\right)\\left(\\theta_\\mathrm{old} - \\theta \\right) \\leq \\delta \\\\<br>&amp;A_{ij} = \\frac{\\partial}{\\partial \\theta_i}\\frac{\\partial}{\\partial \\theta_j}\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right)<br>\\end{align}$$</p>\n<p>문제를 효율적으로 풀기 위하여 approximation을 적용할 수 있습니다. objective function은 first order approximation, constraint는 quadratic approximation을 취하면 효율적으로 문제를 풀 수 있는 형태로 바뀌는데 이것을 natural gradient로 풀 수도 있고 conjugate gradient로 풀 수도 있습니다.</p>\n<p>TRPO는 이렇게 다양한 방법으로 문제를 변형한 것입니다! 이제 좀 더 자세히 살펴보겠습니다.</p>\n<p><br><br></p>\n<h1 id=\"2-Preliminaries\"><a href=\"#2-Preliminaries\" class=\"headerlink\" title=\"2. Preliminaries\"></a>2. Preliminaries</h1><p>다음과 같은 파라미터를 가지는 infinite-horizon discounted Markov decision process (MDP)를 고려합니다. </p>\n<ul>\n<li>$\\mathcal{S}$: finite set of states</li>\n<li>$\\mathcal{A}$: finite set of actions</li>\n<li>$P: \\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$: transition probability distribution</li>\n<li>$r: \\mathcal{S}\\rightarrow\\mathbb{R}$: reward function</li>\n<li>$\\rho_0:\\mathcal{S}\\rightarrow\\mathbb{R}$: distribution of the initial state $s_0$</li>\n<li>$\\gamma\\in(0,1)$: discount factor</li>\n<li>$\\eta(\\pi)=E_{s_0,a_0,\\ldots}\\left[\\sum_{t=0}^\\infty\\gamma^t r\\left(s_t\\right)\\right]$, where $s_0\\sim\\rho_0\\left(s_0\\right), a_t\\sim\\pi\\left(\\left.a_t \\right\\vert s_t\\right), s_{t+1}\\sim P\\left(\\left.s_{t+1}\\right\\vert s_t,a_t\\right)$</li>\n<li>$Q_\\pi\\left(s_t,a_t\\right)=E_{ { \\color{red}{s_{t+1}} },a_{t+1},\\ldots}\\left[\\sum_{l=0}^\\infty \\gamma^l r\\left(s_{t+l}\\right)\\right]$: action value function</li>\n<li>$V_\\pi\\left(s_t\\right)=E_{ { \\color{red}{a_t} }, s_{t+1},a_{t+1},\\ldots}\\left[\\sum_{l=0}^\\infty \\gamma^l r\\left(s_{t+l}\\right)\\right]$: value function (action value function과 expectation의 첨자가 다른 부분을 유의하세요.)</li>\n<li>$A_\\pi(s,a) = Q_\\pi(s,a) - V_\\pi(s)$: advantage function</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-1-Useful-identity-Kakade-amp-Langford-2002\"><a href=\"#2-1-Useful-identity-Kakade-amp-Langford-2002\" class=\"headerlink\" title=\"2.1 Useful identity [Kakade &amp; Langford 2002]\"></a>2.1 Useful identity [Kakade &amp; Langford 2002]</h2><p>우리의 목표는 $\\eta(\\pi)$가 최대화되도록 만드는 것입니다. 하지만 $\\pi$의 변화에 따라 $\\eta$가 어떻게 변하는지 알아내는 것도 쉽지 않습니다. $\\pi$는 기존의 policy이고 $\\tilde\\pi$는 새로운 policy를 나타낸다고 할 때 $\\eta$와 policy update 사이에 다음과 같은 관계가 있다는 것이 밝혀졌습니다.</p>\n<p><strong>Lemma 1.</strong> $\\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E_{s_{0},a_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right]$</p>\n<p><em>Proof.</em> $A_\\pi(s,a)=E_{s’\\sim P\\left(s’\\vert s,a\\right)}\\left[r(s)+\\gamma V_\\pi\\left(s’\\right)-V_\\pi(s)\\right]$로 다시 표현할 수 있습니다. 표기의 편의를 위하여 $\\tau:=\\left(s_0,a_0,s_1,a_1,\\ldots\\right)$를 정의하겠습니다. 다음과 같은 수식 전개가 가능합니다.<br>$$<br>\\begin{align}<br>&amp;E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right] \\\\<br> &amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t \\left(r(s_t)+\\gamma V_\\pi\\left(s_{t+1}\\right)-V_\\pi(s_t)\\right)\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\left(\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right)+\\gamma V_\\pi\\left(s_{1}\\right)-V_\\pi(s_0)+\\gamma^2 V_\\pi\\left(s_{2}\\right)-\\gamma V_\\pi(s_1)+\\gamma^3 V_\\pi\\left(s_{3}\\right)-\\gamma^2 V_\\pi(s_2)+\\cdots\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t)+\\color{red}{\\gamma V_\\pi\\left(s_{1}\\right)}-V_\\pi(s_0)+\\color{red}{\\gamma^2 V_\\pi\\left(s_{2}\\right)}-\\color{red}{\\gamma V_\\pi(s_1)}+\\cdots\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[-V_\\pi(s_0)+\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right] \\\\<br>&amp;\\overset{\\underset{\\mathrm{(a)} }{} }{=} -E_{\\color{red}{s_0}}\\left[V_\\pi(s_0)\\right]+E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right] \\\\<br>&amp;=-\\eta(\\pi) + \\eta\\left(\\tilde\\pi\\right)\\\\<br>&amp;\\quad\\therefore \\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E_{s_{0},a_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right]<br>\\end{align}<br>$$<br>위의 전개의 (a) 부분은 $V_\\pi\\left(s_0\\right)=E_{a_0,a_1,s_1,\\ldots}\\left[\\sum_{t=0}^\\infty\\gamma^l r\\left(s_l\\right)\\right]$이므로 $\\tau$ 중 많은 부분들이 이미 expectation이 취해진 값이므로 무시됩니다. 오직 $s_0$만 expectation이 취해지지 않았기 때문에 남아있습니다.</p>\n<p><strong>Lemma 1</strong>은 새로운 policy와 기존의 policy 사이의 관계를 규정합니다. 다음과 같은 식을 정의하고 이것을 이용해서 <strong>Lemma 1</strong>을 변형시켜 봅시다.</p>\n<ul>\n<li>$\\rho_\\pi(s) = P\\left(s_0=s\\right) + \\gamma P\\left(s_1=s\\right)\\ + \\gamma^2 P\\left(s_2=s\\right) + \\cdots $: (unnormalized) discounted visitation frequencies</li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\eta\\left(\\tilde\\pi\\right) &amp;= \\eta(\\pi) + \\sum_{t=0}^\\infty\\sum_{s}P\\left(\\left.s_t=s\\right\\vert \\tilde\\pi\\right)\\sum_a\\tilde\\pi(a\\vert s)\\gamma^t A_\\pi(s,a) \\\\<br>&amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}{\\sum_{t=0}^\\infty \\gamma^t P\\left(\\left.s_t=s\\right\\vert \\tilde\\pi\\right)} } \\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\\\<br>&amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}{\\rho_\\tilde\\pi(s)} }\\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\\\<br>\\end{align}<br>$$</p>\n<p>이 수식의 의미가 무엇일까요? 만약 $\\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\geq 0$이라면 $\\eta(\\tilde\\pi)$는 항상 $\\eta(\\pi)$보다 큽니다. 즉, policy를 업데이트함으로써 항상 개선됩니다. 즉, 이 수식을 통해서 항상 더 좋은 성능을 내는 policy update가 가져야 할 특징을 알 수 있습니다. 다음과 같은 deterministic policy가 있다고 합시다.</p>\n<p>$$<br>\\tilde\\pi(s) = \\arg\\max_a A_\\pi(s,a)<br>$$</p>\n<p>이 policy는 적어도 하나의 state-action pair에서 0보다 큰 값을 가지는 advantage가 있고 그 때의 확률이 0이 아니라면 항상 성능을 개선시킵니다. 어려운 점은 policy를 바꾸면 $\\rho$도 바뀐다는 점입니다. </p>\n<p>다음 그림을 봅시다. Starting Point에서 여러가지 경로를 거쳐서 Destination으로 갈 수 있습니다. 다른 policy를 이용하는 것은 다른 경로를 이용한 것입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=16s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/policy_change.png\" alt=\"policy_change\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>이 때 다른 policy를 이용함에 따라 state visitation frequency도 변하게 됩니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=36s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/state_visitation_change.png\" alt=\"state_visitation_change\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>이로 인해서 policy를 최적화하는 것은 어려워집니다. 그래서 이러한 변화를 무시하는 다음과 같은 approximation을 취합니다.</p>\n<p>$$<br>\\begin{align}<br>L_\\pi\\left(\\tilde\\pi\\right) &amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}{\\rho_\\pi(s)} } \\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a)<br>\\end{align}<br>$$</p>\n<p>policy가 바뀌었음에도 이전의 state distribution을 계속 이용하는 것입니다. 이것은 policy의 변화가 크지 않다면 어느 정도 허용될 수 있을 것입니다. 그렇지만 얼마나 많은 변화가 허용될까요? 이것을 정하기 위해서 이용하는 것이 trust region입니다.</p>\n<p><br></p>\n<h2 id=\"2-2-Conservative-Policy-Iteration\"><a href=\"#2-2-Conservative-Policy-Iteration\" class=\"headerlink\" title=\"2.2 Conservative Policy Iteration\"></a>2.2 Conservative Policy Iteration</h2><p>policy의 변화를 다루기 용이하게 하기 위해서 policy를 다음과 같이 파라미터를 이용해서 표현합시다.</p>\n<ul>\n<li>$\\pi_\\theta(a\\vert s)$: parameterized policy</li>\n</ul>\n<p>$\\pi_\\theta$는 $\\theta$에 대하여 미분가능한 함수입니다. $L_\\pi\\left(\\tilde\\pi\\right)$을 $\\theta_0$에서 $\\eta(\\pi)$에 대한 first order approximation이라고 하면 다음 식이 성립합니다.</p>\n<p>$$<br>\\begin{align}<br>L_{\\pi_{\\theta_0} }\\left(\\pi_{\\theta_0}\\right) &amp;= \\eta\\left(\\pi_{\\theta_0}\\right) \\\\<br>\\nabla_\\theta \\left.L_{\\pi_{\\theta_0} }\\left(\\pi_{\\theta_0}\\right)\\right\\vert _{\\theta=\\theta_0} &amp;= \\nabla_\\theta\\left.\\eta(\\pi_{\\theta_0})\\right\\vert _{\\theta=\\theta_0}<br>\\end{align}<br>$$</p>\n<p>이것의 의미는 $\\pi_{\\theta_0}$가 매우 작게 변한다면 $L_{\\pi_{\\theta_0} }$를 개선시키는 것이 $\\eta$를 개선시키는 것이라는 것입니다. 그러나 지금까지의 설명만으로는 $\\pi_{\\theta_0}$를 얼마나 작게 변화시켜야 할지에 대해서는 알 수 없습니다.</p>\n<p>Kakade &amp; Langford가 2002년 발표한 논문에서도 이것에 대해서 고민했습니다. 그 논문에서 <em>conservative policy iteration</em>이라는 기법을 제안합니다. 그 논문의 contribution은 다음과 같습니다.</p>\n<ul>\n<li>$\\eta$ 개선의 lower bound를 제공</li>\n<li>기존의 policy를 $\\pi_\\mathrm{old}$라고 하고 $\\pi’$를 $\\pi’=\\arg\\max_{\\pi’}L_{\\pi_\\mathrm{old} }\\left(\\pi’\\right)$과 같이 정의할 때, 새로운 mixture policy $\\pi_\\mathrm{new}$를 다음과 같이 제안<br>$$<br>\\pi_\\mathrm{new}(a\\vert s) = (1-\\alpha)\\pi_\\mathrm{old}(a\\vert s) + \\alpha \\pi’(a\\vert s)<br>$$</li>\n</ul>\n<p>그림으로 표현하면 다음과 같습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=2m46s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/mixure_policy.png\" alt=\"mixure_policy\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<ul>\n<li>다음과 같은 lower bound를 정의<br>$$<br>\\eta\\left(\\pi_\\mathrm{new}\\right) \\geq L_{\\pi_{\\theta_\\mathrm{old} }}\\left(\\pi_\\mathrm{new}\\right) - \\frac{2\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\\\<br>\\mathrm{where\\ }\\epsilon = \\max_s\\left\\vert E_{a\\sim\\pi’(a\\vert s)}\\left[A_\\pi(s,a)\\right]\\right\\vert<br>$$</li>\n</ul>\n<p>하지만 mixture된 policy라는 것은 실용적이지 않습니다. </p>\n<p><br><br></p>\n<h1 id=\"3-Monotonic-Improvement-Guarantee-for-General-Stochastic-Policies\"><a href=\"#3-Monotonic-Improvement-Guarantee-for-General-Stochastic-Policies\" class=\"headerlink\" title=\"3. Monotonic Improvement Guarantee for General Stochastic Policies\"></a>3. Monotonic Improvement Guarantee for General Stochastic Policies</h1><p>이전 장에서 설명한 lower bound는 오직 mixture policy에 대해서만 성립하고 더 많이 사용되는 stochastic policy에는 적용되지 않습니다. 따라서 stochastic policy를 이용할 수 있도록 개선할 필요가 있습니다. 아래와 같이 기존 수식에서 두 가지를 바꿈으로써 이것이 가능합니다.</p>\n<ul>\n<li>$\\alpha\\rightarrow$ distance measure between $\\pi$ and $\\tilde\\pi$</li>\n<li>constant $\\epsilon\\rightarrow\\max_{s,a}\\left\\vert A_\\pi(s,a)\\right\\vert $</li>\n</ul>\n<p>여기서 distance measure로 total variation divergence를 이용합니다. discrete porbability distribution $p$와 $q$에 대하여 다음과 같이 정의됩니다.<br>$$<br>D_\\mathrm{TV}(p\\parallel q) = \\frac{1}{2}\\sum_i\\left\\vert p_i - q_i\\right\\vert<br>$$</p>\n<p>이것을 이용하여 $D_\\mathrm{TV}^\\max$를 다음과 같이 정의합니다.<br>$$<br>D_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D_\\mathrm{TV}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)<br>$$</p>\n<p>그림으로 표현하면 다음과 같습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=3m15s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/tvd.png\" alt=\"tvd\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>이것을 이용하여 다음과 같은 관계식을 얻을 수 있습니다.</p>\n<p><strong>Theorem 1.</strong> Let $\\alpha=D_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi)$. Then the following bound holds:<br>$$<br>\\begin{align}<br>\\eta\\left(\\pi_\\mathrm{new}\\right) &amp;\\geq L_{\\pi_\\mathrm{old} }\\left(\\pi_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2, \\\\<br>\\mathrm{where\\ } \\epsilon&amp;= \\max_{s,a}\\left\\vert A_\\pi(s,a)\\right\\vert<br>\\end{align}<br>$$<br><!--*Proof.* TBD.--></p>\n<p>또다른 distance metric으로 아래 그림과 같은 KL divergence가 있습니다.<br>(그런데 왜 하필 KL divergence로 바꿀까요? 논문의 뒤쪽에서 계산효율을 위해서 conjugate gradient method를 이용하는데 이를 위해서 바꾼게 아닐까 싶습니다. Wasserstein distance 같은 다른 방향으로 발전시킬 수도 있을 것 같습니다. Schulmann이 아마 해봤겠죠?^^a)</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=3m34s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/kld.png\" alt=\"kld\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>total variation divergence와 KL divergence 사이에는 다음과 같은 관계가 있습니다.<br>$$<br>D_\\mathrm{TV}(p\\parallel q)^2 \\leq D_\\mathrm{KL}(p\\parallel q)<br>$$</p>\n<p>다음 수식을 정의합니다.<br>$$<br>D_\\mathrm{KL}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D_\\mathrm{KL}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)<br>$$</p>\n<p><strong>Theorem 1</strong>을 이용하여 다음과 같은 수식이 성립함을 알 수 있습니다.<br>$$<br>\\begin{align}<br>\\eta\\left(\\tilde\\pi\\right) &amp;\\geq L_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max(\\pi, \\tilde\\pi), \\\\<br>\\mathrm{where\\ } C&amp;= \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}<br>\\end{align}<br>$$</p>\n<p>이러한 policy imporvement bound를 기반으로 다음과 같은 approximate policy iteration 알고리듬을 고안해낼 수 있습니다.</p>\n<p><strong>Algorithm 1</strong> Policy iteration algorithm guaranteeing non-decreasing expected return $\\eta$</p>\n<blockquote>\n<p>Initialize $\\pi_0$<br><strong>for</strong> $i=0,1,2,\\ldots$ until convergence <strong>do</strong><br>$\\quad$Compute all advantage values $A_{\\pi_i}(s,a)$<br>$\\quad$Solve the contstrained optimization problem<br>$\\quad\\pi_{i+1}=\\arg\\max_\\pi\\left[L_{\\pi_i} - CD_\\mathrm{KL}^\\max\\left(\\pi_i,\\pi\\right)\\right]$<br>$\\quad\\quad$where $C = 4\\epsilon\\gamma / (1-\\gamma)^2$<br>$\\quad\\quad$and $L_{\\pi_i}\\left(\\pi\\right) = \\eta(\\pi_i) + \\sum_{s}\\rho_{\\pi_i}(s)\\sum_a\\pi(a\\vert s) A_{\\pi_i}(s,a)$<br><strong>end for</strong></p>\n</blockquote>\n<p><strong>Algorithm 1</strong>은 advantage를 정확하게 계산할 수 있다고 가정하고 있습니다. 이 알고리듬은  monotonical한 성능 증가($\\eta(\\pi_0)\\leq\\eta(\\pi_1)\\leq\\cdots$)를 한다는 것을 다음과 같이 보일 수 있습니다. $M_i(\\pi)=L_{\\pi_i}(\\pi) - CD_\\mathrm{KL}^\\max\\left(\\pi_i,\\pi\\right)$라고 합시다.</p>\n<p>$$<br>\\begin{align}<br>\\eta \\left(\\pi_{i+1}\\right) &amp;\\geq M_i\\left(\\pi_{i+1}\\right)\\\\<br>\\eta \\left(\\pi_{i}\\right) &amp;= M_i\\left(\\pi_{i}\\right) \\\\<br>\\eta \\left(\\pi_{i+1}\\right) - \\eta \\left(\\pi_{i}\\right) &amp;\\geq M_i\\left(\\pi_{i+1}\\right) - M_i\\left(\\pi_{i}\\right)<br>\\end{align}<br>$$</p>\n<p>위 수식과 같이 매 iteration 마다 $M_i$를 최대화함으로써, $\\eta$가 감소하지 않는다는 것을 보장할 수 있습니다. 이와 같은 타입의 알고리듬을 <a href=\"https://www.jstor.org/stable/pdf/27643496.pdf?casa_token=0qHamcl60WoAAAAA:-fkZ9JcA_nrY3-zbCUpqvPOgcAMgw7Gr96MajCZg2byHf8m5GU1KTSxyJJcBy1lPZbBTZVCjHHUXilh4k-iuwF91Wka4B5qdltC1IR2qMWk8q1FoV6__\" target=\"_blank\" rel=\"noopener\">minorization-maximization (MM) algorithm</a>이라고 합니다. EM 알고리듬도 이 타입의 알고리듬 중 하나입니다.</p>\n<p><strong>Algorithm 1</strong>은 아래 그림과 같이 동작합니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=3m52s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/surrogate.png\" alt=\"surrogate\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>$M_i$는 $\\pi_i$일 때 equality가 되는 $\\eta$에 대한  surrogate function입니다. TRPO는 이 surrogate function을 최대화하고 KL divergence를 penalty가 아닌 constraint로 두는 알고리듬입니다.</p>\n<p><br><br></p>\n<h1 id=\"4-Optimization-of-Parameterized-Policies\"><a href=\"#4-Optimization-of-Parameterized-Policies\" class=\"headerlink\" title=\"4. Optimization of Parameterized Policies\"></a>4. Optimization of Parameterized Policies</h1><p>표기의 편의를 위해 다음과 같이 notation들을 더 간략하게 정의합니다.</p>\n<ul>\n<li>$\\eta(\\theta):=\\eta\\left(\\pi_\\theta\\right)$</li>\n<li>$L_\\theta\\left(\\tilde\\theta\\right):=L_{\\pi_\\theta}\\left(\\pi_{\\tilde\\theta}\\right)$</li>\n<li>$D_\\mathrm{KL}\\left(\\theta\\parallel\\tilde\\theta\\right):=D_\\mathrm{KL}\\left(\\pi_\\theta\\parallel\\pi_{\\tilde\\theta}\\right)$</li>\n<li>$\\theta_\\mathrm{old}$: previous policy parameter</li>\n</ul>\n<p><br></p>\n<h2 id=\"4-1-Trust-Region-Policy-Optimization\"><a href=\"#4-1-Trust-Region-Policy-Optimization\" class=\"headerlink\" title=\"4.1 Trust Region Policy Optimization\"></a>4.1 Trust Region Policy Optimization</h2><p>이전 장의 중요 결과를 위의 notation으로 다시 표기하면 아래와 같습니다.</p>\n<p>$$<br>\\eta\\left(\\theta\\right) \\geq L_{\\theta_\\mathrm{old} }\\left(\\theta\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)<br>$$</p>\n<p>$\\eta$의 성능 향상을 보장하기 위해서 그것의 lower bound를 최대화할 수 있습니다.</p>\n<p>$$<br>\\max_\\theta \\left[L_{\\theta_\\mathrm{old} }\\left(\\theta\\right) - C D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)\\right]<br>$$</p>\n<p>이 최적화 문제는 step size를 매우 작게 해야 올바른 동작을 합니다. 위에서 살펴봤듯이 first order approximation이기 때문입니다. 좀 더 큰 step size를 가질 수 있도록 이 최적화 문제를 trust region constraint를 도입하여 다음과 같이 바꿉니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>이 최적화문제의 constraint는 모든 state space에 대해서 성립해야 합니다. 또한 maximum값을 매번 찾아야 합니다. state가 많은 경우 constraint의 수가 매우 많아져서 문제를 풀기 어렵게 만듭니다. constraint의 수를 줄이기 위하여 다음과 같은 avergae KL divergence를 이용하는 heuristic approximation을 취합니다. 이것이 최선의 방법은 아닐 수 있지만 실용적인 방법입니다.</p>\n<p>$$<br>\\overline D_\\mathrm{KL}^{\\rho}\\left(\\theta_1, \\theta_2\\right):=E_{s\\sim\\rho}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_1}(\\cdot\\vert s)\\parallel\\pi_{\\theta_2}(\\cdot\\vert s)\\right)\\right]<br>$$</p>\n<p>이것을 기반으로 다음과 같은 최적화 문제를 풀 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>아래 그림처럼 step size에 대해서 고려할 수 있습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=4m35s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/heuristic_approx.png\" alt=\"heuristic_approx\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p><br><br></p>\n<h1 id=\"5-Sample-Based-Estimation-of-the-Objective-and-Constraint\"><a href=\"#5-Sample-Based-Estimation-of-the-Objective-and-Constraint\" class=\"headerlink\" title=\"5. Sample-Based Estimation of the Objective and Constraint\"></a>5. Sample-Based Estimation of the Objective and Constraint</h1><p>실용적인 알고리듬을 만들려고 하는 노력은 아직 끝나지 않았습니다. 이제 앞의 알고리듬을 sample-based estimation 즉, Monte Carlo estimation을 할 수 있도록 바꿔보겠습니다. sampling을 편하게 할 수 있도록 아래와 같이 바꿔줍니다.</p>\n<ul>\n<li>$\\sum_s \\rho_{\\theta_\\mathrm{old} }(s)[\\ldots]\\rightarrow\\frac{1}{1-\\gamma}E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}[\\ldots]$</li>\n<li>$A_{\\theta_\\mathrm{old} }\\rightarrow Q_{\\theta_\\mathrm{old} }$</li>\n<li>$\\sum_a\\pi_{\\theta_\\mathrm{old} }(a\\vert s)A_{\\theta_\\mathrm{old} } \\rightarrow E_{a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\mathrm{old} }\\right]$</li>\n</ul>\n<p>이러한 변화를 그림처럼 도식화할 수 있습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=5m31s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/sample-based.png\" alt=\"sample-based\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>한 가지 짚고 넘어가야 할 점은 action sampling을 할 때 importance sampling을 사용한다는 것입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=5m53s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/importance_sampling.png\" alt=\"importance_sampling\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>바뀐 최적화문제는 아래와 같습니다.</p>\n<ul>\n<li><strong>Equation (14).</strong></li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}<br>$$</p>\n<p>이 때 sampling하는 두 가지 방법이 있습니다.</p>\n<p><br></p>\n<h2 id=\"5-1-Single-Path\"><a href=\"#5-1-Single-Path\" class=\"headerlink\" title=\"5.1 Single Path\"></a>5.1 Single Path</h2><p><em>single path</em>는 개별 trajectory들을 이용하는 방법입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=6m15s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/single.png\" alt=\"single\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p><br></p>\n<h2 id=\"5-2-Vine\"><a href=\"#5-2-Vine\" class=\"headerlink\" title=\"5.2 Vine\"></a>5.2 Vine</h2><p><em>vine</em>은 한 state에서 rollout을 이용하여 여러 action을 수행하는 방법입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=6m32s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/vine1.png\" alt=\"vine1\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=6m49s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/vine2.png\" alt=\"vine2\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>estimation의 variance를 낮출 수 있지만 계산량이 많고 한 state에서 여러 action을 수행할 수 있어야 하기 때문에 현실적인 문제에 적용하기에는 어려움이 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"6-Practical-Algorithm\"><a href=\"#6-Practical-Algorithm\" class=\"headerlink\" title=\"6. Practical Algorithm\"></a>6. Practical Algorithm</h1><p>앞서 single-path, vine 샘플링을 사용하는 두가지 방식의 policy optimization 알고리즘을 살펴봤습니다. 실용적인 알고리듬은 아래의 과정을 반복해서 수행합니다.</p>\n<p>1) Q-values의 몬테카를로 추정을 통해 state-action 쌍 집합을 single path 또는 vine 과정을 통해 수집함.</p>\n<p>2) 샘플 평균으로, (14)식의 목적함수와 제약식을 추정함</p>\n<ul>\n<li><strong>Equation (14).</strong><br>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}<br>$$</li>\n</ul>\n<p>3) policy parameter vector인 $\\theta$를 업데이트 하면서 제약조건이 있는 최적화 문제를 근사적으로 풂. 본 논문에서는 gradient를 직접 계산하는 것보다는 약간 더 계산량이 있는 line search와 conjugate gradient algorithm을 사용했습니다.</p>\n<p>3)에 대해서,  gradient의 covariance matrix를 사용하지 않고 KL divergence의 Hessian을 해석적으로 계산하여 Fisher Information Matrix를 구성했습니다.</p>\n<p>다시말해서</p>\n<p>$\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial}{\\partial \\theta_i} \\log \\pi_{\\theta}(a_n\\vert s_n) \\frac{\\partial}{\\partial \\theta_j} \\log \\pi_{\\theta}(a_n\\vert s_n)$ 대신 $\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} D_{KL}(\\pi_{\\theta_{old} }(\\cdot \\vert  s_n) \\vert \\vert  \\pi_{\\theta}(\\cdot \\vert  s_n))$를 계산한 것입니다.</p>\n<p>이 analytic estimator는 Hessian이나 trajectories의 모든 gradient를 저장하지 않아도 되기 때문에 대규모 환경을 고려할 경우 계산 상 이점이 있습니다. 좀 더 자세하게 설명한 Appendix C 부분을 살펴보겠습니다.</p>\n<p><br></p>\n<h2 id=\"Appendix-C-Efficiently-Solving-the-Trust-Region-Constrained-Optimization-Problem\"><a href=\"#Appendix-C-Efficiently-Solving-the-Trust-Region-Constrained-Optimization-Problem\" class=\"headerlink\" title=\"Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem\"></a>Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem</h2><p>우리가 풀고자 하는 식은 다음과 같습니다.</p>\n<p>$$<br>\\begin{align}<br>\\max\\quad &amp;L(\\theta) \\\\<br>\\mathrm{s.t.\\ } &amp;\\overline{D}_{KL}(\\theta_\\mathrm{old}, \\theta) \\leq \\delta<br>\\end{align}<br>$$</p>\n<p>이 최적화 문제를 풀기 위하여 우리는 gradient 기법을 이용할 것입니다. gradient 기법은 결국 update할 방향과 크기를 결정하는 문제로 생각할 수 있습니다. 이에 따라 아래와 같은 두가지 과정으로 진행합니다.</p>\n<p>1) 탐색 방향을 계산, 목적함수의 first-order approximation(선형근사)와 제약식의 quadratic approximation(2차 근사).</p>\n<p>2) 이동거리 계산을 위해 해당 방향으로 line search 수행.</p>\n<p>탐색 방향은 $Ax = g$ 수식을 근사적으로 풀어서 구합니다. 여기서 $A$ 는 Fisher Information Matrix이고, 이것은 KL divergence 제약식의 2차 근사 $\\overline{D}_\\mathrm{KL} (\\theta_\\mathrm{old}, \\theta) \\approx \\frac{1}{2}(\\theta - \\theta_\\mathrm{old})^T A(\\theta - \\theta_\\mathrm{old})$를 푸는 것입니다. 여기서 $A_{ij} = \\frac{\\partial}{\\partial \\theta_i} \\frac{\\partial}{\\partial \\theta_j} \\overline{D}_\\mathrm{KL}(\\theta_\\mathrm{old}, \\theta)$입니다.</p>\n<p>논문에서 이 부분에 대한 설명이 약간 부족해서 좀 더 내용을 추가하자면, $\\frac{1}{2}x^T A x - x^T g + b = 0$과 같은 quadratic equation의 해는 1차 derivative를 0으로 만드는 $x$를 찾음으로써 알 수 있습니다. 위에서 우리는 목적함수를 1차 근사시키고 제약조건은 2차 근사를 시킴으로써 quadratic equation 형태로 변형했기 때문에 같은 방식으로 풀 수 있습니다. 따라서 $g$는 1차근사시킨 목적함수의 계수가 될 것입니다.</p>\n<p>대규모 문제에서 $A$ 혹은 $A^{-1}$ 을 계산하는 것은 엄청난 계산량이 필요합니다. 하지만 conjugate gradient algorithm을 잘 이용하면 전체 $A$행렬(FIM)을 계산하지 않아도 $Ax=g$ 를 근사적으로 해결할 수 있게 해줍니다.</p>\n<p>탐색 방향 $s \\approx A^{-1}g$과 더불어 최대스텝길이(step size) $\\beta$도 계산할 필요가 있습니다.</p>\n<p>즉, $\\delta = \\overline{D}_\\mathrm{KL} \\approx \\frac{1}{2}(\\beta s)^T A(\\beta s) = \\frac{1}{2}\\beta^2s^TAs$ 이고, </p>\n<p>$\\beta = \\sqrt{2\\delta / s^T As}$가 됩니다. 이때 $\\delta$는 KL divergence의 boundary term 입니다.</p>\n<p>$s^T As$ 항은 Hessian-vector product로 계산할 수 있고, conjugate gradient 과정에서 계산됩니다.</p>\n<p>마지막으로, surrogate objective와 KL divergence 제약식을 위해 다음과 같은 함수를 정의하고 line search를 사용합니다.</p>\n<p>$$<br>L_{\\theta_\\mathrm{old} }(\\theta) - \\chi [\\overline{D}_{KL}(\\theta_\\mathrm{old}, \\theta) \\leq \\delta]<br>$$</p>\n<p>$\\chi[\\cdot]$ 함수는 []안의 조건이 맞으면 0이고 틀리면 무한으로 발산하는 함수입니다.</p>\n<p>위에서 계산한 $\\beta$ 의 최대 값부터 시작해서 목적함수가 개선될때까지 exponential하게 줄여 나갑니다. line search가 없었다면 이 알고리즘은 매우 큰 스텝으로 계산될 것이고 심각한 성능 저하가 발생할 것입니다.</p>\n<p>(여기서 살짝 예고를 하자면, 나중에 이 KL 기반 기법을 개선한 더 단순한 방법이 제안됩니다. PPO라고…)</p>\n<p><br><br></p>\n<h1 id=\"7-Connections-with-Prior-Work\"><a href=\"#7-Connections-with-Prior-Work\" class=\"headerlink\" title=\"7. Connections with Prior Work\"></a>7. Connections with Prior Work</h1><p>Natural Policy Gradient는 $L$의 선형 근사와 $\\overline D_\\mathrm{KL}$ 제약식을 2차근사 하는 아래의 식의 special case입니다.</p>\n<ul>\n<li>Equation (17).</li>\n</ul>\n<p>$L_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A_\\pi(s,a)$</p>\n<p>$\\underset{\\theta}{\\max}\\quad [\\nabla_{\\theta}L_{\\theta_\\mathrm{old} }(\\theta) \\ \\rvert_{\\theta=\\theta_\\mathrm{old} } \\cdot (\\theta - \\theta_\\mathrm{old})]$</p>\n<p>$\\mathrm{s.t.}\\quad\\frac{1}{2}(\\theta_\\mathrm{old} - \\theta)^{T} A(\\theta_\\mathrm{old})(\\theta_\\mathrm{old}-\\theta) \\leq \\delta$</p>\n<p>$\\mathrm{where\\ }A(\\theta_{old})_{ij} = $</p>\n<p>$\\Large \\frac{\\partial}{\\partial \\theta_{i} } \\frac{\\partial}{\\partial \\theta_{j} }<br>E_{s \\sim \\rho_{\\pi} }[D_\\mathrm{KL}(\\pi(\\cdot\\rvert s, \\theta_\\mathrm{old})  \\rvert\\rvert \\pi(\\cdot \\rvert s, \\theta))] \\rvert_{\\theta=\\theta_\\mathrm{old} }$</p>\n<p>업데이트 식은 다음과 같습니다.</p>\n<p>$\\theta_\\mathrm{new} = \\theta_\\mathrm{old} +  \\color{Red}{\\frac{1}{\\lambda} } A(\\theta_\\mathrm{old})^{-1}\\nabla_{\\theta}L(\\theta) \\rvert_{\\theta=\\theta_\\mathrm{old} }$</p>\n<p>여기서 step size인 $\\frac{1}{\\lambda}$ 일반적으로 알고리즘의 파라미터로 취급되지만 TRPO는 각 업데이트 마다 제약식으로 사용한다는 점이 다릅니다. 사소한 차이처럼 보이지만, 큰 차원을 다루는 실험에서 성능을 크게 향상시켰습니다.</p>\n<p>또한 $L^2$ 제약식(혹은 페널티) 를 사용하면 표준 Policy Gradient 업데이트 식을 얻었습니다.</p>\n<ul>\n<li>Equation (18).</li>\n</ul>\n<p>$\\underset{\\theta}{\\max}\\quad[\\nabla_{\\theta} L_{\\theta_\\mathrm{old} }(\\theta) \\rvert_{\\theta=\\theta_\\mathrm{old} } \\cdot (\\theta - \\theta_\\mathrm{old})]$</p>\n<p>$\\mathrm{s.t.}\\quad \\frac{1}{2} \\vert\\vert \\theta - \\theta_\\mathrm{old} \\vert\\vert^2 \\leq \\delta$</p>\n<p>$L_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A_\\pi(s,a) $를 이용해서 제약조건없이 $\\underset{\\pi}{\\max}\\quad L_{\\pi_\\mathrm{old} }(\\pi)$를 풀면<br>Policy Iteration update를 하는 것과 같습니다.</p>\n<p><br><br></p>\n<h1 id=\"8-Experiments\"><a href=\"#8-Experiments\" class=\"headerlink\" title=\"8. Experiments\"></a>8. Experiments</h1><ul>\n<li><p>다음과 같은 3 가지 궁금증을 해결하기 위하여 실험을 설계하였습니다.</p>\n<ol>\n<li><p>Single-path 와 vine 의 성능면에서 특징을 알고싶음.</p>\n</li>\n<li><p>Fixed penalty coefficient(NPG) 보다 fixed KL divergence를 사용하는 것(TRPO)으로<br>변경한 것이 얼마나 뚜렷한 차이를 보일지, 성능 면에서 어떤 영향을 미치는지 알고싶음.</p>\n</li>\n<li><p>TRPO가 큰 스케일의 문제를 해결할 수 있는지, 기존에 연구/적용되어왔던 방법들과<br>성능, 계산시간, 샘플 복잡도 면에서 비교하고 싶음.</p>\n</li>\n</ol>\n</li>\n</ul>\n<ul>\n<li><p>1, 2의 궁금증에 대해서 실험 환경에 single path, vine을 비롯한 여러 사전방법들을 함께 실험하였습니다.</p>\n</li>\n<li><p>3에 대해선, robotic locomotion과 Atari game에 TRPO를 실험하였습니다.</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"8-1-Simulated-Robotic-Locomotion\"><a href=\"#8-1-Simulated-Robotic-Locomotion\" class=\"headerlink\" title=\"8.1 Simulated Robotic Locomotion\"></a>8.1 Simulated Robotic Locomotion</h2><ul>\n<li>MuJoCo 시뮬레이터를 활용해 로봇 locomotion 실험 진행하였습니다.</li>\n<li>Robot State: positions and velocities, joint torques</li>\n</ul>\n<p><img src=\"https://i.imgur.com/bs5ATS3.png\" alt=\"\"></p>\n<ul>\n<li><p>수영, 점프, 걷기 행동을 학습시키고자 합니다.</p>\n<ul>\n<li><p>수영: 10 dimensional state space , reward: $r(x,u)=v_x - 10^{-5}\\vert \\vert u\\vert \\vert ^2$, quadratic penalty.</p>\n</li>\n<li><p>점프: 12 dimensional state space, episode의 끝은 높이와 각도 threshol로 점프를 판단, non-terminal state에 대해 +1 보상 추가.</p>\n</li>\n<li><p>걷기: 18 dimensional state space, 뜀뛰는 듯한 보법(점프에이전트)보다 부드러운 보법을 유도하기위해 페널티를 추가함.</p>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Detailed Experiment Setup and used parameters, used network model</strong><br><img src=\"https://i.imgur.com/zgnsbw6.png\" alt=\"\"></p>\n<p><img src=\"https://i.imgur.com/FqdWC53.png\" alt=\"\"></p>\n<p>equation (12) : $\\max\\quad L_{\\theta_\\mathrm{old}(\\theta)}, \\ \\ \\mathrm{s.t.\\ }\\overline{D}_\\mathrm{KL}^{\\rho_{\\theta_\\mathrm{old} }}(\\theta_\\mathrm{old}, \\theta) \\leq \\delta$</p>\n<p>이 실험에서 $\\delta = 0.01$입니다.</p>\n<ul>\n<li><p>비교에 사용된 모델들</p>\n<ul>\n<li>TRPO Single-path</li>\n<li>TRPO vine</li>\n<li>CEM(cross-entropy method)</li>\n<li>CMA(covariance matrix adaptation)</li>\n<li>NPG (Lagrange Multiplier penalty coefficien를 사용하는 것이 차이점)</li>\n<li>Empirical FIM</li>\n<li>maximum KL(cartpole)</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://i.imgur.com/wSqolvS.png\" alt=\"\"></p>\n<ul>\n<li><p>제안한 single path, vine 모두 task를 잘 학습하는 것을 확인할 수 있습니다.</p>\n</li>\n<li><p>페널티를 고정하는 것보다, KL divergenc를 제약하는 것이 step size를 선택하는 부분에서 Robust합니다.</p>\n</li>\n<li><p>CEM/CMA는 derivative free 알고리즘으로, 샘플 복잡도가 네트워크 파라미터수 만큼 확장되어 high dimension 문제에서는 속도와 성능이 약한 모습을 보입니다.</p>\n</li>\n<li><p>maxKL은 제안방법보다 느린 학습성능을 보입니다.</p>\n</li>\n</ul>\n<p>TRPO video link: <a href=\"https://www.youtube.com/watch?v=jeid0wIrSn4\" target=\"_blank\" rel=\"noopener\">https://www.youtube.com/watch?v=jeid0wIrSn4</a></p>\n<ul>\n<li><p>사전지식이 적은 상태에서 여러 액션을 학습하는 실험으로 TRPO의 성능을 보입니다.</p>\n</li>\n<li><p>지금까지의 균형이나 걸음 같은 개념을 명시적으로 인코딩한 robotic locomotion에서의 사전 연구들과 차이를 보였습니다.</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"8-2-Playing-Games-from-Images\"><a href=\"#8-2-Playing-Games-from-Images\" class=\"headerlink\" title=\"8.2 Playing Games from Images\"></a>8.2 Playing Games from Images</h2><ul>\n<li>부분관찰가능하며 복잡한 observation인 환경에서 TRPO를 평가하기위해 raw image를 입력으로 하는 아타리게임 환경에서 실험하였습니다.</li>\n</ul>\n<ul>\n<li>Challenging point :<ul>\n<li>High dimensional space</li>\n<li>delayed reward</li>\n<li>non-stationary image (Enduro는 배경 이미지가 바뀌고, 반전처리가 됨)</li>\n<li>complex action sequence (Q*bert는 21개의 다른 발판에서 점프 해야함)</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>ALE(Arcade Learning Environment) 에서 테스트</li>\n<li><p>DQN과 전처리 방식은 동일<br>  210 x 160 pixel -&gt; gray-scaling &amp; down-sampling -&gt; 110 x 84  -&gt; crop -&gt; 84 x 84</p>\n</li>\n<li><p>2 convolution layer ( 16 channels) , stride 2, FC layer with 20 units</p>\n</li>\n</ul>\n<p><img src=\"https://i.imgur.com/NJBC69d.png\" alt=\"\"> </p>\n<p><img src=\"https://i.imgur.com/wTe1OEW.png\" alt=\"\"></p>\n<p><img src=\"https://i.imgur.com/j22VtNQ.png\" alt=\"\"></p>\n<ul>\n<li><p>30시간 동안 500 iteration 학습을 하였습니다.</p>\n</li>\n<li><p>DQN과 MCST를 사용한 모델(UCC-I), 사람이 플레이한 것(Human) 과의 비교하였습니다.</p>\n</li>\n<li><p>이전 방법들에 비해 압도적인 점수를 기록하진 못했으나, 다양한 게임에서 적절한 게임 기록하였습니다.</p>\n</li>\n<li><p>이전 방법들과 다른 점은, 특정 task에 초점을 맞춰 설계하지않았는데 robotics나 game playing Task에도 적절한 성능을 내는 점에서 <strong>TRPO의 일반화 성능</strong>을 확인하였습니다.</p>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"9-Discussion\"><a href=\"#9-Discussion\" class=\"headerlink\" title=\"9. Discussion\"></a>9. Discussion</h1><ul>\n<li><p>Trust Region Policy Optimization 을 제안하였습니다.</p>\n</li>\n<li><p>KL Divergence 페널티로 $\\eta(\\pi)$를 최적화하는 알고리즘이 monotonically improve 함을 증명하였습니다.</p>\n</li>\n<li><p>Locomotion 도메인에서 여러 행동(수영, 걷기, 점프)을 제어하는 controller를 학습하였습니다.</p>\n</li>\n<li><p>Robotics와 게임 실험을 결합해, 시각정보와 센서데이터를 사용하는 로봇제어 정책을 학습시킬 수 있는 가능성을 보았습니다.</p>\n</li>\n<li><p>샘플의 복잡도를 상당히 줄일 수 있어 실제상황에 적용가능성을 보았습니다.</p>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"NPG-여행하기\"><a href=\"#NPG-여행하기\" class=\"headerlink\" title=\"NPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/\">NPG 여행하기</a></h2><h2 id=\"NPG-Code\"><a href=\"#NPG-Code\" class=\"headerlink\" title=\"NPG Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py\" target=\"_blank\" rel=\"noopener\">NPG Code</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"TRPO-Code\"><a href=\"#TRPO-Code\" class=\"headerlink\" title=\"TRPO Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></h2><h2 id=\"GAE-여행하기\"><a href=\"#GAE-여행하기\" class=\"headerlink\" title=\"GAE 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/\">GAE 여행하기</a></h2>","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"../../../../img/irl/gail_1.png\" width=\"850\"> </center>\n\n<p>Author : Jonathan Ho, Stefano Ermon<br>Paper Link : <a href=\"https://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf\" target=\"_blank\" rel=\"noopener\">https://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf</a><br>Proceeding : Advances in Neural Information Processing Systems (NIPS) 2016</p>\n<hr>\n<h1 id=\"0-Abstract\"><a href=\"#0-Abstract\" class=\"headerlink\" title=\"0. Abstract\"></a>0. Abstract</h1><p>Trust region policy optimization (TRPO)는 상당히 우수한 성능을 보여주는 policy gradient 기법으로 알려져 있습니다. 높은 차원의 action space를 가진 robot locomotion부터 action은 적지만 화면을 그대로 처리하여 플레이하기 때문에 control parameter가 매우 많은 Atari game까지 각 application에 세부적으로 hyperparameter들을 특화시키지 않아도 두루두루 좋은 성능을 나타내기 때문에 일반화성능이 매우 좋은 기법입니다. 이 TRPO에 대해서 알아보겠습니다.</p>\n<p>※TRPO를 매우 재밌게 설명한 Crazymuse AI의 <a href=\"https://www.youtube.com/watch?v=CKaN5PgkSBc&amp;t=90s\" target=\"_blank\" rel=\"noopener\">Youtube video</a>에서 일부 그림을 차용했습니다. 이 비디오를 시청하시는 것을 강력하게 추천합니다!</p>\n<p><br></p>\n<h2 id=\"1-1-TRPO-흐름-잡기\"><a href=\"#1-1-TRPO-흐름-잡기\" class=\"headerlink\" title=\"1.1 TRPO 흐름 잡기\"></a>1.1 TRPO 흐름 잡기</h2><p>TRPO 논문은 많은 수식이 등장하여 이 수식들을 따라가다보면 큰 그림을 놓칠 수도 있습니다. 세부적인 내용을 살펴보기 전에 기존 연구에서 출발해서 TRPO로 어떻게 발전해나가는지 간략하게 살펴보겠습니다.</p>\n<h3 id=\"1-1-1-Original-Problem\"><a href=\"#1-1-1-Original-Problem\" class=\"headerlink\" title=\"1.1.1 Original Problem\"></a>1.1.1 Original Problem</h3><p>$$\\max_\\pi \\eta(\\pi)$$</p>\n<p>모든 강화학습이 그렇듯이 expected discounted reward를 최대화하는 policy를 찾는 문제로부터 출발합니다.</p>\n<h3 id=\"1-1-2-Conservative-policy-iteration\"><a href=\"#1-1-2-Conservative-policy-iteration\" class=\"headerlink\" title=\"1.1.2 Conservative policy iteration\"></a>1.1.2 Conservative policy iteration</h3><p>$$\\max L_\\pi(\\tilde\\pi) = \\eta(\\pi) + \\sum_s \\rho_\\pi(s)\\sum_a\\tilde\\pi(a\\vert s)A_\\pi(s,a)$$</p>\n<p>$\\eta(\\pi)$를 바로 최대화하는 것은 많은 경우 어렵습니다. $\\eta(\\pi)$의 성능향상을 보장하면서 policy를 update하는 conservative policy iteration 기법이 <a href=\"http://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf\" target=\"_blank\" rel=\"noopener\">Kakade와 Langford</a>에 의하여 제안되었습니다. 이 기법을 이용하면 policy update가 성능을 향상시키는지는 못하더라도 최소한 성능을 악화시키지는 않는다는 것이 이론적으로 보장됩니다.</p>\n<h3 id=\"1-1-3-Theorem-1-of-TRPO\"><a href=\"#1-1-3-Theorem-1-of-TRPO\" class=\"headerlink\" title=\"1.1.3 Theorem 1 of TRPO\"></a>1.1.3 Theorem 1 of TRPO</h3><p>$$\\max L_{\\pi_\\mathrm{old} }\\left(\\pi_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\quad\\left(\\alpha=D_\\mathrm{TV}^\\max\\left(\\pi_\\mathrm{old},\\pi_\\mathrm{new}\\right)\\right)$$</p>\n<p>기존의 conservative policy iteration은 과거 policy와 새로운 policy를 섞어서 사용해서 실용적이지 않다는 단점이 있었는데 이것을 보완하여 온전히 새로운 policy만으로 update할 수 있는 기법을 제안합니다. </p>\n<h3 id=\"1-1-4-KL-divergence-version-of-Theorem-1\"><a href=\"#1-1-4-KL-divergence-version-of-Theorem-1\" class=\"headerlink\" title=\"1.1.4 KL divergence version of Theorem 1\"></a>1.1.4 KL divergence version of Theorem 1</h3><p>$$\\max L_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\pi,\\tilde\\pi\\right),\\quad\\left(C = \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\right)$$</p>\n<p>distance metric을 KL divergence로 바꿀 수 있습니다.</p>\n<h3 id=\"1-1-5-Using-parameterized-policy\"><a href=\"#1-1-5-Using-parameterized-policy\" class=\"headerlink\" title=\"1.1.5 Using parameterized policy\"></a>1.1.5 Using parameterized policy</h3><p>$$\\max_\\theta L_{\\theta_\\mathrm{old} }(\\theta) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)$$</p>\n<p>최적화문제를 더욱 편리하게 풀 수 있도록 낮은 dimension을 가지는 parameter들로 parameterized된 policy를 사용할 수 있습니다.</p>\n<h3 id=\"1-1-6-Trust-region-constraint\"><a href=\"#1-1-6-Trust-region-constraint\" class=\"headerlink\" title=\"1.1.6 Trust region constraint\"></a>1.1.6 Trust region constraint</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>1.1.5까지는 아직 conservative policy iteration을 약간 변형시킨 것입니다. policy를 update할 때 지나치게 많이 변하는 것을 방지하기 위하여 <a href=\"https://en.wikipedia.org/wiki/Trust_region\" target=\"_blank\" rel=\"noopener\">trust region</a>을 constraint로 설정할 수 있습니다. 이 아이디어로 인해서 TRPO라는 명칭을 가지게 됩니다.</p>\n<h3 id=\"1-1-7-Heuristic-approximation\"><a href=\"#1-1-7-Heuristic-approximation\" class=\"headerlink\" title=\"1.1.7 Heuristic approximation\"></a>1.1.7 Heuristic approximation</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>사실 1.1.6의 constraint는 모든 state에 대해서 성립해야 하기 때문에 문제를 매우 어렵게 만듭니다. 이것을 좀 더 다루기 쉽게 state distribution에 대한 평균을 취한 것으로 변형합니다.</p>\n<h3 id=\"1-1-8-Monte-Carlo-simulation\"><a href=\"#1-1-8-Monte-Carlo-simulation\" class=\"headerlink\" title=\"1.1.8 Monte Carlo simulation\"></a>1.1.8 Monte Carlo simulation</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}$$</p>\n<p>Sampling을 통한 계산이 가능하도록 식을 다시 표현할 수 있습니다. </p>\n<h3 id=\"1-1-9-Efficiently-solving-TRPO\"><a href=\"#1-1-9-Efficiently-solving-TRPO\" class=\"headerlink\" title=\"1.1.9 Efficiently solving TRPO\"></a>1.1.9 Efficiently solving TRPO</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;\\nabla_\\theta \\left. L_{\\theta_\\mathrm{old} }(\\theta)\\right\\vert _{\\theta=\\theta_\\mathrm{old} } \\left(\\theta - \\theta_\\mathrm{old}\\right) \\\\<br>\\mathrm{s.t.\\ }&amp;\\frac{1}{2}\\left(\\theta_\\mathrm{old} - \\theta \\right)^T A\\left(\\theta_\\mathrm{old}\\right)\\left(\\theta_\\mathrm{old} - \\theta \\right) \\leq \\delta \\\\<br>&amp;A_{ij} = \\frac{\\partial}{\\partial \\theta_i}\\frac{\\partial}{\\partial \\theta_j}\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right)<br>\\end{align}$$</p>\n<p>문제를 효율적으로 풀기 위하여 approximation을 적용할 수 있습니다. objective function은 first order approximation, constraint는 quadratic approximation을 취하면 효율적으로 문제를 풀 수 있는 형태로 바뀌는데 이것을 natural gradient로 풀 수도 있고 conjugate gradient로 풀 수도 있습니다.</p>\n<p>TRPO는 이렇게 다양한 방법으로 문제를 변형한 것입니다! 이제 좀 더 자세히 살펴보겠습니다.</p>\n<p><br><br></p>\n<h1 id=\"2-Preliminaries\"><a href=\"#2-Preliminaries\" class=\"headerlink\" title=\"2. Preliminaries\"></a>2. Preliminaries</h1><p>다음과 같은 파라미터를 가지는 infinite-horizon discounted Markov decision process (MDP)를 고려합니다. </p>\n<ul>\n<li>$\\mathcal{S}$: finite set of states</li>\n<li>$\\mathcal{A}$: finite set of actions</li>\n<li>$P: \\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$: transition probability distribution</li>\n<li>$r: \\mathcal{S}\\rightarrow\\mathbb{R}$: reward function</li>\n<li>$\\rho_0:\\mathcal{S}\\rightarrow\\mathbb{R}$: distribution of the initial state $s_0$</li>\n<li>$\\gamma\\in(0,1)$: discount factor</li>\n<li>$\\eta(\\pi)=E_{s_0,a_0,\\ldots}\\left[\\sum_{t=0}^\\infty\\gamma^t r\\left(s_t\\right)\\right]$, where $s_0\\sim\\rho_0\\left(s_0\\right), a_t\\sim\\pi\\left(\\left.a_t \\right\\vert s_t\\right), s_{t+1}\\sim P\\left(\\left.s_{t+1}\\right\\vert s_t,a_t\\right)$</li>\n<li>$Q_\\pi\\left(s_t,a_t\\right)=E_{ { \\color{red}{s_{t+1}} },a_{t+1},\\ldots}\\left[\\sum_{l=0}^\\infty \\gamma^l r\\left(s_{t+l}\\right)\\right]$: action value function</li>\n<li>$V_\\pi\\left(s_t\\right)=E_{ { \\color{red}{a_t} }, s_{t+1},a_{t+1},\\ldots}\\left[\\sum_{l=0}^\\infty \\gamma^l r\\left(s_{t+l}\\right)\\right]$: value function (action value function과 expectation의 첨자가 다른 부분을 유의하세요.)</li>\n<li>$A_\\pi(s,a) = Q_\\pi(s,a) - V_\\pi(s)$: advantage function</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-1-Useful-identity-Kakade-amp-Langford-2002\"><a href=\"#2-1-Useful-identity-Kakade-amp-Langford-2002\" class=\"headerlink\" title=\"2.1 Useful identity [Kakade &amp; Langford 2002]\"></a>2.1 Useful identity [Kakade &amp; Langford 2002]</h2><p>우리의 목표는 $\\eta(\\pi)$가 최대화되도록 만드는 것입니다. 하지만 $\\pi$의 변화에 따라 $\\eta$가 어떻게 변하는지 알아내는 것도 쉽지 않습니다. $\\pi$는 기존의 policy이고 $\\tilde\\pi$는 새로운 policy를 나타낸다고 할 때 $\\eta$와 policy update 사이에 다음과 같은 관계가 있다는 것이 밝혀졌습니다.</p>\n<p><strong>Lemma 1.</strong> $\\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E_{s_{0},a_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right]$</p>\n<p><em>Proof.</em> $A_\\pi(s,a)=E_{s’\\sim P\\left(s’\\vert s,a\\right)}\\left[r(s)+\\gamma V_\\pi\\left(s’\\right)-V_\\pi(s)\\right]$로 다시 표현할 수 있습니다. 표기의 편의를 위하여 $\\tau:=\\left(s_0,a_0,s_1,a_1,\\ldots\\right)$를 정의하겠습니다. 다음과 같은 수식 전개가 가능합니다.<br>$$<br>\\begin{align}<br>&amp;E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right] \\\\<br> &amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t \\left(r(s_t)+\\gamma V_\\pi\\left(s_{t+1}\\right)-V_\\pi(s_t)\\right)\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\left(\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right)+\\gamma V_\\pi\\left(s_{1}\\right)-V_\\pi(s_0)+\\gamma^2 V_\\pi\\left(s_{2}\\right)-\\gamma V_\\pi(s_1)+\\gamma^3 V_\\pi\\left(s_{3}\\right)-\\gamma^2 V_\\pi(s_2)+\\cdots\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t)+\\color{red}{\\gamma V_\\pi\\left(s_{1}\\right)}-V_\\pi(s_0)+\\color{red}{\\gamma^2 V_\\pi\\left(s_{2}\\right)}-\\color{red}{\\gamma V_\\pi(s_1)}+\\cdots\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[-V_\\pi(s_0)+\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right] \\\\<br>&amp;\\overset{\\underset{\\mathrm{(a)} }{} }{=} -E_{\\color{red}{s_0}}\\left[V_\\pi(s_0)\\right]+E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right] \\\\<br>&amp;=-\\eta(\\pi) + \\eta\\left(\\tilde\\pi\\right)\\\\<br>&amp;\\quad\\therefore \\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E_{s_{0},a_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right]<br>\\end{align}<br>$$<br>위의 전개의 (a) 부분은 $V_\\pi\\left(s_0\\right)=E_{a_0,a_1,s_1,\\ldots}\\left[\\sum_{t=0}^\\infty\\gamma^l r\\left(s_l\\right)\\right]$이므로 $\\tau$ 중 많은 부분들이 이미 expectation이 취해진 값이므로 무시됩니다. 오직 $s_0$만 expectation이 취해지지 않았기 때문에 남아있습니다.</p>\n<p><strong>Lemma 1</strong>은 새로운 policy와 기존의 policy 사이의 관계를 규정합니다. 다음과 같은 식을 정의하고 이것을 이용해서 <strong>Lemma 1</strong>을 변형시켜 봅시다.</p>\n<ul>\n<li>$\\rho_\\pi(s) = P\\left(s_0=s\\right) + \\gamma P\\left(s_1=s\\right)\\ + \\gamma^2 P\\left(s_2=s\\right) + \\cdots $: (unnormalized) discounted visitation frequencies</li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\eta\\left(\\tilde\\pi\\right) &amp;= \\eta(\\pi) + \\sum_{t=0}^\\infty\\sum_{s}P\\left(\\left.s_t=s\\right\\vert \\tilde\\pi\\right)\\sum_a\\tilde\\pi(a\\vert s)\\gamma^t A_\\pi(s,a) \\\\<br>&amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}{\\sum_{t=0}^\\infty \\gamma^t P\\left(\\left.s_t=s\\right\\vert \\tilde\\pi\\right)} } \\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\\\<br>&amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}{\\rho_\\tilde\\pi(s)} }\\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\\\<br>\\end{align}<br>$$</p>\n<p>이 수식의 의미가 무엇일까요? 만약 $\\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\geq 0$이라면 $\\eta(\\tilde\\pi)$는 항상 $\\eta(\\pi)$보다 큽니다. 즉, policy를 업데이트함으로써 항상 개선됩니다. 즉, 이 수식을 통해서 항상 더 좋은 성능을 내는 policy update가 가져야 할 특징을 알 수 있습니다. 다음과 같은 deterministic policy가 있다고 합시다.</p>\n<p>$$<br>\\tilde\\pi(s) = \\arg\\max_a A_\\pi(s,a)<br>$$</p>\n<p>이 policy는 적어도 하나의 state-action pair에서 0보다 큰 값을 가지는 advantage가 있고 그 때의 확률이 0이 아니라면 항상 성능을 개선시킵니다. 어려운 점은 policy를 바꾸면 $\\rho$도 바뀐다는 점입니다. </p>\n<p>다음 그림을 봅시다. Starting Point에서 여러가지 경로를 거쳐서 Destination으로 갈 수 있습니다. 다른 policy를 이용하는 것은 다른 경로를 이용한 것입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=16s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/policy_change.png\" alt=\"policy_change\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>이 때 다른 policy를 이용함에 따라 state visitation frequency도 변하게 됩니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=36s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/state_visitation_change.png\" alt=\"state_visitation_change\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>이로 인해서 policy를 최적화하는 것은 어려워집니다. 그래서 이러한 변화를 무시하는 다음과 같은 approximation을 취합니다.</p>\n<p>$$<br>\\begin{align}<br>L_\\pi\\left(\\tilde\\pi\\right) &amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}{\\rho_\\pi(s)} } \\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a)<br>\\end{align}<br>$$</p>\n<p>policy가 바뀌었음에도 이전의 state distribution을 계속 이용하는 것입니다. 이것은 policy의 변화가 크지 않다면 어느 정도 허용될 수 있을 것입니다. 그렇지만 얼마나 많은 변화가 허용될까요? 이것을 정하기 위해서 이용하는 것이 trust region입니다.</p>\n<p><br></p>\n<h2 id=\"2-2-Conservative-Policy-Iteration\"><a href=\"#2-2-Conservative-Policy-Iteration\" class=\"headerlink\" title=\"2.2 Conservative Policy Iteration\"></a>2.2 Conservative Policy Iteration</h2><p>policy의 변화를 다루기 용이하게 하기 위해서 policy를 다음과 같이 파라미터를 이용해서 표현합시다.</p>\n<ul>\n<li>$\\pi_\\theta(a\\vert s)$: parameterized policy</li>\n</ul>\n<p>$\\pi_\\theta$는 $\\theta$에 대하여 미분가능한 함수입니다. $L_\\pi\\left(\\tilde\\pi\\right)$을 $\\theta_0$에서 $\\eta(\\pi)$에 대한 first order approximation이라고 하면 다음 식이 성립합니다.</p>\n<p>$$<br>\\begin{align}<br>L_{\\pi_{\\theta_0} }\\left(\\pi_{\\theta_0}\\right) &amp;= \\eta\\left(\\pi_{\\theta_0}\\right) \\\\<br>\\nabla_\\theta \\left.L_{\\pi_{\\theta_0} }\\left(\\pi_{\\theta_0}\\right)\\right\\vert _{\\theta=\\theta_0} &amp;= \\nabla_\\theta\\left.\\eta(\\pi_{\\theta_0})\\right\\vert _{\\theta=\\theta_0}<br>\\end{align}<br>$$</p>\n<p>이것의 의미는 $\\pi_{\\theta_0}$가 매우 작게 변한다면 $L_{\\pi_{\\theta_0} }$를 개선시키는 것이 $\\eta$를 개선시키는 것이라는 것입니다. 그러나 지금까지의 설명만으로는 $\\pi_{\\theta_0}$를 얼마나 작게 변화시켜야 할지에 대해서는 알 수 없습니다.</p>\n<p>Kakade &amp; Langford가 2002년 발표한 논문에서도 이것에 대해서 고민했습니다. 그 논문에서 <em>conservative policy iteration</em>이라는 기법을 제안합니다. 그 논문의 contribution은 다음과 같습니다.</p>\n<ul>\n<li>$\\eta$ 개선의 lower bound를 제공</li>\n<li>기존의 policy를 $\\pi_\\mathrm{old}$라고 하고 $\\pi’$를 $\\pi’=\\arg\\max_{\\pi’}L_{\\pi_\\mathrm{old} }\\left(\\pi’\\right)$과 같이 정의할 때, 새로운 mixture policy $\\pi_\\mathrm{new}$를 다음과 같이 제안<br>$$<br>\\pi_\\mathrm{new}(a\\vert s) = (1-\\alpha)\\pi_\\mathrm{old}(a\\vert s) + \\alpha \\pi’(a\\vert s)<br>$$</li>\n</ul>\n<p>그림으로 표현하면 다음과 같습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=2m46s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/mixure_policy.png\" alt=\"mixure_policy\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<ul>\n<li>다음과 같은 lower bound를 정의<br>$$<br>\\eta\\left(\\pi_\\mathrm{new}\\right) \\geq L_{\\pi_{\\theta_\\mathrm{old} }}\\left(\\pi_\\mathrm{new}\\right) - \\frac{2\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\\\<br>\\mathrm{where\\ }\\epsilon = \\max_s\\left\\vert E_{a\\sim\\pi’(a\\vert s)}\\left[A_\\pi(s,a)\\right]\\right\\vert<br>$$</li>\n</ul>\n<p>하지만 mixture된 policy라는 것은 실용적이지 않습니다. </p>\n<p><br><br></p>\n<h1 id=\"3-Monotonic-Improvement-Guarantee-for-General-Stochastic-Policies\"><a href=\"#3-Monotonic-Improvement-Guarantee-for-General-Stochastic-Policies\" class=\"headerlink\" title=\"3. Monotonic Improvement Guarantee for General Stochastic Policies\"></a>3. Monotonic Improvement Guarantee for General Stochastic Policies</h1><p>이전 장에서 설명한 lower bound는 오직 mixture policy에 대해서만 성립하고 더 많이 사용되는 stochastic policy에는 적용되지 않습니다. 따라서 stochastic policy를 이용할 수 있도록 개선할 필요가 있습니다. 아래와 같이 기존 수식에서 두 가지를 바꿈으로써 이것이 가능합니다.</p>\n<ul>\n<li>$\\alpha\\rightarrow$ distance measure between $\\pi$ and $\\tilde\\pi$</li>\n<li>constant $\\epsilon\\rightarrow\\max_{s,a}\\left\\vert A_\\pi(s,a)\\right\\vert $</li>\n</ul>\n<p>여기서 distance measure로 total variation divergence를 이용합니다. discrete porbability distribution $p$와 $q$에 대하여 다음과 같이 정의됩니다.<br>$$<br>D_\\mathrm{TV}(p\\parallel q) = \\frac{1}{2}\\sum_i\\left\\vert p_i - q_i\\right\\vert<br>$$</p>\n<p>이것을 이용하여 $D_\\mathrm{TV}^\\max$를 다음과 같이 정의합니다.<br>$$<br>D_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D_\\mathrm{TV}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)<br>$$</p>\n<p>그림으로 표현하면 다음과 같습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=3m15s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/tvd.png\" alt=\"tvd\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>이것을 이용하여 다음과 같은 관계식을 얻을 수 있습니다.</p>\n<p><strong>Theorem 1.</strong> Let $\\alpha=D_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi)$. Then the following bound holds:<br>$$<br>\\begin{align}<br>\\eta\\left(\\pi_\\mathrm{new}\\right) &amp;\\geq L_{\\pi_\\mathrm{old} }\\left(\\pi_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2, \\\\<br>\\mathrm{where\\ } \\epsilon&amp;= \\max_{s,a}\\left\\vert A_\\pi(s,a)\\right\\vert<br>\\end{align}<br>$$<br><!--*Proof.* TBD.--></p>\n<p>또다른 distance metric으로 아래 그림과 같은 KL divergence가 있습니다.<br>(그런데 왜 하필 KL divergence로 바꿀까요? 논문의 뒤쪽에서 계산효율을 위해서 conjugate gradient method를 이용하는데 이를 위해서 바꾼게 아닐까 싶습니다. Wasserstein distance 같은 다른 방향으로 발전시킬 수도 있을 것 같습니다. Schulmann이 아마 해봤겠죠?^^a)</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=3m34s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/kld.png\" alt=\"kld\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>total variation divergence와 KL divergence 사이에는 다음과 같은 관계가 있습니다.<br>$$<br>D_\\mathrm{TV}(p\\parallel q)^2 \\leq D_\\mathrm{KL}(p\\parallel q)<br>$$</p>\n<p>다음 수식을 정의합니다.<br>$$<br>D_\\mathrm{KL}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D_\\mathrm{KL}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)<br>$$</p>\n<p><strong>Theorem 1</strong>을 이용하여 다음과 같은 수식이 성립함을 알 수 있습니다.<br>$$<br>\\begin{align}<br>\\eta\\left(\\tilde\\pi\\right) &amp;\\geq L_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max(\\pi, \\tilde\\pi), \\\\<br>\\mathrm{where\\ } C&amp;= \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}<br>\\end{align}<br>$$</p>\n<p>이러한 policy imporvement bound를 기반으로 다음과 같은 approximate policy iteration 알고리듬을 고안해낼 수 있습니다.</p>\n<p><strong>Algorithm 1</strong> Policy iteration algorithm guaranteeing non-decreasing expected return $\\eta$</p>\n<blockquote>\n<p>Initialize $\\pi_0$<br><strong>for</strong> $i=0,1,2,\\ldots$ until convergence <strong>do</strong><br>$\\quad$Compute all advantage values $A_{\\pi_i}(s,a)$<br>$\\quad$Solve the contstrained optimization problem<br>$\\quad\\pi_{i+1}=\\arg\\max_\\pi\\left[L_{\\pi_i} - CD_\\mathrm{KL}^\\max\\left(\\pi_i,\\pi\\right)\\right]$<br>$\\quad\\quad$where $C = 4\\epsilon\\gamma / (1-\\gamma)^2$<br>$\\quad\\quad$and $L_{\\pi_i}\\left(\\pi\\right) = \\eta(\\pi_i) + \\sum_{s}\\rho_{\\pi_i}(s)\\sum_a\\pi(a\\vert s) A_{\\pi_i}(s,a)$<br><strong>end for</strong></p>\n</blockquote>\n<p><strong>Algorithm 1</strong>은 advantage를 정확하게 계산할 수 있다고 가정하고 있습니다. 이 알고리듬은  monotonical한 성능 증가($\\eta(\\pi_0)\\leq\\eta(\\pi_1)\\leq\\cdots$)를 한다는 것을 다음과 같이 보일 수 있습니다. $M_i(\\pi)=L_{\\pi_i}(\\pi) - CD_\\mathrm{KL}^\\max\\left(\\pi_i,\\pi\\right)$라고 합시다.</p>\n<p>$$<br>\\begin{align}<br>\\eta \\left(\\pi_{i+1}\\right) &amp;\\geq M_i\\left(\\pi_{i+1}\\right)\\\\<br>\\eta \\left(\\pi_{i}\\right) &amp;= M_i\\left(\\pi_{i}\\right) \\\\<br>\\eta \\left(\\pi_{i+1}\\right) - \\eta \\left(\\pi_{i}\\right) &amp;\\geq M_i\\left(\\pi_{i+1}\\right) - M_i\\left(\\pi_{i}\\right)<br>\\end{align}<br>$$</p>\n<p>위 수식과 같이 매 iteration 마다 $M_i$를 최대화함으로써, $\\eta$가 감소하지 않는다는 것을 보장할 수 있습니다. 이와 같은 타입의 알고리듬을 <a href=\"https://www.jstor.org/stable/pdf/27643496.pdf?casa_token=0qHamcl60WoAAAAA:-fkZ9JcA_nrY3-zbCUpqvPOgcAMgw7Gr96MajCZg2byHf8m5GU1KTSxyJJcBy1lPZbBTZVCjHHUXilh4k-iuwF91Wka4B5qdltC1IR2qMWk8q1FoV6__\" target=\"_blank\" rel=\"noopener\">minorization-maximization (MM) algorithm</a>이라고 합니다. EM 알고리듬도 이 타입의 알고리듬 중 하나입니다.</p>\n<p><strong>Algorithm 1</strong>은 아래 그림과 같이 동작합니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=3m52s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/surrogate.png\" alt=\"surrogate\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>$M_i$는 $\\pi_i$일 때 equality가 되는 $\\eta$에 대한  surrogate function입니다. TRPO는 이 surrogate function을 최대화하고 KL divergence를 penalty가 아닌 constraint로 두는 알고리듬입니다.</p>\n<p><br><br></p>\n<h1 id=\"4-Optimization-of-Parameterized-Policies\"><a href=\"#4-Optimization-of-Parameterized-Policies\" class=\"headerlink\" title=\"4. Optimization of Parameterized Policies\"></a>4. Optimization of Parameterized Policies</h1><p>표기의 편의를 위해 다음과 같이 notation들을 더 간략하게 정의합니다.</p>\n<ul>\n<li>$\\eta(\\theta):=\\eta\\left(\\pi_\\theta\\right)$</li>\n<li>$L_\\theta\\left(\\tilde\\theta\\right):=L_{\\pi_\\theta}\\left(\\pi_{\\tilde\\theta}\\right)$</li>\n<li>$D_\\mathrm{KL}\\left(\\theta\\parallel\\tilde\\theta\\right):=D_\\mathrm{KL}\\left(\\pi_\\theta\\parallel\\pi_{\\tilde\\theta}\\right)$</li>\n<li>$\\theta_\\mathrm{old}$: previous policy parameter</li>\n</ul>\n<p><br></p>\n<h2 id=\"4-1-Trust-Region-Policy-Optimization\"><a href=\"#4-1-Trust-Region-Policy-Optimization\" class=\"headerlink\" title=\"4.1 Trust Region Policy Optimization\"></a>4.1 Trust Region Policy Optimization</h2><p>이전 장의 중요 결과를 위의 notation으로 다시 표기하면 아래와 같습니다.</p>\n<p>$$<br>\\eta\\left(\\theta\\right) \\geq L_{\\theta_\\mathrm{old} }\\left(\\theta\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)<br>$$</p>\n<p>$\\eta$의 성능 향상을 보장하기 위해서 그것의 lower bound를 최대화할 수 있습니다.</p>\n<p>$$<br>\\max_\\theta \\left[L_{\\theta_\\mathrm{old} }\\left(\\theta\\right) - C D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)\\right]<br>$$</p>\n<p>이 최적화 문제는 step size를 매우 작게 해야 올바른 동작을 합니다. 위에서 살펴봤듯이 first order approximation이기 때문입니다. 좀 더 큰 step size를 가질 수 있도록 이 최적화 문제를 trust region constraint를 도입하여 다음과 같이 바꿉니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>이 최적화문제의 constraint는 모든 state space에 대해서 성립해야 합니다. 또한 maximum값을 매번 찾아야 합니다. state가 많은 경우 constraint의 수가 매우 많아져서 문제를 풀기 어렵게 만듭니다. constraint의 수를 줄이기 위하여 다음과 같은 avergae KL divergence를 이용하는 heuristic approximation을 취합니다. 이것이 최선의 방법은 아닐 수 있지만 실용적인 방법입니다.</p>\n<p>$$<br>\\overline D_\\mathrm{KL}^{\\rho}\\left(\\theta_1, \\theta_2\\right):=E_{s\\sim\\rho}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_1}(\\cdot\\vert s)\\parallel\\pi_{\\theta_2}(\\cdot\\vert s)\\right)\\right]<br>$$</p>\n<p>이것을 기반으로 다음과 같은 최적화 문제를 풀 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>아래 그림처럼 step size에 대해서 고려할 수 있습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=4m35s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/heuristic_approx.png\" alt=\"heuristic_approx\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p><br><br></p>\n<h1 id=\"5-Sample-Based-Estimation-of-the-Objective-and-Constraint\"><a href=\"#5-Sample-Based-Estimation-of-the-Objective-and-Constraint\" class=\"headerlink\" title=\"5. Sample-Based Estimation of the Objective and Constraint\"></a>5. Sample-Based Estimation of the Objective and Constraint</h1><p>실용적인 알고리듬을 만들려고 하는 노력은 아직 끝나지 않았습니다. 이제 앞의 알고리듬을 sample-based estimation 즉, Monte Carlo estimation을 할 수 있도록 바꿔보겠습니다. sampling을 편하게 할 수 있도록 아래와 같이 바꿔줍니다.</p>\n<ul>\n<li>$\\sum_s \\rho_{\\theta_\\mathrm{old} }(s)[\\ldots]\\rightarrow\\frac{1}{1-\\gamma}E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}[\\ldots]$</li>\n<li>$A_{\\theta_\\mathrm{old} }\\rightarrow Q_{\\theta_\\mathrm{old} }$</li>\n<li>$\\sum_a\\pi_{\\theta_\\mathrm{old} }(a\\vert s)A_{\\theta_\\mathrm{old} } \\rightarrow E_{a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\mathrm{old} }\\right]$</li>\n</ul>\n<p>이러한 변화를 그림처럼 도식화할 수 있습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=5m31s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/sample-based.png\" alt=\"sample-based\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>한 가지 짚고 넘어가야 할 점은 action sampling을 할 때 importance sampling을 사용한다는 것입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=5m53s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/importance_sampling.png\" alt=\"importance_sampling\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>바뀐 최적화문제는 아래와 같습니다.</p>\n<ul>\n<li><strong>Equation (14).</strong></li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}<br>$$</p>\n<p>이 때 sampling하는 두 가지 방법이 있습니다.</p>\n<p><br></p>\n<h2 id=\"5-1-Single-Path\"><a href=\"#5-1-Single-Path\" class=\"headerlink\" title=\"5.1 Single Path\"></a>5.1 Single Path</h2><p><em>single path</em>는 개별 trajectory들을 이용하는 방법입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=6m15s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/single.png\" alt=\"single\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p><br></p>\n<h2 id=\"5-2-Vine\"><a href=\"#5-2-Vine\" class=\"headerlink\" title=\"5.2 Vine\"></a>5.2 Vine</h2><p><em>vine</em>은 한 state에서 rollout을 이용하여 여러 action을 수행하는 방법입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=6m32s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/vine1.png\" alt=\"vine1\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=6m49s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/vine2.png\" alt=\"vine2\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>estimation의 variance를 낮출 수 있지만 계산량이 많고 한 state에서 여러 action을 수행할 수 있어야 하기 때문에 현실적인 문제에 적용하기에는 어려움이 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"6-Practical-Algorithm\"><a href=\"#6-Practical-Algorithm\" class=\"headerlink\" title=\"6. Practical Algorithm\"></a>6. Practical Algorithm</h1><p>앞서 single-path, vine 샘플링을 사용하는 두가지 방식의 policy optimization 알고리즘을 살펴봤습니다. 실용적인 알고리듬은 아래의 과정을 반복해서 수행합니다.</p>\n<p>1) Q-values의 몬테카를로 추정을 통해 state-action 쌍 집합을 single path 또는 vine 과정을 통해 수집함.</p>\n<p>2) 샘플 평균으로, (14)식의 목적함수와 제약식을 추정함</p>\n<ul>\n<li><strong>Equation (14).</strong><br>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}<br>$$</li>\n</ul>\n<p>3) policy parameter vector인 $\\theta$를 업데이트 하면서 제약조건이 있는 최적화 문제를 근사적으로 풂. 본 논문에서는 gradient를 직접 계산하는 것보다는 약간 더 계산량이 있는 line search와 conjugate gradient algorithm을 사용했습니다.</p>\n<p>3)에 대해서,  gradient의 covariance matrix를 사용하지 않고 KL divergence의 Hessian을 해석적으로 계산하여 Fisher Information Matrix를 구성했습니다.</p>\n<p>다시말해서</p>\n<p>$\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial}{\\partial \\theta_i} \\log \\pi_{\\theta}(a_n\\vert s_n) \\frac{\\partial}{\\partial \\theta_j} \\log \\pi_{\\theta}(a_n\\vert s_n)$ 대신 $\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} D_{KL}(\\pi_{\\theta_{old} }(\\cdot \\vert  s_n) \\vert \\vert  \\pi_{\\theta}(\\cdot \\vert  s_n))$를 계산한 것입니다.</p>\n<p>이 analytic estimator는 Hessian이나 trajectories의 모든 gradient를 저장하지 않아도 되기 때문에 대규모 환경을 고려할 경우 계산 상 이점이 있습니다. 좀 더 자세하게 설명한 Appendix C 부분을 살펴보겠습니다.</p>\n<p><br></p>\n<h2 id=\"Appendix-C-Efficiently-Solving-the-Trust-Region-Constrained-Optimization-Problem\"><a href=\"#Appendix-C-Efficiently-Solving-the-Trust-Region-Constrained-Optimization-Problem\" class=\"headerlink\" title=\"Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem\"></a>Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem</h2><p>우리가 풀고자 하는 식은 다음과 같습니다.</p>\n<p>$$<br>\\begin{align}<br>\\max\\quad &amp;L(\\theta) \\\\<br>\\mathrm{s.t.\\ } &amp;\\overline{D}_{KL}(\\theta_\\mathrm{old}, \\theta) \\leq \\delta<br>\\end{align}<br>$$</p>\n<p>이 최적화 문제를 풀기 위하여 우리는 gradient 기법을 이용할 것입니다. gradient 기법은 결국 update할 방향과 크기를 결정하는 문제로 생각할 수 있습니다. 이에 따라 아래와 같은 두가지 과정으로 진행합니다.</p>\n<p>1) 탐색 방향을 계산, 목적함수의 first-order approximation(선형근사)와 제약식의 quadratic approximation(2차 근사).</p>\n<p>2) 이동거리 계산을 위해 해당 방향으로 line search 수행.</p>\n<p>탐색 방향은 $Ax = g$ 수식을 근사적으로 풀어서 구합니다. 여기서 $A$ 는 Fisher Information Matrix이고, 이것은 KL divergence 제약식의 2차 근사 $\\overline{D}_\\mathrm{KL} (\\theta_\\mathrm{old}, \\theta) \\approx \\frac{1}{2}(\\theta - \\theta_\\mathrm{old})^T A(\\theta - \\theta_\\mathrm{old})$를 푸는 것입니다. 여기서 $A_{ij} = \\frac{\\partial}{\\partial \\theta_i} \\frac{\\partial}{\\partial \\theta_j} \\overline{D}_\\mathrm{KL}(\\theta_\\mathrm{old}, \\theta)$입니다.</p>\n<p>논문에서 이 부분에 대한 설명이 약간 부족해서 좀 더 내용을 추가하자면, $\\frac{1}{2}x^T A x - x^T g + b = 0$과 같은 quadratic equation의 해는 1차 derivative를 0으로 만드는 $x$를 찾음으로써 알 수 있습니다. 위에서 우리는 목적함수를 1차 근사시키고 제약조건은 2차 근사를 시킴으로써 quadratic equation 형태로 변형했기 때문에 같은 방식으로 풀 수 있습니다. 따라서 $g$는 1차근사시킨 목적함수의 계수가 될 것입니다.</p>\n<p>대규모 문제에서 $A$ 혹은 $A^{-1}$ 을 계산하는 것은 엄청난 계산량이 필요합니다. 하지만 conjugate gradient algorithm을 잘 이용하면 전체 $A$행렬(FIM)을 계산하지 않아도 $Ax=g$ 를 근사적으로 해결할 수 있게 해줍니다.</p>\n<p>탐색 방향 $s \\approx A^{-1}g$과 더불어 최대스텝길이(step size) $\\beta$도 계산할 필요가 있습니다.</p>\n<p>즉, $\\delta = \\overline{D}_\\mathrm{KL} \\approx \\frac{1}{2}(\\beta s)^T A(\\beta s) = \\frac{1}{2}\\beta^2s^TAs$ 이고, </p>\n<p>$\\beta = \\sqrt{2\\delta / s^T As}$가 됩니다. 이때 $\\delta$는 KL divergence의 boundary term 입니다.</p>\n<p>$s^T As$ 항은 Hessian-vector product로 계산할 수 있고, conjugate gradient 과정에서 계산됩니다.</p>\n<p>마지막으로, surrogate objective와 KL divergence 제약식을 위해 다음과 같은 함수를 정의하고 line search를 사용합니다.</p>\n<p>$$<br>L_{\\theta_\\mathrm{old} }(\\theta) - \\chi [\\overline{D}_{KL}(\\theta_\\mathrm{old}, \\theta) \\leq \\delta]<br>$$</p>\n<p>$\\chi[\\cdot]$ 함수는 []안의 조건이 맞으면 0이고 틀리면 무한으로 발산하는 함수입니다.</p>\n<p>위에서 계산한 $\\beta$ 의 최대 값부터 시작해서 목적함수가 개선될때까지 exponential하게 줄여 나갑니다. line search가 없었다면 이 알고리즘은 매우 큰 스텝으로 계산될 것이고 심각한 성능 저하가 발생할 것입니다.</p>\n<p>(여기서 살짝 예고를 하자면, 나중에 이 KL 기반 기법을 개선한 더 단순한 방법이 제안됩니다. PPO라고…)</p>\n<p><br><br></p>\n<h1 id=\"7-Connections-with-Prior-Work\"><a href=\"#7-Connections-with-Prior-Work\" class=\"headerlink\" title=\"7. Connections with Prior Work\"></a>7. Connections with Prior Work</h1><p>Natural Policy Gradient는 $L$의 선형 근사와 $\\overline D_\\mathrm{KL}$ 제약식을 2차근사 하는 아래의 식의 special case입니다.</p>\n<ul>\n<li>Equation (17).</li>\n</ul>\n<p>$L_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A_\\pi(s,a)$</p>\n<p>$\\underset{\\theta}{\\max}\\quad [\\nabla_{\\theta}L_{\\theta_\\mathrm{old} }(\\theta) \\ \\rvert_{\\theta=\\theta_\\mathrm{old} } \\cdot (\\theta - \\theta_\\mathrm{old})]$</p>\n<p>$\\mathrm{s.t.}\\quad\\frac{1}{2}(\\theta_\\mathrm{old} - \\theta)^{T} A(\\theta_\\mathrm{old})(\\theta_\\mathrm{old}-\\theta) \\leq \\delta$</p>\n<p>$\\mathrm{where\\ }A(\\theta_{old})_{ij} = $</p>\n<p>$\\Large \\frac{\\partial}{\\partial \\theta_{i} } \\frac{\\partial}{\\partial \\theta_{j} }<br>E_{s \\sim \\rho_{\\pi} }[D_\\mathrm{KL}(\\pi(\\cdot\\rvert s, \\theta_\\mathrm{old})  \\rvert\\rvert \\pi(\\cdot \\rvert s, \\theta))] \\rvert_{\\theta=\\theta_\\mathrm{old} }$</p>\n<p>업데이트 식은 다음과 같습니다.</p>\n<p>$\\theta_\\mathrm{new} = \\theta_\\mathrm{old} +  \\color{Red}{\\frac{1}{\\lambda} } A(\\theta_\\mathrm{old})^{-1}\\nabla_{\\theta}L(\\theta) \\rvert_{\\theta=\\theta_\\mathrm{old} }$</p>\n<p>여기서 step size인 $\\frac{1}{\\lambda}$ 일반적으로 알고리즘의 파라미터로 취급되지만 TRPO는 각 업데이트 마다 제약식으로 사용한다는 점이 다릅니다. 사소한 차이처럼 보이지만, 큰 차원을 다루는 실험에서 성능을 크게 향상시켰습니다.</p>\n<p>또한 $L^2$ 제약식(혹은 페널티) 를 사용하면 표준 Policy Gradient 업데이트 식을 얻었습니다.</p>\n<ul>\n<li>Equation (18).</li>\n</ul>\n<p>$\\underset{\\theta}{\\max}\\quad[\\nabla_{\\theta} L_{\\theta_\\mathrm{old} }(\\theta) \\rvert_{\\theta=\\theta_\\mathrm{old} } \\cdot (\\theta - \\theta_\\mathrm{old})]$</p>\n<p>$\\mathrm{s.t.}\\quad \\frac{1}{2} \\vert\\vert \\theta - \\theta_\\mathrm{old} \\vert\\vert^2 \\leq \\delta$</p>\n<p>$L_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A_\\pi(s,a) $를 이용해서 제약조건없이 $\\underset{\\pi}{\\max}\\quad L_{\\pi_\\mathrm{old} }(\\pi)$를 풀면<br>Policy Iteration update를 하는 것과 같습니다.</p>\n<p><br><br></p>\n<h1 id=\"8-Experiments\"><a href=\"#8-Experiments\" class=\"headerlink\" title=\"8. Experiments\"></a>8. Experiments</h1><ul>\n<li><p>다음과 같은 3 가지 궁금증을 해결하기 위하여 실험을 설계하였습니다.</p>\n<ol>\n<li><p>Single-path 와 vine 의 성능면에서 특징을 알고싶음.</p>\n</li>\n<li><p>Fixed penalty coefficient(NPG) 보다 fixed KL divergence를 사용하는 것(TRPO)으로<br>변경한 것이 얼마나 뚜렷한 차이를 보일지, 성능 면에서 어떤 영향을 미치는지 알고싶음.</p>\n</li>\n<li><p>TRPO가 큰 스케일의 문제를 해결할 수 있는지, 기존에 연구/적용되어왔던 방법들과<br>성능, 계산시간, 샘플 복잡도 면에서 비교하고 싶음.</p>\n</li>\n</ol>\n</li>\n</ul>\n<ul>\n<li><p>1, 2의 궁금증에 대해서 실험 환경에 single path, vine을 비롯한 여러 사전방법들을 함께 실험하였습니다.</p>\n</li>\n<li><p>3에 대해선, robotic locomotion과 Atari game에 TRPO를 실험하였습니다.</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"8-1-Simulated-Robotic-Locomotion\"><a href=\"#8-1-Simulated-Robotic-Locomotion\" class=\"headerlink\" title=\"8.1 Simulated Robotic Locomotion\"></a>8.1 Simulated Robotic Locomotion</h2><ul>\n<li>MuJoCo 시뮬레이터를 활용해 로봇 locomotion 실험 진행하였습니다.</li>\n<li>Robot State: positions and velocities, joint torques</li>\n</ul>\n<p><img src=\"https://i.imgur.com/bs5ATS3.png\" alt=\"\"></p>\n<ul>\n<li><p>수영, 점프, 걷기 행동을 학습시키고자 합니다.</p>\n<ul>\n<li><p>수영: 10 dimensional state space , reward: $r(x,u)=v_x - 10^{-5}\\vert \\vert u\\vert \\vert ^2$, quadratic penalty.</p>\n</li>\n<li><p>점프: 12 dimensional state space, episode의 끝은 높이와 각도 threshol로 점프를 판단, non-terminal state에 대해 +1 보상 추가.</p>\n</li>\n<li><p>걷기: 18 dimensional state space, 뜀뛰는 듯한 보법(점프에이전트)보다 부드러운 보법을 유도하기위해 페널티를 추가함.</p>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Detailed Experiment Setup and used parameters, used network model</strong><br><img src=\"https://i.imgur.com/zgnsbw6.png\" alt=\"\"></p>\n<p><img src=\"https://i.imgur.com/FqdWC53.png\" alt=\"\"></p>\n<p>equation (12) : $\\max\\quad L_{\\theta_\\mathrm{old}(\\theta)}, \\ \\ \\mathrm{s.t.\\ }\\overline{D}_\\mathrm{KL}^{\\rho_{\\theta_\\mathrm{old} }}(\\theta_\\mathrm{old}, \\theta) \\leq \\delta$</p>\n<p>이 실험에서 $\\delta = 0.01$입니다.</p>\n<ul>\n<li><p>비교에 사용된 모델들</p>\n<ul>\n<li>TRPO Single-path</li>\n<li>TRPO vine</li>\n<li>CEM(cross-entropy method)</li>\n<li>CMA(covariance matrix adaptation)</li>\n<li>NPG (Lagrange Multiplier penalty coefficien를 사용하는 것이 차이점)</li>\n<li>Empirical FIM</li>\n<li>maximum KL(cartpole)</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://i.imgur.com/wSqolvS.png\" alt=\"\"></p>\n<ul>\n<li><p>제안한 single path, vine 모두 task를 잘 학습하는 것을 확인할 수 있습니다.</p>\n</li>\n<li><p>페널티를 고정하는 것보다, KL divergenc를 제약하는 것이 step size를 선택하는 부분에서 Robust합니다.</p>\n</li>\n<li><p>CEM/CMA는 derivative free 알고리즘으로, 샘플 복잡도가 네트워크 파라미터수 만큼 확장되어 high dimension 문제에서는 속도와 성능이 약한 모습을 보입니다.</p>\n</li>\n<li><p>maxKL은 제안방법보다 느린 학습성능을 보입니다.</p>\n</li>\n</ul>\n<p>TRPO video link: <a href=\"https://www.youtube.com/watch?v=jeid0wIrSn4\" target=\"_blank\" rel=\"noopener\">https://www.youtube.com/watch?v=jeid0wIrSn4</a></p>\n<ul>\n<li><p>사전지식이 적은 상태에서 여러 액션을 학습하는 실험으로 TRPO의 성능을 보입니다.</p>\n</li>\n<li><p>지금까지의 균형이나 걸음 같은 개념을 명시적으로 인코딩한 robotic locomotion에서의 사전 연구들과 차이를 보였습니다.</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"8-2-Playing-Games-from-Images\"><a href=\"#8-2-Playing-Games-from-Images\" class=\"headerlink\" title=\"8.2 Playing Games from Images\"></a>8.2 Playing Games from Images</h2><ul>\n<li>부분관찰가능하며 복잡한 observation인 환경에서 TRPO를 평가하기위해 raw image를 입력으로 하는 아타리게임 환경에서 실험하였습니다.</li>\n</ul>\n<ul>\n<li>Challenging point :<ul>\n<li>High dimensional space</li>\n<li>delayed reward</li>\n<li>non-stationary image (Enduro는 배경 이미지가 바뀌고, 반전처리가 됨)</li>\n<li>complex action sequence (Q*bert는 21개의 다른 발판에서 점프 해야함)</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>ALE(Arcade Learning Environment) 에서 테스트</li>\n<li><p>DQN과 전처리 방식은 동일<br>  210 x 160 pixel -&gt; gray-scaling &amp; down-sampling -&gt; 110 x 84  -&gt; crop -&gt; 84 x 84</p>\n</li>\n<li><p>2 convolution layer ( 16 channels) , stride 2, FC layer with 20 units</p>\n</li>\n</ul>\n<p><img src=\"https://i.imgur.com/NJBC69d.png\" alt=\"\"> </p>\n<p><img src=\"https://i.imgur.com/wTe1OEW.png\" alt=\"\"></p>\n<p><img src=\"https://i.imgur.com/j22VtNQ.png\" alt=\"\"></p>\n<ul>\n<li><p>30시간 동안 500 iteration 학습을 하였습니다.</p>\n</li>\n<li><p>DQN과 MCST를 사용한 모델(UCC-I), 사람이 플레이한 것(Human) 과의 비교하였습니다.</p>\n</li>\n<li><p>이전 방법들에 비해 압도적인 점수를 기록하진 못했으나, 다양한 게임에서 적절한 게임 기록하였습니다.</p>\n</li>\n<li><p>이전 방법들과 다른 점은, 특정 task에 초점을 맞춰 설계하지않았는데 robotics나 game playing Task에도 적절한 성능을 내는 점에서 <strong>TRPO의 일반화 성능</strong>을 확인하였습니다.</p>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"9-Discussion\"><a href=\"#9-Discussion\" class=\"headerlink\" title=\"9. Discussion\"></a>9. Discussion</h1><ul>\n<li><p>Trust Region Policy Optimization 을 제안하였습니다.</p>\n</li>\n<li><p>KL Divergence 페널티로 $\\eta(\\pi)$를 최적화하는 알고리즘이 monotonically improve 함을 증명하였습니다.</p>\n</li>\n<li><p>Locomotion 도메인에서 여러 행동(수영, 걷기, 점프)을 제어하는 controller를 학습하였습니다.</p>\n</li>\n<li><p>Robotics와 게임 실험을 결합해, 시각정보와 센서데이터를 사용하는 로봇제어 정책을 학습시킬 수 있는 가능성을 보았습니다.</p>\n</li>\n<li><p>샘플의 복잡도를 상당히 줄일 수 있어 실제상황에 적용가능성을 보았습니다.</p>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"NPG-여행하기\"><a href=\"#NPG-여행하기\" class=\"headerlink\" title=\"NPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/\">NPG 여행하기</a></h2><h2 id=\"NPG-Code\"><a href=\"#NPG-Code\" class=\"headerlink\" title=\"NPG Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py\" target=\"_blank\" rel=\"noopener\">NPG Code</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"TRPO-Code\"><a href=\"#TRPO-Code\" class=\"headerlink\" title=\"TRPO Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></h2><h2 id=\"GAE-여행하기\"><a href=\"#GAE-여행하기\" class=\"headerlink\" title=\"GAE 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/\">GAE 여행하기</a></h2>"},{"title":"Trust Region Policy Optimization","date":"2018-06-24T07:53:12.000Z","author":"공민서, 김동민","subtitle":"피지여행 5번째 논문","_content":"\n<center> <img src=\"https://www.dropbox.com/s/o7cjcn0e17mpizr/Screen%20Shot%202018-07-18%20at%201.14.22%20AM.png?dl=1\" width=\"700\"> </center>\n\n논문 저자 : John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, Pieter Abbeel\n논문 링크 : https://arxiv.org/pdf/1502.05477.pdf\nProceeding : International Conference on Machine Learning (ICML) 2015\n정리 : 공민서, 김동민\n\n---\n\n# 1. 들어가며...\n\nTrust region policy optimization (TRPO)는 상당히 우수한 성능을 보여주는 policy gradient 기법으로 알려져 있습니다. 높은 차원의 action space를 가진 robot locomotion부터 action은 적지만 화면을 그대로 처리하여 플레이하기 때문에 control parameter가 매우 많은 Atari game까지 각 application에 세부적으로 hyperparameter들을 특화시키지 않아도 두루두루 좋은 성능을 나타내기 때문에 일반화성능이 매우 좋은 기법입니다. 이 TRPO에 대해서 알아보겠습니다.\n\n※TRPO를 매우 재밌게 설명한 Crazymuse AI의 [Youtube video](https://www.youtube.com/watch?v=CKaN5PgkSBc&t=90s)에서 일부 그림을 차용했습니다. 이 비디오를 시청하시는 것을 강력하게 추천합니다!\n\n<br>\n## 1.1 TRPO 흐름 잡기\n\nTRPO 논문은 많은 수식이 등장하여 이 수식들을 따라가다보면 큰 그림을 놓칠 수도 있습니다. 세부적인 내용을 살펴보기 전에 기존 연구에서 출발해서 TRPO로 어떻게 발전해나가는지 간략하게 살펴보겠습니다.\n\n### 1.1.1 Original Problem\n\n$$\\max\\_\\pi \\eta(\\pi)$$\n\n모든 강화학습이 그렇듯이 expected discounted reward를 최대화하는 policy를 찾는 문제로부터 출발합니다.\n\n### 1.1.2 Conservative policy iteration\n\n$$\\max L\\_\\pi(\\tilde\\pi) = \\eta(\\pi) + \\sum\\_s \\rho_\\pi(s)\\sum\\_a\\tilde\\pi(a\\vert s)A\\_\\pi(s,a)$$\n\n$\\eta(\\pi)$를 바로 최대화하는 것은 많은 경우 어렵습니다. $\\eta(\\pi)$의 성능향상을 보장하면서 policy를 update하는 conservative policy iteration 기법이 [Kakade와 Langford](http://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf)에 의하여 제안되었습니다. 이 기법을 이용하면 policy update가 성능을 향상시키는지는 못하더라도 최소한 성능을 악화시키지는 않는다는 것이 이론적으로 보장됩니다.\n\n### 1.1.3 Theorem 1 of TRPO\n\n$$\\max L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\quad\\left(\\alpha=D\\_\\mathrm{TV}^\\max\\left(\\pi\\_\\mathrm{old},\\pi\\_\\mathrm{new}\\right)\\right)$$\n\n기존의 conservative policy iteration은 과거 policy와 새로운 policy를 섞어서 사용해서 실용적이지 않다는 단점이 있었는데 이것을 보완하여 온전히 새로운 policy만으로 update할 수 있는 기법을 제안합니다. \n\n### 1.1.4 KL divergence version of Theorem 1\n\n$$\\max L\\_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\pi,\\tilde\\pi\\right),\\quad\\left(C = \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\right)$$\n\ndistance metric을 KL divergence로 바꿀 수 있습니다.\n\n### 1.1.5 Using parameterized policy\n\n$$\\max\\_\\theta L\\_{\\theta\\_\\mathrm{old} }(\\theta) - C\\cdot D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)$$\n\n최적화문제를 더욱 편리하게 풀 수 있도록 낮은 dimension을 가지는 parameter들로 parameterized된 policy를 사용할 수 있습니다.\n\n### 1.1.6 Trust region constraint\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n1.1.5까지는 아직 conservative policy iteration을 약간 변형시킨 것입니다. policy를 update할 때 지나치게 많이 변하는 것을 방지하기 위하여 [trust region](https://en.wikipedia.org/wiki/Trust_region)을 constraint로 설정할 수 있습니다. 이 아이디어로 인해서 TRPO라는 명칭을 가지게 됩니다.\n\n### 1.1.7 Heuristic approximation\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n사실 1.1.6의 constraint는 모든 state에 대해서 성립해야 하기 때문에 문제를 매우 어렵게 만듭니다. 이것을 좀 더 다루기 쉽게 state distribution에 대한 평균을 취한 것으로 변형합니다.\n\n### 1.1.8 Monte Carlo simulation\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}$$\n\nSampling을 통한 계산이 가능하도록 식을 다시 표현할 수 있습니다. \n\n### 1.1.9 Efficiently solving TRPO\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &\\nabla\\_\\theta \\left. L\\_{\\theta\\_\\mathrm{old} }(\\theta)\\right\\vert \\_{\\theta=\\theta\\_\\mathrm{old} } \\left(\\theta - \\theta\\_\\mathrm{old}\\right) \\\\\\\\\n\\mathrm{s.t.\\ }&\\frac{1}{2}\\left(\\theta\\_\\mathrm{old} - \\theta \\right)^T A\\left(\\theta\\_\\mathrm{old}\\right)\\left(\\theta\\_\\mathrm{old} - \\theta \\right) \\leq \\delta \\\\\\\\\n&A\\_{ij} = \\frac{\\partial}{\\partial \\theta\\_i}\\frac{\\partial}{\\partial \\theta\\_j}\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\n\\end{align}$$\n\n문제를 효율적으로 풀기 위하여 approximation을 적용할 수 있습니다. objective function은 first order approximation, constraint는 quadratic approximation을 취하면 효율적으로 문제를 풀 수 있는 형태로 바뀌는데 이것을 natural gradient로 풀 수도 있고 conjugate gradient로 풀 수도 있습니다.\n\nTRPO는 이렇게 다양한 방법으로 문제를 변형한 것입니다! 이제 좀 더 자세히 살펴보겠습니다.\n\n<br><br>\n\n# 2. Preliminaries\n\n다음과 같은 파라미터를 가지는 infinite-horizon discounted Markov decision process (MDP)를 고려합니다. \n\n* $\\mathcal{S}$: finite set of states\n* $\\\\mathcal{A}$: finite set of actions\n* $P: \\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$: transition probability distribution\n* $r: \\mathcal{S}\\rightarrow\\mathbb{R}$: reward function\n* $\\rho\\_0:\\mathcal{S}\\rightarrow\\mathbb{R}$: distribution of the initial state $s\\_0$\n* $\\gamma\\in(0,1)$: discount factor\n* $\\eta(\\pi)=E\\_{s\\_0,a_0,\\ldots}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r\\left(s\\_t\\right)\\right]$, where $s\\_0\\sim\\rho\\_0\\left(s\\_0\\right), a\\_t\\sim\\pi\\left(\\left.a\\_t \\right\\vert s\\_t\\right), s\\_{t+1}\\sim P\\left(\\left.s\\_{t+1}\\right\\vert s\\_t,a\\_t\\right)$\n* $Q\\_\\pi\\left(s\\_t,a\\_t\\right)=E\\_{ { \\color{red}{s\\_{t+1}} },a\\_{t+1},\\ldots}\\left[\\sum\\_{l=0}^\\infty \\gamma^l r\\left(s\\_{t+l}\\right)\\right]$: action value function\n* $V\\_\\pi\\left(s\\_t\\right)=E\\_{ { \\color{red}{a\\_t} }, s\\_{t+1},a\\_{t+1},\\ldots}\\left[\\sum\\_{l=0}^\\infty \\gamma^l r\\left(s\\_{t+l}\\right)\\right]$: value function (action value function과 expectation의 첨자가 다른 부분을 유의하세요.)\n* $A\\_\\pi(s,a) = Q\\_\\pi(s,a) - V\\_\\pi(s)$: advantage function\n\n<br>\n## 2.1 Useful identity [Kakade & Langford 2002]\n\n우리의 목표는 $\\eta\\(\\pi\\)$가 최대화되도록 만드는 것입니다. 하지만 $\\pi$의 변화에 따라 $\\eta$가 어떻게 변하는지 알아내는 것도 쉽지 않습니다. $\\pi$는 기존의 policy이고 $\\tilde\\pi$는 새로운 policy를 나타낸다고 할 때 $\\eta$와 policy update 사이에 다음과 같은 관계가 있다는 것이 밝혀졌습니다.\n\n**Lemma 1.** $\\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E\\_{s\\_{0},a\\_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right]$\n\n*Proof.* $A\\_\\pi(s,a)=E\\_{s'\\sim P\\left(s'\\vert s,a\\right)}\\left[r(s)+\\gamma V\\_\\pi\\left(s'\\right)-V\\_\\pi(s)\\right]$로 다시 표현할 수 있습니다. 표기의 편의를 위하여 $\\tau:=\\left(s\\_0,a\\_0,s\\_1,a\\_1,\\ldots\\right)$를 정의하겠습니다. 다음과 같은 수식 전개가 가능합니다.\n$$\n\\begin{align}\n&E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right] \\\\\\\\\n &= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t \\left(r(s\\_t)+\\gamma V\\_\\pi\\left(s\\_{t+1}\\right)-V\\_\\pi(s\\_t)\\right)\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\left(\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right)+\\gamma V\\_\\pi\\left(s\\_{1}\\right)-V\\_\\pi(s\\_0)+\\gamma^2 V\\_\\pi\\left(s\\_{2}\\right)-\\gamma V\\_\\pi(s\\_1)+\\gamma^3 V\\_\\pi\\left(s\\_{3}\\right)-\\gamma^2 V\\_\\pi(s\\_2)+\\cdots\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)+\\color{red}{\\gamma V\\_\\pi\\left(s\\_{1}\\right)}-V\\_\\pi(s\\_0)+\\color{red}{\\gamma^2 V\\_\\pi\\left(s\\_{2}\\right)}-\\color{red}{\\gamma V\\_\\pi(s\\_1)}+\\cdots\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[-V\\_\\pi(s\\_0)+\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right] \\\\\\\\\n&\\overset{\\underset{\\mathrm{(a)} }{} }{=} -E\\_{\\color{red}{s\\_0}}\\left[V\\_\\pi(s\\_0)\\right]+E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right] \\\\\\\\\n&=-\\eta(\\pi) + \\eta\\left(\\tilde\\pi\\right)\\\\\\\\\n&\\quad\\therefore \\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E\\_{s\\_{0},a\\_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right]\n\\end{align}\n$$\n위의 전개의 (a) 부분은 $V\\_\\pi\\left(s\\_0\\right)=E\\_{a\\_0,a\\_1,s\\_1,\\ldots}\\left[\\sum\\_{t=0}^\\infty\\gamma^l r\\left(s\\_l\\right)\\right]$이므로 $\\tau$ 중 많은 부분들이 이미 expectation이 취해진 값이므로 무시됩니다. 오직 $s\\_0$만 expectation이 취해지지 않았기 때문에 남아있습니다.\n\n**Lemma 1**은 새로운 policy와 기존의 policy 사이의 관계를 규정합니다. 다음과 같은 식을 정의하고 이것을 이용해서 **Lemma 1**을 변형시켜 봅시다.\n\n* $\\rho\\_\\pi(s) = P\\left(s\\_0=s\\right) + \\gamma P\\left(s\\_1=s\\right)\\ + \\gamma^2 P\\left(s\\_2=s\\right) + \\cdots $: (unnormalized) discounted visitation frequencies\n\n\n\n$$\n\\begin{align}\n\\eta\\left(\\tilde\\pi\\right) &= \\eta(\\pi) + \\sum\\_{t=0}^\\infty\\sum\\_{s}P\\left(\\left.s\\_t=s\\right\\vert \\tilde\\pi\\right)\\sum\\_a\\tilde\\pi(a\\vert s)\\gamma^t A\\_\\pi(s,a) \\\\\\\\\n&= \\eta(\\pi) + \\sum\\_{s}{\\color{red}{\\sum\\_{t=0}^\\infty \\gamma^t P\\left(\\left.s\\_t=s\\right\\vert \\tilde\\pi\\right)} } \\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\\\\\\\\n&= \\eta(\\pi) + \\sum\\_{s}{\\color{red}{\\rho\\_\\tilde\\pi(s)} }\\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\\\\\\\\n\\end{align}\n$$\n\n이 수식의 의미가 무엇일까요? 만약 $\\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\geq 0$이라면 $\\eta(\\tilde\\pi)$는 항상 $\\eta(\\pi)$보다 큽니다. 즉, policy를 업데이트함으로써 항상 개선됩니다. 즉, 이 수식을 통해서 항상 더 좋은 성능을 내는 policy update가 가져야 할 특징을 알 수 있습니다. 다음과 같은 deterministic policy가 있다고 합시다.\n\n$$\n\\tilde\\pi(s) = \\arg\\max_a A\\_\\pi(s,a)\n$$\n\n이 policy는 적어도 하나의 state-action pair에서 0보다 큰 값을 가지는 advantage가 있고 그 때의 확률이 0이 아니라면 항상 성능을 개선시킵니다. 어려운 점은 policy를 바꾸면 $\\rho$도 바뀐다는 점입니다. \n\n다음 그림을 봅시다. Starting Point에서 여러가지 경로를 거쳐서 Destination으로 갈 수 있습니다. 다른 policy를 이용하는 것은 다른 경로를 이용한 것입니다.\n\n[![policy_change](../../../../img/policy_change.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=16s)\n\n이 때 다른 policy를 이용함에 따라 state visitation frequency도 변하게 됩니다.\n\n[![state_visitation_change](../../../../img/state_visitation_change.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=36s)\n\n이로 인해서 policy를 최적화하는 것은 어려워집니다. 그래서 이러한 변화를 무시하는 다음과 같은 approximation을 취합니다.\n\n$$\n\\begin{align}\nL_\\pi\\left(\\tilde\\pi\\right) &= \\eta(\\pi) + \\sum\\_{s}{\\color{red}{\\rho\\_\\pi(s)} } \\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a)\n\\end{align}\n$$\n\npolicy가 바뀌었음에도 이전의 state distribution을 계속 이용하는 것입니다. 이것은 policy의 변화가 크지 않다면 어느 정도 허용될 수 있을 것입니다. 그렇지만 얼마나 많은 변화가 허용될까요? 이것을 정하기 위해서 이용하는 것이 trust region입니다.\n\n<br>\n## 2.2 Conservative Policy Iteration\n\npolicy의 변화를 다루기 용이하게 하기 위해서 policy를 다음과 같이 파라미터를 이용해서 표현합시다.\n\n* $\\pi\\_\\theta(a\\vert s)$: parameterized policy\n\n$\\pi\\_\\theta$는 $\\theta$에 대하여 미분가능한 함수입니다. $L\\_\\pi\\left(\\tilde\\pi\\right)$을 $\\theta\\_0$에서 $\\eta(\\pi)$에 대한 first order approximation이라고 하면 다음 식이 성립합니다.\n\n$$\n\\begin{align}\nL\\_{\\pi\\_{\\theta\\_0} }\\left(\\pi\\_{\\theta_0}\\right) &= \\eta\\left(\\pi\\_{\\theta\\_0}\\right) \\\\\\\\\n\\nabla\\_\\theta \\left.L\\_{\\pi\\_{\\theta\\_0} }\\left(\\pi\\_{\\theta\\_0}\\right)\\right\\vert \\_{\\theta=\\theta\\_0} &= \\nabla\\_\\theta\\left.\\eta(\\pi\\_{\\theta\\_0})\\right\\vert \\_{\\theta=\\theta\\_0}\n\\end{align}\n$$\n\n이것의 의미는 $\\pi\\_{\\theta\\_0}$가 매우 작게 변한다면 $L\\_{\\pi\\_{\\theta\\_0} }$를 개선시키는 것이 $\\eta$를 개선시키는 것이라는 것입니다. 그러나 지금까지의 설명만으로는 $\\pi\\_{\\theta\\_0}$를 얼마나 작게 변화시켜야 할지에 대해서는 알 수 없습니다.\n\nKakade & Langford가 2002년 발표한 논문에서도 이것에 대해서 고민했습니다. 그 논문에서 *conservative policy iteration*이라는 기법을 제안합니다. 그 논문의 contribution은 다음과 같습니다.\n\n* $\\eta$ 개선의 lower bound를 제공\n* 기존의 policy를 $\\pi\\_\\mathrm{old}$라고 하고 $\\pi'$를 $\\pi'=\\arg\\max\\_\\{\\pi'}L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi'\\right)$과 같이 정의할 때, 새로운 mixture policy $\\pi\\_\\mathrm{new}$를 다음과 같이 제안\n$$\n\\pi\\_\\mathrm{new}(a\\vert s) = (1-\\alpha)\\pi\\_\\mathrm{old}(a\\vert s) + \\alpha \\pi'(a\\vert s)\n$$\n\n그림으로 표현하면 다음과 같습니다.\n\n[![mixure_policy](../../../../img/mixure_policy.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=2m46s)\n\n* 다음과 같은 lower bound를 정의\n$$\n\\eta\\left(\\pi\\_\\mathrm{new}\\right) \\geq L\\_{\\pi\\_{\\theta\\_\\mathrm{old} }}\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{2\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\\\\\\\\n\\mathrm{where\\ }\\epsilon = \\max_s\\left\\vert E\\_{a\\sim\\pi'(a\\vert s)}\\left[A\\_\\pi(s,a)\\right]\\right\\vert \n$$\n\n하지만 mixture된 policy라는 것은 실용적이지 않습니다. \n\n<br><br>\n\n# 3. Monotonic Improvement Guarantee for General Stochastic Policies\n\n이전 장에서 설명한 lower bound는 오직 mixture policy에 대해서만 성립하고 더 많이 사용되는 stochastic policy에는 적용되지 않습니다. 따라서 stochastic policy를 이용할 수 있도록 개선할 필요가 있습니다. 아래와 같이 기존 수식에서 두 가지를 바꿈으로써 이것이 가능합니다.\n\n* $\\alpha\\rightarrow$ distance measure between $\\pi$ and $\\tilde\\pi$\n* constant $\\epsilon\\rightarrow\\max\\_{s,a}\\left\\vert A\\_\\pi(s,a)\\right\\vert $\n\n여기서 distance measure로 total variation divergence를 이용합니다. discrete porbability distribution $p$와 $q$에 대하여 다음과 같이 정의됩니다.\n$$\nD\\_\\mathrm{TV}(p\\parallel q) = \\frac{1}{2}\\sum\\_i\\left\\vert p\\_i - q\\_i\\right\\vert \n$$\n\n이것을 이용하여 $D\\_\\mathrm{TV}^\\max$를 다음과 같이 정의합니다.\n$$\nD\\_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D\\_\\mathrm{TV}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)\n$$\n\n그림으로 표현하면 다음과 같습니다.\n\n[![tvd](../../../../img/tvd.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=3m15s)\n\n이것을 이용하여 다음과 같은 관계식을 얻을 수 있습니다.\n\n**Theorem 1.** Let $\\alpha=D\\_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi)$. Then the following bound holds:\n$$\n\\begin{align}\n\\eta\\left(\\pi\\_\\mathrm{new}\\right) &\\geq L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2, \\\\\\\\\n\\mathrm{where\\ } \\epsilon&= \\max\\_{s,a}\\left\\vert A\\_\\pi(s,a)\\right\\vert \n\\end{align}\n$$\n<!--*Proof.* TBD.-->\n\n또다른 distance metric으로 아래 그림과 같은 KL divergence가 있습니다.\n(그런데 왜 하필 KL divergence로 바꿀까요? 논문의 뒤쪽에서 계산효율을 위해서 conjugate gradient method를 이용하는데 이를 위해서 바꾼게 아닐까 싶습니다. Wasserstein distance 같은 다른 방향으로 발전시킬 수도 있을 것 같습니다. Schulmann이 아마 해봤겠죠?^^a)\n\n[![kld](../../../../img/kld.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=3m34s)\n\ntotal variation divergence와 KL divergence 사이에는 다음과 같은 관계가 있습니다.\n$$\nD\\_\\mathrm{TV}(p\\parallel q)^2 \\leq D\\_\\mathrm{KL}(p\\parallel q)\n$$\n\n다음 수식을 정의합니다.\n$$\nD\\_\\mathrm{KL}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D\\_\\mathrm{KL}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)\n$$\n\n**Theorem 1**을 이용하여 다음과 같은 수식이 성립함을 알 수 있습니다.\n$$\n\\begin{align}\n\\eta\\left(\\tilde\\pi\\right) &\\geq L\\_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D\\_\\mathrm{KL}^\\max(\\pi, \\tilde\\pi), \\\\\\\\\n\\mathrm{where\\ } C&= \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\n\\end{align}\n$$\n\n이러한 policy imporvement bound를 기반으로 다음과 같은 approximate policy iteration 알고리듬을 고안해낼 수 있습니다.\n\n**Algorithm 1** Policy iteration algorithm guaranteeing non-decreasing expected return $\\eta$\n> Initialize $\\pi\\_0$   \n> **for** $i=0,1,2,\\ldots$ until convergence **do**   \n> $\\quad$Compute all advantage values $A\\_{\\pi\\_i}(s,a)$    \n> $\\quad$Solve the contstrained optimization problem    \n> $\\quad\\pi\\_{i+1}=\\arg\\max\\_\\pi\\left[L\\_{\\pi\\_i} - CD\\_\\mathrm{KL}^\\max\\left(\\pi\\_i,\\pi\\right)\\right]$     \n> $\\quad\\quad$where $C = 4\\epsilon\\gamma / (1-\\gamma)^2$    \n> $\\quad\\quad$and $L\\_{\\pi\\_i}\\left(\\pi\\right) = \\eta(\\pi\\_i) + \\sum\\_{s}\\rho\\_{\\pi\\_i}(s)\\sum\\_a\\pi(a\\vert s) A\\_{\\pi\\_i}(s,a)$    \n> **end for**\n\n**Algorithm 1**은 advantage를 정확하게 계산할 수 있다고 가정하고 있습니다. 이 알고리듬은  monotonical한 성능 증가($\\eta(\\pi\\_0)\\leq\\eta(\\pi\\_1)\\leq\\cdots$)를 한다는 것을 다음과 같이 보일 수 있습니다. $M\\_i(\\pi)=L\\_{\\pi\\_i}(\\pi) - CD\\_\\mathrm{KL}^\\max\\left(\\pi\\_i,\\pi\\right)$라고 합시다.\n\n$$\n\\begin{align}\n\\eta \\left(\\pi\\_{i+1}\\right) &\\geq M\\_i\\left(\\pi\\_{i+1}\\right)\\\\\\\\\n\\eta \\left(\\pi\\_{i}\\right) &= M\\_i\\left(\\pi\\_{i}\\right) \\\\\\\\\n\\eta \\left(\\pi\\_{i+1}\\right) - \\eta \\left(\\pi\\_{i}\\right) &\\geq M\\_i\\left(\\pi\\_{i+1}\\right) - M\\_i\\left(\\pi\\_{i}\\right)\n\\end{align}\n$$\n\n위 수식과 같이 매 iteration 마다 $M\\_i$를 최대화함으로써, $\\eta$가 감소하지 않는다는 것을 보장할 수 있습니다. 이와 같은 타입의 알고리듬을 [minorization-maximization (MM) algorithm](https://www.jstor.org/stable/pdf/27643496.pdf?casa_token=0qHamcl60WoAAAAA:-fkZ9JcA_nrY3-zbCUpqvPOgcAMgw7Gr96MajCZg2byHf8m5GU1KTSxyJJcBy1lPZbBTZVCjHHUXilh4k-iuwF91Wka4B5qdltC1IR2qMWk8q1FoV6__)이라고 합니다. EM 알고리듬도 이 타입의 알고리듬 중 하나입니다.\n\n**Algorithm 1**은 아래 그림과 같이 동작합니다.\n\n[![surrogate](../../../../img/surrogate.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=3m52s)\n\n\n$M\\_i$는 $\\pi\\_i$일 때 equality가 되는 $\\eta$에 대한  surrogate function입니다. TRPO는 이 surrogate function을 최대화하고 KL divergence를 penalty가 아닌 constraint로 두는 알고리듬입니다.\n\n<br><br>\n\n# 4. Optimization of Parameterized Policies\n\n표기의 편의를 위해 다음과 같이 notation들을 더 간략하게 정의합니다.\n\n* $\\eta(\\theta):=\\eta\\left(\\pi\\_\\theta\\right)$\n* $L\\_\\theta\\left(\\tilde\\theta\\right):=L\\_{\\pi\\_\\theta}\\left(\\pi\\_{\\tilde\\theta}\\right)$\n* $D\\_\\mathrm{KL}\\left(\\theta\\parallel\\tilde\\theta\\right):=D\\_\\mathrm{KL}\\left(\\pi\\_\\theta\\parallel\\pi_{\\tilde\\theta}\\right)$\n* $\\theta\\_\\mathrm{old}$: previous policy parameter\n\n<br>\n## 4.1 Trust Region Policy Optimization\n\n이전 장의 중요 결과를 위의 notation으로 다시 표기하면 아래와 같습니다.\n\n$$\n\\eta\\left(\\theta\\right) \\geq L\\_{\\theta\\_\\mathrm{old} }\\left(\\theta\\right) - C\\cdot D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\n$$\n\n$\\eta$의 성능 향상을 보장하기 위해서 그것의 lower bound를 최대화할 수 있습니다.\n\n$$\n\\max\\_\\theta \\left[L\\_{\\theta\\_\\mathrm{old} }\\left(\\theta\\right) - C D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\\right]\n$$\n\n이 최적화 문제는 step size를 매우 작게 해야 올바른 동작을 합니다. 위에서 살펴봤듯이 first order approximation이기 때문입니다. 좀 더 큰 step size를 가질 수 있도록 이 최적화 문제를 trust region constraint를 도입하여 다음과 같이 바꿉니다.\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n이 최적화문제의 constraint는 모든 state space에 대해서 성립해야 합니다. 또한 maximum값을 매번 찾아야 합니다. state가 많은 경우 constraint의 수가 매우 많아져서 문제를 풀기 어렵게 만듭니다. constraint의 수를 줄이기 위하여 다음과 같은 avergae KL divergence를 이용하는 heuristic approximation을 취합니다. 이것이 최선의 방법은 아닐 수 있지만 실용적인 방법입니다.\n\n$$\n\\overline D\\_\\mathrm{KL}^{\\rho}\\left(\\theta\\_1, \\theta\\_2\\right):=E\\_{s\\sim\\rho}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta_1}(\\cdot\\vert s)\\parallel\\pi\\_{\\theta\\_2}(\\cdot\\vert s)\\right)\\right]\n$$\n\n이것을 기반으로 다음과 같은 최적화 문제를 풀 수 있습니다.\n\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n\n아래 그림처럼 step size에 대해서 고려할 수 있습니다.\n\n[![heuristic_approx](../../../../img/heuristic_approx.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=4m35s)\n\n<br><br>\n\n# 5. Sample-Based Estimation of the Objective and Constraint\n\n실용적인 알고리듬을 만들려고 하는 노력은 아직 끝나지 않았습니다. 이제 앞의 알고리듬을 sample-based estimation 즉, Monte Carlo estimation을 할 수 있도록 바꿔보겠습니다. sampling을 편하게 할 수 있도록 아래와 같이 바꿔줍니다.\n\n* $\\sum\\_s \\rho\\_{\\theta\\_\\mathrm{old} }(s)[\\ldots]\\rightarrow\\frac{1}{1-\\gamma}E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}[\\ldots]$\n* $A\\_{\\theta\\_\\mathrm{old} }\\rightarrow Q\\_{\\theta\\_\\mathrm{old} }$\n* $\\sum\\_a\\pi\\_{\\theta\\_\\mathrm{old} }(a\\vert s)A\\_{\\theta\\_\\mathrm{old} } \\rightarrow E\\_{a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}A\\_{\\theta\\_\\mathrm{old} }\\right]$\n\n이러한 변화를 그림처럼 도식화할 수 있습니다.\n\n[![sample-based](../../../../img/sample-based.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=5m31s)\n\n한 가지 짚고 넘어가야 할 점은 action sampling을 할 때 importance sampling을 사용한다는 것입니다.\n\n[![importance_sampling](../../../../img/importance_sampling.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=5m53s)\n\n바뀐 최적화문제는 아래와 같습니다.\n\n- **Equation (14).**\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}\n$$\n\n이 때 sampling하는 두 가지 방법이 있습니다.\n\n<br>\n## 5.1 Single Path\n\n*single path*는 개별 trajectory들을 이용하는 방법입니다.\n\n[![single](../../../../img/single.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=6m15s)\n\n<br>\n## 5.2 Vine\n\n*vine*은 한 state에서 rollout을 이용하여 여러 action을 수행하는 방법입니다.\n\n[![vine1](../../../../img/vine1.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=6m32s)\n\n[![vine2](../../../../img/vine2.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=6m49s)\n\nestimation의 variance를 낮출 수 있지만 계산량이 많고 한 state에서 여러 action을 수행할 수 있어야 하기 때문에 현실적인 문제에 적용하기에는 어려움이 있습니다.\n\n\n<br><br>\n\n# 6. Practical Algorithm\n\n앞서 single-path, vine 샘플링을 사용하는 두가지 방식의 policy optimization 알고리즘을 살펴봤습니다. 실용적인 알고리듬은 아래의 과정을 반복해서 수행합니다.\n\n1) Q-values의 몬테카를로 추정을 통해 state-action 쌍 집합을 single path 또는 vine 과정을 통해 수집함.\n\n2) 샘플 평균으로, (14)식의 목적함수와 제약식을 추정함\n + **Equation (14).**\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}\n$$\n\n3) policy parameter vector인 $\\theta$를 업데이트 하면서 제약조건이 있는 최적화 문제를 근사적으로 풂. 본 논문에서는 gradient를 직접 계산하는 것보다는 약간 더 계산량이 있는 line search와 conjugate gradient algorithm을 사용했습니다.\n\n3)에 대해서,  gradient의 covariance matrix를 사용하지 않고 KL divergence의 Hessian을 해석적으로 계산하여 Fisher Information Matrix를 구성했습니다.\n\n다시말해서\n\n$\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial}{\\partial \\theta_i} \\log \\pi_{\\theta}(a_n\\vert s_n) \\frac{\\partial}{\\partial \\theta_j} \\log \\pi_{\\theta}(a_n\\vert s_n)$ 대신 $\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} D_{KL}(\\pi_{\\theta_{old} }(\\cdot \\vert  s_n) \\vert \\vert  \\pi_{\\theta}(\\cdot \\vert  s_n))$를 계산한 것입니다.\n\n이 analytic estimator는 Hessian이나 trajectories의 모든 gradient를 저장하지 않아도 되기 때문에 대규모 환경을 고려할 경우 계산 상 이점이 있습니다. 좀 더 자세하게 설명한 Appendix C 부분을 살펴보겠습니다.\n\n\n<br>\n## Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem\n\n우리가 풀고자 하는 식은 다음과 같습니다.\n\n$$\n\\begin{align}\n\\max\\quad &L(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ } &\\overline{D}\\_{KL}(\\theta\\_\\mathrm{old}, \\theta) \\leq \\delta\n\\end{align}\n$$\n\n이 최적화 문제를 풀기 위하여 우리는 gradient 기법을 이용할 것입니다. gradient 기법은 결국 update할 방향과 크기를 결정하는 문제로 생각할 수 있습니다. 이에 따라 아래와 같은 두가지 과정으로 진행합니다.\n\n1) 탐색 방향을 계산, 목적함수의 first-order approximation(선형근사)와 제약식의 quadratic approximation(2차 근사).\n\n2) 이동거리 계산을 위해 해당 방향으로 line search 수행.\n\n탐색 방향은 $Ax = g$ 수식을 근사적으로 풀어서 구합니다. 여기서 $A$ 는 Fisher Information Matrix이고, 이것은 KL divergence 제약식의 2차 근사 $\\overline{D}\\_\\mathrm{KL} (\\theta\\_\\mathrm{old}, \\theta) \\approx \\frac{1}{2}(\\theta - \\theta\\_\\mathrm{old})^T A(\\theta - \\theta\\_\\mathrm{old})$를 푸는 것입니다. 여기서 $A\\_{ij} = \\frac{\\partial}{\\partial \\theta\\_i} \\frac{\\partial}{\\partial \\theta\\_j} \\overline{D}\\_\\mathrm{KL}(\\theta\\_\\mathrm{old}, \\theta)$입니다.\n\n논문에서 이 부분에 대한 설명이 약간 부족해서 좀 더 내용을 추가하자면, $\\frac{1}{2}x^T A x - x^T g + b = 0$과 같은 quadratic equation의 해는 1차 derivative를 0으로 만드는 $x$를 찾음으로써 알 수 있습니다. 위에서 우리는 목적함수를 1차 근사시키고 제약조건은 2차 근사를 시킴으로써 quadratic equation 형태로 변형했기 때문에 같은 방식으로 풀 수 있습니다. 따라서 $g$는 1차근사시킨 목적함수의 계수가 될 것입니다.\n\n대규모 문제에서 $A$ 혹은 $A^{-1}$ 을 계산하는 것은 엄청난 계산량이 필요합니다. 하지만 conjugate gradient algorithm을 잘 이용하면 전체 $A$행렬(FIM)을 계산하지 않아도 $Ax=g$ 를 근사적으로 해결할 수 있게 해줍니다.\n\n탐색 방향 $s \\approx A^{-1}g$과 더불어 최대스텝길이(step size) $\\beta$도 계산할 필요가 있습니다.\n\n즉, $\\delta = \\overline{D}\\_\\mathrm{KL} \\approx \\frac{1}{2}(\\beta s)^T A(\\beta s) = \\frac{1}{2}\\beta^2s^TAs$ 이고, \n\n$\\beta = \\sqrt{2\\delta / s^T As}$가 됩니다. 이때 $\\delta$는 KL divergence의 boundary term 입니다.\n\n$s^T As$ 항은 Hessian-vector product로 계산할 수 있고, conjugate gradient 과정에서 계산됩니다.\n\n마지막으로, surrogate objective와 KL divergence 제약식을 위해 다음과 같은 함수를 정의하고 line search를 사용합니다.\n\n$$\nL\\_{\\theta\\_\\mathrm{old} }(\\theta) - \\chi [\\overline{D}\\_{KL}(\\theta\\_\\mathrm{old}, \\theta) \\leq \\delta]\n$$\n\n$\\chi[\\cdot]$ 함수는 []안의 조건이 맞으면 0이고 틀리면 무한으로 발산하는 함수입니다.\n\n위에서 계산한 $\\beta$ 의 최대 값부터 시작해서 목적함수가 개선될때까지 exponential하게 줄여 나갑니다. line search가 없었다면 이 알고리즘은 매우 큰 스텝으로 계산될 것이고 심각한 성능 저하가 발생할 것입니다.\n\n(여기서 살짝 예고를 하자면, 나중에 이 KL 기반 기법을 개선한 더 단순한 방법이 제안됩니다. PPO라고...)\n\n<br><br>\n\n# 7. Connections with Prior Work\n\nNatural Policy Gradient는 $L$의 선형 근사와 $\\overline D\\_\\mathrm{KL}$ 제약식을 2차근사 하는 아래의 식의 special case입니다.\n\n- Equation (17).\n\n$L\\_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho\\_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A\\_\\pi(s,a)$\n\n$\\underset{\\theta}{\\max}\\quad \\[\\nabla\\_{\\theta}L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\ \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} } \\cdot (\\theta - \\theta\\_\\mathrm{old})]$\n\n$\\mathrm{s.t.}\\quad\\frac{1}{2}(\\theta\\_\\mathrm{old} - \\theta)^{T} A(\\theta\\_\\mathrm{old})(\\theta\\_\\mathrm{old}-\\theta) \\leq \\delta$\n\n$\\mathrm{where\\ }A(\\theta_{old})_{ij} = $\n\n$\\Large \\frac{\\partial}{\\partial \\theta\\_{i} } \\frac{\\partial}{\\partial \\theta\\_{j} }\nE\\_{s \\sim \\rho\\_{\\pi} }[D\\_\\mathrm{KL}(\\pi(\\cdot\\rvert s, \\theta\\_\\mathrm{old})  \\rvert\\rvert \\pi(\\cdot \\rvert s, \\theta))] \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} }$\n\n업데이트 식은 다음과 같습니다.\n\n$\\theta\\_\\mathrm{new} = \\theta\\_\\mathrm{old} +  \\color{Red}{\\frac{1}{\\lambda} } A(\\theta\\_\\mathrm{old})^{-1}\\nabla\\_{\\theta}L(\\theta) \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} }$\n\n여기서 step size인 $\\frac{1}{\\lambda}$ 일반적으로 알고리즘의 파라미터로 취급되지만 TRPO는 각 업데이트 마다 제약식으로 사용한다는 점이 다릅니다. 사소한 차이처럼 보이지만, 큰 차원을 다루는 실험에서 성능을 크게 향상시켰습니다.\n\n또한 $L^2$ 제약식(혹은 페널티) 를 사용하면 표준 Policy Gradient 업데이트 식을 얻었습니다.\n\n- Equation (18).\n\n$\\underset{\\theta}{\\max}\\quad[\\nabla\\_{\\theta} L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} } \\cdot (\\theta - \\theta\\_\\mathrm{old})]$\n\n$\\mathrm{s.t.}\\quad \\frac{1}{2} \\vert\\vert \\theta - \\theta\\_\\mathrm{old} \\vert\\vert^2 \\leq \\delta$\n\n\n\n$L\\_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho\\_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A\\_\\pi(s,a) $를 이용해서 제약조건없이 $\\underset{\\pi}{\\max}\\quad L\\_{\\pi\\_\\mathrm{old} }(\\pi)$를 풀면\nPolicy Iteration update를 하는 것과 같습니다.\n\n<br><br>\n\n# 8. Experiments\n\n- 다음과 같은 3 가지 궁금증을 해결하기 위하여 실험을 설계하였습니다.\n\n    1. Single-path 와 vine 의 성능면에서 특징을 알고싶음.\n\n    2. Fixed penalty coefficient(NPG) 보다 fixed KL divergence를 사용하는 것(TRPO)으로    \n    변경한 것이 얼마나 뚜렷한 차이를 보일지, 성능 면에서 어떤 영향을 미치는지 알고싶음.\n\n    3. TRPO가 큰 스케일의 문제를 해결할 수 있는지, 기존에 연구/적용되어왔던 방법들과\n    성능, 계산시간, 샘플 복잡도 면에서 비교하고 싶음.\n\n\n- 1, 2의 궁금증에 대해서 실험 환경에 single path, vine을 비롯한 여러 사전방법들을 함께 실험하였습니다.\n\n- 3에 대해선, robotic locomotion과 Atari game에 TRPO를 실험하였습니다.\n\n\n<br>\n## 8.1 Simulated Robotic Locomotion\n\n- MuJoCo 시뮬레이터를 활용해 로봇 locomotion 실험 진행하였습니다.\n- Robot State: positions and velocities, joint torques\n\n![](https://i.imgur.com/bs5ATS3.png)\n\n- 수영, 점프, 걷기 행동을 학습시키고자 합니다.\n\n    - 수영: 10 dimensional state space , reward: $r(x,u)=v_x - 10^{-5}\\vert \\vert u\\vert \\vert ^2$, quadratic penalty.\n\n    - 점프: 12 dimensional state space, episode의 끝은 높이와 각도 threshol로 점프를 판단, non-terminal state에 대해 +1 보상 추가.\n\n    - 걷기: 18 dimensional state space, 뜀뛰는 듯한 보법(점프에이전트)보다 부드러운 보법을 유도하기위해 페널티를 추가함.\n\n**Detailed Experiment Setup and used parameters, used network model**\n![](https://i.imgur.com/zgnsbw6.png)\n\n![](https://i.imgur.com/FqdWC53.png)\n\nequation (12) : $\\max\\quad L\\_{\\theta\\_\\mathrm{old}(\\theta)}, \\ \\ \\mathrm{s.t.\\ }\\overline{D}\\_\\mathrm{KL}^{\\rho\\_{\\theta\\_\\mathrm{old} }}(\\theta\\_\\mathrm{old}, \\theta) \\leq \\delta$\n\n이 실험에서 $\\delta = 0.01$입니다.\n\n- 비교에 사용된 모델들\n\n    - TRPO Single-path\n    - TRPO vine\n    - CEM(cross-entropy method)\n    - CMA(covariance matrix adaptation)\n    - NPG (Lagrange Multiplier penalty coefficien를 사용하는 것이 차이점)\n    - Empirical FIM\n    - maximum KL(cartpole)\n\n![](https://i.imgur.com/wSqolvS.png)\n\n- 제안한 single path, vine 모두 task를 잘 학습하는 것을 확인할 수 있습니다.\n\n- 페널티를 고정하는 것보다, KL divergenc를 제약하는 것이 step size를 선택하는 부분에서 Robust합니다.\n\n- CEM/CMA는 derivative free 알고리즘으로, 샘플 복잡도가 네트워크 파라미터수 만큼 확장되어 high dimension 문제에서는 속도와 성능이 약한 모습을 보입니다.\n\n- maxKL은 제안방법보다 느린 학습성능을 보입니다.\n\nTRPO video link: https://www.youtube.com/watch?v=jeid0wIrSn4\n\n- 사전지식이 적은 상태에서 여러 액션을 학습하는 실험으로 TRPO의 성능을 보입니다.\n\n- 지금까지의 균형이나 걸음 같은 개념을 명시적으로 인코딩한 robotic locomotion에서의 사전 연구들과 차이를 보였습니다.\n\n<br>\n## 8.2 Playing Games from Images\n\n- 부분관찰가능하며 복잡한 observation인 환경에서 TRPO를 평가하기위해 raw image를 입력으로 하는 아타리게임 환경에서 실험하였습니다.\n\n* Challenging point :\n    - High dimensional space\n    - delayed reward\n    - non-stationary image (Enduro는 배경 이미지가 바뀌고, 반전처리가 됨)\n    - complex action sequence (Q*bert는 21개의 다른 발판에서 점프 해야함)\n\n- ALE(Arcade Learning Environment) 에서 테스트\n- DQN과 전처리 방식은 동일\n    210 x 160 pixel -> gray-scaling & down-sampling -> 110 x 84  -> crop -> 84 x 84\n\n- 2 convolution layer ( 16 channels) , stride 2, FC layer with 20 units\n\n![](https://i.imgur.com/NJBC69d.png) \n\n![](https://i.imgur.com/wTe1OEW.png)\n\n![](https://i.imgur.com/j22VtNQ.png)\n\n- 30시간 동안 500 iteration 학습을 하였습니다.\n\n- DQN과 MCST를 사용한 모델(UCC-I), 사람이 플레이한 것(Human) 과의 비교하였습니다.\n\n- 이전 방법들에 비해 압도적인 점수를 기록하진 못했으나, 다양한 게임에서 적절한 게임 기록하였습니다.\n\n- 이전 방법들과 다른 점은, 특정 task에 초점을 맞춰 설계하지않았는데 robotics나 game playing Task에도 적절한 성능을 내는 점에서 **TRPO의 일반화 성능**을 확인하였습니다.\n\n<br><br>\n\n# 9. Discussion\n\n- Trust Region Policy Optimization 을 제안하였습니다.\n\n- KL Divergence 페널티로 $\\eta(\\pi)$를 최적화하는 알고리즘이 monotonically improve 함을 증명하였습니다.\n\n- Locomotion 도메인에서 여러 행동(수영, 걷기, 점프)을 제어하는 controller를 학습하였습니다.\n\n- Robotics와 게임 실험을 결합해, 시각정보와 센서데이터를 사용하는 로봇제어 정책을 학습시킬 수 있는 가능성을 보았습니다.\n\n- 샘플의 복잡도를 상당히 줄일 수 있어 실제상황에 적용가능성을 보았습니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [NPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/)\n\n## [NPG Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py)\n\n<br>\n\n# 다음으로\n\n## [TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py)\n\n## [GAE 여행하기](https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/)\n","source":"_posts/5_trpo.md","raw":"---\ntitle: Trust Region Policy Optimization\ndate: 2018-06-24 16:53:12\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 공민서, 김동민\nsubtitle: 피지여행 5번째 논문\n---\n\n<center> <img src=\"https://www.dropbox.com/s/o7cjcn0e17mpizr/Screen%20Shot%202018-07-18%20at%201.14.22%20AM.png?dl=1\" width=\"700\"> </center>\n\n논문 저자 : John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, Pieter Abbeel\n논문 링크 : https://arxiv.org/pdf/1502.05477.pdf\nProceeding : International Conference on Machine Learning (ICML) 2015\n정리 : 공민서, 김동민\n\n---\n\n# 1. 들어가며...\n\nTrust region policy optimization (TRPO)는 상당히 우수한 성능을 보여주는 policy gradient 기법으로 알려져 있습니다. 높은 차원의 action space를 가진 robot locomotion부터 action은 적지만 화면을 그대로 처리하여 플레이하기 때문에 control parameter가 매우 많은 Atari game까지 각 application에 세부적으로 hyperparameter들을 특화시키지 않아도 두루두루 좋은 성능을 나타내기 때문에 일반화성능이 매우 좋은 기법입니다. 이 TRPO에 대해서 알아보겠습니다.\n\n※TRPO를 매우 재밌게 설명한 Crazymuse AI의 [Youtube video](https://www.youtube.com/watch?v=CKaN5PgkSBc&t=90s)에서 일부 그림을 차용했습니다. 이 비디오를 시청하시는 것을 강력하게 추천합니다!\n\n<br>\n## 1.1 TRPO 흐름 잡기\n\nTRPO 논문은 많은 수식이 등장하여 이 수식들을 따라가다보면 큰 그림을 놓칠 수도 있습니다. 세부적인 내용을 살펴보기 전에 기존 연구에서 출발해서 TRPO로 어떻게 발전해나가는지 간략하게 살펴보겠습니다.\n\n### 1.1.1 Original Problem\n\n$$\\max\\_\\pi \\eta(\\pi)$$\n\n모든 강화학습이 그렇듯이 expected discounted reward를 최대화하는 policy를 찾는 문제로부터 출발합니다.\n\n### 1.1.2 Conservative policy iteration\n\n$$\\max L\\_\\pi(\\tilde\\pi) = \\eta(\\pi) + \\sum\\_s \\rho_\\pi(s)\\sum\\_a\\tilde\\pi(a\\vert s)A\\_\\pi(s,a)$$\n\n$\\eta(\\pi)$를 바로 최대화하는 것은 많은 경우 어렵습니다. $\\eta(\\pi)$의 성능향상을 보장하면서 policy를 update하는 conservative policy iteration 기법이 [Kakade와 Langford](http://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf)에 의하여 제안되었습니다. 이 기법을 이용하면 policy update가 성능을 향상시키는지는 못하더라도 최소한 성능을 악화시키지는 않는다는 것이 이론적으로 보장됩니다.\n\n### 1.1.3 Theorem 1 of TRPO\n\n$$\\max L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\quad\\left(\\alpha=D\\_\\mathrm{TV}^\\max\\left(\\pi\\_\\mathrm{old},\\pi\\_\\mathrm{new}\\right)\\right)$$\n\n기존의 conservative policy iteration은 과거 policy와 새로운 policy를 섞어서 사용해서 실용적이지 않다는 단점이 있었는데 이것을 보완하여 온전히 새로운 policy만으로 update할 수 있는 기법을 제안합니다. \n\n### 1.1.4 KL divergence version of Theorem 1\n\n$$\\max L\\_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\pi,\\tilde\\pi\\right),\\quad\\left(C = \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\right)$$\n\ndistance metric을 KL divergence로 바꿀 수 있습니다.\n\n### 1.1.5 Using parameterized policy\n\n$$\\max\\_\\theta L\\_{\\theta\\_\\mathrm{old} }(\\theta) - C\\cdot D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)$$\n\n최적화문제를 더욱 편리하게 풀 수 있도록 낮은 dimension을 가지는 parameter들로 parameterized된 policy를 사용할 수 있습니다.\n\n### 1.1.6 Trust region constraint\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n1.1.5까지는 아직 conservative policy iteration을 약간 변형시킨 것입니다. policy를 update할 때 지나치게 많이 변하는 것을 방지하기 위하여 [trust region](https://en.wikipedia.org/wiki/Trust_region)을 constraint로 설정할 수 있습니다. 이 아이디어로 인해서 TRPO라는 명칭을 가지게 됩니다.\n\n### 1.1.7 Heuristic approximation\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n사실 1.1.6의 constraint는 모든 state에 대해서 성립해야 하기 때문에 문제를 매우 어렵게 만듭니다. 이것을 좀 더 다루기 쉽게 state distribution에 대한 평균을 취한 것으로 변형합니다.\n\n### 1.1.8 Monte Carlo simulation\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}$$\n\nSampling을 통한 계산이 가능하도록 식을 다시 표현할 수 있습니다. \n\n### 1.1.9 Efficiently solving TRPO\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &\\nabla\\_\\theta \\left. L\\_{\\theta\\_\\mathrm{old} }(\\theta)\\right\\vert \\_{\\theta=\\theta\\_\\mathrm{old} } \\left(\\theta - \\theta\\_\\mathrm{old}\\right) \\\\\\\\\n\\mathrm{s.t.\\ }&\\frac{1}{2}\\left(\\theta\\_\\mathrm{old} - \\theta \\right)^T A\\left(\\theta\\_\\mathrm{old}\\right)\\left(\\theta\\_\\mathrm{old} - \\theta \\right) \\leq \\delta \\\\\\\\\n&A\\_{ij} = \\frac{\\partial}{\\partial \\theta\\_i}\\frac{\\partial}{\\partial \\theta\\_j}\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\n\\end{align}$$\n\n문제를 효율적으로 풀기 위하여 approximation을 적용할 수 있습니다. objective function은 first order approximation, constraint는 quadratic approximation을 취하면 효율적으로 문제를 풀 수 있는 형태로 바뀌는데 이것을 natural gradient로 풀 수도 있고 conjugate gradient로 풀 수도 있습니다.\n\nTRPO는 이렇게 다양한 방법으로 문제를 변형한 것입니다! 이제 좀 더 자세히 살펴보겠습니다.\n\n<br><br>\n\n# 2. Preliminaries\n\n다음과 같은 파라미터를 가지는 infinite-horizon discounted Markov decision process (MDP)를 고려합니다. \n\n* $\\mathcal{S}$: finite set of states\n* $\\\\mathcal{A}$: finite set of actions\n* $P: \\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$: transition probability distribution\n* $r: \\mathcal{S}\\rightarrow\\mathbb{R}$: reward function\n* $\\rho\\_0:\\mathcal{S}\\rightarrow\\mathbb{R}$: distribution of the initial state $s\\_0$\n* $\\gamma\\in(0,1)$: discount factor\n* $\\eta(\\pi)=E\\_{s\\_0,a_0,\\ldots}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r\\left(s\\_t\\right)\\right]$, where $s\\_0\\sim\\rho\\_0\\left(s\\_0\\right), a\\_t\\sim\\pi\\left(\\left.a\\_t \\right\\vert s\\_t\\right), s\\_{t+1}\\sim P\\left(\\left.s\\_{t+1}\\right\\vert s\\_t,a\\_t\\right)$\n* $Q\\_\\pi\\left(s\\_t,a\\_t\\right)=E\\_{ { \\color{red}{s\\_{t+1}} },a\\_{t+1},\\ldots}\\left[\\sum\\_{l=0}^\\infty \\gamma^l r\\left(s\\_{t+l}\\right)\\right]$: action value function\n* $V\\_\\pi\\left(s\\_t\\right)=E\\_{ { \\color{red}{a\\_t} }, s\\_{t+1},a\\_{t+1},\\ldots}\\left[\\sum\\_{l=0}^\\infty \\gamma^l r\\left(s\\_{t+l}\\right)\\right]$: value function (action value function과 expectation의 첨자가 다른 부분을 유의하세요.)\n* $A\\_\\pi(s,a) = Q\\_\\pi(s,a) - V\\_\\pi(s)$: advantage function\n\n<br>\n## 2.1 Useful identity [Kakade & Langford 2002]\n\n우리의 목표는 $\\eta\\(\\pi\\)$가 최대화되도록 만드는 것입니다. 하지만 $\\pi$의 변화에 따라 $\\eta$가 어떻게 변하는지 알아내는 것도 쉽지 않습니다. $\\pi$는 기존의 policy이고 $\\tilde\\pi$는 새로운 policy를 나타낸다고 할 때 $\\eta$와 policy update 사이에 다음과 같은 관계가 있다는 것이 밝혀졌습니다.\n\n**Lemma 1.** $\\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E\\_{s\\_{0},a\\_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right]$\n\n*Proof.* $A\\_\\pi(s,a)=E\\_{s'\\sim P\\left(s'\\vert s,a\\right)}\\left[r(s)+\\gamma V\\_\\pi\\left(s'\\right)-V\\_\\pi(s)\\right]$로 다시 표현할 수 있습니다. 표기의 편의를 위하여 $\\tau:=\\left(s\\_0,a\\_0,s\\_1,a\\_1,\\ldots\\right)$를 정의하겠습니다. 다음과 같은 수식 전개가 가능합니다.\n$$\n\\begin{align}\n&E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right] \\\\\\\\\n &= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t \\left(r(s\\_t)+\\gamma V\\_\\pi\\left(s\\_{t+1}\\right)-V\\_\\pi(s\\_t)\\right)\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\left(\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right)+\\gamma V\\_\\pi\\left(s\\_{1}\\right)-V\\_\\pi(s\\_0)+\\gamma^2 V\\_\\pi\\left(s\\_{2}\\right)-\\gamma V\\_\\pi(s\\_1)+\\gamma^3 V\\_\\pi\\left(s\\_{3}\\right)-\\gamma^2 V\\_\\pi(s\\_2)+\\cdots\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)+\\color{red}{\\gamma V\\_\\pi\\left(s\\_{1}\\right)}-V\\_\\pi(s\\_0)+\\color{red}{\\gamma^2 V\\_\\pi\\left(s\\_{2}\\right)}-\\color{red}{\\gamma V\\_\\pi(s\\_1)}+\\cdots\\right] \\\\\\\\\n&= E\\_{\\tau\\vert \\tilde\\pi}\\left[-V\\_\\pi(s\\_0)+\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right] \\\\\\\\\n&\\overset{\\underset{\\mathrm{(a)} }{} }{=} -E\\_{\\color{red}{s\\_0}}\\left[V\\_\\pi(s\\_0)\\right]+E\\_{\\tau\\vert \\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t r(s\\_t)\\right] \\\\\\\\\n&=-\\eta(\\pi) + \\eta\\left(\\tilde\\pi\\right)\\\\\\\\\n&\\quad\\therefore \\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E\\_{s\\_{0},a\\_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum\\_{t=0}^\\infty\\gamma^t A\\_\\pi\\left(s\\_t,a\\_t\\right)\\right]\n\\end{align}\n$$\n위의 전개의 (a) 부분은 $V\\_\\pi\\left(s\\_0\\right)=E\\_{a\\_0,a\\_1,s\\_1,\\ldots}\\left[\\sum\\_{t=0}^\\infty\\gamma^l r\\left(s\\_l\\right)\\right]$이므로 $\\tau$ 중 많은 부분들이 이미 expectation이 취해진 값이므로 무시됩니다. 오직 $s\\_0$만 expectation이 취해지지 않았기 때문에 남아있습니다.\n\n**Lemma 1**은 새로운 policy와 기존의 policy 사이의 관계를 규정합니다. 다음과 같은 식을 정의하고 이것을 이용해서 **Lemma 1**을 변형시켜 봅시다.\n\n* $\\rho\\_\\pi(s) = P\\left(s\\_0=s\\right) + \\gamma P\\left(s\\_1=s\\right)\\ + \\gamma^2 P\\left(s\\_2=s\\right) + \\cdots $: (unnormalized) discounted visitation frequencies\n\n\n\n$$\n\\begin{align}\n\\eta\\left(\\tilde\\pi\\right) &= \\eta(\\pi) + \\sum\\_{t=0}^\\infty\\sum\\_{s}P\\left(\\left.s\\_t=s\\right\\vert \\tilde\\pi\\right)\\sum\\_a\\tilde\\pi(a\\vert s)\\gamma^t A\\_\\pi(s,a) \\\\\\\\\n&= \\eta(\\pi) + \\sum\\_{s}{\\color{red}{\\sum\\_{t=0}^\\infty \\gamma^t P\\left(\\left.s\\_t=s\\right\\vert \\tilde\\pi\\right)} } \\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\\\\\\\\n&= \\eta(\\pi) + \\sum\\_{s}{\\color{red}{\\rho\\_\\tilde\\pi(s)} }\\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\\\\\\\\n\\end{align}\n$$\n\n이 수식의 의미가 무엇일까요? 만약 $\\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a) \\geq 0$이라면 $\\eta(\\tilde\\pi)$는 항상 $\\eta(\\pi)$보다 큽니다. 즉, policy를 업데이트함으로써 항상 개선됩니다. 즉, 이 수식을 통해서 항상 더 좋은 성능을 내는 policy update가 가져야 할 특징을 알 수 있습니다. 다음과 같은 deterministic policy가 있다고 합시다.\n\n$$\n\\tilde\\pi(s) = \\arg\\max_a A\\_\\pi(s,a)\n$$\n\n이 policy는 적어도 하나의 state-action pair에서 0보다 큰 값을 가지는 advantage가 있고 그 때의 확률이 0이 아니라면 항상 성능을 개선시킵니다. 어려운 점은 policy를 바꾸면 $\\rho$도 바뀐다는 점입니다. \n\n다음 그림을 봅시다. Starting Point에서 여러가지 경로를 거쳐서 Destination으로 갈 수 있습니다. 다른 policy를 이용하는 것은 다른 경로를 이용한 것입니다.\n\n[![policy_change](../../../../img/policy_change.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=16s)\n\n이 때 다른 policy를 이용함에 따라 state visitation frequency도 변하게 됩니다.\n\n[![state_visitation_change](../../../../img/state_visitation_change.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=36s)\n\n이로 인해서 policy를 최적화하는 것은 어려워집니다. 그래서 이러한 변화를 무시하는 다음과 같은 approximation을 취합니다.\n\n$$\n\\begin{align}\nL_\\pi\\left(\\tilde\\pi\\right) &= \\eta(\\pi) + \\sum\\_{s}{\\color{red}{\\rho\\_\\pi(s)} } \\sum\\_a\\tilde\\pi(a\\vert s) A\\_\\pi(s,a)\n\\end{align}\n$$\n\npolicy가 바뀌었음에도 이전의 state distribution을 계속 이용하는 것입니다. 이것은 policy의 변화가 크지 않다면 어느 정도 허용될 수 있을 것입니다. 그렇지만 얼마나 많은 변화가 허용될까요? 이것을 정하기 위해서 이용하는 것이 trust region입니다.\n\n<br>\n## 2.2 Conservative Policy Iteration\n\npolicy의 변화를 다루기 용이하게 하기 위해서 policy를 다음과 같이 파라미터를 이용해서 표현합시다.\n\n* $\\pi\\_\\theta(a\\vert s)$: parameterized policy\n\n$\\pi\\_\\theta$는 $\\theta$에 대하여 미분가능한 함수입니다. $L\\_\\pi\\left(\\tilde\\pi\\right)$을 $\\theta\\_0$에서 $\\eta(\\pi)$에 대한 first order approximation이라고 하면 다음 식이 성립합니다.\n\n$$\n\\begin{align}\nL\\_{\\pi\\_{\\theta\\_0} }\\left(\\pi\\_{\\theta_0}\\right) &= \\eta\\left(\\pi\\_{\\theta\\_0}\\right) \\\\\\\\\n\\nabla\\_\\theta \\left.L\\_{\\pi\\_{\\theta\\_0} }\\left(\\pi\\_{\\theta\\_0}\\right)\\right\\vert \\_{\\theta=\\theta\\_0} &= \\nabla\\_\\theta\\left.\\eta(\\pi\\_{\\theta\\_0})\\right\\vert \\_{\\theta=\\theta\\_0}\n\\end{align}\n$$\n\n이것의 의미는 $\\pi\\_{\\theta\\_0}$가 매우 작게 변한다면 $L\\_{\\pi\\_{\\theta\\_0} }$를 개선시키는 것이 $\\eta$를 개선시키는 것이라는 것입니다. 그러나 지금까지의 설명만으로는 $\\pi\\_{\\theta\\_0}$를 얼마나 작게 변화시켜야 할지에 대해서는 알 수 없습니다.\n\nKakade & Langford가 2002년 발표한 논문에서도 이것에 대해서 고민했습니다. 그 논문에서 *conservative policy iteration*이라는 기법을 제안합니다. 그 논문의 contribution은 다음과 같습니다.\n\n* $\\eta$ 개선의 lower bound를 제공\n* 기존의 policy를 $\\pi\\_\\mathrm{old}$라고 하고 $\\pi'$를 $\\pi'=\\arg\\max\\_\\{\\pi'}L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi'\\right)$과 같이 정의할 때, 새로운 mixture policy $\\pi\\_\\mathrm{new}$를 다음과 같이 제안\n$$\n\\pi\\_\\mathrm{new}(a\\vert s) = (1-\\alpha)\\pi\\_\\mathrm{old}(a\\vert s) + \\alpha \\pi'(a\\vert s)\n$$\n\n그림으로 표현하면 다음과 같습니다.\n\n[![mixure_policy](../../../../img/mixure_policy.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=2m46s)\n\n* 다음과 같은 lower bound를 정의\n$$\n\\eta\\left(\\pi\\_\\mathrm{new}\\right) \\geq L\\_{\\pi\\_{\\theta\\_\\mathrm{old} }}\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{2\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\\\\\\\\n\\mathrm{where\\ }\\epsilon = \\max_s\\left\\vert E\\_{a\\sim\\pi'(a\\vert s)}\\left[A\\_\\pi(s,a)\\right]\\right\\vert \n$$\n\n하지만 mixture된 policy라는 것은 실용적이지 않습니다. \n\n<br><br>\n\n# 3. Monotonic Improvement Guarantee for General Stochastic Policies\n\n이전 장에서 설명한 lower bound는 오직 mixture policy에 대해서만 성립하고 더 많이 사용되는 stochastic policy에는 적용되지 않습니다. 따라서 stochastic policy를 이용할 수 있도록 개선할 필요가 있습니다. 아래와 같이 기존 수식에서 두 가지를 바꿈으로써 이것이 가능합니다.\n\n* $\\alpha\\rightarrow$ distance measure between $\\pi$ and $\\tilde\\pi$\n* constant $\\epsilon\\rightarrow\\max\\_{s,a}\\left\\vert A\\_\\pi(s,a)\\right\\vert $\n\n여기서 distance measure로 total variation divergence를 이용합니다. discrete porbability distribution $p$와 $q$에 대하여 다음과 같이 정의됩니다.\n$$\nD\\_\\mathrm{TV}(p\\parallel q) = \\frac{1}{2}\\sum\\_i\\left\\vert p\\_i - q\\_i\\right\\vert \n$$\n\n이것을 이용하여 $D\\_\\mathrm{TV}^\\max$를 다음과 같이 정의합니다.\n$$\nD\\_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D\\_\\mathrm{TV}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)\n$$\n\n그림으로 표현하면 다음과 같습니다.\n\n[![tvd](../../../../img/tvd.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=3m15s)\n\n이것을 이용하여 다음과 같은 관계식을 얻을 수 있습니다.\n\n**Theorem 1.** Let $\\alpha=D\\_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi)$. Then the following bound holds:\n$$\n\\begin{align}\n\\eta\\left(\\pi\\_\\mathrm{new}\\right) &\\geq L\\_{\\pi\\_\\mathrm{old} }\\left(\\pi\\_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2, \\\\\\\\\n\\mathrm{where\\ } \\epsilon&= \\max\\_{s,a}\\left\\vert A\\_\\pi(s,a)\\right\\vert \n\\end{align}\n$$\n<!--*Proof.* TBD.-->\n\n또다른 distance metric으로 아래 그림과 같은 KL divergence가 있습니다.\n(그런데 왜 하필 KL divergence로 바꿀까요? 논문의 뒤쪽에서 계산효율을 위해서 conjugate gradient method를 이용하는데 이를 위해서 바꾼게 아닐까 싶습니다. Wasserstein distance 같은 다른 방향으로 발전시킬 수도 있을 것 같습니다. Schulmann이 아마 해봤겠죠?^^a)\n\n[![kld](../../../../img/kld.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=3m34s)\n\ntotal variation divergence와 KL divergence 사이에는 다음과 같은 관계가 있습니다.\n$$\nD\\_\\mathrm{TV}(p\\parallel q)^2 \\leq D\\_\\mathrm{KL}(p\\parallel q)\n$$\n\n다음 수식을 정의합니다.\n$$\nD\\_\\mathrm{KL}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D\\_\\mathrm{KL}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)\n$$\n\n**Theorem 1**을 이용하여 다음과 같은 수식이 성립함을 알 수 있습니다.\n$$\n\\begin{align}\n\\eta\\left(\\tilde\\pi\\right) &\\geq L\\_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D\\_\\mathrm{KL}^\\max(\\pi, \\tilde\\pi), \\\\\\\\\n\\mathrm{where\\ } C&= \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\n\\end{align}\n$$\n\n이러한 policy imporvement bound를 기반으로 다음과 같은 approximate policy iteration 알고리듬을 고안해낼 수 있습니다.\n\n**Algorithm 1** Policy iteration algorithm guaranteeing non-decreasing expected return $\\eta$\n> Initialize $\\pi\\_0$   \n> **for** $i=0,1,2,\\ldots$ until convergence **do**   \n> $\\quad$Compute all advantage values $A\\_{\\pi\\_i}(s,a)$    \n> $\\quad$Solve the contstrained optimization problem    \n> $\\quad\\pi\\_{i+1}=\\arg\\max\\_\\pi\\left[L\\_{\\pi\\_i} - CD\\_\\mathrm{KL}^\\max\\left(\\pi\\_i,\\pi\\right)\\right]$     \n> $\\quad\\quad$where $C = 4\\epsilon\\gamma / (1-\\gamma)^2$    \n> $\\quad\\quad$and $L\\_{\\pi\\_i}\\left(\\pi\\right) = \\eta(\\pi\\_i) + \\sum\\_{s}\\rho\\_{\\pi\\_i}(s)\\sum\\_a\\pi(a\\vert s) A\\_{\\pi\\_i}(s,a)$    \n> **end for**\n\n**Algorithm 1**은 advantage를 정확하게 계산할 수 있다고 가정하고 있습니다. 이 알고리듬은  monotonical한 성능 증가($\\eta(\\pi\\_0)\\leq\\eta(\\pi\\_1)\\leq\\cdots$)를 한다는 것을 다음과 같이 보일 수 있습니다. $M\\_i(\\pi)=L\\_{\\pi\\_i}(\\pi) - CD\\_\\mathrm{KL}^\\max\\left(\\pi\\_i,\\pi\\right)$라고 합시다.\n\n$$\n\\begin{align}\n\\eta \\left(\\pi\\_{i+1}\\right) &\\geq M\\_i\\left(\\pi\\_{i+1}\\right)\\\\\\\\\n\\eta \\left(\\pi\\_{i}\\right) &= M\\_i\\left(\\pi\\_{i}\\right) \\\\\\\\\n\\eta \\left(\\pi\\_{i+1}\\right) - \\eta \\left(\\pi\\_{i}\\right) &\\geq M\\_i\\left(\\pi\\_{i+1}\\right) - M\\_i\\left(\\pi\\_{i}\\right)\n\\end{align}\n$$\n\n위 수식과 같이 매 iteration 마다 $M\\_i$를 최대화함으로써, $\\eta$가 감소하지 않는다는 것을 보장할 수 있습니다. 이와 같은 타입의 알고리듬을 [minorization-maximization (MM) algorithm](https://www.jstor.org/stable/pdf/27643496.pdf?casa_token=0qHamcl60WoAAAAA:-fkZ9JcA_nrY3-zbCUpqvPOgcAMgw7Gr96MajCZg2byHf8m5GU1KTSxyJJcBy1lPZbBTZVCjHHUXilh4k-iuwF91Wka4B5qdltC1IR2qMWk8q1FoV6__)이라고 합니다. EM 알고리듬도 이 타입의 알고리듬 중 하나입니다.\n\n**Algorithm 1**은 아래 그림과 같이 동작합니다.\n\n[![surrogate](../../../../img/surrogate.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=3m52s)\n\n\n$M\\_i$는 $\\pi\\_i$일 때 equality가 되는 $\\eta$에 대한  surrogate function입니다. TRPO는 이 surrogate function을 최대화하고 KL divergence를 penalty가 아닌 constraint로 두는 알고리듬입니다.\n\n<br><br>\n\n# 4. Optimization of Parameterized Policies\n\n표기의 편의를 위해 다음과 같이 notation들을 더 간략하게 정의합니다.\n\n* $\\eta(\\theta):=\\eta\\left(\\pi\\_\\theta\\right)$\n* $L\\_\\theta\\left(\\tilde\\theta\\right):=L\\_{\\pi\\_\\theta}\\left(\\pi\\_{\\tilde\\theta}\\right)$\n* $D\\_\\mathrm{KL}\\left(\\theta\\parallel\\tilde\\theta\\right):=D\\_\\mathrm{KL}\\left(\\pi\\_\\theta\\parallel\\pi_{\\tilde\\theta}\\right)$\n* $\\theta\\_\\mathrm{old}$: previous policy parameter\n\n<br>\n## 4.1 Trust Region Policy Optimization\n\n이전 장의 중요 결과를 위의 notation으로 다시 표기하면 아래와 같습니다.\n\n$$\n\\eta\\left(\\theta\\right) \\geq L\\_{\\theta\\_\\mathrm{old} }\\left(\\theta\\right) - C\\cdot D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\n$$\n\n$\\eta$의 성능 향상을 보장하기 위해서 그것의 lower bound를 최대화할 수 있습니다.\n\n$$\n\\max\\_\\theta \\left[L\\_{\\theta\\_\\mathrm{old} }\\left(\\theta\\right) - C D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right)\\right]\n$$\n\n이 최적화 문제는 step size를 매우 작게 해야 올바른 동작을 합니다. 위에서 살펴봤듯이 first order approximation이기 때문입니다. 좀 더 큰 step size를 가질 수 있도록 이 최적화 문제를 trust region constraint를 도입하여 다음과 같이 바꿉니다.\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&D\\_\\mathrm{KL}^\\max\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n이 최적화문제의 constraint는 모든 state space에 대해서 성립해야 합니다. 또한 maximum값을 매번 찾아야 합니다. state가 많은 경우 constraint의 수가 매우 많아져서 문제를 풀기 어렵게 만듭니다. constraint의 수를 줄이기 위하여 다음과 같은 avergae KL divergence를 이용하는 heuristic approximation을 취합니다. 이것이 최선의 방법은 아닐 수 있지만 실용적인 방법입니다.\n\n$$\n\\overline D\\_\\mathrm{KL}^{\\rho}\\left(\\theta\\_1, \\theta\\_2\\right):=E\\_{s\\sim\\rho}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta_1}(\\cdot\\vert s)\\parallel\\pi\\_{\\theta\\_2}(\\cdot\\vert s)\\right)\\right]\n$$\n\n이것을 기반으로 다음과 같은 최적화 문제를 풀 수 있습니다.\n\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ }&\\overline{D}\\_\\mathrm{KL}^{\\rho\\_\\mathrm{old} }\\left(\\theta\\_\\mathrm{old}, \\theta\\right) \\leq \\delta\n\\end{align}$$\n\n\n아래 그림처럼 step size에 대해서 고려할 수 있습니다.\n\n[![heuristic_approx](../../../../img/heuristic_approx.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=4m35s)\n\n<br><br>\n\n# 5. Sample-Based Estimation of the Objective and Constraint\n\n실용적인 알고리듬을 만들려고 하는 노력은 아직 끝나지 않았습니다. 이제 앞의 알고리듬을 sample-based estimation 즉, Monte Carlo estimation을 할 수 있도록 바꿔보겠습니다. sampling을 편하게 할 수 있도록 아래와 같이 바꿔줍니다.\n\n* $\\sum\\_s \\rho\\_{\\theta\\_\\mathrm{old} }(s)[\\ldots]\\rightarrow\\frac{1}{1-\\gamma}E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}[\\ldots]$\n* $A\\_{\\theta\\_\\mathrm{old} }\\rightarrow Q\\_{\\theta\\_\\mathrm{old} }$\n* $\\sum\\_a\\pi\\_{\\theta\\_\\mathrm{old} }(a\\vert s)A\\_{\\theta\\_\\mathrm{old} } \\rightarrow E\\_{a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}A\\_{\\theta\\_\\mathrm{old} }\\right]$\n\n이러한 변화를 그림처럼 도식화할 수 있습니다.\n\n[![sample-based](../../../../img/sample-based.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=5m31s)\n\n한 가지 짚고 넘어가야 할 점은 action sampling을 할 때 importance sampling을 사용한다는 것입니다.\n\n[![importance_sampling](../../../../img/importance_sampling.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=5m53s)\n\n바뀐 최적화문제는 아래와 같습니다.\n\n- **Equation (14).**\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}\n$$\n\n이 때 sampling하는 두 가지 방법이 있습니다.\n\n<br>\n## 5.1 Single Path\n\n*single path*는 개별 trajectory들을 이용하는 방법입니다.\n\n[![single](../../../../img/single.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=6m15s)\n\n<br>\n## 5.2 Vine\n\n*vine*은 한 state에서 rollout을 이용하여 여러 action을 수행하는 방법입니다.\n\n[![vine1](../../../../img/vine1.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=6m32s)\n\n[![vine2](../../../../img/vine2.png \"https://youtu.be/CKaN5PgkSBc\")](https://youtu.be/CKaN5PgkSBc?t=6m49s)\n\nestimation의 variance를 낮출 수 있지만 계산량이 많고 한 state에서 여러 action을 수행할 수 있어야 하기 때문에 현실적인 문제에 적용하기에는 어려움이 있습니다.\n\n\n<br><br>\n\n# 6. Practical Algorithm\n\n앞서 single-path, vine 샘플링을 사용하는 두가지 방식의 policy optimization 알고리즘을 살펴봤습니다. 실용적인 알고리듬은 아래의 과정을 반복해서 수행합니다.\n\n1) Q-values의 몬테카를로 추정을 통해 state-action 쌍 집합을 single path 또는 vine 과정을 통해 수집함.\n\n2) 샘플 평균으로, (14)식의 목적함수와 제약식을 추정함\n + **Equation (14).**\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}\n$$\n\n3) policy parameter vector인 $\\theta$를 업데이트 하면서 제약조건이 있는 최적화 문제를 근사적으로 풂. 본 논문에서는 gradient를 직접 계산하는 것보다는 약간 더 계산량이 있는 line search와 conjugate gradient algorithm을 사용했습니다.\n\n3)에 대해서,  gradient의 covariance matrix를 사용하지 않고 KL divergence의 Hessian을 해석적으로 계산하여 Fisher Information Matrix를 구성했습니다.\n\n다시말해서\n\n$\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial}{\\partial \\theta_i} \\log \\pi_{\\theta}(a_n\\vert s_n) \\frac{\\partial}{\\partial \\theta_j} \\log \\pi_{\\theta}(a_n\\vert s_n)$ 대신 $\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} D_{KL}(\\pi_{\\theta_{old} }(\\cdot \\vert  s_n) \\vert \\vert  \\pi_{\\theta}(\\cdot \\vert  s_n))$를 계산한 것입니다.\n\n이 analytic estimator는 Hessian이나 trajectories의 모든 gradient를 저장하지 않아도 되기 때문에 대규모 환경을 고려할 경우 계산 상 이점이 있습니다. 좀 더 자세하게 설명한 Appendix C 부분을 살펴보겠습니다.\n\n\n<br>\n## Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem\n\n우리가 풀고자 하는 식은 다음과 같습니다.\n\n$$\n\\begin{align}\n\\max\\quad &L(\\theta) \\\\\\\\\n\\mathrm{s.t.\\ } &\\overline{D}\\_{KL}(\\theta\\_\\mathrm{old}, \\theta) \\leq \\delta\n\\end{align}\n$$\n\n이 최적화 문제를 풀기 위하여 우리는 gradient 기법을 이용할 것입니다. gradient 기법은 결국 update할 방향과 크기를 결정하는 문제로 생각할 수 있습니다. 이에 따라 아래와 같은 두가지 과정으로 진행합니다.\n\n1) 탐색 방향을 계산, 목적함수의 first-order approximation(선형근사)와 제약식의 quadratic approximation(2차 근사).\n\n2) 이동거리 계산을 위해 해당 방향으로 line search 수행.\n\n탐색 방향은 $Ax = g$ 수식을 근사적으로 풀어서 구합니다. 여기서 $A$ 는 Fisher Information Matrix이고, 이것은 KL divergence 제약식의 2차 근사 $\\overline{D}\\_\\mathrm{KL} (\\theta\\_\\mathrm{old}, \\theta) \\approx \\frac{1}{2}(\\theta - \\theta\\_\\mathrm{old})^T A(\\theta - \\theta\\_\\mathrm{old})$를 푸는 것입니다. 여기서 $A\\_{ij} = \\frac{\\partial}{\\partial \\theta\\_i} \\frac{\\partial}{\\partial \\theta\\_j} \\overline{D}\\_\\mathrm{KL}(\\theta\\_\\mathrm{old}, \\theta)$입니다.\n\n논문에서 이 부분에 대한 설명이 약간 부족해서 좀 더 내용을 추가하자면, $\\frac{1}{2}x^T A x - x^T g + b = 0$과 같은 quadratic equation의 해는 1차 derivative를 0으로 만드는 $x$를 찾음으로써 알 수 있습니다. 위에서 우리는 목적함수를 1차 근사시키고 제약조건은 2차 근사를 시킴으로써 quadratic equation 형태로 변형했기 때문에 같은 방식으로 풀 수 있습니다. 따라서 $g$는 1차근사시킨 목적함수의 계수가 될 것입니다.\n\n대규모 문제에서 $A$ 혹은 $A^{-1}$ 을 계산하는 것은 엄청난 계산량이 필요합니다. 하지만 conjugate gradient algorithm을 잘 이용하면 전체 $A$행렬(FIM)을 계산하지 않아도 $Ax=g$ 를 근사적으로 해결할 수 있게 해줍니다.\n\n탐색 방향 $s \\approx A^{-1}g$과 더불어 최대스텝길이(step size) $\\beta$도 계산할 필요가 있습니다.\n\n즉, $\\delta = \\overline{D}\\_\\mathrm{KL} \\approx \\frac{1}{2}(\\beta s)^T A(\\beta s) = \\frac{1}{2}\\beta^2s^TAs$ 이고, \n\n$\\beta = \\sqrt{2\\delta / s^T As}$가 됩니다. 이때 $\\delta$는 KL divergence의 boundary term 입니다.\n\n$s^T As$ 항은 Hessian-vector product로 계산할 수 있고, conjugate gradient 과정에서 계산됩니다.\n\n마지막으로, surrogate objective와 KL divergence 제약식을 위해 다음과 같은 함수를 정의하고 line search를 사용합니다.\n\n$$\nL\\_{\\theta\\_\\mathrm{old} }(\\theta) - \\chi [\\overline{D}\\_{KL}(\\theta\\_\\mathrm{old}, \\theta) \\leq \\delta]\n$$\n\n$\\chi[\\cdot]$ 함수는 []안의 조건이 맞으면 0이고 틀리면 무한으로 발산하는 함수입니다.\n\n위에서 계산한 $\\beta$ 의 최대 값부터 시작해서 목적함수가 개선될때까지 exponential하게 줄여 나갑니다. line search가 없었다면 이 알고리즘은 매우 큰 스텝으로 계산될 것이고 심각한 성능 저하가 발생할 것입니다.\n\n(여기서 살짝 예고를 하자면, 나중에 이 KL 기반 기법을 개선한 더 단순한 방법이 제안됩니다. PPO라고...)\n\n<br><br>\n\n# 7. Connections with Prior Work\n\nNatural Policy Gradient는 $L$의 선형 근사와 $\\overline D\\_\\mathrm{KL}$ 제약식을 2차근사 하는 아래의 식의 special case입니다.\n\n- Equation (17).\n\n$L\\_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho\\_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A\\_\\pi(s,a)$\n\n$\\underset{\\theta}{\\max}\\quad \\[\\nabla\\_{\\theta}L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\ \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} } \\cdot (\\theta - \\theta\\_\\mathrm{old})]$\n\n$\\mathrm{s.t.}\\quad\\frac{1}{2}(\\theta\\_\\mathrm{old} - \\theta)^{T} A(\\theta\\_\\mathrm{old})(\\theta\\_\\mathrm{old}-\\theta) \\leq \\delta$\n\n$\\mathrm{where\\ }A(\\theta_{old})_{ij} = $\n\n$\\Large \\frac{\\partial}{\\partial \\theta\\_{i} } \\frac{\\partial}{\\partial \\theta\\_{j} }\nE\\_{s \\sim \\rho\\_{\\pi} }[D\\_\\mathrm{KL}(\\pi(\\cdot\\rvert s, \\theta\\_\\mathrm{old})  \\rvert\\rvert \\pi(\\cdot \\rvert s, \\theta))] \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} }$\n\n업데이트 식은 다음과 같습니다.\n\n$\\theta\\_\\mathrm{new} = \\theta\\_\\mathrm{old} +  \\color{Red}{\\frac{1}{\\lambda} } A(\\theta\\_\\mathrm{old})^{-1}\\nabla\\_{\\theta}L(\\theta) \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} }$\n\n여기서 step size인 $\\frac{1}{\\lambda}$ 일반적으로 알고리즘의 파라미터로 취급되지만 TRPO는 각 업데이트 마다 제약식으로 사용한다는 점이 다릅니다. 사소한 차이처럼 보이지만, 큰 차원을 다루는 실험에서 성능을 크게 향상시켰습니다.\n\n또한 $L^2$ 제약식(혹은 페널티) 를 사용하면 표준 Policy Gradient 업데이트 식을 얻었습니다.\n\n- Equation (18).\n\n$\\underset{\\theta}{\\max}\\quad[\\nabla\\_{\\theta} L\\_{\\theta\\_\\mathrm{old} }(\\theta) \\rvert\\_{\\theta=\\theta\\_\\mathrm{old} } \\cdot (\\theta - \\theta\\_\\mathrm{old})]$\n\n$\\mathrm{s.t.}\\quad \\frac{1}{2} \\vert\\vert \\theta - \\theta\\_\\mathrm{old} \\vert\\vert^2 \\leq \\delta$\n\n\n\n$L\\_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho\\_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A\\_\\pi(s,a) $를 이용해서 제약조건없이 $\\underset{\\pi}{\\max}\\quad L\\_{\\pi\\_\\mathrm{old} }(\\pi)$를 풀면\nPolicy Iteration update를 하는 것과 같습니다.\n\n<br><br>\n\n# 8. Experiments\n\n- 다음과 같은 3 가지 궁금증을 해결하기 위하여 실험을 설계하였습니다.\n\n    1. Single-path 와 vine 의 성능면에서 특징을 알고싶음.\n\n    2. Fixed penalty coefficient(NPG) 보다 fixed KL divergence를 사용하는 것(TRPO)으로    \n    변경한 것이 얼마나 뚜렷한 차이를 보일지, 성능 면에서 어떤 영향을 미치는지 알고싶음.\n\n    3. TRPO가 큰 스케일의 문제를 해결할 수 있는지, 기존에 연구/적용되어왔던 방법들과\n    성능, 계산시간, 샘플 복잡도 면에서 비교하고 싶음.\n\n\n- 1, 2의 궁금증에 대해서 실험 환경에 single path, vine을 비롯한 여러 사전방법들을 함께 실험하였습니다.\n\n- 3에 대해선, robotic locomotion과 Atari game에 TRPO를 실험하였습니다.\n\n\n<br>\n## 8.1 Simulated Robotic Locomotion\n\n- MuJoCo 시뮬레이터를 활용해 로봇 locomotion 실험 진행하였습니다.\n- Robot State: positions and velocities, joint torques\n\n![](https://i.imgur.com/bs5ATS3.png)\n\n- 수영, 점프, 걷기 행동을 학습시키고자 합니다.\n\n    - 수영: 10 dimensional state space , reward: $r(x,u)=v_x - 10^{-5}\\vert \\vert u\\vert \\vert ^2$, quadratic penalty.\n\n    - 점프: 12 dimensional state space, episode의 끝은 높이와 각도 threshol로 점프를 판단, non-terminal state에 대해 +1 보상 추가.\n\n    - 걷기: 18 dimensional state space, 뜀뛰는 듯한 보법(점프에이전트)보다 부드러운 보법을 유도하기위해 페널티를 추가함.\n\n**Detailed Experiment Setup and used parameters, used network model**\n![](https://i.imgur.com/zgnsbw6.png)\n\n![](https://i.imgur.com/FqdWC53.png)\n\nequation (12) : $\\max\\quad L\\_{\\theta\\_\\mathrm{old}(\\theta)}, \\ \\ \\mathrm{s.t.\\ }\\overline{D}\\_\\mathrm{KL}^{\\rho\\_{\\theta\\_\\mathrm{old} }}(\\theta\\_\\mathrm{old}, \\theta) \\leq \\delta$\n\n이 실험에서 $\\delta = 0.01$입니다.\n\n- 비교에 사용된 모델들\n\n    - TRPO Single-path\n    - TRPO vine\n    - CEM(cross-entropy method)\n    - CMA(covariance matrix adaptation)\n    - NPG (Lagrange Multiplier penalty coefficien를 사용하는 것이 차이점)\n    - Empirical FIM\n    - maximum KL(cartpole)\n\n![](https://i.imgur.com/wSqolvS.png)\n\n- 제안한 single path, vine 모두 task를 잘 학습하는 것을 확인할 수 있습니다.\n\n- 페널티를 고정하는 것보다, KL divergenc를 제약하는 것이 step size를 선택하는 부분에서 Robust합니다.\n\n- CEM/CMA는 derivative free 알고리즘으로, 샘플 복잡도가 네트워크 파라미터수 만큼 확장되어 high dimension 문제에서는 속도와 성능이 약한 모습을 보입니다.\n\n- maxKL은 제안방법보다 느린 학습성능을 보입니다.\n\nTRPO video link: https://www.youtube.com/watch?v=jeid0wIrSn4\n\n- 사전지식이 적은 상태에서 여러 액션을 학습하는 실험으로 TRPO의 성능을 보입니다.\n\n- 지금까지의 균형이나 걸음 같은 개념을 명시적으로 인코딩한 robotic locomotion에서의 사전 연구들과 차이를 보였습니다.\n\n<br>\n## 8.2 Playing Games from Images\n\n- 부분관찰가능하며 복잡한 observation인 환경에서 TRPO를 평가하기위해 raw image를 입력으로 하는 아타리게임 환경에서 실험하였습니다.\n\n* Challenging point :\n    - High dimensional space\n    - delayed reward\n    - non-stationary image (Enduro는 배경 이미지가 바뀌고, 반전처리가 됨)\n    - complex action sequence (Q*bert는 21개의 다른 발판에서 점프 해야함)\n\n- ALE(Arcade Learning Environment) 에서 테스트\n- DQN과 전처리 방식은 동일\n    210 x 160 pixel -> gray-scaling & down-sampling -> 110 x 84  -> crop -> 84 x 84\n\n- 2 convolution layer ( 16 channels) , stride 2, FC layer with 20 units\n\n![](https://i.imgur.com/NJBC69d.png) \n\n![](https://i.imgur.com/wTe1OEW.png)\n\n![](https://i.imgur.com/j22VtNQ.png)\n\n- 30시간 동안 500 iteration 학습을 하였습니다.\n\n- DQN과 MCST를 사용한 모델(UCC-I), 사람이 플레이한 것(Human) 과의 비교하였습니다.\n\n- 이전 방법들에 비해 압도적인 점수를 기록하진 못했으나, 다양한 게임에서 적절한 게임 기록하였습니다.\n\n- 이전 방법들과 다른 점은, 특정 task에 초점을 맞춰 설계하지않았는데 robotics나 game playing Task에도 적절한 성능을 내는 점에서 **TRPO의 일반화 성능**을 확인하였습니다.\n\n<br><br>\n\n# 9. Discussion\n\n- Trust Region Policy Optimization 을 제안하였습니다.\n\n- KL Divergence 페널티로 $\\eta(\\pi)$를 최적화하는 알고리즘이 monotonically improve 함을 증명하였습니다.\n\n- Locomotion 도메인에서 여러 행동(수영, 걷기, 점프)을 제어하는 controller를 학습하였습니다.\n\n- Robotics와 게임 실험을 결합해, 시각정보와 센서데이터를 사용하는 로봇제어 정책을 학습시킬 수 있는 가능성을 보았습니다.\n\n- 샘플의 복잡도를 상당히 줄일 수 있어 실제상황에 적용가능성을 보았습니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [NPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/)\n\n## [NPG Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py)\n\n<br>\n\n# 다음으로\n\n## [TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py)\n\n## [GAE 여행하기](https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/)\n","slug":"5_trpo","published":1,"updated":"2019-02-07T11:21:31.958Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjrujlu1i00225wfe77ibd3es","content":"<center> <img src=\"https://www.dropbox.com/s/o7cjcn0e17mpizr/Screen%20Shot%202018-07-18%20at%201.14.22%20AM.png?dl=1\" width=\"700\"> </center>\n\n<p>논문 저자 : John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, Pieter Abbeel<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1502.05477.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1502.05477.pdf</a><br>Proceeding : International Conference on Machine Learning (ICML) 2015<br>정리 : 공민서, 김동민</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p>Trust region policy optimization (TRPO)는 상당히 우수한 성능을 보여주는 policy gradient 기법으로 알려져 있습니다. 높은 차원의 action space를 가진 robot locomotion부터 action은 적지만 화면을 그대로 처리하여 플레이하기 때문에 control parameter가 매우 많은 Atari game까지 각 application에 세부적으로 hyperparameter들을 특화시키지 않아도 두루두루 좋은 성능을 나타내기 때문에 일반화성능이 매우 좋은 기법입니다. 이 TRPO에 대해서 알아보겠습니다.</p>\n<p>※TRPO를 매우 재밌게 설명한 Crazymuse AI의 <a href=\"https://www.youtube.com/watch?v=CKaN5PgkSBc&amp;t=90s\" target=\"_blank\" rel=\"noopener\">Youtube video</a>에서 일부 그림을 차용했습니다. 이 비디오를 시청하시는 것을 강력하게 추천합니다!</p>\n<p><br></p>\n<h2 id=\"1-1-TRPO-흐름-잡기\"><a href=\"#1-1-TRPO-흐름-잡기\" class=\"headerlink\" title=\"1.1 TRPO 흐름 잡기\"></a>1.1 TRPO 흐름 잡기</h2><p>TRPO 논문은 많은 수식이 등장하여 이 수식들을 따라가다보면 큰 그림을 놓칠 수도 있습니다. 세부적인 내용을 살펴보기 전에 기존 연구에서 출발해서 TRPO로 어떻게 발전해나가는지 간략하게 살펴보겠습니다.</p>\n<h3 id=\"1-1-1-Original-Problem\"><a href=\"#1-1-1-Original-Problem\" class=\"headerlink\" title=\"1.1.1 Original Problem\"></a>1.1.1 Original Problem</h3><p>$$\\max_\\pi \\eta(\\pi)$$</p>\n<p>모든 강화학습이 그렇듯이 expected discounted reward를 최대화하는 policy를 찾는 문제로부터 출발합니다.</p>\n<h3 id=\"1-1-2-Conservative-policy-iteration\"><a href=\"#1-1-2-Conservative-policy-iteration\" class=\"headerlink\" title=\"1.1.2 Conservative policy iteration\"></a>1.1.2 Conservative policy iteration</h3><p>$$\\max L_\\pi(\\tilde\\pi) = \\eta(\\pi) + \\sum_s \\rho_\\pi(s)\\sum_a\\tilde\\pi(a\\vert s)A_\\pi(s,a)$$</p>\n<p>$\\eta(\\pi)$를 바로 최대화하는 것은 많은 경우 어렵습니다. $\\eta(\\pi)$의 성능향상을 보장하면서 policy를 update하는 conservative policy iteration 기법이 <a href=\"http://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf\" target=\"_blank\" rel=\"noopener\">Kakade와 Langford</a>에 의하여 제안되었습니다. 이 기법을 이용하면 policy update가 성능을 향상시키는지는 못하더라도 최소한 성능을 악화시키지는 않는다는 것이 이론적으로 보장됩니다.</p>\n<h3 id=\"1-1-3-Theorem-1-of-TRPO\"><a href=\"#1-1-3-Theorem-1-of-TRPO\" class=\"headerlink\" title=\"1.1.3 Theorem 1 of TRPO\"></a>1.1.3 Theorem 1 of TRPO</h3><p>$$\\max L_{\\pi_\\mathrm{old} }\\left(\\pi_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\quad\\left(\\alpha=D_\\mathrm{TV}^\\max\\left(\\pi_\\mathrm{old},\\pi_\\mathrm{new}\\right)\\right)$$</p>\n<p>기존의 conservative policy iteration은 과거 policy와 새로운 policy를 섞어서 사용해서 실용적이지 않다는 단점이 있었는데 이것을 보완하여 온전히 새로운 policy만으로 update할 수 있는 기법을 제안합니다. </p>\n<h3 id=\"1-1-4-KL-divergence-version-of-Theorem-1\"><a href=\"#1-1-4-KL-divergence-version-of-Theorem-1\" class=\"headerlink\" title=\"1.1.4 KL divergence version of Theorem 1\"></a>1.1.4 KL divergence version of Theorem 1</h3><p>$$\\max L_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\pi,\\tilde\\pi\\right),\\quad\\left(C = \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\right)$$</p>\n<p>distance metric을 KL divergence로 바꿀 수 있습니다.</p>\n<h3 id=\"1-1-5-Using-parameterized-policy\"><a href=\"#1-1-5-Using-parameterized-policy\" class=\"headerlink\" title=\"1.1.5 Using parameterized policy\"></a>1.1.5 Using parameterized policy</h3><p>$$\\max_\\theta L_{\\theta_\\mathrm{old} }(\\theta) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)$$</p>\n<p>최적화문제를 더욱 편리하게 풀 수 있도록 낮은 dimension을 가지는 parameter들로 parameterized된 policy를 사용할 수 있습니다.</p>\n<h3 id=\"1-1-6-Trust-region-constraint\"><a href=\"#1-1-6-Trust-region-constraint\" class=\"headerlink\" title=\"1.1.6 Trust region constraint\"></a>1.1.6 Trust region constraint</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>1.1.5까지는 아직 conservative policy iteration을 약간 변형시킨 것입니다. policy를 update할 때 지나치게 많이 변하는 것을 방지하기 위하여 <a href=\"https://en.wikipedia.org/wiki/Trust_region\" target=\"_blank\" rel=\"noopener\">trust region</a>을 constraint로 설정할 수 있습니다. 이 아이디어로 인해서 TRPO라는 명칭을 가지게 됩니다.</p>\n<h3 id=\"1-1-7-Heuristic-approximation\"><a href=\"#1-1-7-Heuristic-approximation\" class=\"headerlink\" title=\"1.1.7 Heuristic approximation\"></a>1.1.7 Heuristic approximation</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>사실 1.1.6의 constraint는 모든 state에 대해서 성립해야 하기 때문에 문제를 매우 어렵게 만듭니다. 이것을 좀 더 다루기 쉽게 state distribution에 대한 평균을 취한 것으로 변형합니다.</p>\n<h3 id=\"1-1-8-Monte-Carlo-simulation\"><a href=\"#1-1-8-Monte-Carlo-simulation\" class=\"headerlink\" title=\"1.1.8 Monte Carlo simulation\"></a>1.1.8 Monte Carlo simulation</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}$$</p>\n<p>Sampling을 통한 계산이 가능하도록 식을 다시 표현할 수 있습니다. </p>\n<h3 id=\"1-1-9-Efficiently-solving-TRPO\"><a href=\"#1-1-9-Efficiently-solving-TRPO\" class=\"headerlink\" title=\"1.1.9 Efficiently solving TRPO\"></a>1.1.9 Efficiently solving TRPO</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;\\nabla_\\theta \\left. L_{\\theta_\\mathrm{old} }(\\theta)\\right\\vert _{\\theta=\\theta_\\mathrm{old} } \\left(\\theta - \\theta_\\mathrm{old}\\right) \\\\<br>\\mathrm{s.t.\\ }&amp;\\frac{1}{2}\\left(\\theta_\\mathrm{old} - \\theta \\right)^T A\\left(\\theta_\\mathrm{old}\\right)\\left(\\theta_\\mathrm{old} - \\theta \\right) \\leq \\delta \\\\<br>&amp;A_{ij} = \\frac{\\partial}{\\partial \\theta_i}\\frac{\\partial}{\\partial \\theta_j}\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right)<br>\\end{align}$$</p>\n<p>문제를 효율적으로 풀기 위하여 approximation을 적용할 수 있습니다. objective function은 first order approximation, constraint는 quadratic approximation을 취하면 효율적으로 문제를 풀 수 있는 형태로 바뀌는데 이것을 natural gradient로 풀 수도 있고 conjugate gradient로 풀 수도 있습니다.</p>\n<p>TRPO는 이렇게 다양한 방법으로 문제를 변형한 것입니다! 이제 좀 더 자세히 살펴보겠습니다.</p>\n<p><br><br></p>\n<h1 id=\"2-Preliminaries\"><a href=\"#2-Preliminaries\" class=\"headerlink\" title=\"2. Preliminaries\"></a>2. Preliminaries</h1><p>다음과 같은 파라미터를 가지는 infinite-horizon discounted Markov decision process (MDP)를 고려합니다. </p>\n<ul>\n<li>$\\mathcal{S}$: finite set of states</li>\n<li>$\\mathcal{A}$: finite set of actions</li>\n<li>$P: \\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$: transition probability distribution</li>\n<li>$r: \\mathcal{S}\\rightarrow\\mathbb{R}$: reward function</li>\n<li>$\\rho_0:\\mathcal{S}\\rightarrow\\mathbb{R}$: distribution of the initial state $s_0$</li>\n<li>$\\gamma\\in(0,1)$: discount factor</li>\n<li>$\\eta(\\pi)=E_{s_0,a_0,\\ldots}\\left[\\sum_{t=0}^\\infty\\gamma^t r\\left(s_t\\right)\\right]$, where $s_0\\sim\\rho_0\\left(s_0\\right), a_t\\sim\\pi\\left(\\left.a_t \\right\\vert s_t\\right), s_{t+1}\\sim P\\left(\\left.s_{t+1}\\right\\vert s_t,a_t\\right)$</li>\n<li>$Q_\\pi\\left(s_t,a_t\\right)=E_{ { \\color{red}{s_{t+1}} },a_{t+1},\\ldots}\\left[\\sum_{l=0}^\\infty \\gamma^l r\\left(s_{t+l}\\right)\\right]$: action value function</li>\n<li>$V_\\pi\\left(s_t\\right)=E_{ { \\color{red}{a_t} }, s_{t+1},a_{t+1},\\ldots}\\left[\\sum_{l=0}^\\infty \\gamma^l r\\left(s_{t+l}\\right)\\right]$: value function (action value function과 expectation의 첨자가 다른 부분을 유의하세요.)</li>\n<li>$A_\\pi(s,a) = Q_\\pi(s,a) - V_\\pi(s)$: advantage function</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-1-Useful-identity-Kakade-amp-Langford-2002\"><a href=\"#2-1-Useful-identity-Kakade-amp-Langford-2002\" class=\"headerlink\" title=\"2.1 Useful identity [Kakade &amp; Langford 2002]\"></a>2.1 Useful identity [Kakade &amp; Langford 2002]</h2><p>우리의 목표는 $\\eta(\\pi)$가 최대화되도록 만드는 것입니다. 하지만 $\\pi$의 변화에 따라 $\\eta$가 어떻게 변하는지 알아내는 것도 쉽지 않습니다. $\\pi$는 기존의 policy이고 $\\tilde\\pi$는 새로운 policy를 나타낸다고 할 때 $\\eta$와 policy update 사이에 다음과 같은 관계가 있다는 것이 밝혀졌습니다.</p>\n<p><strong>Lemma 1.</strong> $\\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E_{s_{0},a_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right]$</p>\n<p><em>Proof.</em> $A_\\pi(s,a)=E_{s’\\sim P\\left(s’\\vert s,a\\right)}\\left[r(s)+\\gamma V_\\pi\\left(s’\\right)-V_\\pi(s)\\right]$로 다시 표현할 수 있습니다. 표기의 편의를 위하여 $\\tau:=\\left(s_0,a_0,s_1,a_1,\\ldots\\right)$를 정의하겠습니다. 다음과 같은 수식 전개가 가능합니다.<br>$$<br>\\begin{align}<br>&amp;E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right] \\\\<br> &amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t \\left(r(s_t)+\\gamma V_\\pi\\left(s_{t+1}\\right)-V_\\pi(s_t)\\right)\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\left(\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right)+\\gamma V_\\pi\\left(s_{1}\\right)-V_\\pi(s_0)+\\gamma^2 V_\\pi\\left(s_{2}\\right)-\\gamma V_\\pi(s_1)+\\gamma^3 V_\\pi\\left(s_{3}\\right)-\\gamma^2 V_\\pi(s_2)+\\cdots\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t)+\\color{red}{\\gamma V_\\pi\\left(s_{1}\\right)}-V_\\pi(s_0)+\\color{red}{\\gamma^2 V_\\pi\\left(s_{2}\\right)}-\\color{red}{\\gamma V_\\pi(s_1)}+\\cdots\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[-V_\\pi(s_0)+\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right] \\\\<br>&amp;\\overset{\\underset{\\mathrm{(a)} }{} }{=} -E_{\\color{red}{s_0}}\\left[V_\\pi(s_0)\\right]+E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right] \\\\<br>&amp;=-\\eta(\\pi) + \\eta\\left(\\tilde\\pi\\right)\\\\<br>&amp;\\quad\\therefore \\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E_{s_{0},a_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right]<br>\\end{align}<br>$$<br>위의 전개의 (a) 부분은 $V_\\pi\\left(s_0\\right)=E_{a_0,a_1,s_1,\\ldots}\\left[\\sum_{t=0}^\\infty\\gamma^l r\\left(s_l\\right)\\right]$이므로 $\\tau$ 중 많은 부분들이 이미 expectation이 취해진 값이므로 무시됩니다. 오직 $s_0$만 expectation이 취해지지 않았기 때문에 남아있습니다.</p>\n<p><strong>Lemma 1</strong>은 새로운 policy와 기존의 policy 사이의 관계를 규정합니다. 다음과 같은 식을 정의하고 이것을 이용해서 <strong>Lemma 1</strong>을 변형시켜 봅시다.</p>\n<ul>\n<li>$\\rho_\\pi(s) = P\\left(s_0=s\\right) + \\gamma P\\left(s_1=s\\right)\\ + \\gamma^2 P\\left(s_2=s\\right) + \\cdots $: (unnormalized) discounted visitation frequencies</li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\eta\\left(\\tilde\\pi\\right) &amp;= \\eta(\\pi) + \\sum_{t=0}^\\infty\\sum_{s}P\\left(\\left.s_t=s\\right\\vert \\tilde\\pi\\right)\\sum_a\\tilde\\pi(a\\vert s)\\gamma^t A_\\pi(s,a) \\\\<br>&amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}{\\sum_{t=0}^\\infty \\gamma^t P\\left(\\left.s_t=s\\right\\vert \\tilde\\pi\\right)} } \\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\\\<br>&amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}{\\rho_\\tilde\\pi(s)} }\\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\\\<br>\\end{align}<br>$$</p>\n<p>이 수식의 의미가 무엇일까요? 만약 $\\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\geq 0$이라면 $\\eta(\\tilde\\pi)$는 항상 $\\eta(\\pi)$보다 큽니다. 즉, policy를 업데이트함으로써 항상 개선됩니다. 즉, 이 수식을 통해서 항상 더 좋은 성능을 내는 policy update가 가져야 할 특징을 알 수 있습니다. 다음과 같은 deterministic policy가 있다고 합시다.</p>\n<p>$$<br>\\tilde\\pi(s) = \\arg\\max_a A_\\pi(s,a)<br>$$</p>\n<p>이 policy는 적어도 하나의 state-action pair에서 0보다 큰 값을 가지는 advantage가 있고 그 때의 확률이 0이 아니라면 항상 성능을 개선시킵니다. 어려운 점은 policy를 바꾸면 $\\rho$도 바뀐다는 점입니다. </p>\n<p>다음 그림을 봅시다. Starting Point에서 여러가지 경로를 거쳐서 Destination으로 갈 수 있습니다. 다른 policy를 이용하는 것은 다른 경로를 이용한 것입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=16s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/policy_change.png\" alt=\"policy_change\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>이 때 다른 policy를 이용함에 따라 state visitation frequency도 변하게 됩니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=36s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/state_visitation_change.png\" alt=\"state_visitation_change\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>이로 인해서 policy를 최적화하는 것은 어려워집니다. 그래서 이러한 변화를 무시하는 다음과 같은 approximation을 취합니다.</p>\n<p>$$<br>\\begin{align}<br>L_\\pi\\left(\\tilde\\pi\\right) &amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}{\\rho_\\pi(s)} } \\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a)<br>\\end{align}<br>$$</p>\n<p>policy가 바뀌었음에도 이전의 state distribution을 계속 이용하는 것입니다. 이것은 policy의 변화가 크지 않다면 어느 정도 허용될 수 있을 것입니다. 그렇지만 얼마나 많은 변화가 허용될까요? 이것을 정하기 위해서 이용하는 것이 trust region입니다.</p>\n<p><br></p>\n<h2 id=\"2-2-Conservative-Policy-Iteration\"><a href=\"#2-2-Conservative-Policy-Iteration\" class=\"headerlink\" title=\"2.2 Conservative Policy Iteration\"></a>2.2 Conservative Policy Iteration</h2><p>policy의 변화를 다루기 용이하게 하기 위해서 policy를 다음과 같이 파라미터를 이용해서 표현합시다.</p>\n<ul>\n<li>$\\pi_\\theta(a\\vert s)$: parameterized policy</li>\n</ul>\n<p>$\\pi_\\theta$는 $\\theta$에 대하여 미분가능한 함수입니다. $L_\\pi\\left(\\tilde\\pi\\right)$을 $\\theta_0$에서 $\\eta(\\pi)$에 대한 first order approximation이라고 하면 다음 식이 성립합니다.</p>\n<p>$$<br>\\begin{align}<br>L_{\\pi_{\\theta_0} }\\left(\\pi_{\\theta_0}\\right) &amp;= \\eta\\left(\\pi_{\\theta_0}\\right) \\\\<br>\\nabla_\\theta \\left.L_{\\pi_{\\theta_0} }\\left(\\pi_{\\theta_0}\\right)\\right\\vert _{\\theta=\\theta_0} &amp;= \\nabla_\\theta\\left.\\eta(\\pi_{\\theta_0})\\right\\vert _{\\theta=\\theta_0}<br>\\end{align}<br>$$</p>\n<p>이것의 의미는 $\\pi_{\\theta_0}$가 매우 작게 변한다면 $L_{\\pi_{\\theta_0} }$를 개선시키는 것이 $\\eta$를 개선시키는 것이라는 것입니다. 그러나 지금까지의 설명만으로는 $\\pi_{\\theta_0}$를 얼마나 작게 변화시켜야 할지에 대해서는 알 수 없습니다.</p>\n<p>Kakade &amp; Langford가 2002년 발표한 논문에서도 이것에 대해서 고민했습니다. 그 논문에서 <em>conservative policy iteration</em>이라는 기법을 제안합니다. 그 논문의 contribution은 다음과 같습니다.</p>\n<ul>\n<li>$\\eta$ 개선의 lower bound를 제공</li>\n<li>기존의 policy를 $\\pi_\\mathrm{old}$라고 하고 $\\pi’$를 $\\pi’=\\arg\\max_{\\pi’}L_{\\pi_\\mathrm{old} }\\left(\\pi’\\right)$과 같이 정의할 때, 새로운 mixture policy $\\pi_\\mathrm{new}$를 다음과 같이 제안<br>$$<br>\\pi_\\mathrm{new}(a\\vert s) = (1-\\alpha)\\pi_\\mathrm{old}(a\\vert s) + \\alpha \\pi’(a\\vert s)<br>$$</li>\n</ul>\n<p>그림으로 표현하면 다음과 같습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=2m46s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/mixure_policy.png\" alt=\"mixure_policy\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<ul>\n<li>다음과 같은 lower bound를 정의<br>$$<br>\\eta\\left(\\pi_\\mathrm{new}\\right) \\geq L_{\\pi_{\\theta_\\mathrm{old} }}\\left(\\pi_\\mathrm{new}\\right) - \\frac{2\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\\\<br>\\mathrm{where\\ }\\epsilon = \\max_s\\left\\vert E_{a\\sim\\pi’(a\\vert s)}\\left[A_\\pi(s,a)\\right]\\right\\vert<br>$$</li>\n</ul>\n<p>하지만 mixture된 policy라는 것은 실용적이지 않습니다. </p>\n<p><br><br></p>\n<h1 id=\"3-Monotonic-Improvement-Guarantee-for-General-Stochastic-Policies\"><a href=\"#3-Monotonic-Improvement-Guarantee-for-General-Stochastic-Policies\" class=\"headerlink\" title=\"3. Monotonic Improvement Guarantee for General Stochastic Policies\"></a>3. Monotonic Improvement Guarantee for General Stochastic Policies</h1><p>이전 장에서 설명한 lower bound는 오직 mixture policy에 대해서만 성립하고 더 많이 사용되는 stochastic policy에는 적용되지 않습니다. 따라서 stochastic policy를 이용할 수 있도록 개선할 필요가 있습니다. 아래와 같이 기존 수식에서 두 가지를 바꿈으로써 이것이 가능합니다.</p>\n<ul>\n<li>$\\alpha\\rightarrow$ distance measure between $\\pi$ and $\\tilde\\pi$</li>\n<li>constant $\\epsilon\\rightarrow\\max_{s,a}\\left\\vert A_\\pi(s,a)\\right\\vert $</li>\n</ul>\n<p>여기서 distance measure로 total variation divergence를 이용합니다. discrete porbability distribution $p$와 $q$에 대하여 다음과 같이 정의됩니다.<br>$$<br>D_\\mathrm{TV}(p\\parallel q) = \\frac{1}{2}\\sum_i\\left\\vert p_i - q_i\\right\\vert<br>$$</p>\n<p>이것을 이용하여 $D_\\mathrm{TV}^\\max$를 다음과 같이 정의합니다.<br>$$<br>D_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D_\\mathrm{TV}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)<br>$$</p>\n<p>그림으로 표현하면 다음과 같습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=3m15s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/tvd.png\" alt=\"tvd\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>이것을 이용하여 다음과 같은 관계식을 얻을 수 있습니다.</p>\n<p><strong>Theorem 1.</strong> Let $\\alpha=D_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi)$. Then the following bound holds:<br>$$<br>\\begin{align}<br>\\eta\\left(\\pi_\\mathrm{new}\\right) &amp;\\geq L_{\\pi_\\mathrm{old} }\\left(\\pi_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2, \\\\<br>\\mathrm{where\\ } \\epsilon&amp;= \\max_{s,a}\\left\\vert A_\\pi(s,a)\\right\\vert<br>\\end{align}<br>$$<br><!--*Proof.* TBD.--></p>\n<p>또다른 distance metric으로 아래 그림과 같은 KL divergence가 있습니다.<br>(그런데 왜 하필 KL divergence로 바꿀까요? 논문의 뒤쪽에서 계산효율을 위해서 conjugate gradient method를 이용하는데 이를 위해서 바꾼게 아닐까 싶습니다. Wasserstein distance 같은 다른 방향으로 발전시킬 수도 있을 것 같습니다. Schulmann이 아마 해봤겠죠?^^a)</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=3m34s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/kld.png\" alt=\"kld\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>total variation divergence와 KL divergence 사이에는 다음과 같은 관계가 있습니다.<br>$$<br>D_\\mathrm{TV}(p\\parallel q)^2 \\leq D_\\mathrm{KL}(p\\parallel q)<br>$$</p>\n<p>다음 수식을 정의합니다.<br>$$<br>D_\\mathrm{KL}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D_\\mathrm{KL}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)<br>$$</p>\n<p><strong>Theorem 1</strong>을 이용하여 다음과 같은 수식이 성립함을 알 수 있습니다.<br>$$<br>\\begin{align}<br>\\eta\\left(\\tilde\\pi\\right) &amp;\\geq L_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max(\\pi, \\tilde\\pi), \\\\<br>\\mathrm{where\\ } C&amp;= \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}<br>\\end{align}<br>$$</p>\n<p>이러한 policy imporvement bound를 기반으로 다음과 같은 approximate policy iteration 알고리듬을 고안해낼 수 있습니다.</p>\n<p><strong>Algorithm 1</strong> Policy iteration algorithm guaranteeing non-decreasing expected return $\\eta$</p>\n<blockquote>\n<p>Initialize $\\pi_0$<br><strong>for</strong> $i=0,1,2,\\ldots$ until convergence <strong>do</strong><br>$\\quad$Compute all advantage values $A_{\\pi_i}(s,a)$<br>$\\quad$Solve the contstrained optimization problem<br>$\\quad\\pi_{i+1}=\\arg\\max_\\pi\\left[L_{\\pi_i} - CD_\\mathrm{KL}^\\max\\left(\\pi_i,\\pi\\right)\\right]$<br>$\\quad\\quad$where $C = 4\\epsilon\\gamma / (1-\\gamma)^2$<br>$\\quad\\quad$and $L_{\\pi_i}\\left(\\pi\\right) = \\eta(\\pi_i) + \\sum_{s}\\rho_{\\pi_i}(s)\\sum_a\\pi(a\\vert s) A_{\\pi_i}(s,a)$<br><strong>end for</strong></p>\n</blockquote>\n<p><strong>Algorithm 1</strong>은 advantage를 정확하게 계산할 수 있다고 가정하고 있습니다. 이 알고리듬은  monotonical한 성능 증가($\\eta(\\pi_0)\\leq\\eta(\\pi_1)\\leq\\cdots$)를 한다는 것을 다음과 같이 보일 수 있습니다. $M_i(\\pi)=L_{\\pi_i}(\\pi) - CD_\\mathrm{KL}^\\max\\left(\\pi_i,\\pi\\right)$라고 합시다.</p>\n<p>$$<br>\\begin{align}<br>\\eta \\left(\\pi_{i+1}\\right) &amp;\\geq M_i\\left(\\pi_{i+1}\\right)\\\\<br>\\eta \\left(\\pi_{i}\\right) &amp;= M_i\\left(\\pi_{i}\\right) \\\\<br>\\eta \\left(\\pi_{i+1}\\right) - \\eta \\left(\\pi_{i}\\right) &amp;\\geq M_i\\left(\\pi_{i+1}\\right) - M_i\\left(\\pi_{i}\\right)<br>\\end{align}<br>$$</p>\n<p>위 수식과 같이 매 iteration 마다 $M_i$를 최대화함으로써, $\\eta$가 감소하지 않는다는 것을 보장할 수 있습니다. 이와 같은 타입의 알고리듬을 <a href=\"https://www.jstor.org/stable/pdf/27643496.pdf?casa_token=0qHamcl60WoAAAAA:-fkZ9JcA_nrY3-zbCUpqvPOgcAMgw7Gr96MajCZg2byHf8m5GU1KTSxyJJcBy1lPZbBTZVCjHHUXilh4k-iuwF91Wka4B5qdltC1IR2qMWk8q1FoV6__\" target=\"_blank\" rel=\"noopener\">minorization-maximization (MM) algorithm</a>이라고 합니다. EM 알고리듬도 이 타입의 알고리듬 중 하나입니다.</p>\n<p><strong>Algorithm 1</strong>은 아래 그림과 같이 동작합니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=3m52s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/surrogate.png\" alt=\"surrogate\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>$M_i$는 $\\pi_i$일 때 equality가 되는 $\\eta$에 대한  surrogate function입니다. TRPO는 이 surrogate function을 최대화하고 KL divergence를 penalty가 아닌 constraint로 두는 알고리듬입니다.</p>\n<p><br><br></p>\n<h1 id=\"4-Optimization-of-Parameterized-Policies\"><a href=\"#4-Optimization-of-Parameterized-Policies\" class=\"headerlink\" title=\"4. Optimization of Parameterized Policies\"></a>4. Optimization of Parameterized Policies</h1><p>표기의 편의를 위해 다음과 같이 notation들을 더 간략하게 정의합니다.</p>\n<ul>\n<li>$\\eta(\\theta):=\\eta\\left(\\pi_\\theta\\right)$</li>\n<li>$L_\\theta\\left(\\tilde\\theta\\right):=L_{\\pi_\\theta}\\left(\\pi_{\\tilde\\theta}\\right)$</li>\n<li>$D_\\mathrm{KL}\\left(\\theta\\parallel\\tilde\\theta\\right):=D_\\mathrm{KL}\\left(\\pi_\\theta\\parallel\\pi_{\\tilde\\theta}\\right)$</li>\n<li>$\\theta_\\mathrm{old}$: previous policy parameter</li>\n</ul>\n<p><br></p>\n<h2 id=\"4-1-Trust-Region-Policy-Optimization\"><a href=\"#4-1-Trust-Region-Policy-Optimization\" class=\"headerlink\" title=\"4.1 Trust Region Policy Optimization\"></a>4.1 Trust Region Policy Optimization</h2><p>이전 장의 중요 결과를 위의 notation으로 다시 표기하면 아래와 같습니다.</p>\n<p>$$<br>\\eta\\left(\\theta\\right) \\geq L_{\\theta_\\mathrm{old} }\\left(\\theta\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)<br>$$</p>\n<p>$\\eta$의 성능 향상을 보장하기 위해서 그것의 lower bound를 최대화할 수 있습니다.</p>\n<p>$$<br>\\max_\\theta \\left[L_{\\theta_\\mathrm{old} }\\left(\\theta\\right) - C D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)\\right]<br>$$</p>\n<p>이 최적화 문제는 step size를 매우 작게 해야 올바른 동작을 합니다. 위에서 살펴봤듯이 first order approximation이기 때문입니다. 좀 더 큰 step size를 가질 수 있도록 이 최적화 문제를 trust region constraint를 도입하여 다음과 같이 바꿉니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>이 최적화문제의 constraint는 모든 state space에 대해서 성립해야 합니다. 또한 maximum값을 매번 찾아야 합니다. state가 많은 경우 constraint의 수가 매우 많아져서 문제를 풀기 어렵게 만듭니다. constraint의 수를 줄이기 위하여 다음과 같은 avergae KL divergence를 이용하는 heuristic approximation을 취합니다. 이것이 최선의 방법은 아닐 수 있지만 실용적인 방법입니다.</p>\n<p>$$<br>\\overline D_\\mathrm{KL}^{\\rho}\\left(\\theta_1, \\theta_2\\right):=E_{s\\sim\\rho}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_1}(\\cdot\\vert s)\\parallel\\pi_{\\theta_2}(\\cdot\\vert s)\\right)\\right]<br>$$</p>\n<p>이것을 기반으로 다음과 같은 최적화 문제를 풀 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>아래 그림처럼 step size에 대해서 고려할 수 있습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=4m35s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/heuristic_approx.png\" alt=\"heuristic_approx\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p><br><br></p>\n<h1 id=\"5-Sample-Based-Estimation-of-the-Objective-and-Constraint\"><a href=\"#5-Sample-Based-Estimation-of-the-Objective-and-Constraint\" class=\"headerlink\" title=\"5. Sample-Based Estimation of the Objective and Constraint\"></a>5. Sample-Based Estimation of the Objective and Constraint</h1><p>실용적인 알고리듬을 만들려고 하는 노력은 아직 끝나지 않았습니다. 이제 앞의 알고리듬을 sample-based estimation 즉, Monte Carlo estimation을 할 수 있도록 바꿔보겠습니다. sampling을 편하게 할 수 있도록 아래와 같이 바꿔줍니다.</p>\n<ul>\n<li>$\\sum_s \\rho_{\\theta_\\mathrm{old} }(s)[\\ldots]\\rightarrow\\frac{1}{1-\\gamma}E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}[\\ldots]$</li>\n<li>$A_{\\theta_\\mathrm{old} }\\rightarrow Q_{\\theta_\\mathrm{old} }$</li>\n<li>$\\sum_a\\pi_{\\theta_\\mathrm{old} }(a\\vert s)A_{\\theta_\\mathrm{old} } \\rightarrow E_{a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\mathrm{old} }\\right]$</li>\n</ul>\n<p>이러한 변화를 그림처럼 도식화할 수 있습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=5m31s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/sample-based.png\" alt=\"sample-based\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>한 가지 짚고 넘어가야 할 점은 action sampling을 할 때 importance sampling을 사용한다는 것입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=5m53s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/importance_sampling.png\" alt=\"importance_sampling\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>바뀐 최적화문제는 아래와 같습니다.</p>\n<ul>\n<li><strong>Equation (14).</strong></li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}<br>$$</p>\n<p>이 때 sampling하는 두 가지 방법이 있습니다.</p>\n<p><br></p>\n<h2 id=\"5-1-Single-Path\"><a href=\"#5-1-Single-Path\" class=\"headerlink\" title=\"5.1 Single Path\"></a>5.1 Single Path</h2><p><em>single path</em>는 개별 trajectory들을 이용하는 방법입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=6m15s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/single.png\" alt=\"single\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p><br></p>\n<h2 id=\"5-2-Vine\"><a href=\"#5-2-Vine\" class=\"headerlink\" title=\"5.2 Vine\"></a>5.2 Vine</h2><p><em>vine</em>은 한 state에서 rollout을 이용하여 여러 action을 수행하는 방법입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=6m32s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/vine1.png\" alt=\"vine1\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=6m49s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/vine2.png\" alt=\"vine2\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>estimation의 variance를 낮출 수 있지만 계산량이 많고 한 state에서 여러 action을 수행할 수 있어야 하기 때문에 현실적인 문제에 적용하기에는 어려움이 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"6-Practical-Algorithm\"><a href=\"#6-Practical-Algorithm\" class=\"headerlink\" title=\"6. Practical Algorithm\"></a>6. Practical Algorithm</h1><p>앞서 single-path, vine 샘플링을 사용하는 두가지 방식의 policy optimization 알고리즘을 살펴봤습니다. 실용적인 알고리듬은 아래의 과정을 반복해서 수행합니다.</p>\n<p>1) Q-values의 몬테카를로 추정을 통해 state-action 쌍 집합을 single path 또는 vine 과정을 통해 수집함.</p>\n<p>2) 샘플 평균으로, (14)식의 목적함수와 제약식을 추정함</p>\n<ul>\n<li><strong>Equation (14).</strong><br>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}<br>$$</li>\n</ul>\n<p>3) policy parameter vector인 $\\theta$를 업데이트 하면서 제약조건이 있는 최적화 문제를 근사적으로 풂. 본 논문에서는 gradient를 직접 계산하는 것보다는 약간 더 계산량이 있는 line search와 conjugate gradient algorithm을 사용했습니다.</p>\n<p>3)에 대해서,  gradient의 covariance matrix를 사용하지 않고 KL divergence의 Hessian을 해석적으로 계산하여 Fisher Information Matrix를 구성했습니다.</p>\n<p>다시말해서</p>\n<p>$\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial}{\\partial \\theta_i} \\log \\pi_{\\theta}(a_n\\vert s_n) \\frac{\\partial}{\\partial \\theta_j} \\log \\pi_{\\theta}(a_n\\vert s_n)$ 대신 $\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} D_{KL}(\\pi_{\\theta_{old} }(\\cdot \\vert  s_n) \\vert \\vert  \\pi_{\\theta}(\\cdot \\vert  s_n))$를 계산한 것입니다.</p>\n<p>이 analytic estimator는 Hessian이나 trajectories의 모든 gradient를 저장하지 않아도 되기 때문에 대규모 환경을 고려할 경우 계산 상 이점이 있습니다. 좀 더 자세하게 설명한 Appendix C 부분을 살펴보겠습니다.</p>\n<p><br></p>\n<h2 id=\"Appendix-C-Efficiently-Solving-the-Trust-Region-Constrained-Optimization-Problem\"><a href=\"#Appendix-C-Efficiently-Solving-the-Trust-Region-Constrained-Optimization-Problem\" class=\"headerlink\" title=\"Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem\"></a>Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem</h2><p>우리가 풀고자 하는 식은 다음과 같습니다.</p>\n<p>$$<br>\\begin{align}<br>\\max\\quad &amp;L(\\theta) \\\\<br>\\mathrm{s.t.\\ } &amp;\\overline{D}_{KL}(\\theta_\\mathrm{old}, \\theta) \\leq \\delta<br>\\end{align}<br>$$</p>\n<p>이 최적화 문제를 풀기 위하여 우리는 gradient 기법을 이용할 것입니다. gradient 기법은 결국 update할 방향과 크기를 결정하는 문제로 생각할 수 있습니다. 이에 따라 아래와 같은 두가지 과정으로 진행합니다.</p>\n<p>1) 탐색 방향을 계산, 목적함수의 first-order approximation(선형근사)와 제약식의 quadratic approximation(2차 근사).</p>\n<p>2) 이동거리 계산을 위해 해당 방향으로 line search 수행.</p>\n<p>탐색 방향은 $Ax = g$ 수식을 근사적으로 풀어서 구합니다. 여기서 $A$ 는 Fisher Information Matrix이고, 이것은 KL divergence 제약식의 2차 근사 $\\overline{D}_\\mathrm{KL} (\\theta_\\mathrm{old}, \\theta) \\approx \\frac{1}{2}(\\theta - \\theta_\\mathrm{old})^T A(\\theta - \\theta_\\mathrm{old})$를 푸는 것입니다. 여기서 $A_{ij} = \\frac{\\partial}{\\partial \\theta_i} \\frac{\\partial}{\\partial \\theta_j} \\overline{D}_\\mathrm{KL}(\\theta_\\mathrm{old}, \\theta)$입니다.</p>\n<p>논문에서 이 부분에 대한 설명이 약간 부족해서 좀 더 내용을 추가하자면, $\\frac{1}{2}x^T A x - x^T g + b = 0$과 같은 quadratic equation의 해는 1차 derivative를 0으로 만드는 $x$를 찾음으로써 알 수 있습니다. 위에서 우리는 목적함수를 1차 근사시키고 제약조건은 2차 근사를 시킴으로써 quadratic equation 형태로 변형했기 때문에 같은 방식으로 풀 수 있습니다. 따라서 $g$는 1차근사시킨 목적함수의 계수가 될 것입니다.</p>\n<p>대규모 문제에서 $A$ 혹은 $A^{-1}$ 을 계산하는 것은 엄청난 계산량이 필요합니다. 하지만 conjugate gradient algorithm을 잘 이용하면 전체 $A$행렬(FIM)을 계산하지 않아도 $Ax=g$ 를 근사적으로 해결할 수 있게 해줍니다.</p>\n<p>탐색 방향 $s \\approx A^{-1}g$과 더불어 최대스텝길이(step size) $\\beta$도 계산할 필요가 있습니다.</p>\n<p>즉, $\\delta = \\overline{D}_\\mathrm{KL} \\approx \\frac{1}{2}(\\beta s)^T A(\\beta s) = \\frac{1}{2}\\beta^2s^TAs$ 이고, </p>\n<p>$\\beta = \\sqrt{2\\delta / s^T As}$가 됩니다. 이때 $\\delta$는 KL divergence의 boundary term 입니다.</p>\n<p>$s^T As$ 항은 Hessian-vector product로 계산할 수 있고, conjugate gradient 과정에서 계산됩니다.</p>\n<p>마지막으로, surrogate objective와 KL divergence 제약식을 위해 다음과 같은 함수를 정의하고 line search를 사용합니다.</p>\n<p>$$<br>L_{\\theta_\\mathrm{old} }(\\theta) - \\chi [\\overline{D}_{KL}(\\theta_\\mathrm{old}, \\theta) \\leq \\delta]<br>$$</p>\n<p>$\\chi[\\cdot]$ 함수는 []안의 조건이 맞으면 0이고 틀리면 무한으로 발산하는 함수입니다.</p>\n<p>위에서 계산한 $\\beta$ 의 최대 값부터 시작해서 목적함수가 개선될때까지 exponential하게 줄여 나갑니다. line search가 없었다면 이 알고리즘은 매우 큰 스텝으로 계산될 것이고 심각한 성능 저하가 발생할 것입니다.</p>\n<p>(여기서 살짝 예고를 하자면, 나중에 이 KL 기반 기법을 개선한 더 단순한 방법이 제안됩니다. PPO라고…)</p>\n<p><br><br></p>\n<h1 id=\"7-Connections-with-Prior-Work\"><a href=\"#7-Connections-with-Prior-Work\" class=\"headerlink\" title=\"7. Connections with Prior Work\"></a>7. Connections with Prior Work</h1><p>Natural Policy Gradient는 $L$의 선형 근사와 $\\overline D_\\mathrm{KL}$ 제약식을 2차근사 하는 아래의 식의 special case입니다.</p>\n<ul>\n<li>Equation (17).</li>\n</ul>\n<p>$L_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A_\\pi(s,a)$</p>\n<p>$\\underset{\\theta}{\\max}\\quad [\\nabla_{\\theta}L_{\\theta_\\mathrm{old} }(\\theta) \\ \\rvert_{\\theta=\\theta_\\mathrm{old} } \\cdot (\\theta - \\theta_\\mathrm{old})]$</p>\n<p>$\\mathrm{s.t.}\\quad\\frac{1}{2}(\\theta_\\mathrm{old} - \\theta)^{T} A(\\theta_\\mathrm{old})(\\theta_\\mathrm{old}-\\theta) \\leq \\delta$</p>\n<p>$\\mathrm{where\\ }A(\\theta_{old})_{ij} = $</p>\n<p>$\\Large \\frac{\\partial}{\\partial \\theta_{i} } \\frac{\\partial}{\\partial \\theta_{j} }<br>E_{s \\sim \\rho_{\\pi} }[D_\\mathrm{KL}(\\pi(\\cdot\\rvert s, \\theta_\\mathrm{old})  \\rvert\\rvert \\pi(\\cdot \\rvert s, \\theta))] \\rvert_{\\theta=\\theta_\\mathrm{old} }$</p>\n<p>업데이트 식은 다음과 같습니다.</p>\n<p>$\\theta_\\mathrm{new} = \\theta_\\mathrm{old} +  \\color{Red}{\\frac{1}{\\lambda} } A(\\theta_\\mathrm{old})^{-1}\\nabla_{\\theta}L(\\theta) \\rvert_{\\theta=\\theta_\\mathrm{old} }$</p>\n<p>여기서 step size인 $\\frac{1}{\\lambda}$ 일반적으로 알고리즘의 파라미터로 취급되지만 TRPO는 각 업데이트 마다 제약식으로 사용한다는 점이 다릅니다. 사소한 차이처럼 보이지만, 큰 차원을 다루는 실험에서 성능을 크게 향상시켰습니다.</p>\n<p>또한 $L^2$ 제약식(혹은 페널티) 를 사용하면 표준 Policy Gradient 업데이트 식을 얻었습니다.</p>\n<ul>\n<li>Equation (18).</li>\n</ul>\n<p>$\\underset{\\theta}{\\max}\\quad[\\nabla_{\\theta} L_{\\theta_\\mathrm{old} }(\\theta) \\rvert_{\\theta=\\theta_\\mathrm{old} } \\cdot (\\theta - \\theta_\\mathrm{old})]$</p>\n<p>$\\mathrm{s.t.}\\quad \\frac{1}{2} \\vert\\vert \\theta - \\theta_\\mathrm{old} \\vert\\vert^2 \\leq \\delta$</p>\n<p>$L_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A_\\pi(s,a) $를 이용해서 제약조건없이 $\\underset{\\pi}{\\max}\\quad L_{\\pi_\\mathrm{old} }(\\pi)$를 풀면<br>Policy Iteration update를 하는 것과 같습니다.</p>\n<p><br><br></p>\n<h1 id=\"8-Experiments\"><a href=\"#8-Experiments\" class=\"headerlink\" title=\"8. Experiments\"></a>8. Experiments</h1><ul>\n<li><p>다음과 같은 3 가지 궁금증을 해결하기 위하여 실험을 설계하였습니다.</p>\n<ol>\n<li><p>Single-path 와 vine 의 성능면에서 특징을 알고싶음.</p>\n</li>\n<li><p>Fixed penalty coefficient(NPG) 보다 fixed KL divergence를 사용하는 것(TRPO)으로<br>변경한 것이 얼마나 뚜렷한 차이를 보일지, 성능 면에서 어떤 영향을 미치는지 알고싶음.</p>\n</li>\n<li><p>TRPO가 큰 스케일의 문제를 해결할 수 있는지, 기존에 연구/적용되어왔던 방법들과<br>성능, 계산시간, 샘플 복잡도 면에서 비교하고 싶음.</p>\n</li>\n</ol>\n</li>\n</ul>\n<ul>\n<li><p>1, 2의 궁금증에 대해서 실험 환경에 single path, vine을 비롯한 여러 사전방법들을 함께 실험하였습니다.</p>\n</li>\n<li><p>3에 대해선, robotic locomotion과 Atari game에 TRPO를 실험하였습니다.</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"8-1-Simulated-Robotic-Locomotion\"><a href=\"#8-1-Simulated-Robotic-Locomotion\" class=\"headerlink\" title=\"8.1 Simulated Robotic Locomotion\"></a>8.1 Simulated Robotic Locomotion</h2><ul>\n<li>MuJoCo 시뮬레이터를 활용해 로봇 locomotion 실험 진행하였습니다.</li>\n<li>Robot State: positions and velocities, joint torques</li>\n</ul>\n<p><img src=\"https://i.imgur.com/bs5ATS3.png\" alt=\"\"></p>\n<ul>\n<li><p>수영, 점프, 걷기 행동을 학습시키고자 합니다.</p>\n<ul>\n<li><p>수영: 10 dimensional state space , reward: $r(x,u)=v_x - 10^{-5}\\vert \\vert u\\vert \\vert ^2$, quadratic penalty.</p>\n</li>\n<li><p>점프: 12 dimensional state space, episode의 끝은 높이와 각도 threshol로 점프를 판단, non-terminal state에 대해 +1 보상 추가.</p>\n</li>\n<li><p>걷기: 18 dimensional state space, 뜀뛰는 듯한 보법(점프에이전트)보다 부드러운 보법을 유도하기위해 페널티를 추가함.</p>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Detailed Experiment Setup and used parameters, used network model</strong><br><img src=\"https://i.imgur.com/zgnsbw6.png\" alt=\"\"></p>\n<p><img src=\"https://i.imgur.com/FqdWC53.png\" alt=\"\"></p>\n<p>equation (12) : $\\max\\quad L_{\\theta_\\mathrm{old}(\\theta)}, \\ \\ \\mathrm{s.t.\\ }\\overline{D}_\\mathrm{KL}^{\\rho_{\\theta_\\mathrm{old} }}(\\theta_\\mathrm{old}, \\theta) \\leq \\delta$</p>\n<p>이 실험에서 $\\delta = 0.01$입니다.</p>\n<ul>\n<li><p>비교에 사용된 모델들</p>\n<ul>\n<li>TRPO Single-path</li>\n<li>TRPO vine</li>\n<li>CEM(cross-entropy method)</li>\n<li>CMA(covariance matrix adaptation)</li>\n<li>NPG (Lagrange Multiplier penalty coefficien를 사용하는 것이 차이점)</li>\n<li>Empirical FIM</li>\n<li>maximum KL(cartpole)</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://i.imgur.com/wSqolvS.png\" alt=\"\"></p>\n<ul>\n<li><p>제안한 single path, vine 모두 task를 잘 학습하는 것을 확인할 수 있습니다.</p>\n</li>\n<li><p>페널티를 고정하는 것보다, KL divergenc를 제약하는 것이 step size를 선택하는 부분에서 Robust합니다.</p>\n</li>\n<li><p>CEM/CMA는 derivative free 알고리즘으로, 샘플 복잡도가 네트워크 파라미터수 만큼 확장되어 high dimension 문제에서는 속도와 성능이 약한 모습을 보입니다.</p>\n</li>\n<li><p>maxKL은 제안방법보다 느린 학습성능을 보입니다.</p>\n</li>\n</ul>\n<p>TRPO video link: <a href=\"https://www.youtube.com/watch?v=jeid0wIrSn4\" target=\"_blank\" rel=\"noopener\">https://www.youtube.com/watch?v=jeid0wIrSn4</a></p>\n<ul>\n<li><p>사전지식이 적은 상태에서 여러 액션을 학습하는 실험으로 TRPO의 성능을 보입니다.</p>\n</li>\n<li><p>지금까지의 균형이나 걸음 같은 개념을 명시적으로 인코딩한 robotic locomotion에서의 사전 연구들과 차이를 보였습니다.</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"8-2-Playing-Games-from-Images\"><a href=\"#8-2-Playing-Games-from-Images\" class=\"headerlink\" title=\"8.2 Playing Games from Images\"></a>8.2 Playing Games from Images</h2><ul>\n<li>부분관찰가능하며 복잡한 observation인 환경에서 TRPO를 평가하기위해 raw image를 입력으로 하는 아타리게임 환경에서 실험하였습니다.</li>\n</ul>\n<ul>\n<li>Challenging point :<ul>\n<li>High dimensional space</li>\n<li>delayed reward</li>\n<li>non-stationary image (Enduro는 배경 이미지가 바뀌고, 반전처리가 됨)</li>\n<li>complex action sequence (Q*bert는 21개의 다른 발판에서 점프 해야함)</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>ALE(Arcade Learning Environment) 에서 테스트</li>\n<li><p>DQN과 전처리 방식은 동일<br>  210 x 160 pixel -&gt; gray-scaling &amp; down-sampling -&gt; 110 x 84  -&gt; crop -&gt; 84 x 84</p>\n</li>\n<li><p>2 convolution layer ( 16 channels) , stride 2, FC layer with 20 units</p>\n</li>\n</ul>\n<p><img src=\"https://i.imgur.com/NJBC69d.png\" alt=\"\"> </p>\n<p><img src=\"https://i.imgur.com/wTe1OEW.png\" alt=\"\"></p>\n<p><img src=\"https://i.imgur.com/j22VtNQ.png\" alt=\"\"></p>\n<ul>\n<li><p>30시간 동안 500 iteration 학습을 하였습니다.</p>\n</li>\n<li><p>DQN과 MCST를 사용한 모델(UCC-I), 사람이 플레이한 것(Human) 과의 비교하였습니다.</p>\n</li>\n<li><p>이전 방법들에 비해 압도적인 점수를 기록하진 못했으나, 다양한 게임에서 적절한 게임 기록하였습니다.</p>\n</li>\n<li><p>이전 방법들과 다른 점은, 특정 task에 초점을 맞춰 설계하지않았는데 robotics나 game playing Task에도 적절한 성능을 내는 점에서 <strong>TRPO의 일반화 성능</strong>을 확인하였습니다.</p>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"9-Discussion\"><a href=\"#9-Discussion\" class=\"headerlink\" title=\"9. Discussion\"></a>9. Discussion</h1><ul>\n<li><p>Trust Region Policy Optimization 을 제안하였습니다.</p>\n</li>\n<li><p>KL Divergence 페널티로 $\\eta(\\pi)$를 최적화하는 알고리즘이 monotonically improve 함을 증명하였습니다.</p>\n</li>\n<li><p>Locomotion 도메인에서 여러 행동(수영, 걷기, 점프)을 제어하는 controller를 학습하였습니다.</p>\n</li>\n<li><p>Robotics와 게임 실험을 결합해, 시각정보와 센서데이터를 사용하는 로봇제어 정책을 학습시킬 수 있는 가능성을 보았습니다.</p>\n</li>\n<li><p>샘플의 복잡도를 상당히 줄일 수 있어 실제상황에 적용가능성을 보았습니다.</p>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"NPG-여행하기\"><a href=\"#NPG-여행하기\" class=\"headerlink\" title=\"NPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/\">NPG 여행하기</a></h2><h2 id=\"NPG-Code\"><a href=\"#NPG-Code\" class=\"headerlink\" title=\"NPG Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py\" target=\"_blank\" rel=\"noopener\">NPG Code</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"TRPO-Code\"><a href=\"#TRPO-Code\" class=\"headerlink\" title=\"TRPO Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></h2><h2 id=\"GAE-여행하기\"><a href=\"#GAE-여행하기\" class=\"headerlink\" title=\"GAE 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/\">GAE 여행하기</a></h2>","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"https://www.dropbox.com/s/o7cjcn0e17mpizr/Screen%20Shot%202018-07-18%20at%201.14.22%20AM.png?dl=1\" width=\"700\"> </center>\n\n<p>논문 저자 : John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, Pieter Abbeel<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1502.05477.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1502.05477.pdf</a><br>Proceeding : International Conference on Machine Learning (ICML) 2015<br>정리 : 공민서, 김동민</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p>Trust region policy optimization (TRPO)는 상당히 우수한 성능을 보여주는 policy gradient 기법으로 알려져 있습니다. 높은 차원의 action space를 가진 robot locomotion부터 action은 적지만 화면을 그대로 처리하여 플레이하기 때문에 control parameter가 매우 많은 Atari game까지 각 application에 세부적으로 hyperparameter들을 특화시키지 않아도 두루두루 좋은 성능을 나타내기 때문에 일반화성능이 매우 좋은 기법입니다. 이 TRPO에 대해서 알아보겠습니다.</p>\n<p>※TRPO를 매우 재밌게 설명한 Crazymuse AI의 <a href=\"https://www.youtube.com/watch?v=CKaN5PgkSBc&amp;t=90s\" target=\"_blank\" rel=\"noopener\">Youtube video</a>에서 일부 그림을 차용했습니다. 이 비디오를 시청하시는 것을 강력하게 추천합니다!</p>\n<p><br></p>\n<h2 id=\"1-1-TRPO-흐름-잡기\"><a href=\"#1-1-TRPO-흐름-잡기\" class=\"headerlink\" title=\"1.1 TRPO 흐름 잡기\"></a>1.1 TRPO 흐름 잡기</h2><p>TRPO 논문은 많은 수식이 등장하여 이 수식들을 따라가다보면 큰 그림을 놓칠 수도 있습니다. 세부적인 내용을 살펴보기 전에 기존 연구에서 출발해서 TRPO로 어떻게 발전해나가는지 간략하게 살펴보겠습니다.</p>\n<h3 id=\"1-1-1-Original-Problem\"><a href=\"#1-1-1-Original-Problem\" class=\"headerlink\" title=\"1.1.1 Original Problem\"></a>1.1.1 Original Problem</h3><p>$$\\max_\\pi \\eta(\\pi)$$</p>\n<p>모든 강화학습이 그렇듯이 expected discounted reward를 최대화하는 policy를 찾는 문제로부터 출발합니다.</p>\n<h3 id=\"1-1-2-Conservative-policy-iteration\"><a href=\"#1-1-2-Conservative-policy-iteration\" class=\"headerlink\" title=\"1.1.2 Conservative policy iteration\"></a>1.1.2 Conservative policy iteration</h3><p>$$\\max L_\\pi(\\tilde\\pi) = \\eta(\\pi) + \\sum_s \\rho_\\pi(s)\\sum_a\\tilde\\pi(a\\vert s)A_\\pi(s,a)$$</p>\n<p>$\\eta(\\pi)$를 바로 최대화하는 것은 많은 경우 어렵습니다. $\\eta(\\pi)$의 성능향상을 보장하면서 policy를 update하는 conservative policy iteration 기법이 <a href=\"http://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf\" target=\"_blank\" rel=\"noopener\">Kakade와 Langford</a>에 의하여 제안되었습니다. 이 기법을 이용하면 policy update가 성능을 향상시키는지는 못하더라도 최소한 성능을 악화시키지는 않는다는 것이 이론적으로 보장됩니다.</p>\n<h3 id=\"1-1-3-Theorem-1-of-TRPO\"><a href=\"#1-1-3-Theorem-1-of-TRPO\" class=\"headerlink\" title=\"1.1.3 Theorem 1 of TRPO\"></a>1.1.3 Theorem 1 of TRPO</h3><p>$$\\max L_{\\pi_\\mathrm{old} }\\left(\\pi_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\quad\\left(\\alpha=D_\\mathrm{TV}^\\max\\left(\\pi_\\mathrm{old},\\pi_\\mathrm{new}\\right)\\right)$$</p>\n<p>기존의 conservative policy iteration은 과거 policy와 새로운 policy를 섞어서 사용해서 실용적이지 않다는 단점이 있었는데 이것을 보완하여 온전히 새로운 policy만으로 update할 수 있는 기법을 제안합니다. </p>\n<h3 id=\"1-1-4-KL-divergence-version-of-Theorem-1\"><a href=\"#1-1-4-KL-divergence-version-of-Theorem-1\" class=\"headerlink\" title=\"1.1.4 KL divergence version of Theorem 1\"></a>1.1.4 KL divergence version of Theorem 1</h3><p>$$\\max L_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\pi,\\tilde\\pi\\right),\\quad\\left(C = \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\right)$$</p>\n<p>distance metric을 KL divergence로 바꿀 수 있습니다.</p>\n<h3 id=\"1-1-5-Using-parameterized-policy\"><a href=\"#1-1-5-Using-parameterized-policy\" class=\"headerlink\" title=\"1.1.5 Using parameterized policy\"></a>1.1.5 Using parameterized policy</h3><p>$$\\max_\\theta L_{\\theta_\\mathrm{old} }(\\theta) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)$$</p>\n<p>최적화문제를 더욱 편리하게 풀 수 있도록 낮은 dimension을 가지는 parameter들로 parameterized된 policy를 사용할 수 있습니다.</p>\n<h3 id=\"1-1-6-Trust-region-constraint\"><a href=\"#1-1-6-Trust-region-constraint\" class=\"headerlink\" title=\"1.1.6 Trust region constraint\"></a>1.1.6 Trust region constraint</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>1.1.5까지는 아직 conservative policy iteration을 약간 변형시킨 것입니다. policy를 update할 때 지나치게 많이 변하는 것을 방지하기 위하여 <a href=\"https://en.wikipedia.org/wiki/Trust_region\" target=\"_blank\" rel=\"noopener\">trust region</a>을 constraint로 설정할 수 있습니다. 이 아이디어로 인해서 TRPO라는 명칭을 가지게 됩니다.</p>\n<h3 id=\"1-1-7-Heuristic-approximation\"><a href=\"#1-1-7-Heuristic-approximation\" class=\"headerlink\" title=\"1.1.7 Heuristic approximation\"></a>1.1.7 Heuristic approximation</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>사실 1.1.6의 constraint는 모든 state에 대해서 성립해야 하기 때문에 문제를 매우 어렵게 만듭니다. 이것을 좀 더 다루기 쉽게 state distribution에 대한 평균을 취한 것으로 변형합니다.</p>\n<h3 id=\"1-1-8-Monte-Carlo-simulation\"><a href=\"#1-1-8-Monte-Carlo-simulation\" class=\"headerlink\" title=\"1.1.8 Monte Carlo simulation\"></a>1.1.8 Monte Carlo simulation</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}$$</p>\n<p>Sampling을 통한 계산이 가능하도록 식을 다시 표현할 수 있습니다. </p>\n<h3 id=\"1-1-9-Efficiently-solving-TRPO\"><a href=\"#1-1-9-Efficiently-solving-TRPO\" class=\"headerlink\" title=\"1.1.9 Efficiently solving TRPO\"></a>1.1.9 Efficiently solving TRPO</h3><p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;\\nabla_\\theta \\left. L_{\\theta_\\mathrm{old} }(\\theta)\\right\\vert _{\\theta=\\theta_\\mathrm{old} } \\left(\\theta - \\theta_\\mathrm{old}\\right) \\\\<br>\\mathrm{s.t.\\ }&amp;\\frac{1}{2}\\left(\\theta_\\mathrm{old} - \\theta \\right)^T A\\left(\\theta_\\mathrm{old}\\right)\\left(\\theta_\\mathrm{old} - \\theta \\right) \\leq \\delta \\\\<br>&amp;A_{ij} = \\frac{\\partial}{\\partial \\theta_i}\\frac{\\partial}{\\partial \\theta_j}\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right)<br>\\end{align}$$</p>\n<p>문제를 효율적으로 풀기 위하여 approximation을 적용할 수 있습니다. objective function은 first order approximation, constraint는 quadratic approximation을 취하면 효율적으로 문제를 풀 수 있는 형태로 바뀌는데 이것을 natural gradient로 풀 수도 있고 conjugate gradient로 풀 수도 있습니다.</p>\n<p>TRPO는 이렇게 다양한 방법으로 문제를 변형한 것입니다! 이제 좀 더 자세히 살펴보겠습니다.</p>\n<p><br><br></p>\n<h1 id=\"2-Preliminaries\"><a href=\"#2-Preliminaries\" class=\"headerlink\" title=\"2. Preliminaries\"></a>2. Preliminaries</h1><p>다음과 같은 파라미터를 가지는 infinite-horizon discounted Markov decision process (MDP)를 고려합니다. </p>\n<ul>\n<li>$\\mathcal{S}$: finite set of states</li>\n<li>$\\mathcal{A}$: finite set of actions</li>\n<li>$P: \\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$: transition probability distribution</li>\n<li>$r: \\mathcal{S}\\rightarrow\\mathbb{R}$: reward function</li>\n<li>$\\rho_0:\\mathcal{S}\\rightarrow\\mathbb{R}$: distribution of the initial state $s_0$</li>\n<li>$\\gamma\\in(0,1)$: discount factor</li>\n<li>$\\eta(\\pi)=E_{s_0,a_0,\\ldots}\\left[\\sum_{t=0}^\\infty\\gamma^t r\\left(s_t\\right)\\right]$, where $s_0\\sim\\rho_0\\left(s_0\\right), a_t\\sim\\pi\\left(\\left.a_t \\right\\vert s_t\\right), s_{t+1}\\sim P\\left(\\left.s_{t+1}\\right\\vert s_t,a_t\\right)$</li>\n<li>$Q_\\pi\\left(s_t,a_t\\right)=E_{ { \\color{red}{s_{t+1}} },a_{t+1},\\ldots}\\left[\\sum_{l=0}^\\infty \\gamma^l r\\left(s_{t+l}\\right)\\right]$: action value function</li>\n<li>$V_\\pi\\left(s_t\\right)=E_{ { \\color{red}{a_t} }, s_{t+1},a_{t+1},\\ldots}\\left[\\sum_{l=0}^\\infty \\gamma^l r\\left(s_{t+l}\\right)\\right]$: value function (action value function과 expectation의 첨자가 다른 부분을 유의하세요.)</li>\n<li>$A_\\pi(s,a) = Q_\\pi(s,a) - V_\\pi(s)$: advantage function</li>\n</ul>\n<p><br></p>\n<h2 id=\"2-1-Useful-identity-Kakade-amp-Langford-2002\"><a href=\"#2-1-Useful-identity-Kakade-amp-Langford-2002\" class=\"headerlink\" title=\"2.1 Useful identity [Kakade &amp; Langford 2002]\"></a>2.1 Useful identity [Kakade &amp; Langford 2002]</h2><p>우리의 목표는 $\\eta(\\pi)$가 최대화되도록 만드는 것입니다. 하지만 $\\pi$의 변화에 따라 $\\eta$가 어떻게 변하는지 알아내는 것도 쉽지 않습니다. $\\pi$는 기존의 policy이고 $\\tilde\\pi$는 새로운 policy를 나타낸다고 할 때 $\\eta$와 policy update 사이에 다음과 같은 관계가 있다는 것이 밝혀졌습니다.</p>\n<p><strong>Lemma 1.</strong> $\\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E_{s_{0},a_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right]$</p>\n<p><em>Proof.</em> $A_\\pi(s,a)=E_{s’\\sim P\\left(s’\\vert s,a\\right)}\\left[r(s)+\\gamma V_\\pi\\left(s’\\right)-V_\\pi(s)\\right]$로 다시 표현할 수 있습니다. 표기의 편의를 위하여 $\\tau:=\\left(s_0,a_0,s_1,a_1,\\ldots\\right)$를 정의하겠습니다. 다음과 같은 수식 전개가 가능합니다.<br>$$<br>\\begin{align}<br>&amp;E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right] \\\\<br> &amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t \\left(r(s_t)+\\gamma V_\\pi\\left(s_{t+1}\\right)-V_\\pi(s_t)\\right)\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\left(\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right)+\\gamma V_\\pi\\left(s_{1}\\right)-V_\\pi(s_0)+\\gamma^2 V_\\pi\\left(s_{2}\\right)-\\gamma V_\\pi(s_1)+\\gamma^3 V_\\pi\\left(s_{3}\\right)-\\gamma^2 V_\\pi(s_2)+\\cdots\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t)+\\color{red}{\\gamma V_\\pi\\left(s_{1}\\right)}-V_\\pi(s_0)+\\color{red}{\\gamma^2 V_\\pi\\left(s_{2}\\right)}-\\color{red}{\\gamma V_\\pi(s_1)}+\\cdots\\right] \\\\<br>&amp;= E_{\\tau\\vert \\tilde\\pi}\\left[-V_\\pi(s_0)+\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right] \\\\<br>&amp;\\overset{\\underset{\\mathrm{(a)} }{} }{=} -E_{\\color{red}{s_0}}\\left[V_\\pi(s_0)\\right]+E_{\\tau\\vert \\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t)\\right] \\\\<br>&amp;=-\\eta(\\pi) + \\eta\\left(\\tilde\\pi\\right)\\\\<br>&amp;\\quad\\therefore \\eta\\left(\\tilde\\pi\\right) = \\eta(\\pi) + E_{s_{0},a_{0},\\ldots\\sim\\tilde\\pi}\\left[\\sum_{t=0}^\\infty\\gamma^t A_\\pi\\left(s_t,a_t\\right)\\right]<br>\\end{align}<br>$$<br>위의 전개의 (a) 부분은 $V_\\pi\\left(s_0\\right)=E_{a_0,a_1,s_1,\\ldots}\\left[\\sum_{t=0}^\\infty\\gamma^l r\\left(s_l\\right)\\right]$이므로 $\\tau$ 중 많은 부분들이 이미 expectation이 취해진 값이므로 무시됩니다. 오직 $s_0$만 expectation이 취해지지 않았기 때문에 남아있습니다.</p>\n<p><strong>Lemma 1</strong>은 새로운 policy와 기존의 policy 사이의 관계를 규정합니다. 다음과 같은 식을 정의하고 이것을 이용해서 <strong>Lemma 1</strong>을 변형시켜 봅시다.</p>\n<ul>\n<li>$\\rho_\\pi(s) = P\\left(s_0=s\\right) + \\gamma P\\left(s_1=s\\right)\\ + \\gamma^2 P\\left(s_2=s\\right) + \\cdots $: (unnormalized) discounted visitation frequencies</li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\eta\\left(\\tilde\\pi\\right) &amp;= \\eta(\\pi) + \\sum_{t=0}^\\infty\\sum_{s}P\\left(\\left.s_t=s\\right\\vert \\tilde\\pi\\right)\\sum_a\\tilde\\pi(a\\vert s)\\gamma^t A_\\pi(s,a) \\\\<br>&amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}{\\sum_{t=0}^\\infty \\gamma^t P\\left(\\left.s_t=s\\right\\vert \\tilde\\pi\\right)} } \\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\\\<br>&amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}{\\rho_\\tilde\\pi(s)} }\\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\\\<br>\\end{align}<br>$$</p>\n<p>이 수식의 의미가 무엇일까요? 만약 $\\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a) \\geq 0$이라면 $\\eta(\\tilde\\pi)$는 항상 $\\eta(\\pi)$보다 큽니다. 즉, policy를 업데이트함으로써 항상 개선됩니다. 즉, 이 수식을 통해서 항상 더 좋은 성능을 내는 policy update가 가져야 할 특징을 알 수 있습니다. 다음과 같은 deterministic policy가 있다고 합시다.</p>\n<p>$$<br>\\tilde\\pi(s) = \\arg\\max_a A_\\pi(s,a)<br>$$</p>\n<p>이 policy는 적어도 하나의 state-action pair에서 0보다 큰 값을 가지는 advantage가 있고 그 때의 확률이 0이 아니라면 항상 성능을 개선시킵니다. 어려운 점은 policy를 바꾸면 $\\rho$도 바뀐다는 점입니다. </p>\n<p>다음 그림을 봅시다. Starting Point에서 여러가지 경로를 거쳐서 Destination으로 갈 수 있습니다. 다른 policy를 이용하는 것은 다른 경로를 이용한 것입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=16s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/policy_change.png\" alt=\"policy_change\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>이 때 다른 policy를 이용함에 따라 state visitation frequency도 변하게 됩니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=36s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/state_visitation_change.png\" alt=\"state_visitation_change\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>이로 인해서 policy를 최적화하는 것은 어려워집니다. 그래서 이러한 변화를 무시하는 다음과 같은 approximation을 취합니다.</p>\n<p>$$<br>\\begin{align}<br>L_\\pi\\left(\\tilde\\pi\\right) &amp;= \\eta(\\pi) + \\sum_{s}{\\color{red}{\\rho_\\pi(s)} } \\sum_a\\tilde\\pi(a\\vert s) A_\\pi(s,a)<br>\\end{align}<br>$$</p>\n<p>policy가 바뀌었음에도 이전의 state distribution을 계속 이용하는 것입니다. 이것은 policy의 변화가 크지 않다면 어느 정도 허용될 수 있을 것입니다. 그렇지만 얼마나 많은 변화가 허용될까요? 이것을 정하기 위해서 이용하는 것이 trust region입니다.</p>\n<p><br></p>\n<h2 id=\"2-2-Conservative-Policy-Iteration\"><a href=\"#2-2-Conservative-Policy-Iteration\" class=\"headerlink\" title=\"2.2 Conservative Policy Iteration\"></a>2.2 Conservative Policy Iteration</h2><p>policy의 변화를 다루기 용이하게 하기 위해서 policy를 다음과 같이 파라미터를 이용해서 표현합시다.</p>\n<ul>\n<li>$\\pi_\\theta(a\\vert s)$: parameterized policy</li>\n</ul>\n<p>$\\pi_\\theta$는 $\\theta$에 대하여 미분가능한 함수입니다. $L_\\pi\\left(\\tilde\\pi\\right)$을 $\\theta_0$에서 $\\eta(\\pi)$에 대한 first order approximation이라고 하면 다음 식이 성립합니다.</p>\n<p>$$<br>\\begin{align}<br>L_{\\pi_{\\theta_0} }\\left(\\pi_{\\theta_0}\\right) &amp;= \\eta\\left(\\pi_{\\theta_0}\\right) \\\\<br>\\nabla_\\theta \\left.L_{\\pi_{\\theta_0} }\\left(\\pi_{\\theta_0}\\right)\\right\\vert _{\\theta=\\theta_0} &amp;= \\nabla_\\theta\\left.\\eta(\\pi_{\\theta_0})\\right\\vert _{\\theta=\\theta_0}<br>\\end{align}<br>$$</p>\n<p>이것의 의미는 $\\pi_{\\theta_0}$가 매우 작게 변한다면 $L_{\\pi_{\\theta_0} }$를 개선시키는 것이 $\\eta$를 개선시키는 것이라는 것입니다. 그러나 지금까지의 설명만으로는 $\\pi_{\\theta_0}$를 얼마나 작게 변화시켜야 할지에 대해서는 알 수 없습니다.</p>\n<p>Kakade &amp; Langford가 2002년 발표한 논문에서도 이것에 대해서 고민했습니다. 그 논문에서 <em>conservative policy iteration</em>이라는 기법을 제안합니다. 그 논문의 contribution은 다음과 같습니다.</p>\n<ul>\n<li>$\\eta$ 개선의 lower bound를 제공</li>\n<li>기존의 policy를 $\\pi_\\mathrm{old}$라고 하고 $\\pi’$를 $\\pi’=\\arg\\max_{\\pi’}L_{\\pi_\\mathrm{old} }\\left(\\pi’\\right)$과 같이 정의할 때, 새로운 mixture policy $\\pi_\\mathrm{new}$를 다음과 같이 제안<br>$$<br>\\pi_\\mathrm{new}(a\\vert s) = (1-\\alpha)\\pi_\\mathrm{old}(a\\vert s) + \\alpha \\pi’(a\\vert s)<br>$$</li>\n</ul>\n<p>그림으로 표현하면 다음과 같습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=2m46s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/mixure_policy.png\" alt=\"mixure_policy\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<ul>\n<li>다음과 같은 lower bound를 정의<br>$$<br>\\eta\\left(\\pi_\\mathrm{new}\\right) \\geq L_{\\pi_{\\theta_\\mathrm{old} }}\\left(\\pi_\\mathrm{new}\\right) - \\frac{2\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2,\\\\<br>\\mathrm{where\\ }\\epsilon = \\max_s\\left\\vert E_{a\\sim\\pi’(a\\vert s)}\\left[A_\\pi(s,a)\\right]\\right\\vert<br>$$</li>\n</ul>\n<p>하지만 mixture된 policy라는 것은 실용적이지 않습니다. </p>\n<p><br><br></p>\n<h1 id=\"3-Monotonic-Improvement-Guarantee-for-General-Stochastic-Policies\"><a href=\"#3-Monotonic-Improvement-Guarantee-for-General-Stochastic-Policies\" class=\"headerlink\" title=\"3. Monotonic Improvement Guarantee for General Stochastic Policies\"></a>3. Monotonic Improvement Guarantee for General Stochastic Policies</h1><p>이전 장에서 설명한 lower bound는 오직 mixture policy에 대해서만 성립하고 더 많이 사용되는 stochastic policy에는 적용되지 않습니다. 따라서 stochastic policy를 이용할 수 있도록 개선할 필요가 있습니다. 아래와 같이 기존 수식에서 두 가지를 바꿈으로써 이것이 가능합니다.</p>\n<ul>\n<li>$\\alpha\\rightarrow$ distance measure between $\\pi$ and $\\tilde\\pi$</li>\n<li>constant $\\epsilon\\rightarrow\\max_{s,a}\\left\\vert A_\\pi(s,a)\\right\\vert $</li>\n</ul>\n<p>여기서 distance measure로 total variation divergence를 이용합니다. discrete porbability distribution $p$와 $q$에 대하여 다음과 같이 정의됩니다.<br>$$<br>D_\\mathrm{TV}(p\\parallel q) = \\frac{1}{2}\\sum_i\\left\\vert p_i - q_i\\right\\vert<br>$$</p>\n<p>이것을 이용하여 $D_\\mathrm{TV}^\\max$를 다음과 같이 정의합니다.<br>$$<br>D_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D_\\mathrm{TV}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)<br>$$</p>\n<p>그림으로 표현하면 다음과 같습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=3m15s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/tvd.png\" alt=\"tvd\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>이것을 이용하여 다음과 같은 관계식을 얻을 수 있습니다.</p>\n<p><strong>Theorem 1.</strong> Let $\\alpha=D_\\mathrm{TV}^\\max(\\pi\\parallel \\tilde\\pi)$. Then the following bound holds:<br>$$<br>\\begin{align}<br>\\eta\\left(\\pi_\\mathrm{new}\\right) &amp;\\geq L_{\\pi_\\mathrm{old} }\\left(\\pi_\\mathrm{new}\\right) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2, \\\\<br>\\mathrm{where\\ } \\epsilon&amp;= \\max_{s,a}\\left\\vert A_\\pi(s,a)\\right\\vert<br>\\end{align}<br>$$<br><!--*Proof.* TBD.--></p>\n<p>또다른 distance metric으로 아래 그림과 같은 KL divergence가 있습니다.<br>(그런데 왜 하필 KL divergence로 바꿀까요? 논문의 뒤쪽에서 계산효율을 위해서 conjugate gradient method를 이용하는데 이를 위해서 바꾼게 아닐까 싶습니다. Wasserstein distance 같은 다른 방향으로 발전시킬 수도 있을 것 같습니다. Schulmann이 아마 해봤겠죠?^^a)</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=3m34s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/kld.png\" alt=\"kld\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>total variation divergence와 KL divergence 사이에는 다음과 같은 관계가 있습니다.<br>$$<br>D_\\mathrm{TV}(p\\parallel q)^2 \\leq D_\\mathrm{KL}(p\\parallel q)<br>$$</p>\n<p>다음 수식을 정의합니다.<br>$$<br>D_\\mathrm{KL}^\\max(\\pi\\parallel \\tilde\\pi) = \\max_s D_\\mathrm{KL}\\left(\\pi(\\cdot\\vert s)\\parallel\\tilde\\pi(\\cdot\\vert s)\\right)<br>$$</p>\n<p><strong>Theorem 1</strong>을 이용하여 다음과 같은 수식이 성립함을 알 수 있습니다.<br>$$<br>\\begin{align}<br>\\eta\\left(\\tilde\\pi\\right) &amp;\\geq L_{\\pi}\\left(\\tilde\\pi\\right) - C\\cdot D_\\mathrm{KL}^\\max(\\pi, \\tilde\\pi), \\\\<br>\\mathrm{where\\ } C&amp;= \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}<br>\\end{align}<br>$$</p>\n<p>이러한 policy imporvement bound를 기반으로 다음과 같은 approximate policy iteration 알고리듬을 고안해낼 수 있습니다.</p>\n<p><strong>Algorithm 1</strong> Policy iteration algorithm guaranteeing non-decreasing expected return $\\eta$</p>\n<blockquote>\n<p>Initialize $\\pi_0$<br><strong>for</strong> $i=0,1,2,\\ldots$ until convergence <strong>do</strong><br>$\\quad$Compute all advantage values $A_{\\pi_i}(s,a)$<br>$\\quad$Solve the contstrained optimization problem<br>$\\quad\\pi_{i+1}=\\arg\\max_\\pi\\left[L_{\\pi_i} - CD_\\mathrm{KL}^\\max\\left(\\pi_i,\\pi\\right)\\right]$<br>$\\quad\\quad$where $C = 4\\epsilon\\gamma / (1-\\gamma)^2$<br>$\\quad\\quad$and $L_{\\pi_i}\\left(\\pi\\right) = \\eta(\\pi_i) + \\sum_{s}\\rho_{\\pi_i}(s)\\sum_a\\pi(a\\vert s) A_{\\pi_i}(s,a)$<br><strong>end for</strong></p>\n</blockquote>\n<p><strong>Algorithm 1</strong>은 advantage를 정확하게 계산할 수 있다고 가정하고 있습니다. 이 알고리듬은  monotonical한 성능 증가($\\eta(\\pi_0)\\leq\\eta(\\pi_1)\\leq\\cdots$)를 한다는 것을 다음과 같이 보일 수 있습니다. $M_i(\\pi)=L_{\\pi_i}(\\pi) - CD_\\mathrm{KL}^\\max\\left(\\pi_i,\\pi\\right)$라고 합시다.</p>\n<p>$$<br>\\begin{align}<br>\\eta \\left(\\pi_{i+1}\\right) &amp;\\geq M_i\\left(\\pi_{i+1}\\right)\\\\<br>\\eta \\left(\\pi_{i}\\right) &amp;= M_i\\left(\\pi_{i}\\right) \\\\<br>\\eta \\left(\\pi_{i+1}\\right) - \\eta \\left(\\pi_{i}\\right) &amp;\\geq M_i\\left(\\pi_{i+1}\\right) - M_i\\left(\\pi_{i}\\right)<br>\\end{align}<br>$$</p>\n<p>위 수식과 같이 매 iteration 마다 $M_i$를 최대화함으로써, $\\eta$가 감소하지 않는다는 것을 보장할 수 있습니다. 이와 같은 타입의 알고리듬을 <a href=\"https://www.jstor.org/stable/pdf/27643496.pdf?casa_token=0qHamcl60WoAAAAA:-fkZ9JcA_nrY3-zbCUpqvPOgcAMgw7Gr96MajCZg2byHf8m5GU1KTSxyJJcBy1lPZbBTZVCjHHUXilh4k-iuwF91Wka4B5qdltC1IR2qMWk8q1FoV6__\" target=\"_blank\" rel=\"noopener\">minorization-maximization (MM) algorithm</a>이라고 합니다. EM 알고리듬도 이 타입의 알고리듬 중 하나입니다.</p>\n<p><strong>Algorithm 1</strong>은 아래 그림과 같이 동작합니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=3m52s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/surrogate.png\" alt=\"surrogate\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>$M_i$는 $\\pi_i$일 때 equality가 되는 $\\eta$에 대한  surrogate function입니다. TRPO는 이 surrogate function을 최대화하고 KL divergence를 penalty가 아닌 constraint로 두는 알고리듬입니다.</p>\n<p><br><br></p>\n<h1 id=\"4-Optimization-of-Parameterized-Policies\"><a href=\"#4-Optimization-of-Parameterized-Policies\" class=\"headerlink\" title=\"4. Optimization of Parameterized Policies\"></a>4. Optimization of Parameterized Policies</h1><p>표기의 편의를 위해 다음과 같이 notation들을 더 간략하게 정의합니다.</p>\n<ul>\n<li>$\\eta(\\theta):=\\eta\\left(\\pi_\\theta\\right)$</li>\n<li>$L_\\theta\\left(\\tilde\\theta\\right):=L_{\\pi_\\theta}\\left(\\pi_{\\tilde\\theta}\\right)$</li>\n<li>$D_\\mathrm{KL}\\left(\\theta\\parallel\\tilde\\theta\\right):=D_\\mathrm{KL}\\left(\\pi_\\theta\\parallel\\pi_{\\tilde\\theta}\\right)$</li>\n<li>$\\theta_\\mathrm{old}$: previous policy parameter</li>\n</ul>\n<p><br></p>\n<h2 id=\"4-1-Trust-Region-Policy-Optimization\"><a href=\"#4-1-Trust-Region-Policy-Optimization\" class=\"headerlink\" title=\"4.1 Trust Region Policy Optimization\"></a>4.1 Trust Region Policy Optimization</h2><p>이전 장의 중요 결과를 위의 notation으로 다시 표기하면 아래와 같습니다.</p>\n<p>$$<br>\\eta\\left(\\theta\\right) \\geq L_{\\theta_\\mathrm{old} }\\left(\\theta\\right) - C\\cdot D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)<br>$$</p>\n<p>$\\eta$의 성능 향상을 보장하기 위해서 그것의 lower bound를 최대화할 수 있습니다.</p>\n<p>$$<br>\\max_\\theta \\left[L_{\\theta_\\mathrm{old} }\\left(\\theta\\right) - C D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right)\\right]<br>$$</p>\n<p>이 최적화 문제는 step size를 매우 작게 해야 올바른 동작을 합니다. 위에서 살펴봤듯이 first order approximation이기 때문입니다. 좀 더 큰 step size를 가질 수 있도록 이 최적화 문제를 trust region constraint를 도입하여 다음과 같이 바꿉니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;D_\\mathrm{KL}^\\max\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>이 최적화문제의 constraint는 모든 state space에 대해서 성립해야 합니다. 또한 maximum값을 매번 찾아야 합니다. state가 많은 경우 constraint의 수가 매우 많아져서 문제를 풀기 어렵게 만듭니다. constraint의 수를 줄이기 위하여 다음과 같은 avergae KL divergence를 이용하는 heuristic approximation을 취합니다. 이것이 최선의 방법은 아닐 수 있지만 실용적인 방법입니다.</p>\n<p>$$<br>\\overline D_\\mathrm{KL}^{\\rho}\\left(\\theta_1, \\theta_2\\right):=E_{s\\sim\\rho}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_1}(\\cdot\\vert s)\\parallel\\pi_{\\theta_2}(\\cdot\\vert s)\\right)\\right]<br>$$</p>\n<p>이것을 기반으로 다음과 같은 최적화 문제를 풀 수 있습니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;L_{\\theta_\\mathrm{old} }(\\theta) \\\\<br>\\mathrm{s.t.\\ }&amp;\\overline{D}_\\mathrm{KL}^{\\rho_\\mathrm{old} }\\left(\\theta_\\mathrm{old}, \\theta\\right) \\leq \\delta<br>\\end{align}$$</p>\n<p>아래 그림처럼 step size에 대해서 고려할 수 있습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=4m35s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/heuristic_approx.png\" alt=\"heuristic_approx\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p><br><br></p>\n<h1 id=\"5-Sample-Based-Estimation-of-the-Objective-and-Constraint\"><a href=\"#5-Sample-Based-Estimation-of-the-Objective-and-Constraint\" class=\"headerlink\" title=\"5. Sample-Based Estimation of the Objective and Constraint\"></a>5. Sample-Based Estimation of the Objective and Constraint</h1><p>실용적인 알고리듬을 만들려고 하는 노력은 아직 끝나지 않았습니다. 이제 앞의 알고리듬을 sample-based estimation 즉, Monte Carlo estimation을 할 수 있도록 바꿔보겠습니다. sampling을 편하게 할 수 있도록 아래와 같이 바꿔줍니다.</p>\n<ul>\n<li>$\\sum_s \\rho_{\\theta_\\mathrm{old} }(s)[\\ldots]\\rightarrow\\frac{1}{1-\\gamma}E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}[\\ldots]$</li>\n<li>$A_{\\theta_\\mathrm{old} }\\rightarrow Q_{\\theta_\\mathrm{old} }$</li>\n<li>$\\sum_a\\pi_{\\theta_\\mathrm{old} }(a\\vert s)A_{\\theta_\\mathrm{old} } \\rightarrow E_{a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\mathrm{old} }\\right]$</li>\n</ul>\n<p>이러한 변화를 그림처럼 도식화할 수 있습니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=5m31s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/sample-based.png\" alt=\"sample-based\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>한 가지 짚고 넘어가야 할 점은 action sampling을 할 때 importance sampling을 사용한다는 것입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=5m53s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/importance_sampling.png\" alt=\"importance_sampling\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>바뀐 최적화문제는 아래와 같습니다.</p>\n<ul>\n<li><strong>Equation (14).</strong></li>\n</ul>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}<br>$$</p>\n<p>이 때 sampling하는 두 가지 방법이 있습니다.</p>\n<p><br></p>\n<h2 id=\"5-1-Single-Path\"><a href=\"#5-1-Single-Path\" class=\"headerlink\" title=\"5.1 Single Path\"></a>5.1 Single Path</h2><p><em>single path</em>는 개별 trajectory들을 이용하는 방법입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=6m15s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/single.png\" alt=\"single\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p><br></p>\n<h2 id=\"5-2-Vine\"><a href=\"#5-2-Vine\" class=\"headerlink\" title=\"5.2 Vine\"></a>5.2 Vine</h2><p><em>vine</em>은 한 state에서 rollout을 이용하여 여러 action을 수행하는 방법입니다.</p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=6m32s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/vine1.png\" alt=\"vine1\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p><a href=\"https://youtu.be/CKaN5PgkSBc?t=6m49s\" target=\"_blank\" rel=\"noopener\"><img src=\"../../../../img/vine2.png\" alt=\"vine2\" title=\"https://youtu.be/CKaN5PgkSBc\"></a></p>\n<p>estimation의 variance를 낮출 수 있지만 계산량이 많고 한 state에서 여러 action을 수행할 수 있어야 하기 때문에 현실적인 문제에 적용하기에는 어려움이 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"6-Practical-Algorithm\"><a href=\"#6-Practical-Algorithm\" class=\"headerlink\" title=\"6. Practical Algorithm\"></a>6. Practical Algorithm</h1><p>앞서 single-path, vine 샘플링을 사용하는 두가지 방식의 policy optimization 알고리즘을 살펴봤습니다. 실용적인 알고리듬은 아래의 과정을 반복해서 수행합니다.</p>\n<p>1) Q-values의 몬테카를로 추정을 통해 state-action 쌍 집합을 single path 또는 vine 과정을 통해 수집함.</p>\n<p>2) 샘플 평균으로, (14)식의 목적함수와 제약식을 추정함</p>\n<ul>\n<li><strong>Equation (14).</strong><br>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}<br>$$</li>\n</ul>\n<p>3) policy parameter vector인 $\\theta$를 업데이트 하면서 제약조건이 있는 최적화 문제를 근사적으로 풂. 본 논문에서는 gradient를 직접 계산하는 것보다는 약간 더 계산량이 있는 line search와 conjugate gradient algorithm을 사용했습니다.</p>\n<p>3)에 대해서,  gradient의 covariance matrix를 사용하지 않고 KL divergence의 Hessian을 해석적으로 계산하여 Fisher Information Matrix를 구성했습니다.</p>\n<p>다시말해서</p>\n<p>$\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial}{\\partial \\theta_i} \\log \\pi_{\\theta}(a_n\\vert s_n) \\frac{\\partial}{\\partial \\theta_j} \\log \\pi_{\\theta}(a_n\\vert s_n)$ 대신 $\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} D_{KL}(\\pi_{\\theta_{old} }(\\cdot \\vert  s_n) \\vert \\vert  \\pi_{\\theta}(\\cdot \\vert  s_n))$를 계산한 것입니다.</p>\n<p>이 analytic estimator는 Hessian이나 trajectories의 모든 gradient를 저장하지 않아도 되기 때문에 대규모 환경을 고려할 경우 계산 상 이점이 있습니다. 좀 더 자세하게 설명한 Appendix C 부분을 살펴보겠습니다.</p>\n<p><br></p>\n<h2 id=\"Appendix-C-Efficiently-Solving-the-Trust-Region-Constrained-Optimization-Problem\"><a href=\"#Appendix-C-Efficiently-Solving-the-Trust-Region-Constrained-Optimization-Problem\" class=\"headerlink\" title=\"Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem\"></a>Appendix C. Efficiently Solving the Trust-Region Constrained Optimization Problem</h2><p>우리가 풀고자 하는 식은 다음과 같습니다.</p>\n<p>$$<br>\\begin{align}<br>\\max\\quad &amp;L(\\theta) \\\\<br>\\mathrm{s.t.\\ } &amp;\\overline{D}_{KL}(\\theta_\\mathrm{old}, \\theta) \\leq \\delta<br>\\end{align}<br>$$</p>\n<p>이 최적화 문제를 풀기 위하여 우리는 gradient 기법을 이용할 것입니다. gradient 기법은 결국 update할 방향과 크기를 결정하는 문제로 생각할 수 있습니다. 이에 따라 아래와 같은 두가지 과정으로 진행합니다.</p>\n<p>1) 탐색 방향을 계산, 목적함수의 first-order approximation(선형근사)와 제약식의 quadratic approximation(2차 근사).</p>\n<p>2) 이동거리 계산을 위해 해당 방향으로 line search 수행.</p>\n<p>탐색 방향은 $Ax = g$ 수식을 근사적으로 풀어서 구합니다. 여기서 $A$ 는 Fisher Information Matrix이고, 이것은 KL divergence 제약식의 2차 근사 $\\overline{D}_\\mathrm{KL} (\\theta_\\mathrm{old}, \\theta) \\approx \\frac{1}{2}(\\theta - \\theta_\\mathrm{old})^T A(\\theta - \\theta_\\mathrm{old})$를 푸는 것입니다. 여기서 $A_{ij} = \\frac{\\partial}{\\partial \\theta_i} \\frac{\\partial}{\\partial \\theta_j} \\overline{D}_\\mathrm{KL}(\\theta_\\mathrm{old}, \\theta)$입니다.</p>\n<p>논문에서 이 부분에 대한 설명이 약간 부족해서 좀 더 내용을 추가하자면, $\\frac{1}{2}x^T A x - x^T g + b = 0$과 같은 quadratic equation의 해는 1차 derivative를 0으로 만드는 $x$를 찾음으로써 알 수 있습니다. 위에서 우리는 목적함수를 1차 근사시키고 제약조건은 2차 근사를 시킴으로써 quadratic equation 형태로 변형했기 때문에 같은 방식으로 풀 수 있습니다. 따라서 $g$는 1차근사시킨 목적함수의 계수가 될 것입니다.</p>\n<p>대규모 문제에서 $A$ 혹은 $A^{-1}$ 을 계산하는 것은 엄청난 계산량이 필요합니다. 하지만 conjugate gradient algorithm을 잘 이용하면 전체 $A$행렬(FIM)을 계산하지 않아도 $Ax=g$ 를 근사적으로 해결할 수 있게 해줍니다.</p>\n<p>탐색 방향 $s \\approx A^{-1}g$과 더불어 최대스텝길이(step size) $\\beta$도 계산할 필요가 있습니다.</p>\n<p>즉, $\\delta = \\overline{D}_\\mathrm{KL} \\approx \\frac{1}{2}(\\beta s)^T A(\\beta s) = \\frac{1}{2}\\beta^2s^TAs$ 이고, </p>\n<p>$\\beta = \\sqrt{2\\delta / s^T As}$가 됩니다. 이때 $\\delta$는 KL divergence의 boundary term 입니다.</p>\n<p>$s^T As$ 항은 Hessian-vector product로 계산할 수 있고, conjugate gradient 과정에서 계산됩니다.</p>\n<p>마지막으로, surrogate objective와 KL divergence 제약식을 위해 다음과 같은 함수를 정의하고 line search를 사용합니다.</p>\n<p>$$<br>L_{\\theta_\\mathrm{old} }(\\theta) - \\chi [\\overline{D}_{KL}(\\theta_\\mathrm{old}, \\theta) \\leq \\delta]<br>$$</p>\n<p>$\\chi[\\cdot]$ 함수는 []안의 조건이 맞으면 0이고 틀리면 무한으로 발산하는 함수입니다.</p>\n<p>위에서 계산한 $\\beta$ 의 최대 값부터 시작해서 목적함수가 개선될때까지 exponential하게 줄여 나갑니다. line search가 없었다면 이 알고리즘은 매우 큰 스텝으로 계산될 것이고 심각한 성능 저하가 발생할 것입니다.</p>\n<p>(여기서 살짝 예고를 하자면, 나중에 이 KL 기반 기법을 개선한 더 단순한 방법이 제안됩니다. PPO라고…)</p>\n<p><br><br></p>\n<h1 id=\"7-Connections-with-Prior-Work\"><a href=\"#7-Connections-with-Prior-Work\" class=\"headerlink\" title=\"7. Connections with Prior Work\"></a>7. Connections with Prior Work</h1><p>Natural Policy Gradient는 $L$의 선형 근사와 $\\overline D_\\mathrm{KL}$ 제약식을 2차근사 하는 아래의 식의 special case입니다.</p>\n<ul>\n<li>Equation (17).</li>\n</ul>\n<p>$L_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A_\\pi(s,a)$</p>\n<p>$\\underset{\\theta}{\\max}\\quad [\\nabla_{\\theta}L_{\\theta_\\mathrm{old} }(\\theta) \\ \\rvert_{\\theta=\\theta_\\mathrm{old} } \\cdot (\\theta - \\theta_\\mathrm{old})]$</p>\n<p>$\\mathrm{s.t.}\\quad\\frac{1}{2}(\\theta_\\mathrm{old} - \\theta)^{T} A(\\theta_\\mathrm{old})(\\theta_\\mathrm{old}-\\theta) \\leq \\delta$</p>\n<p>$\\mathrm{where\\ }A(\\theta_{old})_{ij} = $</p>\n<p>$\\Large \\frac{\\partial}{\\partial \\theta_{i} } \\frac{\\partial}{\\partial \\theta_{j} }<br>E_{s \\sim \\rho_{\\pi} }[D_\\mathrm{KL}(\\pi(\\cdot\\rvert s, \\theta_\\mathrm{old})  \\rvert\\rvert \\pi(\\cdot \\rvert s, \\theta))] \\rvert_{\\theta=\\theta_\\mathrm{old} }$</p>\n<p>업데이트 식은 다음과 같습니다.</p>\n<p>$\\theta_\\mathrm{new} = \\theta_\\mathrm{old} +  \\color{Red}{\\frac{1}{\\lambda} } A(\\theta_\\mathrm{old})^{-1}\\nabla_{\\theta}L(\\theta) \\rvert_{\\theta=\\theta_\\mathrm{old} }$</p>\n<p>여기서 step size인 $\\frac{1}{\\lambda}$ 일반적으로 알고리즘의 파라미터로 취급되지만 TRPO는 각 업데이트 마다 제약식으로 사용한다는 점이 다릅니다. 사소한 차이처럼 보이지만, 큰 차원을 다루는 실험에서 성능을 크게 향상시켰습니다.</p>\n<p>또한 $L^2$ 제약식(혹은 페널티) 를 사용하면 표준 Policy Gradient 업데이트 식을 얻었습니다.</p>\n<ul>\n<li>Equation (18).</li>\n</ul>\n<p>$\\underset{\\theta}{\\max}\\quad[\\nabla_{\\theta} L_{\\theta_\\mathrm{old} }(\\theta) \\rvert_{\\theta=\\theta_\\mathrm{old} } \\cdot (\\theta - \\theta_\\mathrm{old})]$</p>\n<p>$\\mathrm{s.t.}\\quad \\frac{1}{2} \\vert\\vert \\theta - \\theta_\\mathrm{old} \\vert\\vert^2 \\leq \\delta$</p>\n<p>$L_{\\pi}(\\tilde\\pi) = \\eta(\\pi) + \\underset{s}{\\sum}\\rho_{\\pi}(s) \\underset{a}{\\sum}\\tilde\\pi(a\\vert s)A_\\pi(s,a) $를 이용해서 제약조건없이 $\\underset{\\pi}{\\max}\\quad L_{\\pi_\\mathrm{old} }(\\pi)$를 풀면<br>Policy Iteration update를 하는 것과 같습니다.</p>\n<p><br><br></p>\n<h1 id=\"8-Experiments\"><a href=\"#8-Experiments\" class=\"headerlink\" title=\"8. Experiments\"></a>8. Experiments</h1><ul>\n<li><p>다음과 같은 3 가지 궁금증을 해결하기 위하여 실험을 설계하였습니다.</p>\n<ol>\n<li><p>Single-path 와 vine 의 성능면에서 특징을 알고싶음.</p>\n</li>\n<li><p>Fixed penalty coefficient(NPG) 보다 fixed KL divergence를 사용하는 것(TRPO)으로<br>변경한 것이 얼마나 뚜렷한 차이를 보일지, 성능 면에서 어떤 영향을 미치는지 알고싶음.</p>\n</li>\n<li><p>TRPO가 큰 스케일의 문제를 해결할 수 있는지, 기존에 연구/적용되어왔던 방법들과<br>성능, 계산시간, 샘플 복잡도 면에서 비교하고 싶음.</p>\n</li>\n</ol>\n</li>\n</ul>\n<ul>\n<li><p>1, 2의 궁금증에 대해서 실험 환경에 single path, vine을 비롯한 여러 사전방법들을 함께 실험하였습니다.</p>\n</li>\n<li><p>3에 대해선, robotic locomotion과 Atari game에 TRPO를 실험하였습니다.</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"8-1-Simulated-Robotic-Locomotion\"><a href=\"#8-1-Simulated-Robotic-Locomotion\" class=\"headerlink\" title=\"8.1 Simulated Robotic Locomotion\"></a>8.1 Simulated Robotic Locomotion</h2><ul>\n<li>MuJoCo 시뮬레이터를 활용해 로봇 locomotion 실험 진행하였습니다.</li>\n<li>Robot State: positions and velocities, joint torques</li>\n</ul>\n<p><img src=\"https://i.imgur.com/bs5ATS3.png\" alt=\"\"></p>\n<ul>\n<li><p>수영, 점프, 걷기 행동을 학습시키고자 합니다.</p>\n<ul>\n<li><p>수영: 10 dimensional state space , reward: $r(x,u)=v_x - 10^{-5}\\vert \\vert u\\vert \\vert ^2$, quadratic penalty.</p>\n</li>\n<li><p>점프: 12 dimensional state space, episode의 끝은 높이와 각도 threshol로 점프를 판단, non-terminal state에 대해 +1 보상 추가.</p>\n</li>\n<li><p>걷기: 18 dimensional state space, 뜀뛰는 듯한 보법(점프에이전트)보다 부드러운 보법을 유도하기위해 페널티를 추가함.</p>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Detailed Experiment Setup and used parameters, used network model</strong><br><img src=\"https://i.imgur.com/zgnsbw6.png\" alt=\"\"></p>\n<p><img src=\"https://i.imgur.com/FqdWC53.png\" alt=\"\"></p>\n<p>equation (12) : $\\max\\quad L_{\\theta_\\mathrm{old}(\\theta)}, \\ \\ \\mathrm{s.t.\\ }\\overline{D}_\\mathrm{KL}^{\\rho_{\\theta_\\mathrm{old} }}(\\theta_\\mathrm{old}, \\theta) \\leq \\delta$</p>\n<p>이 실험에서 $\\delta = 0.01$입니다.</p>\n<ul>\n<li><p>비교에 사용된 모델들</p>\n<ul>\n<li>TRPO Single-path</li>\n<li>TRPO vine</li>\n<li>CEM(cross-entropy method)</li>\n<li>CMA(covariance matrix adaptation)</li>\n<li>NPG (Lagrange Multiplier penalty coefficien를 사용하는 것이 차이점)</li>\n<li>Empirical FIM</li>\n<li>maximum KL(cartpole)</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://i.imgur.com/wSqolvS.png\" alt=\"\"></p>\n<ul>\n<li><p>제안한 single path, vine 모두 task를 잘 학습하는 것을 확인할 수 있습니다.</p>\n</li>\n<li><p>페널티를 고정하는 것보다, KL divergenc를 제약하는 것이 step size를 선택하는 부분에서 Robust합니다.</p>\n</li>\n<li><p>CEM/CMA는 derivative free 알고리즘으로, 샘플 복잡도가 네트워크 파라미터수 만큼 확장되어 high dimension 문제에서는 속도와 성능이 약한 모습을 보입니다.</p>\n</li>\n<li><p>maxKL은 제안방법보다 느린 학습성능을 보입니다.</p>\n</li>\n</ul>\n<p>TRPO video link: <a href=\"https://www.youtube.com/watch?v=jeid0wIrSn4\" target=\"_blank\" rel=\"noopener\">https://www.youtube.com/watch?v=jeid0wIrSn4</a></p>\n<ul>\n<li><p>사전지식이 적은 상태에서 여러 액션을 학습하는 실험으로 TRPO의 성능을 보입니다.</p>\n</li>\n<li><p>지금까지의 균형이나 걸음 같은 개념을 명시적으로 인코딩한 robotic locomotion에서의 사전 연구들과 차이를 보였습니다.</p>\n</li>\n</ul>\n<p><br></p>\n<h2 id=\"8-2-Playing-Games-from-Images\"><a href=\"#8-2-Playing-Games-from-Images\" class=\"headerlink\" title=\"8.2 Playing Games from Images\"></a>8.2 Playing Games from Images</h2><ul>\n<li>부분관찰가능하며 복잡한 observation인 환경에서 TRPO를 평가하기위해 raw image를 입력으로 하는 아타리게임 환경에서 실험하였습니다.</li>\n</ul>\n<ul>\n<li>Challenging point :<ul>\n<li>High dimensional space</li>\n<li>delayed reward</li>\n<li>non-stationary image (Enduro는 배경 이미지가 바뀌고, 반전처리가 됨)</li>\n<li>complex action sequence (Q*bert는 21개의 다른 발판에서 점프 해야함)</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>ALE(Arcade Learning Environment) 에서 테스트</li>\n<li><p>DQN과 전처리 방식은 동일<br>  210 x 160 pixel -&gt; gray-scaling &amp; down-sampling -&gt; 110 x 84  -&gt; crop -&gt; 84 x 84</p>\n</li>\n<li><p>2 convolution layer ( 16 channels) , stride 2, FC layer with 20 units</p>\n</li>\n</ul>\n<p><img src=\"https://i.imgur.com/NJBC69d.png\" alt=\"\"> </p>\n<p><img src=\"https://i.imgur.com/wTe1OEW.png\" alt=\"\"></p>\n<p><img src=\"https://i.imgur.com/j22VtNQ.png\" alt=\"\"></p>\n<ul>\n<li><p>30시간 동안 500 iteration 학습을 하였습니다.</p>\n</li>\n<li><p>DQN과 MCST를 사용한 모델(UCC-I), 사람이 플레이한 것(Human) 과의 비교하였습니다.</p>\n</li>\n<li><p>이전 방법들에 비해 압도적인 점수를 기록하진 못했으나, 다양한 게임에서 적절한 게임 기록하였습니다.</p>\n</li>\n<li><p>이전 방법들과 다른 점은, 특정 task에 초점을 맞춰 설계하지않았는데 robotics나 game playing Task에도 적절한 성능을 내는 점에서 <strong>TRPO의 일반화 성능</strong>을 확인하였습니다.</p>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"9-Discussion\"><a href=\"#9-Discussion\" class=\"headerlink\" title=\"9. Discussion\"></a>9. Discussion</h1><ul>\n<li><p>Trust Region Policy Optimization 을 제안하였습니다.</p>\n</li>\n<li><p>KL Divergence 페널티로 $\\eta(\\pi)$를 최적화하는 알고리즘이 monotonically improve 함을 증명하였습니다.</p>\n</li>\n<li><p>Locomotion 도메인에서 여러 행동(수영, 걷기, 점프)을 제어하는 controller를 학습하였습니다.</p>\n</li>\n<li><p>Robotics와 게임 실험을 결합해, 시각정보와 센서데이터를 사용하는 로봇제어 정책을 학습시킬 수 있는 가능성을 보았습니다.</p>\n</li>\n<li><p>샘플의 복잡도를 상당히 줄일 수 있어 실제상황에 적용가능성을 보았습니다.</p>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"NPG-여행하기\"><a href=\"#NPG-여행하기\" class=\"headerlink\" title=\"NPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/25/4_npg/\">NPG 여행하기</a></h2><h2 id=\"NPG-Code\"><a href=\"#NPG-Code\" class=\"headerlink\" title=\"NPG Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py\" target=\"_blank\" rel=\"noopener\">NPG Code</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"TRPO-Code\"><a href=\"#TRPO-Code\" class=\"headerlink\" title=\"TRPO Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></h2><h2 id=\"GAE-여행하기\"><a href=\"#GAE-여행하기\" class=\"headerlink\" title=\"GAE 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/\">GAE 여행하기</a></h2>"},{"title":"Variational Discriminator Bottleneck. Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow","date":"2019-02-24T15:00:00.000Z","author":"이동민","subtitle":"Inverse RL 6번째 논문","_content":"\n<center> <img src=\"../../../../img/irl/vail_1.png\" width=\"850\"> </center>\n\n논문 저자 : John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan and Pieter Abbeel\n논문 링크 : https://arxiv.org/pdf/1810.00821.pdf\nProceeding : International Conference of Learning Representations (ICLR) 2019\n\n---\n\n# 0. Abstract\n\n현존하는 Policy Gradient Method들의 목적은 누적되는 reward들을 optimization하는 것입니다. 하지만 학습할 때에 많은 양의 sample이 필요로 하고, 들어오는 data가 nonstationarity임에도 불구하고 stable and steady improvement가 어렵습니다.\n\n그래서 이 논문에서는 다음과 같은 방법을 제시합니다.\n\n- TD($\\lambda$)와 유사한 advantage function의 exponentially-weighted estimator를 사용하여 policy gradient estimate의 variance를 줄이는 것 \n- policy와 value function에 대한 Trust Region Optimization 사용하는 것\n\n3D locomotion tasks에 대한 empirical results는 다음과 같습니다.\n\n- bipedal and quadrupedal simulated robots의 달리는 자세를 학습\n- bipedal 사람이 땅에 누워있다가 일어서는 것을 학습\n\n※ TD($\\lambda$)를 혹시 모르실 경우도 있을 것 같아 간략하게 정리하였습니다. http://dongminlee.tistory.com/10 를 먼저 보시고 아래의 내용을 봐주시기 바랍니다!\n\n<br><br>\n\n# 2. Introduction\n\n기본적으로 \"parameterized stochastic policy\"를 가정합니다. 이 때 expected total returns의 gradient에 대한 unbiased estimate를 얻을 수 있는데 이것을 REINFORCE라고 부릅니다. 하지만 하나의 action의 결과가 과거와 미래의 action의 결과로 혼동되기 때문에 gradient estimator의 high variance는 시간에 따라 scaling됩니다.\n\n또 다른 방법은 Actor-Critic이 있습니다. 이 방법은 empirical returns보다 하나의 value function을 사용합니다. 또한 bias하고 lower variance를 가진 estimator입니다. 구체적으로 말하자면, high variance하다면 더 sampling을 하면 되는 반면에 bias는 매우 치명적입니다. 다시 말해 bias는 algorithm이 converge하는 데에 실패하거나 또는 local optimum이 아닌 poor solution에 converge하도록 만듭니다.\n\n따라서 이 논문에서는 $\\gamma\\in [0,1]$ and $\\lambda\\in [0,1]$에 의해 parameterized estimation scheme인 Generalized Advantage Estimator(GAE)를 다룹니다.\n\n이 논문을 대략적으로 요약하자면 다음과 같습니다.\n\n- 효과적인 variance reduction scheme인 GAE를 다룹니다. 또한 실험할 때 batch trust region algorithm에 더하여 다양한 algorithm들에 적용됩니다.\n- value function에 대해 trust region optimization method를 사용합니다. 이렇게 함으로서 더 robust하고 efficient한 방법이 됩니다.\n- A와 B를 합쳐서, 실험적으로 control task에 neural network policies를 learning하는 데에 있어서 효과적인 algorithm을 얻습니다. 이러한 결과는 high-demensional continuous control에 RL을 사용함으로서 state of the art로 확장되었습니다.\n\n<br><br>\n\n# 3. Preliminaries\n\n먼저 policy optimization의 \"undiscounted formulation\"을 가정합니다. (undiscounted formulation에 주목합시다.)\n\n- initial state $s_0$는 distribution $\\rho_0$으로부터 sampling된 것입니다.\n- 하나의 trajectory ($s_0, a_0, s_1, a_1, ...$)는 terminal state에 도달될 때 까지 policy $a_t$ ~ $\\pi(a_t | s_t)$에 따라서 action을 sampling하고, dynamics $s_{t+1}$ ~ $P(s_{t+1} | s_t, a_t)$에 따라서 state를 sampling함으로써 생성됩니다.\n- reward $r_t = r(s_t, a_t, s_{t+1})$은 매 time step마다 받아집니다.\n- 목표는 모든 policies에 대해 finite하다고 가정됨으로서 expected total reward $\\sum_{t=0}^{\\infty} r_t$를 maximize하는 것입니다.\n\n여기서 중요한 점은 $\\gamma$를 discount의 parameter로 사용하는 것이 아니라 \"bias-variance tradeoff를 조절하는 parameter로 사용한다\" 는 것입니다.\n\npolicy gradient method는 gradient $g := \\nabla_\\theta \\mathbb{E} [\\sum_{t=0}^\\infty r_t]$를 반복적으로 estimate함으로써 expected total reward를 maximize하는 것인데, policy gradient에는 여러 다른 표현들이 있습니다.\n$$g = \\mathbb{E} [\\sum_{t=0}^\\infty \\Phi_t \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$\n\n여기서 $\\Phi_t$는 아래 중의 하나일 수 있습니다.\n\n1. $\\sum_{t=0}^\\infty r_t$: trajectory의 total reward\n2. $\\sum_{t'=t}^\\infty r_t'$: action $a_t$ 후의 reward\n3. $\\sum_{t'=t}^\\infty r_t' - b(s_t)$: 2의 baselined version\n4. $Q^\\pi (s_t, a_t)$: state-action value function\n5. $A^\\pi (s_t, a_t)$: advantage function\n6. $r_t + V^\\pi (s_{t+1}) - V^\\pi (s_t)$: TD residual\n\n위의 번호 중 4, 5, 6의 수식들은 다음의 정의를 사용합니다.\n\n- $V^\\pi (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty r_{t+1}]$\n- $Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty r_{t+l}]$\n- $A^\\pi (s_t, a_t) := Q^\\pi (s_t, a_t) - V^\\pi (s_t)$, (Advantage function)\n\n추가적으로 colon notation $a : b$는 포괄적인 범위 $(a, a+1, ... , b)$입니다. (잘 기억해둡시다. 뒤에 계속해서 colon notation이 나옵니다.)\n\n여기서부터 parameter $\\gamma$에 대해 좀 더 자세히 알아봅시다. parameter $\\gamma$는 bias하면서 동시에 reward를 downweighting함으로써 variance를 줄입니다. 다시 말해 MDP의 discounted formulation에서 사용된 discounted factor와 일치하지만, 이 논문에서는 \"$\\gamma$를 undiscounted MDP에서 variance reduction parameter\"로 다룹니다. -> 결과는 같지만, 의미와 의도가 다릅니다.\n\ndiscounted value function들은 다음과 같습니다.\n\n- $V^{\\pi, \\gamma} (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$ \n- $Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$\n- $A^{\\pi, \\gamma} (s_t, a_t) := Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)$\n\n따라서 policy gradient에서의 discounted approximation은 다음과 같이 나타낼 수 있습니다.\n$$g^\\gamma := \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\sum_{t=0}^\\infty A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$\n뒤이어 나오는 section에서는 위의 수식에서 $A^{\\pi, \\gamma}$에 대해 biased (but not too biased) estimator를 얻는 방법에 대해서 나옵니다.\n\n다음으로 advantage function에서 새롭게 접하는 \"$\\gamma$-just estimator\"의 notation에 대해 알아봅시다.\n\n- 먼저 $\\gamma$-just estimator는 $g^\\gamma$ ${}^1$를 estimate하기 위해 위의 수식에서 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 사용할 때, bias하지 않은 estimator라고 합시다. 그리고 이 $\\gamma$-just advantage estimator를 <img src=\"https://www.dropbox.com/s/7g2glqx2kbfynaa/Screen%20Shot%202018-08-22%20at%209.03.46%20PM.png?dl=1\" width=\"100\"> 라고 하고, 전체의 trajectory에 대한 하나의 function이라고 합시다.\n- ${}^1$에서 이 논문의 저자가 하고 싶은 말이 있는 것 같습니다. 개인적으로 $\\gamma$-just estimator를 이해하는 데에 있어서 중요한 주석이라 정확히 해석하고자 합니다.\n    - $A^\\pi$를 $A^{\\pi, \\gamma}$로 사용함으로써 이미 bias하다라고 말했지만, \"이 논문에서 하고자 하는 것은 $g^\\gamma$에 대해 unbiased estimate를 얻고 싶은 것입니다.\" 하지만 undiscounted MDP의 policy gradient에 대해서는 당연히 $\\gamma$를 사용하기 때문에 biased estimate를 얻습니다. 개인적으로 이것은 일단 무시하고 $\\gamma$를 사용할 때 어떻게 unbiased estimate를 얻을 지에 대해 좀 더 포커스를 맞추고 있는 것 같습니다.\n    - 그러니까 저 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 바꿔줌으로써 unbiased estimate를 하고 싶다는 것이 뒤이어 나오는 정의와 명제의 핵심이라고 할 수 있습니다.\n\nDefinition 1.\n먼저 가정을 합니다. (가정을 바탕으로 이루어지는 정의라는 것을 주목합시다.) 만약\n<center ><img src=\"https://www.dropbox.com/s/aia9f1l99wyxgah/Screen%20Shot%202018-08-22%20at%209.07.05%20PM.png?dl=1\" width=\"700\"> </center>\n두 수식이 같다면, estimator $\\hat{A}_t$는 $\\gamma$-just입니다.\n\n그리고 만약 모든 t에 대해서 $\\hat{A}_t$이 $\\gamma$-just이라면, 다음과 같이 표현할 수 있습니다.\n\n<center ><img src=\"https://www.dropbox.com/s/r4m6ktw8geeu7pe/Screen%20Shot%202018-08-22%20at%209.29.18%20PM.png?dl=1\" width=\"420\"> </center>\n\n위의 수식이 바로 unbiased estimate입니다.\n\n$\\gamma$-just인 $\\hat{A}_t$에 대한 한가지 조건은 $\\hat{A}_t$이 두 가지 function $Q_t$ and $b_t$로 나뉠 수 있다는 것입니다.\n\n- $Q_t$는 $\\gamma$-discounted Q-function의 unbiased estimator입니다.\n- $b_t$는 action $a_t$전에 sampling된 states and actions의 arbitrary function이다. \n\nProposition 1.\n모든 $(s_t, a_t)$에 대해,\n$$\\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty | s_t, a_t} [Q_t (s_{t:\\infty}, a_{t:\\infty})] = Q^{\\pi, \\gamma} (s_t, a_t)$$\n로 인하여 $\\hat{A}_t$이 \n\n<center> <img src=\"https://www.dropbox.com/s/59ydsav6djwyevl/Screen%20Shot%202018-08-22%20at%209.27.13%20PM.png?dl=1\" width=\"380\"> </center>\n\n형태라고 가정합시다. (가정을 바탕으로 이루어지는 명제라는 점을 주목합시다.)\n\n그 때, $\\hat{A}_t$은 $\\gamma$-just입니다.\n\n이 명제에 대한 증명은 Appendix B에 있습니. 그리고 $\\hat{A}_t$에 대한 $\\gamma$-just advantage estimator들은 다음과 같습니다.\n\n- $\\sum_{l=0}^\\infty \\gamma^l r_{t+1}$\n- $A^{\\pi, \\gamma} (s_t, a_t)$\n- $Q^{\\pi, \\gamma} (s_t, a_t)$\n- $r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)$\n\n증명을 보기 전에 먼저 Advantage function에 대해서 먼저 살펴봅시다. 아래의 내용은 Sutton이 쓴 논문인 Policy Gradient Methods for Reinforcement Learning with Function Approximation(2000)에서 나온 내용을 리뷰한 것입니다. \n\n<center> <img src=\"https://www.dropbox.com/s/x737yq97ut6gp1a/Screen%20Shot%202018-07-15%20at%201.16.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n여기서는 \"이 수식은 $\\sum_a \\frac{\\partial \\pi (s,a)}{\\partial \\theta} = 0$이기 때문에 가능해진다.\", \"즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없다\"라는 두 문장을 기억해둡시다.\n\n이제 증명을 보면 됩니다.\n\n<center> <img src=\"https://www.dropbox.com/s/e7sj2fwcm1f7hof/figure2.jpg?dl=1\" width=\"600\"> </center>\n<center> <img src=\"https://www.dropbox.com/s/kyk4jes1202az4m/figure3.jpg?dl=1\" width=\"600\"> </center>\n\n위의 증명 수식에서 Q와 b의 각각 세번째 수식과 마지막 수식을 자세히 봅시다.\n\n1. $Q$에 대한 증명\n    - 빨간색 부분\n        - $\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [Q_t (s_{0:\\infty}, a_{0:\\infty})]$를 쉽게 말하자면 \"모든(과거, 현재, 미래) state와 action에 대해서 expectation을 $t+1$부터 $\\infty$까지 하겠다.\"라는 말입니다. 이렇게 함으로써 원래는 $Q^\\pi (s_t, a_t)$가 나와야 맞는 건데, 앞에서 살펴봤다시피 4번째 수식 밑줄 친 자리에는 advantage 역할을 하는 함수들을 넣어도 결과값에 아무런 영향을 끼치지 않기 때문에 의도적으로 $A^\\pi (s_t, a_t)$로 바꾼 것입니다. 또한 이 증명을 바탕으로 $\\hat{A}$이 $\\gamma$-just라는 것을 표현하고 싶기 때문이라고 봐도 됩니다.\n\n    - 파란색 부분\n        - 또한 $a_{0:t}$에서 $a_{0:t-1}$로 변한 것은 $A^\\pi (s_t, a_t)$에 baseline이 들어가기 때문에 바꿔준 것입니다.\n\n2. $b$에 대한 증명\n    - 빨간색 부분\n        - $\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$가 0으로 바뀌는 것은 위에서 설명했듯이 $\\nabla_\\theta$자체가 $\\log \\pi_\\theta$만 gradient하기 때문에 이것을 expectation을 취하면 0이 됩니다.\n\n<br><br>\n\n# 4. Advantage Function Estimation\n\n이번 section에는 discounted advantage function $A^{\\pi, \\gamma} (s_t, a_t)$의 accurate estimate $\\hat{A}_t$에 대해서 살펴봅시다. 이에 따른 수식은 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/sn16g5iyzzflju0/Screen%20Shot%202018-08-22%20at%209.22.49%20PM.png?dl=1\" width=\"300\"> </center>\n\n여기서 n은 a batch of episodes에 대하여 index한 것입니다.\n\n$V$를 approximate value function이라고 합시다. 그리고 $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$이라고 합시다.\n\n만약 (이전과 마찬가지로 가정부터 합니다.) correct value function $V = V^{\\pi, \\gamma}$가 있다고 한다면, 이것은 $\\gamma$-just advantage estimator입니다. 실제로, $A^{\\pi, \\gamma}$의 unbiased estimator는 다음과 같습니다.\n$$\\mathbb{E}_{s_{t+1}} [\\delta_t^{V^{\\pi, \\gamma}}] = \\mathbb{E}_{s_{t+1}} [r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)]$$\n$$= \\mathbb{E}_{s_{t+1}} [Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)] = A^{\\pi, \\gamma} (s_t, a_t)$$\n그러나, \"이 estimator는 유일하게 $V = V^{\\pi, \\gamma}$에 대한 $\\gamma$-just입니다.\" 다른 경우라면 이것은 biased policy gradient estimate일 것입니다. (우리가 하고 싶은 것은 $V$에 대해서만 unbiased estimator가 아니라 advantage function에 대해서 일반화된 unbiased estimator를 얻고 싶은 것입니다. 그래서 아래에서도 나오겠지만, $\\gamma$와 함께 $\\lambda$를 이용한 estimator가 나옵니다.ㅏ)\n\n그래서 $\\delta$에 대해 $k$의 sum으로 생각해봅시다. 이것을 $\\hat{A}_t^{(k)}$라고 하자. 그러면 아래와 같이 표현할 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ra7hxksveg2hz45/figure4.jpg?dl=1\" width=\"600\"> </center>\n\n$\\hat{A}_t^{(k)}$은 returns의 $k$-step estimate와 연관지을 수 있고, $\\delta_t^V = \\hat{A}_t^{(1)}$의 case와 유사하게도 $\\hat{A}_t^{(k)}$를 advantage function의 estimator로 생각할 수 있습니다.\n\n여기서 $k \\rightarrow \\infty$로 생각해보면 bias가 일반적으로 점점 더 작아집니다. 왜냐하면 $\\gamma^k V(s_{t+k})$가 점점 많이 discounted되서 $-V(s_t)$가 bias에 영향을 미치지 못하기 때문입니다. $k \\rightarrow \\infty$를 취하면 다음과 같은 수식이 나옵니다.\n\n<center> <img src=\"https://www.dropbox.com/s/13fn9wcup8pfh9u/Screen%20Shot%202018-08-22%20at%209.19.52%20PM.png?dl=1\" width=\"320\"> </center>\n\n우변의 수식과 같이 empirical returns에서 value function baseline을 뺀 것으로 나타낼 수 있습니다.\n\nGeneralized Advantage Estimator GAE($\\gamma, \\lambda$)는 $k$-step estimators의 exponentially-weighted average로 나타낼 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/yg1ybmfkep3towu/figure5.jpg?dl=1\" width=\"600\"> </center>\n\n(TD($\\lambda$)를 떠올려주세요..!)\n\n위의 마지막 수식에서도 알 수 있듯이, advantage estimator는 Bellman residual terms의 discounted sum과 관련있는 간단한 수식입니다. 다음 section에서 modified reward function을 가진 MDP에 returns로서의 관점에서 위의 마지막 수식을 더 자세히 살펴봅시다. 위의 수식은 TD($\\lambda$)와 많이 유사합니다. 그러나 TD($\\lambda$)는 value function를 estimator하고, 여기서는 advantage function을 estimator합니다.\n\n위의 수식에서 $\\lambda = 0$ and $\\lambda = 1$에 대해서는 특별한 case가 존재한다. 수식으로 표현하면 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ufglxzanhcnbi36/figure6.jpg?dl=1\" width=\"600\"> </center>\n\n- $GAE(\\gamma, 1)$은 $V$의 정확도와 관계없이 $\\gamma$-just입니다. 그러나 returns의 sum때문에 high variance합니다.\n- $GAE(\\gamma, 0)$은 $V = V^{\\pi, \\gamma}$에 대해 $\\gamma$-just입니다. 그리고 bias하지만 일반적으로 훨씬 lower variance를 가집니다. \n- $0 < \\lambda < 1$에 대해 GAE는 parameter $\\lambda$를 control하여 bias와 variance사이에 compromise를 만듭니다.\n\n두 가지 별개의 parameter $\\gamma$ and $\\lambda$를 가진 advantage estimator는 bias-variance tradeoff에 도움을 줍니다. 그러나 이 두 가지 parameter는 각각 다른 목적을 가지고 작동합니다.\n\n- $\\gamma$는 가장 중요하게 value function $V^{\\pi, \\gamma}$ 의 scale을 결정합니다. 또한 $\\gamma$는 $\\lambda$에 의존하지 않습니다. $\\gamma < 1$로 정하는 것은 policy gradient estimate에서 bias합니다.\n- 반면에, $\\lambda < 1$는 유일하게 value function이 부정확할 때 bias합니다. 그리고 경험상, $\\lambda$의 best value는 $\\gamma$의 best value보다 훨씬 더 낮습니다. 왜냐하면 $\\lambda$가 정확한 value function에 대해 $\\gamma$보다 훨씬 덜 bias하기 때문입니다.\n\nGAE를 사용할 때, $g^\\gamma$의 biased estimator를 구성할 수 있습니다. 수식은 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ok57dsk52o7pl9r/Screen%20Shot%202018-08-22%20at%209.17.54%20PM.png?dl=1\" width=\"670\"> </center>\n\n여기서 $\\lambda = 1$일 때 동일해집니다.\n\n<br><br>\n\n# 5. Interpretation as Reward Shaping\n\n이번 section에서는 앞서 다뤘던 수식 $\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V$를 modified reward function의 MDP의 관점으로 생각해봅시다. 조금 더 구체적으로 말하자면, MDP에서 reward shaping transformation을 실행한 후에 적용된 extra discounted factor로서 $\\lambda$를 어떻게 볼 것인지에 대해서 다룹니다.\n\n(개인적인 comment) 한 가지 먼저 언급하자면, 본래의 목적은 reward shaping이 아니라 variance reduction입니다. 이번 section은 그저 이전에 이러한 개념이 있었고, gae를 다른 관점에서 생각해보자라는 뜻에서 나온 section인 것 같습니다. '아~ 이러한 개념이 있구나~!' 정도로만 알면 될 것 같습니다. 'gae가 reward shaping의 효과까지 있어서 이러한 section을 넣은 것일까?' 라는 생각도 해봤지만 아직 잘 모르겠습니다. 실험부분에도 딱히 하지 않은 걸로 봐서는.. 아닌 것 같기도 하고.. 아직까지는 그저 'reward shaping의 관점에서 봤을 때에도 gae로 만들어줄 수 있다.' 정도만 생각하려고 합니다.\n\nReward Shaping이란 개념 자체는 일반적인 알고리즘 분야(최단경로 문제 등)에도 있습니다. 하지만 이 개념이 머신러닝에 적용된 건 Andrew Y. Ng이 쓴 Policy Invariance under Reward Transformations Theory and Application to Reward Shaping(1999) 논문입니다. 논문에서 Reward Shaping을 설명하는 부분은 간략하게 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/0w9cuxjkhz2mbcy/figure7.jpg?dl=1\" width=\"400\"> </center>\n<center> <img src=\"https://www.dropbox.com/s/xcp9eqvcbv3268g/figure8.jpg?dl=1\" width=\"400\"> </center>\n\n위의 글만 보고 이해가 잘 안됩니다. 그래서 다른 자료들을 찾던 중에 다음의 그림을 찾게 되었습니다. (아래의 그림은 Youtube에 있는 [Udacity 영상](https://www.youtube.com/watch?v=xManAGjbx2k&t=95s)에서 있는 그림입니다.)\n\n<center> <img src=\"https://www.dropbox.com/s/87kacngez9cl2e6/figure9.jpg?dl=1\" width=\"400\"> </center>\n\n이 그림을 통해서 Reward Shaping을 이해해봅시다.\n\nReward Shaping(보상 형성)을 통해서 뭘 하고 싶은지 목적부터 봅시다. reward space가 sparse 한 경우에 reward가 너무 드문드문 나옵니다. 따라서 이것을 꾸준히 reward를 받을 수 있도록 바꿉니다. potential-based shaping function인 $\\Phi$를 만들어서 더하고 빼줍니다. 저 $\\Phi$ 자리에는 대표적으로 state value function이 많이 들어간다고 합니다. (아직 목적이 이해가 안될 수 있습니다. Reward Shaping에 대해서 다 보고 나서 다시 한 번 읽어봅시다.)\n\n위의 그림은 돌고래가 점프하여 불구멍을 통과해야하는 환경입니다. 하지만 저 불구멍을 통과하기 위해서는 (1) 점프도 해야하고, (2) 불도 피해야하고, (3) 알맞게 착지까지 완료해야합니다. 이렇게 해야 reward를 +1 얻습니다. \n\n생각해봅시다. 어느 세월에 점프도 해야하고.. 불도 피해야하고.. 알맞게 착지까지해서 reward를 +1 얻겠습니까? 따라서 그 전에도 잘하고 있는 지, 못하고 있는 지를 판단하기 위해, 다시 말해 reward를 꾸준히 받도록 다음과 같이 transformed reward function $\\tilde{r}$을 정의합니다.\n\n$$\\tilde{r} (s, a, s') = r(s, a, s') + \\gamma \\Phi (s') - \\Phi (s)$$\n\n- 여기서 $\\Phi : S \\rightarrow \\mathbb{R}$를 state space에서의 arbitrary scalar-valued function을 나타냅니다. 그리고 $\\Phi$자리에는 대표적으로 state value function이 들어간다고 생각합시다.\n- 형태가 TD residual term의 결과와 비슷하지만 의미와 의도가 다릅니다. reward shaping은 sparse reward 때문이고, 이전 section에서 봤던 gae는 variance reduction때문에 나온 것입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/tkfpqniaazwfbld/figure10.jpg?dl=1\" width=\"600\"> </center>\n\n이 transformation은 discounted advantage function $A^{\\pi, \\gamma}$으로도 둘 수 있습니다. state $s_t$를 시작하여 하나의 trajectory의 rewards의 discounted sum을 표현하면 다음과 같습니다.\n$$\\sum_{l=0}^\\infty \\gamma^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty \\gamma^l r(s_{t+l}, a_{t+l}, s_{t+l+1}) - \\Phi(s_t)$$\n\n$\\tilde{Q}^{\\pi, \\gamma}, \\tilde{V}^{\\pi, \\gamma}, \\tilde{A}^{\\pi, \\gamma}$를  transformed MDP의 value function과 advantage function이라고 하면, 다음과 같은 수식이 나옵니다.\n$$\\tilde{Q}^{\\pi, \\gamma} (s, a) = Q^{\\pi, \\gamma} (s, a) - \\Phi (s)$$\n$$\\tilde{V}^{\\pi, \\gamma} (s, a) = V^{\\pi, \\gamma} (s, a) - \\Phi (s)$$\n$$\\tilde{A}^{\\pi, \\gamma} (s, a) = (Q^{\\pi, \\gamma} (s, a) - \\Phi (s)) - (V^\\pi (s) - \\Phi (s)) = A^{\\pi, \\gamma} (s, a)$$\n\n이제 reward shaping의 idea를 가지고 어떻게 policy gradient estimate를 얻을 수 있는 지에 대해서 알아봅시다.\n\n- $0 \\le \\lambda \\le 1$의 범위에 있는 steeper discount $\\gamma \\lambda$를 사용합니다.\n- shaped reward $\\tilde{r}$는 Bellman residual term $\\delta^V$와 동일합니다.\n- 그리고 $\\Phi = V$와 같다고 보면 다음과 같은 수식이 나옵니다.\n\n$$\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V = \\hat{A}_t^{GAE(\\gamma, \\lambda)}$$\n이렇게 shaped rewards의 $\\gamma \\lambda$-discounted sum을 고려함으로써 GAE를 얻을 수 있습니다. 더 정확하게 $\\lambda = 1$은 $g^\\gamma$의 unbiased estimate이고, 반면에 $\\lambda < 1$은 biased estimate입니다.\n\n좀 더 나아가서, shaping transformation과 parameters $\\gamma$ and $\\lambda$의 결과를 보기 위해, response function $\\chi$를 이용하면 다음과 같은 수식이 나옵니다.\n$$\\chi (l; s_t, a_t) = \\mathbb{E} [r_{t+l} | s_t, a_t] - \\mathbb{E} [r_{t+l} | s_t]$$\n추가적으로 $A^{\\pi, \\gamma} (s, a) = \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$입니다.\n그래서 discounted policy gradient estimator는 다음과 같이 쓸 수 있다.\n$$\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) A^{\\pi, \\gamma} (s_t, a_t) = \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$$\n\n<br><br>\n\n# 6. Value Fuction Estimation\n\n이번 section에서는 Trust Region Optimization Scheme에 따라 Value function을 Estimation합니다.\n\n<br>\n## 6.1 Simplest approach\n\n$$minimize_{\\phi} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi}(s_n) - \\hat{V_n} \\vert\\vert^{2}$$\n\n- 위는 가장 간단하게 non-linear approximation으로 푸는 방법입니다.\n- $\\hat{V_t} = \\sum_{l=0}^{\\infty}\\gamma^l r_{t+l}$은 reward 에 대한 discounted sum을 의미합니다.\n\n<br>\n## 6.2 Trust region method to optimize the value function\n\n- Value function을 최적화 하기 위해 trust region method를 사용합니다.\n- Trust region은 최근 데이터에 대해 overfitting되는 것을 막아줍니다.\n\nTrust region문제를 풀기 위해서는 다음 스텝을 따릅니다.\n\n- $\\sigma^2 = \\frac{1}{N} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi old}(s_n) - \\hat{V_n} \\vert\\vert^{2}$을 계산합니다.\n- 그 후에 다음과 같은 constrained opimization문제를 풉니다.\n\n<center> <img src=\"https://www.dropbox.com/s/d74cg3votxi2dbg/Screen%20Shot%202018-08-22%20at%209.13.28%20PM.png?dl=1\" width=\"300\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/jrkjax71t3j3vk4/Screen%20Shot%202018-08-22%20at%209.13.35%20PM.png?dl=1\" width=\"370\"> </center>\n\n- 위의 수식은 사실 old Value function과 새로운 Value function KL distance가 $\\epsilon$ 다 작아야한다는 수식과 같습니다. Value function이 평균은 $V_{\\phi}(s)$이고 분산이 $\\sigma^2$인 conditional Gaussian distribution으로 parameterize되었을 뿐입니다.\n\n<img width =\"400px\" src=\"https://www.dropbox.com/s/uw0v05feu8chqmc/Screenshot%202018-07-08%2010.04.38.png?dl=1\"> \n\n이 trust region문제의 답을 conjudate gradient algorithm을 이용하여 근사값을 구할 수 있습니다. 특히, 다음과 같은  quadratic program을 풀게됩니다.\n\n<center> <img src=\"https://www.dropbox.com/s/pawjkm8b9ycwsl5/Screen%20Shot%202018-08-22%20at%209.10.36%20PM.png?dl=1\" width=\"250\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/va7wms8uqxnw5rq/Screen%20Shot%202018-08-22%20at%209.10.40%20PM.png?dl=1\" width=\"370\"> </center>\n\n- 여기서 $g$는 objective 의 gradient입니다.\n- $j_n = \\nabla_{\\phi} V_{\\phi}(s_n)$일때, $H = \\frac{1}{N} \\sum_{n} j_n j^T_n$이며, $H$는 objective의 hessian에 대해서 gaussian newton method로 근사한 값입니다. 따라서, value function을 conditional probability로 해석한다면 Fisher information matrix가 됩니다.\n- 구현할때의 방법은 TRPO에서 사용한 방법과 모두 같습니다.\n\n<br><br>\n\n# 7.Experiments\n\n실험은 다음 두 가지 물음에 대해서 디자인 되었습니다.\n\n- GAE에 따라서 episodic total reward를 최적화 할때, $\\lambda$와 $\\gamma$가 변함에 따라서 어떤 경험적인 효과를 볼 수 있는지?\n- GAE와 trust region alogorithm을 policy와 value function 모두에 함께 사용했을 때 어려운 문제에 적용되는 큰 뉴럴넷을 최적화 할 수 있을지?\n\n<br>\n## 7.1 Policy Optimization Algorithm\n\nPolicy update는 TRPO를 사용합니다. TRPO에 대한 설명은 여기서는 생략하겠습니다. TRPO 포스트를 보고 돌아와주세요!\n\n- 이전 TRPO에서 이미 TRPO와 다른 많은 알고리즘들을 비교하였기 때문에, 여기서는 똑같은 짓을 반복하지 않고 $\\lambda$, $\\gamma$가 변함에 따라 어떤 영향이 있는 지에 대한 실험에 집중하겠다고 합니다. (귀찮았던거죠..)\n\n- TRPO를 적용한 GAE의 최종 알고리즘은 다음과 같습니다.\n\n<center> <img width = \"600px\" src=\"https://www.dropbox.com/s/b1klz11f2frrvg4/Screenshot%202018-07-08%2010.33.54.png?dl=1\"> </center>\n<center> <img width = \"300px\" src=\"https://www.dropbox.com/s/35fxte05pqtiel7/Screenshot%202018-07-08%2010.35.22.png?dl=1\"> </center>\n<center> <img width = \"300px\" src=\"https://www.dropbox.com/s/u35pjow3w50bvz1/Screenshot%202018-07-08%2010.36.03.png?dl=1\"> </center>\n\n- 여기서 주의할 점은 Policy update($\\theta_i \\rightarrow \\theta_{i+1}$)에서 $V_{\\phi_i}$를 사용했다는 점입니다.\n- 만약 Value function을 먼저 update하게 된다면 추가적인 bias가 발생합니다.\n- 극단적으로 생각해보아서, 우리가 Value function을 완벽하게 overfit을 해낸다면 Bellman residual($r_t + \\gamma V(s_{t+1}) - V(S_t)$)은 0이 됩니다. 그럼 Policy gradient의 estimation도 거의 0이 될 것입니다.\n\n<br>\n## 7.2 Expermint details\n\n### 7.2.1 Environment \n실험에서 사용된 환경은 다음 네 가지 입니다.\n\n1. classic cart-pole (x 3D)\n2. bipedal locomotion\n3. quadrupedal locomotion\n4. dynamically standing up for the biped\n\n### 7.2.2 Architecture\n\n- 3D robot task에 대해서는 같은 모델을 사용하였습니다.\n    - layers  = [100, 50, 25] 각각 tanh 사용. (Policy와 Value 네트워크 모두)\n    - Final output layer은 linear\n- Cartpole에 대해서는 1개의 layer 안에 20개의 hidden unit만 있는 linear policy를 사용했다고 합니다.\n\n### 7.2.3 Task\n\n- Cartpole\n    - 한 배치당 20 개의 trajectory를 모았고, maximum length는 1000입니다.\n- 3D biped locomotion\n    - 33 dim state , 10 dim action\n    - 50000 time step per batch\n- 3D quadruped locomotion\n    - 29 dim state, 8 dim action\n    - 200000 time step per batch\n- 3D biped locomotion Standing\n    - 33 dim state , 10 dim action\n    - 200000 time step per batch\n\n\n### 7.2.3 results\ncost의 관점에서 결과를 나타내었다고 합니다. Cost는 negative reward와 이것이 최소화 되었는가로 정의되었습니다.\n\n#### 7.2.3.1 Cartpole\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/x9pbms1wvg38lda/Screenshot%202018-07-08%2011.08.22.png?dl=1\"> </center>\n\n- 왼쪽 그림은 $\\gamma$를 0.99로 고정시켜놓은 상태에서 $\\lambda$를 변화시킴에 따라서 cost를 측정한 것입니다. \n- 오른쪽은 $\\gamma$와 $\\lambda$를 둘 다 변화 시키면서 성능을 그림으로 나타낸 표입니다. 흰색에 가까울 수록 좋은 성능입니다. \n\n#### 7.2.3.2 3D BIPEDAL LOCOMOTINO\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/i9wj4p6ijojsy82/Screenshot%202018-07-08%2011.25.08.png?dl=1\"> </center>\n\n\n- 다른 random seed로 부터 9번 씩 시도한 결과를 mean을 취해서 사용합니다.\n- Best performance는   $\\gamma \\in [0.99, 0.995]$ 그리고  $\\lambda \\in [0.96, 0.99]$일때. \n- 1000 iteration 후에 빠르고 부드럽고 안정적인 걸음거이가 나옵니다.\n- 실제로 걸린 시간은 0.01(타입스텝당 시간) * 50000(배치당 타임스텝) * 1000(배치) * 3600(초->시간) * 24 = 5.8일 정도가 걸렸습니다.\n\n#### 7.2.3.3 다른 ROBOT TASKS\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/fuuat65we52quht/Screenshot%202018-07-08%2011.33.11.png?dl=1\"> </center>\n\n- 다른 로봇 TASK에 대해서는 아주 제한적인 실험만 진행합니다.(시간이 부족했던듯 하네요..)\n- Quadruped에 대해서는 $\\gamma = 0.995$로 fix, $\\lambda \\in {0, 0.96}$\n- Standingup에 대해서는 $\\gamma = 0.99$로 fix, $\\lambda \\in {0, 0.96}$\n\n<br><br>\n\n# 8. Discussion\n\n<br>\n## 8.1 Main discussion\n\n지금까지 복잡하고 어려운 control problem에서 Reinforcement Learning(RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 \"variance reduction\"에 대해 연구하였습니다.\n\n\"Generalized Advantage Estimator(GAE)\"라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.\n또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는 지를 보였습니다.\n\n이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.\n\nGAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.\n\n<br>\n## 8.2 Future work\n\nValue function estimation error와 Policy gradient estimation error사이의 관계를 알아낸다면, 우리는 Value function fitting에 더 잘 맞는 error metric(policy gradient estimation 의 정확성과 더 잘 맞는 value function)을 사용할 수 있습니다. 여기서 Policy와 Value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.\n\n추가적으로 DDPG는 별로라고 합니다. 그리고 TD(0)는 bias가 너무 크고, poor performance로 이끈다고 합니다. 특히나 이 논문에서는 low-dimention의 쉬운 문제들만 해결했습니다.\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/nhc7t9psul5lr3x/Screenshot%202018-07-08%2011.45.15.png?dl=1\"> </center>\n\n<br>\n## 8.3 FAQ\n\n- Compatible features와는 무슨 관계?\n     - Compatible features는 value function을 이용하는 policy gradient 알고리즘들과 함께 자주 언급됩니다.\n     - Actor Critic의 저자는 policy의 제한된 representation power때문에, policy gradient는 단지 advantage function space의 subspace에만 의존하게 됩니다.\n     - 이 subspace는 compatible features에 의해 span됩니다.\n     - 이 이론은 현재 문제 구조를 어떻게 이용해야 advantage function에 대해 더 나은 estimation을 할 수 있는 지에 대한 지침을 주지 않습니다. GAE 논문의 idea와 orthogonal합니다.\n- 왜 Q function을 사용하지 않는가?\n     - 먼저 state-value function이 더 낮은 차원의 input을 가진다. 그래서 Q function보다 더 학습하기가 쉽습니다.\n     - 두 번째로 이 논문에서 제안하는 방법으로는 high bias estimator에서 low bias estimator로 $\\lambda$를 통해서 부드럽게 interpolate를 할 수 있습니다.\n     - 반면에 Q를 사용하면 단지 high-bias estimator 밖에 사용할 수 없습니다.\n     - 특히나 return에 대한 one-step estimation은 엄두를 못낼 정도로 bias가 큽니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [TRPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/)\n\n## [TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py)\n\n<br>\n\n# 다음으로\n\n## [PPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/)\n","source":"_posts/6_vail.md","raw":"---\ntitle: Variational Discriminator Bottleneck. Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow\ndate: 2019-02-25\ntags: [\"프로젝트\", \"GAIL하자!\"]\ncategories: 프로젝트\nauthor: 이동민\nsubtitle: Inverse RL 6번째 논문\n---\n\n<center> <img src=\"../../../../img/irl/vail_1.png\" width=\"850\"> </center>\n\n논문 저자 : John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan and Pieter Abbeel\n논문 링크 : https://arxiv.org/pdf/1810.00821.pdf\nProceeding : International Conference of Learning Representations (ICLR) 2019\n\n---\n\n# 0. Abstract\n\n현존하는 Policy Gradient Method들의 목적은 누적되는 reward들을 optimization하는 것입니다. 하지만 학습할 때에 많은 양의 sample이 필요로 하고, 들어오는 data가 nonstationarity임에도 불구하고 stable and steady improvement가 어렵습니다.\n\n그래서 이 논문에서는 다음과 같은 방법을 제시합니다.\n\n- TD($\\lambda$)와 유사한 advantage function의 exponentially-weighted estimator를 사용하여 policy gradient estimate의 variance를 줄이는 것 \n- policy와 value function에 대한 Trust Region Optimization 사용하는 것\n\n3D locomotion tasks에 대한 empirical results는 다음과 같습니다.\n\n- bipedal and quadrupedal simulated robots의 달리는 자세를 학습\n- bipedal 사람이 땅에 누워있다가 일어서는 것을 학습\n\n※ TD($\\lambda$)를 혹시 모르실 경우도 있을 것 같아 간략하게 정리하였습니다. http://dongminlee.tistory.com/10 를 먼저 보시고 아래의 내용을 봐주시기 바랍니다!\n\n<br><br>\n\n# 2. Introduction\n\n기본적으로 \"parameterized stochastic policy\"를 가정합니다. 이 때 expected total returns의 gradient에 대한 unbiased estimate를 얻을 수 있는데 이것을 REINFORCE라고 부릅니다. 하지만 하나의 action의 결과가 과거와 미래의 action의 결과로 혼동되기 때문에 gradient estimator의 high variance는 시간에 따라 scaling됩니다.\n\n또 다른 방법은 Actor-Critic이 있습니다. 이 방법은 empirical returns보다 하나의 value function을 사용합니다. 또한 bias하고 lower variance를 가진 estimator입니다. 구체적으로 말하자면, high variance하다면 더 sampling을 하면 되는 반면에 bias는 매우 치명적입니다. 다시 말해 bias는 algorithm이 converge하는 데에 실패하거나 또는 local optimum이 아닌 poor solution에 converge하도록 만듭니다.\n\n따라서 이 논문에서는 $\\gamma\\in [0,1]$ and $\\lambda\\in [0,1]$에 의해 parameterized estimation scheme인 Generalized Advantage Estimator(GAE)를 다룹니다.\n\n이 논문을 대략적으로 요약하자면 다음과 같습니다.\n\n- 효과적인 variance reduction scheme인 GAE를 다룹니다. 또한 실험할 때 batch trust region algorithm에 더하여 다양한 algorithm들에 적용됩니다.\n- value function에 대해 trust region optimization method를 사용합니다. 이렇게 함으로서 더 robust하고 efficient한 방법이 됩니다.\n- A와 B를 합쳐서, 실험적으로 control task에 neural network policies를 learning하는 데에 있어서 효과적인 algorithm을 얻습니다. 이러한 결과는 high-demensional continuous control에 RL을 사용함으로서 state of the art로 확장되었습니다.\n\n<br><br>\n\n# 3. Preliminaries\n\n먼저 policy optimization의 \"undiscounted formulation\"을 가정합니다. (undiscounted formulation에 주목합시다.)\n\n- initial state $s_0$는 distribution $\\rho_0$으로부터 sampling된 것입니다.\n- 하나의 trajectory ($s_0, a_0, s_1, a_1, ...$)는 terminal state에 도달될 때 까지 policy $a_t$ ~ $\\pi(a_t | s_t)$에 따라서 action을 sampling하고, dynamics $s_{t+1}$ ~ $P(s_{t+1} | s_t, a_t)$에 따라서 state를 sampling함으로써 생성됩니다.\n- reward $r_t = r(s_t, a_t, s_{t+1})$은 매 time step마다 받아집니다.\n- 목표는 모든 policies에 대해 finite하다고 가정됨으로서 expected total reward $\\sum_{t=0}^{\\infty} r_t$를 maximize하는 것입니다.\n\n여기서 중요한 점은 $\\gamma$를 discount의 parameter로 사용하는 것이 아니라 \"bias-variance tradeoff를 조절하는 parameter로 사용한다\" 는 것입니다.\n\npolicy gradient method는 gradient $g := \\nabla_\\theta \\mathbb{E} [\\sum_{t=0}^\\infty r_t]$를 반복적으로 estimate함으로써 expected total reward를 maximize하는 것인데, policy gradient에는 여러 다른 표현들이 있습니다.\n$$g = \\mathbb{E} [\\sum_{t=0}^\\infty \\Phi_t \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$\n\n여기서 $\\Phi_t$는 아래 중의 하나일 수 있습니다.\n\n1. $\\sum_{t=0}^\\infty r_t$: trajectory의 total reward\n2. $\\sum_{t'=t}^\\infty r_t'$: action $a_t$ 후의 reward\n3. $\\sum_{t'=t}^\\infty r_t' - b(s_t)$: 2의 baselined version\n4. $Q^\\pi (s_t, a_t)$: state-action value function\n5. $A^\\pi (s_t, a_t)$: advantage function\n6. $r_t + V^\\pi (s_{t+1}) - V^\\pi (s_t)$: TD residual\n\n위의 번호 중 4, 5, 6의 수식들은 다음의 정의를 사용합니다.\n\n- $V^\\pi (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty r_{t+1}]$\n- $Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty r_{t+l}]$\n- $A^\\pi (s_t, a_t) := Q^\\pi (s_t, a_t) - V^\\pi (s_t)$, (Advantage function)\n\n추가적으로 colon notation $a : b$는 포괄적인 범위 $(a, a+1, ... , b)$입니다. (잘 기억해둡시다. 뒤에 계속해서 colon notation이 나옵니다.)\n\n여기서부터 parameter $\\gamma$에 대해 좀 더 자세히 알아봅시다. parameter $\\gamma$는 bias하면서 동시에 reward를 downweighting함으로써 variance를 줄입니다. 다시 말해 MDP의 discounted formulation에서 사용된 discounted factor와 일치하지만, 이 논문에서는 \"$\\gamma$를 undiscounted MDP에서 variance reduction parameter\"로 다룹니다. -> 결과는 같지만, 의미와 의도가 다릅니다.\n\ndiscounted value function들은 다음과 같습니다.\n\n- $V^{\\pi, \\gamma} (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$ \n- $Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$\n- $A^{\\pi, \\gamma} (s_t, a_t) := Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)$\n\n따라서 policy gradient에서의 discounted approximation은 다음과 같이 나타낼 수 있습니다.\n$$g^\\gamma := \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\sum_{t=0}^\\infty A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$\n뒤이어 나오는 section에서는 위의 수식에서 $A^{\\pi, \\gamma}$에 대해 biased (but not too biased) estimator를 얻는 방법에 대해서 나옵니다.\n\n다음으로 advantage function에서 새롭게 접하는 \"$\\gamma$-just estimator\"의 notation에 대해 알아봅시다.\n\n- 먼저 $\\gamma$-just estimator는 $g^\\gamma$ ${}^1$를 estimate하기 위해 위의 수식에서 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 사용할 때, bias하지 않은 estimator라고 합시다. 그리고 이 $\\gamma$-just advantage estimator를 <img src=\"https://www.dropbox.com/s/7g2glqx2kbfynaa/Screen%20Shot%202018-08-22%20at%209.03.46%20PM.png?dl=1\" width=\"100\"> 라고 하고, 전체의 trajectory에 대한 하나의 function이라고 합시다.\n- ${}^1$에서 이 논문의 저자가 하고 싶은 말이 있는 것 같습니다. 개인적으로 $\\gamma$-just estimator를 이해하는 데에 있어서 중요한 주석이라 정확히 해석하고자 합니다.\n    - $A^\\pi$를 $A^{\\pi, \\gamma}$로 사용함으로써 이미 bias하다라고 말했지만, \"이 논문에서 하고자 하는 것은 $g^\\gamma$에 대해 unbiased estimate를 얻고 싶은 것입니다.\" 하지만 undiscounted MDP의 policy gradient에 대해서는 당연히 $\\gamma$를 사용하기 때문에 biased estimate를 얻습니다. 개인적으로 이것은 일단 무시하고 $\\gamma$를 사용할 때 어떻게 unbiased estimate를 얻을 지에 대해 좀 더 포커스를 맞추고 있는 것 같습니다.\n    - 그러니까 저 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 바꿔줌으로써 unbiased estimate를 하고 싶다는 것이 뒤이어 나오는 정의와 명제의 핵심이라고 할 수 있습니다.\n\nDefinition 1.\n먼저 가정을 합니다. (가정을 바탕으로 이루어지는 정의라는 것을 주목합시다.) 만약\n<center ><img src=\"https://www.dropbox.com/s/aia9f1l99wyxgah/Screen%20Shot%202018-08-22%20at%209.07.05%20PM.png?dl=1\" width=\"700\"> </center>\n두 수식이 같다면, estimator $\\hat{A}_t$는 $\\gamma$-just입니다.\n\n그리고 만약 모든 t에 대해서 $\\hat{A}_t$이 $\\gamma$-just이라면, 다음과 같이 표현할 수 있습니다.\n\n<center ><img src=\"https://www.dropbox.com/s/r4m6ktw8geeu7pe/Screen%20Shot%202018-08-22%20at%209.29.18%20PM.png?dl=1\" width=\"420\"> </center>\n\n위의 수식이 바로 unbiased estimate입니다.\n\n$\\gamma$-just인 $\\hat{A}_t$에 대한 한가지 조건은 $\\hat{A}_t$이 두 가지 function $Q_t$ and $b_t$로 나뉠 수 있다는 것입니다.\n\n- $Q_t$는 $\\gamma$-discounted Q-function의 unbiased estimator입니다.\n- $b_t$는 action $a_t$전에 sampling된 states and actions의 arbitrary function이다. \n\nProposition 1.\n모든 $(s_t, a_t)$에 대해,\n$$\\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty | s_t, a_t} [Q_t (s_{t:\\infty}, a_{t:\\infty})] = Q^{\\pi, \\gamma} (s_t, a_t)$$\n로 인하여 $\\hat{A}_t$이 \n\n<center> <img src=\"https://www.dropbox.com/s/59ydsav6djwyevl/Screen%20Shot%202018-08-22%20at%209.27.13%20PM.png?dl=1\" width=\"380\"> </center>\n\n형태라고 가정합시다. (가정을 바탕으로 이루어지는 명제라는 점을 주목합시다.)\n\n그 때, $\\hat{A}_t$은 $\\gamma$-just입니다.\n\n이 명제에 대한 증명은 Appendix B에 있습니. 그리고 $\\hat{A}_t$에 대한 $\\gamma$-just advantage estimator들은 다음과 같습니다.\n\n- $\\sum_{l=0}^\\infty \\gamma^l r_{t+1}$\n- $A^{\\pi, \\gamma} (s_t, a_t)$\n- $Q^{\\pi, \\gamma} (s_t, a_t)$\n- $r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)$\n\n증명을 보기 전에 먼저 Advantage function에 대해서 먼저 살펴봅시다. 아래의 내용은 Sutton이 쓴 논문인 Policy Gradient Methods for Reinforcement Learning with Function Approximation(2000)에서 나온 내용을 리뷰한 것입니다. \n\n<center> <img src=\"https://www.dropbox.com/s/x737yq97ut6gp1a/Screen%20Shot%202018-07-15%20at%201.16.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n여기서는 \"이 수식은 $\\sum_a \\frac{\\partial \\pi (s,a)}{\\partial \\theta} = 0$이기 때문에 가능해진다.\", \"즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없다\"라는 두 문장을 기억해둡시다.\n\n이제 증명을 보면 됩니다.\n\n<center> <img src=\"https://www.dropbox.com/s/e7sj2fwcm1f7hof/figure2.jpg?dl=1\" width=\"600\"> </center>\n<center> <img src=\"https://www.dropbox.com/s/kyk4jes1202az4m/figure3.jpg?dl=1\" width=\"600\"> </center>\n\n위의 증명 수식에서 Q와 b의 각각 세번째 수식과 마지막 수식을 자세히 봅시다.\n\n1. $Q$에 대한 증명\n    - 빨간색 부분\n        - $\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [Q_t (s_{0:\\infty}, a_{0:\\infty})]$를 쉽게 말하자면 \"모든(과거, 현재, 미래) state와 action에 대해서 expectation을 $t+1$부터 $\\infty$까지 하겠다.\"라는 말입니다. 이렇게 함으로써 원래는 $Q^\\pi (s_t, a_t)$가 나와야 맞는 건데, 앞에서 살펴봤다시피 4번째 수식 밑줄 친 자리에는 advantage 역할을 하는 함수들을 넣어도 결과값에 아무런 영향을 끼치지 않기 때문에 의도적으로 $A^\\pi (s_t, a_t)$로 바꾼 것입니다. 또한 이 증명을 바탕으로 $\\hat{A}$이 $\\gamma$-just라는 것을 표현하고 싶기 때문이라고 봐도 됩니다.\n\n    - 파란색 부분\n        - 또한 $a_{0:t}$에서 $a_{0:t-1}$로 변한 것은 $A^\\pi (s_t, a_t)$에 baseline이 들어가기 때문에 바꿔준 것입니다.\n\n2. $b$에 대한 증명\n    - 빨간색 부분\n        - $\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$가 0으로 바뀌는 것은 위에서 설명했듯이 $\\nabla_\\theta$자체가 $\\log \\pi_\\theta$만 gradient하기 때문에 이것을 expectation을 취하면 0이 됩니다.\n\n<br><br>\n\n# 4. Advantage Function Estimation\n\n이번 section에는 discounted advantage function $A^{\\pi, \\gamma} (s_t, a_t)$의 accurate estimate $\\hat{A}_t$에 대해서 살펴봅시다. 이에 따른 수식은 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/sn16g5iyzzflju0/Screen%20Shot%202018-08-22%20at%209.22.49%20PM.png?dl=1\" width=\"300\"> </center>\n\n여기서 n은 a batch of episodes에 대하여 index한 것입니다.\n\n$V$를 approximate value function이라고 합시다. 그리고 $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$이라고 합시다.\n\n만약 (이전과 마찬가지로 가정부터 합니다.) correct value function $V = V^{\\pi, \\gamma}$가 있다고 한다면, 이것은 $\\gamma$-just advantage estimator입니다. 실제로, $A^{\\pi, \\gamma}$의 unbiased estimator는 다음과 같습니다.\n$$\\mathbb{E}_{s_{t+1}} [\\delta_t^{V^{\\pi, \\gamma}}] = \\mathbb{E}_{s_{t+1}} [r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)]$$\n$$= \\mathbb{E}_{s_{t+1}} [Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)] = A^{\\pi, \\gamma} (s_t, a_t)$$\n그러나, \"이 estimator는 유일하게 $V = V^{\\pi, \\gamma}$에 대한 $\\gamma$-just입니다.\" 다른 경우라면 이것은 biased policy gradient estimate일 것입니다. (우리가 하고 싶은 것은 $V$에 대해서만 unbiased estimator가 아니라 advantage function에 대해서 일반화된 unbiased estimator를 얻고 싶은 것입니다. 그래서 아래에서도 나오겠지만, $\\gamma$와 함께 $\\lambda$를 이용한 estimator가 나옵니다.ㅏ)\n\n그래서 $\\delta$에 대해 $k$의 sum으로 생각해봅시다. 이것을 $\\hat{A}_t^{(k)}$라고 하자. 그러면 아래와 같이 표현할 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ra7hxksveg2hz45/figure4.jpg?dl=1\" width=\"600\"> </center>\n\n$\\hat{A}_t^{(k)}$은 returns의 $k$-step estimate와 연관지을 수 있고, $\\delta_t^V = \\hat{A}_t^{(1)}$의 case와 유사하게도 $\\hat{A}_t^{(k)}$를 advantage function의 estimator로 생각할 수 있습니다.\n\n여기서 $k \\rightarrow \\infty$로 생각해보면 bias가 일반적으로 점점 더 작아집니다. 왜냐하면 $\\gamma^k V(s_{t+k})$가 점점 많이 discounted되서 $-V(s_t)$가 bias에 영향을 미치지 못하기 때문입니다. $k \\rightarrow \\infty$를 취하면 다음과 같은 수식이 나옵니다.\n\n<center> <img src=\"https://www.dropbox.com/s/13fn9wcup8pfh9u/Screen%20Shot%202018-08-22%20at%209.19.52%20PM.png?dl=1\" width=\"320\"> </center>\n\n우변의 수식과 같이 empirical returns에서 value function baseline을 뺀 것으로 나타낼 수 있습니다.\n\nGeneralized Advantage Estimator GAE($\\gamma, \\lambda$)는 $k$-step estimators의 exponentially-weighted average로 나타낼 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/yg1ybmfkep3towu/figure5.jpg?dl=1\" width=\"600\"> </center>\n\n(TD($\\lambda$)를 떠올려주세요..!)\n\n위의 마지막 수식에서도 알 수 있듯이, advantage estimator는 Bellman residual terms의 discounted sum과 관련있는 간단한 수식입니다. 다음 section에서 modified reward function을 가진 MDP에 returns로서의 관점에서 위의 마지막 수식을 더 자세히 살펴봅시다. 위의 수식은 TD($\\lambda$)와 많이 유사합니다. 그러나 TD($\\lambda$)는 value function를 estimator하고, 여기서는 advantage function을 estimator합니다.\n\n위의 수식에서 $\\lambda = 0$ and $\\lambda = 1$에 대해서는 특별한 case가 존재한다. 수식으로 표현하면 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ufglxzanhcnbi36/figure6.jpg?dl=1\" width=\"600\"> </center>\n\n- $GAE(\\gamma, 1)$은 $V$의 정확도와 관계없이 $\\gamma$-just입니다. 그러나 returns의 sum때문에 high variance합니다.\n- $GAE(\\gamma, 0)$은 $V = V^{\\pi, \\gamma}$에 대해 $\\gamma$-just입니다. 그리고 bias하지만 일반적으로 훨씬 lower variance를 가집니다. \n- $0 < \\lambda < 1$에 대해 GAE는 parameter $\\lambda$를 control하여 bias와 variance사이에 compromise를 만듭니다.\n\n두 가지 별개의 parameter $\\gamma$ and $\\lambda$를 가진 advantage estimator는 bias-variance tradeoff에 도움을 줍니다. 그러나 이 두 가지 parameter는 각각 다른 목적을 가지고 작동합니다.\n\n- $\\gamma$는 가장 중요하게 value function $V^{\\pi, \\gamma}$ 의 scale을 결정합니다. 또한 $\\gamma$는 $\\lambda$에 의존하지 않습니다. $\\gamma < 1$로 정하는 것은 policy gradient estimate에서 bias합니다.\n- 반면에, $\\lambda < 1$는 유일하게 value function이 부정확할 때 bias합니다. 그리고 경험상, $\\lambda$의 best value는 $\\gamma$의 best value보다 훨씬 더 낮습니다. 왜냐하면 $\\lambda$가 정확한 value function에 대해 $\\gamma$보다 훨씬 덜 bias하기 때문입니다.\n\nGAE를 사용할 때, $g^\\gamma$의 biased estimator를 구성할 수 있습니다. 수식은 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ok57dsk52o7pl9r/Screen%20Shot%202018-08-22%20at%209.17.54%20PM.png?dl=1\" width=\"670\"> </center>\n\n여기서 $\\lambda = 1$일 때 동일해집니다.\n\n<br><br>\n\n# 5. Interpretation as Reward Shaping\n\n이번 section에서는 앞서 다뤘던 수식 $\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V$를 modified reward function의 MDP의 관점으로 생각해봅시다. 조금 더 구체적으로 말하자면, MDP에서 reward shaping transformation을 실행한 후에 적용된 extra discounted factor로서 $\\lambda$를 어떻게 볼 것인지에 대해서 다룹니다.\n\n(개인적인 comment) 한 가지 먼저 언급하자면, 본래의 목적은 reward shaping이 아니라 variance reduction입니다. 이번 section은 그저 이전에 이러한 개념이 있었고, gae를 다른 관점에서 생각해보자라는 뜻에서 나온 section인 것 같습니다. '아~ 이러한 개념이 있구나~!' 정도로만 알면 될 것 같습니다. 'gae가 reward shaping의 효과까지 있어서 이러한 section을 넣은 것일까?' 라는 생각도 해봤지만 아직 잘 모르겠습니다. 실험부분에도 딱히 하지 않은 걸로 봐서는.. 아닌 것 같기도 하고.. 아직까지는 그저 'reward shaping의 관점에서 봤을 때에도 gae로 만들어줄 수 있다.' 정도만 생각하려고 합니다.\n\nReward Shaping이란 개념 자체는 일반적인 알고리즘 분야(최단경로 문제 등)에도 있습니다. 하지만 이 개념이 머신러닝에 적용된 건 Andrew Y. Ng이 쓴 Policy Invariance under Reward Transformations Theory and Application to Reward Shaping(1999) 논문입니다. 논문에서 Reward Shaping을 설명하는 부분은 간략하게 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/0w9cuxjkhz2mbcy/figure7.jpg?dl=1\" width=\"400\"> </center>\n<center> <img src=\"https://www.dropbox.com/s/xcp9eqvcbv3268g/figure8.jpg?dl=1\" width=\"400\"> </center>\n\n위의 글만 보고 이해가 잘 안됩니다. 그래서 다른 자료들을 찾던 중에 다음의 그림을 찾게 되었습니다. (아래의 그림은 Youtube에 있는 [Udacity 영상](https://www.youtube.com/watch?v=xManAGjbx2k&t=95s)에서 있는 그림입니다.)\n\n<center> <img src=\"https://www.dropbox.com/s/87kacngez9cl2e6/figure9.jpg?dl=1\" width=\"400\"> </center>\n\n이 그림을 통해서 Reward Shaping을 이해해봅시다.\n\nReward Shaping(보상 형성)을 통해서 뭘 하고 싶은지 목적부터 봅시다. reward space가 sparse 한 경우에 reward가 너무 드문드문 나옵니다. 따라서 이것을 꾸준히 reward를 받을 수 있도록 바꿉니다. potential-based shaping function인 $\\Phi$를 만들어서 더하고 빼줍니다. 저 $\\Phi$ 자리에는 대표적으로 state value function이 많이 들어간다고 합니다. (아직 목적이 이해가 안될 수 있습니다. Reward Shaping에 대해서 다 보고 나서 다시 한 번 읽어봅시다.)\n\n위의 그림은 돌고래가 점프하여 불구멍을 통과해야하는 환경입니다. 하지만 저 불구멍을 통과하기 위해서는 (1) 점프도 해야하고, (2) 불도 피해야하고, (3) 알맞게 착지까지 완료해야합니다. 이렇게 해야 reward를 +1 얻습니다. \n\n생각해봅시다. 어느 세월에 점프도 해야하고.. 불도 피해야하고.. 알맞게 착지까지해서 reward를 +1 얻겠습니까? 따라서 그 전에도 잘하고 있는 지, 못하고 있는 지를 판단하기 위해, 다시 말해 reward를 꾸준히 받도록 다음과 같이 transformed reward function $\\tilde{r}$을 정의합니다.\n\n$$\\tilde{r} (s, a, s') = r(s, a, s') + \\gamma \\Phi (s') - \\Phi (s)$$\n\n- 여기서 $\\Phi : S \\rightarrow \\mathbb{R}$를 state space에서의 arbitrary scalar-valued function을 나타냅니다. 그리고 $\\Phi$자리에는 대표적으로 state value function이 들어간다고 생각합시다.\n- 형태가 TD residual term의 결과와 비슷하지만 의미와 의도가 다릅니다. reward shaping은 sparse reward 때문이고, 이전 section에서 봤던 gae는 variance reduction때문에 나온 것입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/tkfpqniaazwfbld/figure10.jpg?dl=1\" width=\"600\"> </center>\n\n이 transformation은 discounted advantage function $A^{\\pi, \\gamma}$으로도 둘 수 있습니다. state $s_t$를 시작하여 하나의 trajectory의 rewards의 discounted sum을 표현하면 다음과 같습니다.\n$$\\sum_{l=0}^\\infty \\gamma^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty \\gamma^l r(s_{t+l}, a_{t+l}, s_{t+l+1}) - \\Phi(s_t)$$\n\n$\\tilde{Q}^{\\pi, \\gamma}, \\tilde{V}^{\\pi, \\gamma}, \\tilde{A}^{\\pi, \\gamma}$를  transformed MDP의 value function과 advantage function이라고 하면, 다음과 같은 수식이 나옵니다.\n$$\\tilde{Q}^{\\pi, \\gamma} (s, a) = Q^{\\pi, \\gamma} (s, a) - \\Phi (s)$$\n$$\\tilde{V}^{\\pi, \\gamma} (s, a) = V^{\\pi, \\gamma} (s, a) - \\Phi (s)$$\n$$\\tilde{A}^{\\pi, \\gamma} (s, a) = (Q^{\\pi, \\gamma} (s, a) - \\Phi (s)) - (V^\\pi (s) - \\Phi (s)) = A^{\\pi, \\gamma} (s, a)$$\n\n이제 reward shaping의 idea를 가지고 어떻게 policy gradient estimate를 얻을 수 있는 지에 대해서 알아봅시다.\n\n- $0 \\le \\lambda \\le 1$의 범위에 있는 steeper discount $\\gamma \\lambda$를 사용합니다.\n- shaped reward $\\tilde{r}$는 Bellman residual term $\\delta^V$와 동일합니다.\n- 그리고 $\\Phi = V$와 같다고 보면 다음과 같은 수식이 나옵니다.\n\n$$\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V = \\hat{A}_t^{GAE(\\gamma, \\lambda)}$$\n이렇게 shaped rewards의 $\\gamma \\lambda$-discounted sum을 고려함으로써 GAE를 얻을 수 있습니다. 더 정확하게 $\\lambda = 1$은 $g^\\gamma$의 unbiased estimate이고, 반면에 $\\lambda < 1$은 biased estimate입니다.\n\n좀 더 나아가서, shaping transformation과 parameters $\\gamma$ and $\\lambda$의 결과를 보기 위해, response function $\\chi$를 이용하면 다음과 같은 수식이 나옵니다.\n$$\\chi (l; s_t, a_t) = \\mathbb{E} [r_{t+l} | s_t, a_t] - \\mathbb{E} [r_{t+l} | s_t]$$\n추가적으로 $A^{\\pi, \\gamma} (s, a) = \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$입니다.\n그래서 discounted policy gradient estimator는 다음과 같이 쓸 수 있다.\n$$\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) A^{\\pi, \\gamma} (s_t, a_t) = \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$$\n\n<br><br>\n\n# 6. Value Fuction Estimation\n\n이번 section에서는 Trust Region Optimization Scheme에 따라 Value function을 Estimation합니다.\n\n<br>\n## 6.1 Simplest approach\n\n$$minimize_{\\phi} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi}(s_n) - \\hat{V_n} \\vert\\vert^{2}$$\n\n- 위는 가장 간단하게 non-linear approximation으로 푸는 방법입니다.\n- $\\hat{V_t} = \\sum_{l=0}^{\\infty}\\gamma^l r_{t+l}$은 reward 에 대한 discounted sum을 의미합니다.\n\n<br>\n## 6.2 Trust region method to optimize the value function\n\n- Value function을 최적화 하기 위해 trust region method를 사용합니다.\n- Trust region은 최근 데이터에 대해 overfitting되는 것을 막아줍니다.\n\nTrust region문제를 풀기 위해서는 다음 스텝을 따릅니다.\n\n- $\\sigma^2 = \\frac{1}{N} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi old}(s_n) - \\hat{V_n} \\vert\\vert^{2}$을 계산합니다.\n- 그 후에 다음과 같은 constrained opimization문제를 풉니다.\n\n<center> <img src=\"https://www.dropbox.com/s/d74cg3votxi2dbg/Screen%20Shot%202018-08-22%20at%209.13.28%20PM.png?dl=1\" width=\"300\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/jrkjax71t3j3vk4/Screen%20Shot%202018-08-22%20at%209.13.35%20PM.png?dl=1\" width=\"370\"> </center>\n\n- 위의 수식은 사실 old Value function과 새로운 Value function KL distance가 $\\epsilon$ 다 작아야한다는 수식과 같습니다. Value function이 평균은 $V_{\\phi}(s)$이고 분산이 $\\sigma^2$인 conditional Gaussian distribution으로 parameterize되었을 뿐입니다.\n\n<img width =\"400px\" src=\"https://www.dropbox.com/s/uw0v05feu8chqmc/Screenshot%202018-07-08%2010.04.38.png?dl=1\"> \n\n이 trust region문제의 답을 conjudate gradient algorithm을 이용하여 근사값을 구할 수 있습니다. 특히, 다음과 같은  quadratic program을 풀게됩니다.\n\n<center> <img src=\"https://www.dropbox.com/s/pawjkm8b9ycwsl5/Screen%20Shot%202018-08-22%20at%209.10.36%20PM.png?dl=1\" width=\"250\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/va7wms8uqxnw5rq/Screen%20Shot%202018-08-22%20at%209.10.40%20PM.png?dl=1\" width=\"370\"> </center>\n\n- 여기서 $g$는 objective 의 gradient입니다.\n- $j_n = \\nabla_{\\phi} V_{\\phi}(s_n)$일때, $H = \\frac{1}{N} \\sum_{n} j_n j^T_n$이며, $H$는 objective의 hessian에 대해서 gaussian newton method로 근사한 값입니다. 따라서, value function을 conditional probability로 해석한다면 Fisher information matrix가 됩니다.\n- 구현할때의 방법은 TRPO에서 사용한 방법과 모두 같습니다.\n\n<br><br>\n\n# 7.Experiments\n\n실험은 다음 두 가지 물음에 대해서 디자인 되었습니다.\n\n- GAE에 따라서 episodic total reward를 최적화 할때, $\\lambda$와 $\\gamma$가 변함에 따라서 어떤 경험적인 효과를 볼 수 있는지?\n- GAE와 trust region alogorithm을 policy와 value function 모두에 함께 사용했을 때 어려운 문제에 적용되는 큰 뉴럴넷을 최적화 할 수 있을지?\n\n<br>\n## 7.1 Policy Optimization Algorithm\n\nPolicy update는 TRPO를 사용합니다. TRPO에 대한 설명은 여기서는 생략하겠습니다. TRPO 포스트를 보고 돌아와주세요!\n\n- 이전 TRPO에서 이미 TRPO와 다른 많은 알고리즘들을 비교하였기 때문에, 여기서는 똑같은 짓을 반복하지 않고 $\\lambda$, $\\gamma$가 변함에 따라 어떤 영향이 있는 지에 대한 실험에 집중하겠다고 합니다. (귀찮았던거죠..)\n\n- TRPO를 적용한 GAE의 최종 알고리즘은 다음과 같습니다.\n\n<center> <img width = \"600px\" src=\"https://www.dropbox.com/s/b1klz11f2frrvg4/Screenshot%202018-07-08%2010.33.54.png?dl=1\"> </center>\n<center> <img width = \"300px\" src=\"https://www.dropbox.com/s/35fxte05pqtiel7/Screenshot%202018-07-08%2010.35.22.png?dl=1\"> </center>\n<center> <img width = \"300px\" src=\"https://www.dropbox.com/s/u35pjow3w50bvz1/Screenshot%202018-07-08%2010.36.03.png?dl=1\"> </center>\n\n- 여기서 주의할 점은 Policy update($\\theta_i \\rightarrow \\theta_{i+1}$)에서 $V_{\\phi_i}$를 사용했다는 점입니다.\n- 만약 Value function을 먼저 update하게 된다면 추가적인 bias가 발생합니다.\n- 극단적으로 생각해보아서, 우리가 Value function을 완벽하게 overfit을 해낸다면 Bellman residual($r_t + \\gamma V(s_{t+1}) - V(S_t)$)은 0이 됩니다. 그럼 Policy gradient의 estimation도 거의 0이 될 것입니다.\n\n<br>\n## 7.2 Expermint details\n\n### 7.2.1 Environment \n실험에서 사용된 환경은 다음 네 가지 입니다.\n\n1. classic cart-pole (x 3D)\n2. bipedal locomotion\n3. quadrupedal locomotion\n4. dynamically standing up for the biped\n\n### 7.2.2 Architecture\n\n- 3D robot task에 대해서는 같은 모델을 사용하였습니다.\n    - layers  = [100, 50, 25] 각각 tanh 사용. (Policy와 Value 네트워크 모두)\n    - Final output layer은 linear\n- Cartpole에 대해서는 1개의 layer 안에 20개의 hidden unit만 있는 linear policy를 사용했다고 합니다.\n\n### 7.2.3 Task\n\n- Cartpole\n    - 한 배치당 20 개의 trajectory를 모았고, maximum length는 1000입니다.\n- 3D biped locomotion\n    - 33 dim state , 10 dim action\n    - 50000 time step per batch\n- 3D quadruped locomotion\n    - 29 dim state, 8 dim action\n    - 200000 time step per batch\n- 3D biped locomotion Standing\n    - 33 dim state , 10 dim action\n    - 200000 time step per batch\n\n\n### 7.2.3 results\ncost의 관점에서 결과를 나타내었다고 합니다. Cost는 negative reward와 이것이 최소화 되었는가로 정의되었습니다.\n\n#### 7.2.3.1 Cartpole\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/x9pbms1wvg38lda/Screenshot%202018-07-08%2011.08.22.png?dl=1\"> </center>\n\n- 왼쪽 그림은 $\\gamma$를 0.99로 고정시켜놓은 상태에서 $\\lambda$를 변화시킴에 따라서 cost를 측정한 것입니다. \n- 오른쪽은 $\\gamma$와 $\\lambda$를 둘 다 변화 시키면서 성능을 그림으로 나타낸 표입니다. 흰색에 가까울 수록 좋은 성능입니다. \n\n#### 7.2.3.2 3D BIPEDAL LOCOMOTINO\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/i9wj4p6ijojsy82/Screenshot%202018-07-08%2011.25.08.png?dl=1\"> </center>\n\n\n- 다른 random seed로 부터 9번 씩 시도한 결과를 mean을 취해서 사용합니다.\n- Best performance는   $\\gamma \\in [0.99, 0.995]$ 그리고  $\\lambda \\in [0.96, 0.99]$일때. \n- 1000 iteration 후에 빠르고 부드럽고 안정적인 걸음거이가 나옵니다.\n- 실제로 걸린 시간은 0.01(타입스텝당 시간) * 50000(배치당 타임스텝) * 1000(배치) * 3600(초->시간) * 24 = 5.8일 정도가 걸렸습니다.\n\n#### 7.2.3.3 다른 ROBOT TASKS\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/fuuat65we52quht/Screenshot%202018-07-08%2011.33.11.png?dl=1\"> </center>\n\n- 다른 로봇 TASK에 대해서는 아주 제한적인 실험만 진행합니다.(시간이 부족했던듯 하네요..)\n- Quadruped에 대해서는 $\\gamma = 0.995$로 fix, $\\lambda \\in {0, 0.96}$\n- Standingup에 대해서는 $\\gamma = 0.99$로 fix, $\\lambda \\in {0, 0.96}$\n\n<br><br>\n\n# 8. Discussion\n\n<br>\n## 8.1 Main discussion\n\n지금까지 복잡하고 어려운 control problem에서 Reinforcement Learning(RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 \"variance reduction\"에 대해 연구하였습니다.\n\n\"Generalized Advantage Estimator(GAE)\"라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.\n또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는 지를 보였습니다.\n\n이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.\n\nGAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.\n\n<br>\n## 8.2 Future work\n\nValue function estimation error와 Policy gradient estimation error사이의 관계를 알아낸다면, 우리는 Value function fitting에 더 잘 맞는 error metric(policy gradient estimation 의 정확성과 더 잘 맞는 value function)을 사용할 수 있습니다. 여기서 Policy와 Value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.\n\n추가적으로 DDPG는 별로라고 합니다. 그리고 TD(0)는 bias가 너무 크고, poor performance로 이끈다고 합니다. 특히나 이 논문에서는 low-dimention의 쉬운 문제들만 해결했습니다.\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/nhc7t9psul5lr3x/Screenshot%202018-07-08%2011.45.15.png?dl=1\"> </center>\n\n<br>\n## 8.3 FAQ\n\n- Compatible features와는 무슨 관계?\n     - Compatible features는 value function을 이용하는 policy gradient 알고리즘들과 함께 자주 언급됩니다.\n     - Actor Critic의 저자는 policy의 제한된 representation power때문에, policy gradient는 단지 advantage function space의 subspace에만 의존하게 됩니다.\n     - 이 subspace는 compatible features에 의해 span됩니다.\n     - 이 이론은 현재 문제 구조를 어떻게 이용해야 advantage function에 대해 더 나은 estimation을 할 수 있는 지에 대한 지침을 주지 않습니다. GAE 논문의 idea와 orthogonal합니다.\n- 왜 Q function을 사용하지 않는가?\n     - 먼저 state-value function이 더 낮은 차원의 input을 가진다. 그래서 Q function보다 더 학습하기가 쉽습니다.\n     - 두 번째로 이 논문에서 제안하는 방법으로는 high bias estimator에서 low bias estimator로 $\\lambda$를 통해서 부드럽게 interpolate를 할 수 있습니다.\n     - 반면에 Q를 사용하면 단지 high-bias estimator 밖에 사용할 수 없습니다.\n     - 특히나 return에 대한 one-step estimation은 엄두를 못낼 정도로 bias가 큽니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [TRPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/)\n\n## [TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py)\n\n<br>\n\n# 다음으로\n\n## [PPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/)\n","slug":"6_vail","published":1,"updated":"2019-02-07T11:21:31.967Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjrujlu1k00245wfe304hcq04","content":"<center> <img src=\"../../../../img/irl/vail_1.png\" width=\"850\"> </center>\n\n<p>논문 저자 : John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan and Pieter Abbeel<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1810.00821.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1810.00821.pdf</a><br>Proceeding : International Conference of Learning Representations (ICLR) 2019</p>\n<hr>\n<h1 id=\"0-Abstract\"><a href=\"#0-Abstract\" class=\"headerlink\" title=\"0. Abstract\"></a>0. Abstract</h1><p>현존하는 Policy Gradient Method들의 목적은 누적되는 reward들을 optimization하는 것입니다. 하지만 학습할 때에 많은 양의 sample이 필요로 하고, 들어오는 data가 nonstationarity임에도 불구하고 stable and steady improvement가 어렵습니다.</p>\n<p>그래서 이 논문에서는 다음과 같은 방법을 제시합니다.</p>\n<ul>\n<li>TD($\\lambda$)와 유사한 advantage function의 exponentially-weighted estimator를 사용하여 policy gradient estimate의 variance를 줄이는 것 </li>\n<li>policy와 value function에 대한 Trust Region Optimization 사용하는 것</li>\n</ul>\n<p>3D locomotion tasks에 대한 empirical results는 다음과 같습니다.</p>\n<ul>\n<li>bipedal and quadrupedal simulated robots의 달리는 자세를 학습</li>\n<li>bipedal 사람이 땅에 누워있다가 일어서는 것을 학습</li>\n</ul>\n<p>※ TD($\\lambda$)를 혹시 모르실 경우도 있을 것 같아 간략하게 정리하였습니다. <a href=\"http://dongminlee.tistory.com/10\" target=\"_blank\" rel=\"noopener\">http://dongminlee.tistory.com/10</a> 를 먼저 보시고 아래의 내용을 봐주시기 바랍니다!</p>\n<p><br><br></p>\n<h1 id=\"2-Introduction\"><a href=\"#2-Introduction\" class=\"headerlink\" title=\"2. Introduction\"></a>2. Introduction</h1><p>기본적으로 “parameterized stochastic policy”를 가정합니다. 이 때 expected total returns의 gradient에 대한 unbiased estimate를 얻을 수 있는데 이것을 REINFORCE라고 부릅니다. 하지만 하나의 action의 결과가 과거와 미래의 action의 결과로 혼동되기 때문에 gradient estimator의 high variance는 시간에 따라 scaling됩니다.</p>\n<p>또 다른 방법은 Actor-Critic이 있습니다. 이 방법은 empirical returns보다 하나의 value function을 사용합니다. 또한 bias하고 lower variance를 가진 estimator입니다. 구체적으로 말하자면, high variance하다면 더 sampling을 하면 되는 반면에 bias는 매우 치명적입니다. 다시 말해 bias는 algorithm이 converge하는 데에 실패하거나 또는 local optimum이 아닌 poor solution에 converge하도록 만듭니다.</p>\n<p>따라서 이 논문에서는 $\\gamma\\in [0,1]$ and $\\lambda\\in [0,1]$에 의해 parameterized estimation scheme인 Generalized Advantage Estimator(GAE)를 다룹니다.</p>\n<p>이 논문을 대략적으로 요약하자면 다음과 같습니다.</p>\n<ul>\n<li>효과적인 variance reduction scheme인 GAE를 다룹니다. 또한 실험할 때 batch trust region algorithm에 더하여 다양한 algorithm들에 적용됩니다.</li>\n<li>value function에 대해 trust region optimization method를 사용합니다. 이렇게 함으로서 더 robust하고 efficient한 방법이 됩니다.</li>\n<li>A와 B를 합쳐서, 실험적으로 control task에 neural network policies를 learning하는 데에 있어서 효과적인 algorithm을 얻습니다. 이러한 결과는 high-demensional continuous control에 RL을 사용함으로서 state of the art로 확장되었습니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"3-Preliminaries\"><a href=\"#3-Preliminaries\" class=\"headerlink\" title=\"3. Preliminaries\"></a>3. Preliminaries</h1><p>먼저 policy optimization의 “undiscounted formulation”을 가정합니다. (undiscounted formulation에 주목합시다.)</p>\n<ul>\n<li>initial state $s_0$는 distribution $\\rho_0$으로부터 sampling된 것입니다.</li>\n<li>하나의 trajectory ($s_0, a_0, s_1, a_1, …$)는 terminal state에 도달될 때 까지 policy $a_t$ ~ $\\pi(a_t | s_t)$에 따라서 action을 sampling하고, dynamics $s_{t+1}$ ~ $P(s_{t+1} | s_t, a_t)$에 따라서 state를 sampling함으로써 생성됩니다.</li>\n<li>reward $r_t = r(s_t, a_t, s_{t+1})$은 매 time step마다 받아집니다.</li>\n<li>목표는 모든 policies에 대해 finite하다고 가정됨으로서 expected total reward $\\sum_{t=0}^{\\infty} r_t$를 maximize하는 것입니다.</li>\n</ul>\n<p>여기서 중요한 점은 $\\gamma$를 discount의 parameter로 사용하는 것이 아니라 “bias-variance tradeoff를 조절하는 parameter로 사용한다” 는 것입니다.</p>\n<p>policy gradient method는 gradient $g := \\nabla_\\theta \\mathbb{E} [\\sum_{t=0}^\\infty r_t]$를 반복적으로 estimate함으로써 expected total reward를 maximize하는 것인데, policy gradient에는 여러 다른 표현들이 있습니다.<br>$$g = \\mathbb{E} [\\sum_{t=0}^\\infty \\Phi_t \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$</p>\n<p>여기서 $\\Phi_t$는 아래 중의 하나일 수 있습니다.</p>\n<ol>\n<li>$\\sum_{t=0}^\\infty r_t$: trajectory의 total reward</li>\n<li>$\\sum_{t’=t}^\\infty r_t’$: action $a_t$ 후의 reward</li>\n<li>$\\sum_{t’=t}^\\infty r_t’ - b(s_t)$: 2의 baselined version</li>\n<li>$Q^\\pi (s_t, a_t)$: state-action value function</li>\n<li>$A^\\pi (s_t, a_t)$: advantage function</li>\n<li>$r_t + V^\\pi (s_{t+1}) - V^\\pi (s_t)$: TD residual</li>\n</ol>\n<p>위의 번호 중 4, 5, 6의 수식들은 다음의 정의를 사용합니다.</p>\n<ul>\n<li>$V^\\pi (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty r_{t+1}]$</li>\n<li>$Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty r_{t+l}]$</li>\n<li>$A^\\pi (s_t, a_t) := Q^\\pi (s_t, a_t) - V^\\pi (s_t)$, (Advantage function)</li>\n</ul>\n<p>추가적으로 colon notation $a : b$는 포괄적인 범위 $(a, a+1, … , b)$입니다. (잘 기억해둡시다. 뒤에 계속해서 colon notation이 나옵니다.)</p>\n<p>여기서부터 parameter $\\gamma$에 대해 좀 더 자세히 알아봅시다. parameter $\\gamma$는 bias하면서 동시에 reward를 downweighting함으로써 variance를 줄입니다. 다시 말해 MDP의 discounted formulation에서 사용된 discounted factor와 일치하지만, 이 논문에서는 “$\\gamma$를 undiscounted MDP에서 variance reduction parameter”로 다룹니다. -&gt; 결과는 같지만, 의미와 의도가 다릅니다.</p>\n<p>discounted value function들은 다음과 같습니다.</p>\n<ul>\n<li>$V^{\\pi, \\gamma} (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$ </li>\n<li>$Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$</li>\n<li>$A^{\\pi, \\gamma} (s_t, a_t) := Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)$</li>\n</ul>\n<p>따라서 policy gradient에서의 discounted approximation은 다음과 같이 나타낼 수 있습니다.<br>$$g^\\gamma := \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\sum_{t=0}^\\infty A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$<br>뒤이어 나오는 section에서는 위의 수식에서 $A^{\\pi, \\gamma}$에 대해 biased (but not too biased) estimator를 얻는 방법에 대해서 나옵니다.</p>\n<p>다음으로 advantage function에서 새롭게 접하는 “$\\gamma$-just estimator”의 notation에 대해 알아봅시다.</p>\n<ul>\n<li>먼저 $\\gamma$-just estimator는 $g^\\gamma$ ${}^1$를 estimate하기 위해 위의 수식에서 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 사용할 때, bias하지 않은 estimator라고 합시다. 그리고 이 $\\gamma$-just advantage estimator를 <img src=\"https://www.dropbox.com/s/7g2glqx2kbfynaa/Screen%20Shot%202018-08-22%20at%209.03.46%20PM.png?dl=1\" width=\"100\"> 라고 하고, 전체의 trajectory에 대한 하나의 function이라고 합시다.</li>\n<li>${}^1$에서 이 논문의 저자가 하고 싶은 말이 있는 것 같습니다. 개인적으로 $\\gamma$-just estimator를 이해하는 데에 있어서 중요한 주석이라 정확히 해석하고자 합니다.<ul>\n<li>$A^\\pi$를 $A^{\\pi, \\gamma}$로 사용함으로써 이미 bias하다라고 말했지만, “이 논문에서 하고자 하는 것은 $g^\\gamma$에 대해 unbiased estimate를 얻고 싶은 것입니다.” 하지만 undiscounted MDP의 policy gradient에 대해서는 당연히 $\\gamma$를 사용하기 때문에 biased estimate를 얻습니다. 개인적으로 이것은 일단 무시하고 $\\gamma$를 사용할 때 어떻게 unbiased estimate를 얻을 지에 대해 좀 더 포커스를 맞추고 있는 것 같습니다.</li>\n<li>그러니까 저 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 바꿔줌으로써 unbiased estimate를 하고 싶다는 것이 뒤이어 나오는 정의와 명제의 핵심이라고 할 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n<p>Definition 1.<br>먼저 가정을 합니다. (가정을 바탕으로 이루어지는 정의라는 것을 주목합시다.) 만약</p>\n<center><img src=\"https://www.dropbox.com/s/aia9f1l99wyxgah/Screen%20Shot%202018-08-22%20at%209.07.05%20PM.png?dl=1\" width=\"700\"> </center><br>두 수식이 같다면, estimator $\\hat{A}_t$는 $\\gamma$-just입니다.<br><br>그리고 만약 모든 t에 대해서 $\\hat{A}_t$이 $\\gamma$-just이라면, 다음과 같이 표현할 수 있습니다.<br><br><center><img src=\"https://www.dropbox.com/s/r4m6ktw8geeu7pe/Screen%20Shot%202018-08-22%20at%209.29.18%20PM.png?dl=1\" width=\"420\"> </center>\n\n<p>위의 수식이 바로 unbiased estimate입니다.</p>\n<p>$\\gamma$-just인 $\\hat{A}_t$에 대한 한가지 조건은 $\\hat{A}_t$이 두 가지 function $Q_t$ and $b_t$로 나뉠 수 있다는 것입니다.</p>\n<ul>\n<li>$Q_t$는 $\\gamma$-discounted Q-function의 unbiased estimator입니다.</li>\n<li>$b_t$는 action $a_t$전에 sampling된 states and actions의 arbitrary function이다. </li>\n</ul>\n<p>Proposition 1.<br>모든 $(s_t, a_t)$에 대해,<br>$$\\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty | s_t, a_t} [Q_t (s_{t:\\infty}, a_{t:\\infty})] = Q^{\\pi, \\gamma} (s_t, a_t)$$<br>로 인하여 $\\hat{A}_t$이 </p>\n<center> <img src=\"https://www.dropbox.com/s/59ydsav6djwyevl/Screen%20Shot%202018-08-22%20at%209.27.13%20PM.png?dl=1\" width=\"380\"> </center>\n\n<p>형태라고 가정합시다. (가정을 바탕으로 이루어지는 명제라는 점을 주목합시다.)</p>\n<p>그 때, $\\hat{A}_t$은 $\\gamma$-just입니다.</p>\n<p>이 명제에 대한 증명은 Appendix B에 있습니. 그리고 $\\hat{A}_t$에 대한 $\\gamma$-just advantage estimator들은 다음과 같습니다.</p>\n<ul>\n<li>$\\sum_{l=0}^\\infty \\gamma^l r_{t+1}$</li>\n<li>$A^{\\pi, \\gamma} (s_t, a_t)$</li>\n<li>$Q^{\\pi, \\gamma} (s_t, a_t)$</li>\n<li>$r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)$</li>\n</ul>\n<p>증명을 보기 전에 먼저 Advantage function에 대해서 먼저 살펴봅시다. 아래의 내용은 Sutton이 쓴 논문인 Policy Gradient Methods for Reinforcement Learning with Function Approximation(2000)에서 나온 내용을 리뷰한 것입니다. </p>\n<center> <img src=\"https://www.dropbox.com/s/x737yq97ut6gp1a/Screen%20Shot%202018-07-15%20at%201.16.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n<p>여기서는 “이 수식은 $\\sum_a \\frac{\\partial \\pi (s,a)}{\\partial \\theta} = 0$이기 때문에 가능해진다.”, “즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없다”라는 두 문장을 기억해둡시다.</p>\n<p>이제 증명을 보면 됩니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/e7sj2fwcm1f7hof/figure2.jpg?dl=1\" width=\"600\"> </center><br><center> <img src=\"https://www.dropbox.com/s/kyk4jes1202az4m/figure3.jpg?dl=1\" width=\"600\"> </center>\n\n<p>위의 증명 수식에서 Q와 b의 각각 세번째 수식과 마지막 수식을 자세히 봅시다.</p>\n<ol>\n<li><p>$Q$에 대한 증명</p>\n<ul>\n<li><p>빨간색 부분</p>\n<ul>\n<li>$\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [Q_t (s_{0:\\infty}, a_{0:\\infty})]$를 쉽게 말하자면 “모든(과거, 현재, 미래) state와 action에 대해서 expectation을 $t+1$부터 $\\infty$까지 하겠다.”라는 말입니다. 이렇게 함으로써 원래는 $Q^\\pi (s_t, a_t)$가 나와야 맞는 건데, 앞에서 살펴봤다시피 4번째 수식 밑줄 친 자리에는 advantage 역할을 하는 함수들을 넣어도 결과값에 아무런 영향을 끼치지 않기 때문에 의도적으로 $A^\\pi (s_t, a_t)$로 바꾼 것입니다. 또한 이 증명을 바탕으로 $\\hat{A}$이 $\\gamma$-just라는 것을 표현하고 싶기 때문이라고 봐도 됩니다.</li>\n</ul>\n</li>\n<li><p>파란색 부분</p>\n<ul>\n<li>또한 $a_{0:t}$에서 $a_{0:t-1}$로 변한 것은 $A^\\pi (s_t, a_t)$에 baseline이 들어가기 때문에 바꿔준 것입니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>$b$에 대한 증명</p>\n<ul>\n<li>빨간색 부분<ul>\n<li>$\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$가 0으로 바뀌는 것은 위에서 설명했듯이 $\\nabla_\\theta$자체가 $\\log \\pi_\\theta$만 gradient하기 때문에 이것을 expectation을 취하면 0이 됩니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p><br><br></p>\n<h1 id=\"4-Advantage-Function-Estimation\"><a href=\"#4-Advantage-Function-Estimation\" class=\"headerlink\" title=\"4. Advantage Function Estimation\"></a>4. Advantage Function Estimation</h1><p>이번 section에는 discounted advantage function $A^{\\pi, \\gamma} (s_t, a_t)$의 accurate estimate $\\hat{A}_t$에 대해서 살펴봅시다. 이에 따른 수식은 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/sn16g5iyzzflju0/Screen%20Shot%202018-08-22%20at%209.22.49%20PM.png?dl=1\" width=\"300\"> </center>\n\n<p>여기서 n은 a batch of episodes에 대하여 index한 것입니다.</p>\n<p>$V$를 approximate value function이라고 합시다. 그리고 $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$이라고 합시다.</p>\n<p>만약 (이전과 마찬가지로 가정부터 합니다.) correct value function $V = V^{\\pi, \\gamma}$가 있다고 한다면, 이것은 $\\gamma$-just advantage estimator입니다. 실제로, $A^{\\pi, \\gamma}$의 unbiased estimator는 다음과 같습니다.<br>$$\\mathbb{E}_{s_{t+1}} [\\delta_t^{V^{\\pi, \\gamma}}] = \\mathbb{E}_{s_{t+1}} [r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)]$$<br>$$= \\mathbb{E}_{s_{t+1}} [Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)] = A^{\\pi, \\gamma} (s_t, a_t)$$<br>그러나, “이 estimator는 유일하게 $V = V^{\\pi, \\gamma}$에 대한 $\\gamma$-just입니다.” 다른 경우라면 이것은 biased policy gradient estimate일 것입니다. (우리가 하고 싶은 것은 $V$에 대해서만 unbiased estimator가 아니라 advantage function에 대해서 일반화된 unbiased estimator를 얻고 싶은 것입니다. 그래서 아래에서도 나오겠지만, $\\gamma$와 함께 $\\lambda$를 이용한 estimator가 나옵니다.ㅏ)</p>\n<p>그래서 $\\delta$에 대해 $k$의 sum으로 생각해봅시다. 이것을 $\\hat{A}_t^{(k)}$라고 하자. 그러면 아래와 같이 표현할 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ra7hxksveg2hz45/figure4.jpg?dl=1\" width=\"600\"> </center>\n\n<p>$\\hat{A}_t^{(k)}$은 returns의 $k$-step estimate와 연관지을 수 있고, $\\delta_t^V = \\hat{A}_t^{(1)}$의 case와 유사하게도 $\\hat{A}_t^{(k)}$를 advantage function의 estimator로 생각할 수 있습니다.</p>\n<p>여기서 $k \\rightarrow \\infty$로 생각해보면 bias가 일반적으로 점점 더 작아집니다. 왜냐하면 $\\gamma^k V(s_{t+k})$가 점점 많이 discounted되서 $-V(s_t)$가 bias에 영향을 미치지 못하기 때문입니다. $k \\rightarrow \\infty$를 취하면 다음과 같은 수식이 나옵니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/13fn9wcup8pfh9u/Screen%20Shot%202018-08-22%20at%209.19.52%20PM.png?dl=1\" width=\"320\"> </center>\n\n<p>우변의 수식과 같이 empirical returns에서 value function baseline을 뺀 것으로 나타낼 수 있습니다.</p>\n<p>Generalized Advantage Estimator GAE($\\gamma, \\lambda$)는 $k$-step estimators의 exponentially-weighted average로 나타낼 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/yg1ybmfkep3towu/figure5.jpg?dl=1\" width=\"600\"> </center>\n\n<p>(TD($\\lambda$)를 떠올려주세요..!)</p>\n<p>위의 마지막 수식에서도 알 수 있듯이, advantage estimator는 Bellman residual terms의 discounted sum과 관련있는 간단한 수식입니다. 다음 section에서 modified reward function을 가진 MDP에 returns로서의 관점에서 위의 마지막 수식을 더 자세히 살펴봅시다. 위의 수식은 TD($\\lambda$)와 많이 유사합니다. 그러나 TD($\\lambda$)는 value function를 estimator하고, 여기서는 advantage function을 estimator합니다.</p>\n<p>위의 수식에서 $\\lambda = 0$ and $\\lambda = 1$에 대해서는 특별한 case가 존재한다. 수식으로 표현하면 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ufglxzanhcnbi36/figure6.jpg?dl=1\" width=\"600\"> </center>\n\n<ul>\n<li>$GAE(\\gamma, 1)$은 $V$의 정확도와 관계없이 $\\gamma$-just입니다. 그러나 returns의 sum때문에 high variance합니다.</li>\n<li>$GAE(\\gamma, 0)$은 $V = V^{\\pi, \\gamma}$에 대해 $\\gamma$-just입니다. 그리고 bias하지만 일반적으로 훨씬 lower variance를 가집니다. </li>\n<li>$0 &lt; \\lambda &lt; 1$에 대해 GAE는 parameter $\\lambda$를 control하여 bias와 variance사이에 compromise를 만듭니다.</li>\n</ul>\n<p>두 가지 별개의 parameter $\\gamma$ and $\\lambda$를 가진 advantage estimator는 bias-variance tradeoff에 도움을 줍니다. 그러나 이 두 가지 parameter는 각각 다른 목적을 가지고 작동합니다.</p>\n<ul>\n<li>$\\gamma$는 가장 중요하게 value function $V^{\\pi, \\gamma}$ 의 scale을 결정합니다. 또한 $\\gamma$는 $\\lambda$에 의존하지 않습니다. $\\gamma &lt; 1$로 정하는 것은 policy gradient estimate에서 bias합니다.</li>\n<li>반면에, $\\lambda &lt; 1$는 유일하게 value function이 부정확할 때 bias합니다. 그리고 경험상, $\\lambda$의 best value는 $\\gamma$의 best value보다 훨씬 더 낮습니다. 왜냐하면 $\\lambda$가 정확한 value function에 대해 $\\gamma$보다 훨씬 덜 bias하기 때문입니다.</li>\n</ul>\n<p>GAE를 사용할 때, $g^\\gamma$의 biased estimator를 구성할 수 있습니다. 수식은 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ok57dsk52o7pl9r/Screen%20Shot%202018-08-22%20at%209.17.54%20PM.png?dl=1\" width=\"670\"> </center>\n\n<p>여기서 $\\lambda = 1$일 때 동일해집니다.</p>\n<p><br><br></p>\n<h1 id=\"5-Interpretation-as-Reward-Shaping\"><a href=\"#5-Interpretation-as-Reward-Shaping\" class=\"headerlink\" title=\"5. Interpretation as Reward Shaping\"></a>5. Interpretation as Reward Shaping</h1><p>이번 section에서는 앞서 다뤘던 수식 $\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V$를 modified reward function의 MDP의 관점으로 생각해봅시다. 조금 더 구체적으로 말하자면, MDP에서 reward shaping transformation을 실행한 후에 적용된 extra discounted factor로서 $\\lambda$를 어떻게 볼 것인지에 대해서 다룹니다.</p>\n<p>(개인적인 comment) 한 가지 먼저 언급하자면, 본래의 목적은 reward shaping이 아니라 variance reduction입니다. 이번 section은 그저 이전에 이러한 개념이 있었고, gae를 다른 관점에서 생각해보자라는 뜻에서 나온 section인 것 같습니다. ‘아~ 이러한 개념이 있구나~!’ 정도로만 알면 될 것 같습니다. ‘gae가 reward shaping의 효과까지 있어서 이러한 section을 넣은 것일까?’ 라는 생각도 해봤지만 아직 잘 모르겠습니다. 실험부분에도 딱히 하지 않은 걸로 봐서는.. 아닌 것 같기도 하고.. 아직까지는 그저 ‘reward shaping의 관점에서 봤을 때에도 gae로 만들어줄 수 있다.’ 정도만 생각하려고 합니다.</p>\n<p>Reward Shaping이란 개념 자체는 일반적인 알고리즘 분야(최단경로 문제 등)에도 있습니다. 하지만 이 개념이 머신러닝에 적용된 건 Andrew Y. Ng이 쓴 Policy Invariance under Reward Transformations Theory and Application to Reward Shaping(1999) 논문입니다. 논문에서 Reward Shaping을 설명하는 부분은 간략하게 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/0w9cuxjkhz2mbcy/figure7.jpg?dl=1\" width=\"400\"> </center><br><center> <img src=\"https://www.dropbox.com/s/xcp9eqvcbv3268g/figure8.jpg?dl=1\" width=\"400\"> </center>\n\n<p>위의 글만 보고 이해가 잘 안됩니다. 그래서 다른 자료들을 찾던 중에 다음의 그림을 찾게 되었습니다. (아래의 그림은 Youtube에 있는 <a href=\"https://www.youtube.com/watch?v=xManAGjbx2k&amp;t=95s\" target=\"_blank\" rel=\"noopener\">Udacity 영상</a>에서 있는 그림입니다.)</p>\n<center> <img src=\"https://www.dropbox.com/s/87kacngez9cl2e6/figure9.jpg?dl=1\" width=\"400\"> </center>\n\n<p>이 그림을 통해서 Reward Shaping을 이해해봅시다.</p>\n<p>Reward Shaping(보상 형성)을 통해서 뭘 하고 싶은지 목적부터 봅시다. reward space가 sparse 한 경우에 reward가 너무 드문드문 나옵니다. 따라서 이것을 꾸준히 reward를 받을 수 있도록 바꿉니다. potential-based shaping function인 $\\Phi$를 만들어서 더하고 빼줍니다. 저 $\\Phi$ 자리에는 대표적으로 state value function이 많이 들어간다고 합니다. (아직 목적이 이해가 안될 수 있습니다. Reward Shaping에 대해서 다 보고 나서 다시 한 번 읽어봅시다.)</p>\n<p>위의 그림은 돌고래가 점프하여 불구멍을 통과해야하는 환경입니다. 하지만 저 불구멍을 통과하기 위해서는 (1) 점프도 해야하고, (2) 불도 피해야하고, (3) 알맞게 착지까지 완료해야합니다. 이렇게 해야 reward를 +1 얻습니다. </p>\n<p>생각해봅시다. 어느 세월에 점프도 해야하고.. 불도 피해야하고.. 알맞게 착지까지해서 reward를 +1 얻겠습니까? 따라서 그 전에도 잘하고 있는 지, 못하고 있는 지를 판단하기 위해, 다시 말해 reward를 꾸준히 받도록 다음과 같이 transformed reward function $\\tilde{r}$을 정의합니다.</p>\n<p>$$\\tilde{r} (s, a, s’) = r(s, a, s’) + \\gamma \\Phi (s’) - \\Phi (s)$$</p>\n<ul>\n<li>여기서 $\\Phi : S \\rightarrow \\mathbb{R}$를 state space에서의 arbitrary scalar-valued function을 나타냅니다. 그리고 $\\Phi$자리에는 대표적으로 state value function이 들어간다고 생각합시다.</li>\n<li>형태가 TD residual term의 결과와 비슷하지만 의미와 의도가 다릅니다. reward shaping은 sparse reward 때문이고, 이전 section에서 봤던 gae는 variance reduction때문에 나온 것입니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/tkfpqniaazwfbld/figure10.jpg?dl=1\" width=\"600\"> </center>\n\n<p>이 transformation은 discounted advantage function $A^{\\pi, \\gamma}$으로도 둘 수 있습니다. state $s_t$를 시작하여 하나의 trajectory의 rewards의 discounted sum을 표현하면 다음과 같습니다.<br>$$\\sum_{l=0}^\\infty \\gamma^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty \\gamma^l r(s_{t+l}, a_{t+l}, s_{t+l+1}) - \\Phi(s_t)$$</p>\n<p>$\\tilde{Q}^{\\pi, \\gamma}, \\tilde{V}^{\\pi, \\gamma}, \\tilde{A}^{\\pi, \\gamma}$를  transformed MDP의 value function과 advantage function이라고 하면, 다음과 같은 수식이 나옵니다.<br>$$\\tilde{Q}^{\\pi, \\gamma} (s, a) = Q^{\\pi, \\gamma} (s, a) - \\Phi (s)$$<br>$$\\tilde{V}^{\\pi, \\gamma} (s, a) = V^{\\pi, \\gamma} (s, a) - \\Phi (s)$$<br>$$\\tilde{A}^{\\pi, \\gamma} (s, a) = (Q^{\\pi, \\gamma} (s, a) - \\Phi (s)) - (V^\\pi (s) - \\Phi (s)) = A^{\\pi, \\gamma} (s, a)$$</p>\n<p>이제 reward shaping의 idea를 가지고 어떻게 policy gradient estimate를 얻을 수 있는 지에 대해서 알아봅시다.</p>\n<ul>\n<li>$0 \\le \\lambda \\le 1$의 범위에 있는 steeper discount $\\gamma \\lambda$를 사용합니다.</li>\n<li>shaped reward $\\tilde{r}$는 Bellman residual term $\\delta^V$와 동일합니다.</li>\n<li>그리고 $\\Phi = V$와 같다고 보면 다음과 같은 수식이 나옵니다.</li>\n</ul>\n<p>$$\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V = \\hat{A}_t^{GAE(\\gamma, \\lambda)}$$<br>이렇게 shaped rewards의 $\\gamma \\lambda$-discounted sum을 고려함으로써 GAE를 얻을 수 있습니다. 더 정확하게 $\\lambda = 1$은 $g^\\gamma$의 unbiased estimate이고, 반면에 $\\lambda &lt; 1$은 biased estimate입니다.</p>\n<p>좀 더 나아가서, shaping transformation과 parameters $\\gamma$ and $\\lambda$의 결과를 보기 위해, response function $\\chi$를 이용하면 다음과 같은 수식이 나옵니다.<br>$$\\chi (l; s_t, a_t) = \\mathbb{E} [r_{t+l} | s_t, a_t] - \\mathbb{E} [r_{t+l} | s_t]$$<br>추가적으로 $A^{\\pi, \\gamma} (s, a) = \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$입니다.<br>그래서 discounted policy gradient estimator는 다음과 같이 쓸 수 있다.<br>$$\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) A^{\\pi, \\gamma} (s_t, a_t) = \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$$</p>\n<p><br><br></p>\n<h1 id=\"6-Value-Fuction-Estimation\"><a href=\"#6-Value-Fuction-Estimation\" class=\"headerlink\" title=\"6. Value Fuction Estimation\"></a>6. Value Fuction Estimation</h1><p>이번 section에서는 Trust Region Optimization Scheme에 따라 Value function을 Estimation합니다.</p>\n<p><br></p>\n<h2 id=\"6-1-Simplest-approach\"><a href=\"#6-1-Simplest-approach\" class=\"headerlink\" title=\"6.1 Simplest approach\"></a>6.1 Simplest approach</h2><p>$$minimize_{\\phi} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi}(s_n) - \\hat{V_n} \\vert\\vert^{2}$$</p>\n<ul>\n<li>위는 가장 간단하게 non-linear approximation으로 푸는 방법입니다.</li>\n<li>$\\hat{V_t} = \\sum_{l=0}^{\\infty}\\gamma^l r_{t+l}$은 reward 에 대한 discounted sum을 의미합니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"6-2-Trust-region-method-to-optimize-the-value-function\"><a href=\"#6-2-Trust-region-method-to-optimize-the-value-function\" class=\"headerlink\" title=\"6.2 Trust region method to optimize the value function\"></a>6.2 Trust region method to optimize the value function</h2><ul>\n<li>Value function을 최적화 하기 위해 trust region method를 사용합니다.</li>\n<li>Trust region은 최근 데이터에 대해 overfitting되는 것을 막아줍니다.</li>\n</ul>\n<p>Trust region문제를 풀기 위해서는 다음 스텝을 따릅니다.</p>\n<ul>\n<li>$\\sigma^2 = \\frac{1}{N} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi old}(s_n) - \\hat{V_n} \\vert\\vert^{2}$을 계산합니다.</li>\n<li>그 후에 다음과 같은 constrained opimization문제를 풉니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/d74cg3votxi2dbg/Screen%20Shot%202018-08-22%20at%209.13.28%20PM.png?dl=1\" width=\"300\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/jrkjax71t3j3vk4/Screen%20Shot%202018-08-22%20at%209.13.35%20PM.png?dl=1\" width=\"370\"> </center>\n\n<ul>\n<li>위의 수식은 사실 old Value function과 새로운 Value function KL distance가 $\\epsilon$ 다 작아야한다는 수식과 같습니다. Value function이 평균은 $V_{\\phi}(s)$이고 분산이 $\\sigma^2$인 conditional Gaussian distribution으로 parameterize되었을 뿐입니다.</li>\n</ul>\n<p><img width=\"400px\" src=\"https://www.dropbox.com/s/uw0v05feu8chqmc/Screenshot%202018-07-08%2010.04.38.png?dl=1\"> </p>\n<p>이 trust region문제의 답을 conjudate gradient algorithm을 이용하여 근사값을 구할 수 있습니다. 특히, 다음과 같은  quadratic program을 풀게됩니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/pawjkm8b9ycwsl5/Screen%20Shot%202018-08-22%20at%209.10.36%20PM.png?dl=1\" width=\"250\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/va7wms8uqxnw5rq/Screen%20Shot%202018-08-22%20at%209.10.40%20PM.png?dl=1\" width=\"370\"> </center>\n\n<ul>\n<li>여기서 $g$는 objective 의 gradient입니다.</li>\n<li>$j_n = \\nabla_{\\phi} V_{\\phi}(s_n)$일때, $H = \\frac{1}{N} \\sum_{n} j_n j^T_n$이며, $H$는 objective의 hessian에 대해서 gaussian newton method로 근사한 값입니다. 따라서, value function을 conditional probability로 해석한다면 Fisher information matrix가 됩니다.</li>\n<li>구현할때의 방법은 TRPO에서 사용한 방법과 모두 같습니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"7-Experiments\"><a href=\"#7-Experiments\" class=\"headerlink\" title=\"7.Experiments\"></a>7.Experiments</h1><p>실험은 다음 두 가지 물음에 대해서 디자인 되었습니다.</p>\n<ul>\n<li>GAE에 따라서 episodic total reward를 최적화 할때, $\\lambda$와 $\\gamma$가 변함에 따라서 어떤 경험적인 효과를 볼 수 있는지?</li>\n<li>GAE와 trust region alogorithm을 policy와 value function 모두에 함께 사용했을 때 어려운 문제에 적용되는 큰 뉴럴넷을 최적화 할 수 있을지?</li>\n</ul>\n<p><br></p>\n<h2 id=\"7-1-Policy-Optimization-Algorithm\"><a href=\"#7-1-Policy-Optimization-Algorithm\" class=\"headerlink\" title=\"7.1 Policy Optimization Algorithm\"></a>7.1 Policy Optimization Algorithm</h2><p>Policy update는 TRPO를 사용합니다. TRPO에 대한 설명은 여기서는 생략하겠습니다. TRPO 포스트를 보고 돌아와주세요!</p>\n<ul>\n<li><p>이전 TRPO에서 이미 TRPO와 다른 많은 알고리즘들을 비교하였기 때문에, 여기서는 똑같은 짓을 반복하지 않고 $\\lambda$, $\\gamma$가 변함에 따라 어떤 영향이 있는 지에 대한 실험에 집중하겠다고 합니다. (귀찮았던거죠..)</p>\n</li>\n<li><p>TRPO를 적용한 GAE의 최종 알고리즘은 다음과 같습니다.</p>\n</li>\n</ul>\n<center> <img width=\"600px\" src=\"https://www.dropbox.com/s/b1klz11f2frrvg4/Screenshot%202018-07-08%2010.33.54.png?dl=1\"> </center><br><center> <img width=\"300px\" src=\"https://www.dropbox.com/s/35fxte05pqtiel7/Screenshot%202018-07-08%2010.35.22.png?dl=1\"> </center><br><center> <img width=\"300px\" src=\"https://www.dropbox.com/s/u35pjow3w50bvz1/Screenshot%202018-07-08%2010.36.03.png?dl=1\"> </center>\n\n<ul>\n<li>여기서 주의할 점은 Policy update($\\theta_i \\rightarrow \\theta_{i+1}$)에서 $V_{\\phi_i}$를 사용했다는 점입니다.</li>\n<li>만약 Value function을 먼저 update하게 된다면 추가적인 bias가 발생합니다.</li>\n<li>극단적으로 생각해보아서, 우리가 Value function을 완벽하게 overfit을 해낸다면 Bellman residual($r_t + \\gamma V(s_{t+1}) - V(S_t)$)은 0이 됩니다. 그럼 Policy gradient의 estimation도 거의 0이 될 것입니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"7-2-Expermint-details\"><a href=\"#7-2-Expermint-details\" class=\"headerlink\" title=\"7.2 Expermint details\"></a>7.2 Expermint details</h2><h3 id=\"7-2-1-Environment\"><a href=\"#7-2-1-Environment\" class=\"headerlink\" title=\"7.2.1 Environment\"></a>7.2.1 Environment</h3><p>실험에서 사용된 환경은 다음 네 가지 입니다.</p>\n<ol>\n<li>classic cart-pole (x 3D)</li>\n<li>bipedal locomotion</li>\n<li>quadrupedal locomotion</li>\n<li>dynamically standing up for the biped</li>\n</ol>\n<h3 id=\"7-2-2-Architecture\"><a href=\"#7-2-2-Architecture\" class=\"headerlink\" title=\"7.2.2 Architecture\"></a>7.2.2 Architecture</h3><ul>\n<li>3D robot task에 대해서는 같은 모델을 사용하였습니다.<ul>\n<li>layers  = [100, 50, 25] 각각 tanh 사용. (Policy와 Value 네트워크 모두)</li>\n<li>Final output layer은 linear</li>\n</ul>\n</li>\n<li>Cartpole에 대해서는 1개의 layer 안에 20개의 hidden unit만 있는 linear policy를 사용했다고 합니다.</li>\n</ul>\n<h3 id=\"7-2-3-Task\"><a href=\"#7-2-3-Task\" class=\"headerlink\" title=\"7.2.3 Task\"></a>7.2.3 Task</h3><ul>\n<li>Cartpole<ul>\n<li>한 배치당 20 개의 trajectory를 모았고, maximum length는 1000입니다.</li>\n</ul>\n</li>\n<li>3D biped locomotion<ul>\n<li>33 dim state , 10 dim action</li>\n<li>50000 time step per batch</li>\n</ul>\n</li>\n<li>3D quadruped locomotion<ul>\n<li>29 dim state, 8 dim action</li>\n<li>200000 time step per batch</li>\n</ul>\n</li>\n<li>3D biped locomotion Standing<ul>\n<li>33 dim state , 10 dim action</li>\n<li>200000 time step per batch</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"7-2-3-results\"><a href=\"#7-2-3-results\" class=\"headerlink\" title=\"7.2.3 results\"></a>7.2.3 results</h3><p>cost의 관점에서 결과를 나타내었다고 합니다. Cost는 negative reward와 이것이 최소화 되었는가로 정의되었습니다.</p>\n<h4 id=\"7-2-3-1-Cartpole\"><a href=\"#7-2-3-1-Cartpole\" class=\"headerlink\" title=\"7.2.3.1 Cartpole\"></a>7.2.3.1 Cartpole</h4><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/x9pbms1wvg38lda/Screenshot%202018-07-08%2011.08.22.png?dl=1\"> </center>\n\n<ul>\n<li>왼쪽 그림은 $\\gamma$를 0.99로 고정시켜놓은 상태에서 $\\lambda$를 변화시킴에 따라서 cost를 측정한 것입니다. </li>\n<li>오른쪽은 $\\gamma$와 $\\lambda$를 둘 다 변화 시키면서 성능을 그림으로 나타낸 표입니다. 흰색에 가까울 수록 좋은 성능입니다. </li>\n</ul>\n<h4 id=\"7-2-3-2-3D-BIPEDAL-LOCOMOTINO\"><a href=\"#7-2-3-2-3D-BIPEDAL-LOCOMOTINO\" class=\"headerlink\" title=\"7.2.3.2 3D BIPEDAL LOCOMOTINO\"></a>7.2.3.2 3D BIPEDAL LOCOMOTINO</h4><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/i9wj4p6ijojsy82/Screenshot%202018-07-08%2011.25.08.png?dl=1\"> </center>\n\n\n<ul>\n<li>다른 random seed로 부터 9번 씩 시도한 결과를 mean을 취해서 사용합니다.</li>\n<li>Best performance는   $\\gamma \\in [0.99, 0.995]$ 그리고  $\\lambda \\in [0.96, 0.99]$일때. </li>\n<li>1000 iteration 후에 빠르고 부드럽고 안정적인 걸음거이가 나옵니다.</li>\n<li>실제로 걸린 시간은 0.01(타입스텝당 시간) <em> 50000(배치당 타임스텝) </em> 1000(배치) <em> 3600(초-&gt;시간) </em> 24 = 5.8일 정도가 걸렸습니다.</li>\n</ul>\n<h4 id=\"7-2-3-3-다른-ROBOT-TASKS\"><a href=\"#7-2-3-3-다른-ROBOT-TASKS\" class=\"headerlink\" title=\"7.2.3.3 다른 ROBOT TASKS\"></a>7.2.3.3 다른 ROBOT TASKS</h4><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/fuuat65we52quht/Screenshot%202018-07-08%2011.33.11.png?dl=1\"> </center>\n\n<ul>\n<li>다른 로봇 TASK에 대해서는 아주 제한적인 실험만 진행합니다.(시간이 부족했던듯 하네요..)</li>\n<li>Quadruped에 대해서는 $\\gamma = 0.995$로 fix, $\\lambda \\in {0, 0.96}$</li>\n<li>Standingup에 대해서는 $\\gamma = 0.99$로 fix, $\\lambda \\in {0, 0.96}$</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"8-Discussion\"><a href=\"#8-Discussion\" class=\"headerlink\" title=\"8. Discussion\"></a>8. Discussion</h1><p><br></p>\n<h2 id=\"8-1-Main-discussion\"><a href=\"#8-1-Main-discussion\" class=\"headerlink\" title=\"8.1 Main discussion\"></a>8.1 Main discussion</h2><p>지금까지 복잡하고 어려운 control problem에서 Reinforcement Learning(RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 “variance reduction”에 대해 연구하였습니다.</p>\n<p>“Generalized Advantage Estimator(GAE)”라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.<br>또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는 지를 보였습니다.</p>\n<p>이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.</p>\n<p>GAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.</p>\n<p><br></p>\n<h2 id=\"8-2-Future-work\"><a href=\"#8-2-Future-work\" class=\"headerlink\" title=\"8.2 Future work\"></a>8.2 Future work</h2><p>Value function estimation error와 Policy gradient estimation error사이의 관계를 알아낸다면, 우리는 Value function fitting에 더 잘 맞는 error metric(policy gradient estimation 의 정확성과 더 잘 맞는 value function)을 사용할 수 있습니다. 여기서 Policy와 Value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.</p>\n<p>추가적으로 DDPG는 별로라고 합니다. 그리고 TD(0)는 bias가 너무 크고, poor performance로 이끈다고 합니다. 특히나 이 논문에서는 low-dimention의 쉬운 문제들만 해결했습니다.</p>\n<center> <img width=\"500px\" src=\"https://www.dropbox.com/s/nhc7t9psul5lr3x/Screenshot%202018-07-08%2011.45.15.png?dl=1\"> </center>\n\n<p><br></p>\n<h2 id=\"8-3-FAQ\"><a href=\"#8-3-FAQ\" class=\"headerlink\" title=\"8.3 FAQ\"></a>8.3 FAQ</h2><ul>\n<li>Compatible features와는 무슨 관계?<ul>\n<li>Compatible features는 value function을 이용하는 policy gradient 알고리즘들과 함께 자주 언급됩니다.</li>\n<li>Actor Critic의 저자는 policy의 제한된 representation power때문에, policy gradient는 단지 advantage function space의 subspace에만 의존하게 됩니다.</li>\n<li>이 subspace는 compatible features에 의해 span됩니다.</li>\n<li>이 이론은 현재 문제 구조를 어떻게 이용해야 advantage function에 대해 더 나은 estimation을 할 수 있는 지에 대한 지침을 주지 않습니다. GAE 논문의 idea와 orthogonal합니다.</li>\n</ul>\n</li>\n<li>왜 Q function을 사용하지 않는가?<ul>\n<li>먼저 state-value function이 더 낮은 차원의 input을 가진다. 그래서 Q function보다 더 학습하기가 쉽습니다.</li>\n<li>두 번째로 이 논문에서 제안하는 방법으로는 high bias estimator에서 low bias estimator로 $\\lambda$를 통해서 부드럽게 interpolate를 할 수 있습니다.</li>\n<li>반면에 Q를 사용하면 단지 high-bias estimator 밖에 사용할 수 없습니다.</li>\n<li>특히나 return에 대한 one-step estimation은 엄두를 못낼 정도로 bias가 큽니다.</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"TRPO-여행하기\"><a href=\"#TRPO-여행하기\" class=\"headerlink\" title=\"TRPO 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/\">TRPO 여행하기</a></h2><h2 id=\"TRPO-Code\"><a href=\"#TRPO-Code\" class=\"headerlink\" title=\"TRPO Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"PPO-여행하기\"><a href=\"#PPO-여행하기\" class=\"headerlink\" title=\"PPO 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/\">PPO 여행하기</a></h2>","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"../../../../img/irl/vail_1.png\" width=\"850\"> </center>\n\n<p>논문 저자 : John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan and Pieter Abbeel<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1810.00821.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1810.00821.pdf</a><br>Proceeding : International Conference of Learning Representations (ICLR) 2019</p>\n<hr>\n<h1 id=\"0-Abstract\"><a href=\"#0-Abstract\" class=\"headerlink\" title=\"0. Abstract\"></a>0. Abstract</h1><p>현존하는 Policy Gradient Method들의 목적은 누적되는 reward들을 optimization하는 것입니다. 하지만 학습할 때에 많은 양의 sample이 필요로 하고, 들어오는 data가 nonstationarity임에도 불구하고 stable and steady improvement가 어렵습니다.</p>\n<p>그래서 이 논문에서는 다음과 같은 방법을 제시합니다.</p>\n<ul>\n<li>TD($\\lambda$)와 유사한 advantage function의 exponentially-weighted estimator를 사용하여 policy gradient estimate의 variance를 줄이는 것 </li>\n<li>policy와 value function에 대한 Trust Region Optimization 사용하는 것</li>\n</ul>\n<p>3D locomotion tasks에 대한 empirical results는 다음과 같습니다.</p>\n<ul>\n<li>bipedal and quadrupedal simulated robots의 달리는 자세를 학습</li>\n<li>bipedal 사람이 땅에 누워있다가 일어서는 것을 학습</li>\n</ul>\n<p>※ TD($\\lambda$)를 혹시 모르실 경우도 있을 것 같아 간략하게 정리하였습니다. <a href=\"http://dongminlee.tistory.com/10\" target=\"_blank\" rel=\"noopener\">http://dongminlee.tistory.com/10</a> 를 먼저 보시고 아래의 내용을 봐주시기 바랍니다!</p>\n<p><br><br></p>\n<h1 id=\"2-Introduction\"><a href=\"#2-Introduction\" class=\"headerlink\" title=\"2. Introduction\"></a>2. Introduction</h1><p>기본적으로 “parameterized stochastic policy”를 가정합니다. 이 때 expected total returns의 gradient에 대한 unbiased estimate를 얻을 수 있는데 이것을 REINFORCE라고 부릅니다. 하지만 하나의 action의 결과가 과거와 미래의 action의 결과로 혼동되기 때문에 gradient estimator의 high variance는 시간에 따라 scaling됩니다.</p>\n<p>또 다른 방법은 Actor-Critic이 있습니다. 이 방법은 empirical returns보다 하나의 value function을 사용합니다. 또한 bias하고 lower variance를 가진 estimator입니다. 구체적으로 말하자면, high variance하다면 더 sampling을 하면 되는 반면에 bias는 매우 치명적입니다. 다시 말해 bias는 algorithm이 converge하는 데에 실패하거나 또는 local optimum이 아닌 poor solution에 converge하도록 만듭니다.</p>\n<p>따라서 이 논문에서는 $\\gamma\\in [0,1]$ and $\\lambda\\in [0,1]$에 의해 parameterized estimation scheme인 Generalized Advantage Estimator(GAE)를 다룹니다.</p>\n<p>이 논문을 대략적으로 요약하자면 다음과 같습니다.</p>\n<ul>\n<li>효과적인 variance reduction scheme인 GAE를 다룹니다. 또한 실험할 때 batch trust region algorithm에 더하여 다양한 algorithm들에 적용됩니다.</li>\n<li>value function에 대해 trust region optimization method를 사용합니다. 이렇게 함으로서 더 robust하고 efficient한 방법이 됩니다.</li>\n<li>A와 B를 합쳐서, 실험적으로 control task에 neural network policies를 learning하는 데에 있어서 효과적인 algorithm을 얻습니다. 이러한 결과는 high-demensional continuous control에 RL을 사용함으로서 state of the art로 확장되었습니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"3-Preliminaries\"><a href=\"#3-Preliminaries\" class=\"headerlink\" title=\"3. Preliminaries\"></a>3. Preliminaries</h1><p>먼저 policy optimization의 “undiscounted formulation”을 가정합니다. (undiscounted formulation에 주목합시다.)</p>\n<ul>\n<li>initial state $s_0$는 distribution $\\rho_0$으로부터 sampling된 것입니다.</li>\n<li>하나의 trajectory ($s_0, a_0, s_1, a_1, …$)는 terminal state에 도달될 때 까지 policy $a_t$ ~ $\\pi(a_t | s_t)$에 따라서 action을 sampling하고, dynamics $s_{t+1}$ ~ $P(s_{t+1} | s_t, a_t)$에 따라서 state를 sampling함으로써 생성됩니다.</li>\n<li>reward $r_t = r(s_t, a_t, s_{t+1})$은 매 time step마다 받아집니다.</li>\n<li>목표는 모든 policies에 대해 finite하다고 가정됨으로서 expected total reward $\\sum_{t=0}^{\\infty} r_t$를 maximize하는 것입니다.</li>\n</ul>\n<p>여기서 중요한 점은 $\\gamma$를 discount의 parameter로 사용하는 것이 아니라 “bias-variance tradeoff를 조절하는 parameter로 사용한다” 는 것입니다.</p>\n<p>policy gradient method는 gradient $g := \\nabla_\\theta \\mathbb{E} [\\sum_{t=0}^\\infty r_t]$를 반복적으로 estimate함으로써 expected total reward를 maximize하는 것인데, policy gradient에는 여러 다른 표현들이 있습니다.<br>$$g = \\mathbb{E} [\\sum_{t=0}^\\infty \\Phi_t \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$</p>\n<p>여기서 $\\Phi_t$는 아래 중의 하나일 수 있습니다.</p>\n<ol>\n<li>$\\sum_{t=0}^\\infty r_t$: trajectory의 total reward</li>\n<li>$\\sum_{t’=t}^\\infty r_t’$: action $a_t$ 후의 reward</li>\n<li>$\\sum_{t’=t}^\\infty r_t’ - b(s_t)$: 2의 baselined version</li>\n<li>$Q^\\pi (s_t, a_t)$: state-action value function</li>\n<li>$A^\\pi (s_t, a_t)$: advantage function</li>\n<li>$r_t + V^\\pi (s_{t+1}) - V^\\pi (s_t)$: TD residual</li>\n</ol>\n<p>위의 번호 중 4, 5, 6의 수식들은 다음의 정의를 사용합니다.</p>\n<ul>\n<li>$V^\\pi (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty r_{t+1}]$</li>\n<li>$Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty r_{t+l}]$</li>\n<li>$A^\\pi (s_t, a_t) := Q^\\pi (s_t, a_t) - V^\\pi (s_t)$, (Advantage function)</li>\n</ul>\n<p>추가적으로 colon notation $a : b$는 포괄적인 범위 $(a, a+1, … , b)$입니다. (잘 기억해둡시다. 뒤에 계속해서 colon notation이 나옵니다.)</p>\n<p>여기서부터 parameter $\\gamma$에 대해 좀 더 자세히 알아봅시다. parameter $\\gamma$는 bias하면서 동시에 reward를 downweighting함으로써 variance를 줄입니다. 다시 말해 MDP의 discounted formulation에서 사용된 discounted factor와 일치하지만, 이 논문에서는 “$\\gamma$를 undiscounted MDP에서 variance reduction parameter”로 다룹니다. -&gt; 결과는 같지만, 의미와 의도가 다릅니다.</p>\n<p>discounted value function들은 다음과 같습니다.</p>\n<ul>\n<li>$V^{\\pi, \\gamma} (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$ </li>\n<li>$Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$</li>\n<li>$A^{\\pi, \\gamma} (s_t, a_t) := Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)$</li>\n</ul>\n<p>따라서 policy gradient에서의 discounted approximation은 다음과 같이 나타낼 수 있습니다.<br>$$g^\\gamma := \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\sum_{t=0}^\\infty A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$<br>뒤이어 나오는 section에서는 위의 수식에서 $A^{\\pi, \\gamma}$에 대해 biased (but not too biased) estimator를 얻는 방법에 대해서 나옵니다.</p>\n<p>다음으로 advantage function에서 새롭게 접하는 “$\\gamma$-just estimator”의 notation에 대해 알아봅시다.</p>\n<ul>\n<li>먼저 $\\gamma$-just estimator는 $g^\\gamma$ ${}^1$를 estimate하기 위해 위의 수식에서 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 사용할 때, bias하지 않은 estimator라고 합시다. 그리고 이 $\\gamma$-just advantage estimator를 <img src=\"https://www.dropbox.com/s/7g2glqx2kbfynaa/Screen%20Shot%202018-08-22%20at%209.03.46%20PM.png?dl=1\" width=\"100\"> 라고 하고, 전체의 trajectory에 대한 하나의 function이라고 합시다.</li>\n<li>${}^1$에서 이 논문의 저자가 하고 싶은 말이 있는 것 같습니다. 개인적으로 $\\gamma$-just estimator를 이해하는 데에 있어서 중요한 주석이라 정확히 해석하고자 합니다.<ul>\n<li>$A^\\pi$를 $A^{\\pi, \\gamma}$로 사용함으로써 이미 bias하다라고 말했지만, “이 논문에서 하고자 하는 것은 $g^\\gamma$에 대해 unbiased estimate를 얻고 싶은 것입니다.” 하지만 undiscounted MDP의 policy gradient에 대해서는 당연히 $\\gamma$를 사용하기 때문에 biased estimate를 얻습니다. 개인적으로 이것은 일단 무시하고 $\\gamma$를 사용할 때 어떻게 unbiased estimate를 얻을 지에 대해 좀 더 포커스를 맞추고 있는 것 같습니다.</li>\n<li>그러니까 저 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 바꿔줌으로써 unbiased estimate를 하고 싶다는 것이 뒤이어 나오는 정의와 명제의 핵심이라고 할 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n<p>Definition 1.<br>먼저 가정을 합니다. (가정을 바탕으로 이루어지는 정의라는 것을 주목합시다.) 만약</p>\n<center><img src=\"https://www.dropbox.com/s/aia9f1l99wyxgah/Screen%20Shot%202018-08-22%20at%209.07.05%20PM.png?dl=1\" width=\"700\"> </center><br>두 수식이 같다면, estimator $\\hat{A}_t$는 $\\gamma$-just입니다.<br><br>그리고 만약 모든 t에 대해서 $\\hat{A}_t$이 $\\gamma$-just이라면, 다음과 같이 표현할 수 있습니다.<br><br><center><img src=\"https://www.dropbox.com/s/r4m6ktw8geeu7pe/Screen%20Shot%202018-08-22%20at%209.29.18%20PM.png?dl=1\" width=\"420\"> </center>\n\n<p>위의 수식이 바로 unbiased estimate입니다.</p>\n<p>$\\gamma$-just인 $\\hat{A}_t$에 대한 한가지 조건은 $\\hat{A}_t$이 두 가지 function $Q_t$ and $b_t$로 나뉠 수 있다는 것입니다.</p>\n<ul>\n<li>$Q_t$는 $\\gamma$-discounted Q-function의 unbiased estimator입니다.</li>\n<li>$b_t$는 action $a_t$전에 sampling된 states and actions의 arbitrary function이다. </li>\n</ul>\n<p>Proposition 1.<br>모든 $(s_t, a_t)$에 대해,<br>$$\\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty | s_t, a_t} [Q_t (s_{t:\\infty}, a_{t:\\infty})] = Q^{\\pi, \\gamma} (s_t, a_t)$$<br>로 인하여 $\\hat{A}_t$이 </p>\n<center> <img src=\"https://www.dropbox.com/s/59ydsav6djwyevl/Screen%20Shot%202018-08-22%20at%209.27.13%20PM.png?dl=1\" width=\"380\"> </center>\n\n<p>형태라고 가정합시다. (가정을 바탕으로 이루어지는 명제라는 점을 주목합시다.)</p>\n<p>그 때, $\\hat{A}_t$은 $\\gamma$-just입니다.</p>\n<p>이 명제에 대한 증명은 Appendix B에 있습니. 그리고 $\\hat{A}_t$에 대한 $\\gamma$-just advantage estimator들은 다음과 같습니다.</p>\n<ul>\n<li>$\\sum_{l=0}^\\infty \\gamma^l r_{t+1}$</li>\n<li>$A^{\\pi, \\gamma} (s_t, a_t)$</li>\n<li>$Q^{\\pi, \\gamma} (s_t, a_t)$</li>\n<li>$r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)$</li>\n</ul>\n<p>증명을 보기 전에 먼저 Advantage function에 대해서 먼저 살펴봅시다. 아래의 내용은 Sutton이 쓴 논문인 Policy Gradient Methods for Reinforcement Learning with Function Approximation(2000)에서 나온 내용을 리뷰한 것입니다. </p>\n<center> <img src=\"https://www.dropbox.com/s/x737yq97ut6gp1a/Screen%20Shot%202018-07-15%20at%201.16.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n<p>여기서는 “이 수식은 $\\sum_a \\frac{\\partial \\pi (s,a)}{\\partial \\theta} = 0$이기 때문에 가능해진다.”, “즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없다”라는 두 문장을 기억해둡시다.</p>\n<p>이제 증명을 보면 됩니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/e7sj2fwcm1f7hof/figure2.jpg?dl=1\" width=\"600\"> </center><br><center> <img src=\"https://www.dropbox.com/s/kyk4jes1202az4m/figure3.jpg?dl=1\" width=\"600\"> </center>\n\n<p>위의 증명 수식에서 Q와 b의 각각 세번째 수식과 마지막 수식을 자세히 봅시다.</p>\n<ol>\n<li><p>$Q$에 대한 증명</p>\n<ul>\n<li><p>빨간색 부분</p>\n<ul>\n<li>$\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [Q_t (s_{0:\\infty}, a_{0:\\infty})]$를 쉽게 말하자면 “모든(과거, 현재, 미래) state와 action에 대해서 expectation을 $t+1$부터 $\\infty$까지 하겠다.”라는 말입니다. 이렇게 함으로써 원래는 $Q^\\pi (s_t, a_t)$가 나와야 맞는 건데, 앞에서 살펴봤다시피 4번째 수식 밑줄 친 자리에는 advantage 역할을 하는 함수들을 넣어도 결과값에 아무런 영향을 끼치지 않기 때문에 의도적으로 $A^\\pi (s_t, a_t)$로 바꾼 것입니다. 또한 이 증명을 바탕으로 $\\hat{A}$이 $\\gamma$-just라는 것을 표현하고 싶기 때문이라고 봐도 됩니다.</li>\n</ul>\n</li>\n<li><p>파란색 부분</p>\n<ul>\n<li>또한 $a_{0:t}$에서 $a_{0:t-1}$로 변한 것은 $A^\\pi (s_t, a_t)$에 baseline이 들어가기 때문에 바꿔준 것입니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>$b$에 대한 증명</p>\n<ul>\n<li>빨간색 부분<ul>\n<li>$\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$가 0으로 바뀌는 것은 위에서 설명했듯이 $\\nabla_\\theta$자체가 $\\log \\pi_\\theta$만 gradient하기 때문에 이것을 expectation을 취하면 0이 됩니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p><br><br></p>\n<h1 id=\"4-Advantage-Function-Estimation\"><a href=\"#4-Advantage-Function-Estimation\" class=\"headerlink\" title=\"4. Advantage Function Estimation\"></a>4. Advantage Function Estimation</h1><p>이번 section에는 discounted advantage function $A^{\\pi, \\gamma} (s_t, a_t)$의 accurate estimate $\\hat{A}_t$에 대해서 살펴봅시다. 이에 따른 수식은 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/sn16g5iyzzflju0/Screen%20Shot%202018-08-22%20at%209.22.49%20PM.png?dl=1\" width=\"300\"> </center>\n\n<p>여기서 n은 a batch of episodes에 대하여 index한 것입니다.</p>\n<p>$V$를 approximate value function이라고 합시다. 그리고 $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$이라고 합시다.</p>\n<p>만약 (이전과 마찬가지로 가정부터 합니다.) correct value function $V = V^{\\pi, \\gamma}$가 있다고 한다면, 이것은 $\\gamma$-just advantage estimator입니다. 실제로, $A^{\\pi, \\gamma}$의 unbiased estimator는 다음과 같습니다.<br>$$\\mathbb{E}_{s_{t+1}} [\\delta_t^{V^{\\pi, \\gamma}}] = \\mathbb{E}_{s_{t+1}} [r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)]$$<br>$$= \\mathbb{E}_{s_{t+1}} [Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)] = A^{\\pi, \\gamma} (s_t, a_t)$$<br>그러나, “이 estimator는 유일하게 $V = V^{\\pi, \\gamma}$에 대한 $\\gamma$-just입니다.” 다른 경우라면 이것은 biased policy gradient estimate일 것입니다. (우리가 하고 싶은 것은 $V$에 대해서만 unbiased estimator가 아니라 advantage function에 대해서 일반화된 unbiased estimator를 얻고 싶은 것입니다. 그래서 아래에서도 나오겠지만, $\\gamma$와 함께 $\\lambda$를 이용한 estimator가 나옵니다.ㅏ)</p>\n<p>그래서 $\\delta$에 대해 $k$의 sum으로 생각해봅시다. 이것을 $\\hat{A}_t^{(k)}$라고 하자. 그러면 아래와 같이 표현할 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ra7hxksveg2hz45/figure4.jpg?dl=1\" width=\"600\"> </center>\n\n<p>$\\hat{A}_t^{(k)}$은 returns의 $k$-step estimate와 연관지을 수 있고, $\\delta_t^V = \\hat{A}_t^{(1)}$의 case와 유사하게도 $\\hat{A}_t^{(k)}$를 advantage function의 estimator로 생각할 수 있습니다.</p>\n<p>여기서 $k \\rightarrow \\infty$로 생각해보면 bias가 일반적으로 점점 더 작아집니다. 왜냐하면 $\\gamma^k V(s_{t+k})$가 점점 많이 discounted되서 $-V(s_t)$가 bias에 영향을 미치지 못하기 때문입니다. $k \\rightarrow \\infty$를 취하면 다음과 같은 수식이 나옵니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/13fn9wcup8pfh9u/Screen%20Shot%202018-08-22%20at%209.19.52%20PM.png?dl=1\" width=\"320\"> </center>\n\n<p>우변의 수식과 같이 empirical returns에서 value function baseline을 뺀 것으로 나타낼 수 있습니다.</p>\n<p>Generalized Advantage Estimator GAE($\\gamma, \\lambda$)는 $k$-step estimators의 exponentially-weighted average로 나타낼 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/yg1ybmfkep3towu/figure5.jpg?dl=1\" width=\"600\"> </center>\n\n<p>(TD($\\lambda$)를 떠올려주세요..!)</p>\n<p>위의 마지막 수식에서도 알 수 있듯이, advantage estimator는 Bellman residual terms의 discounted sum과 관련있는 간단한 수식입니다. 다음 section에서 modified reward function을 가진 MDP에 returns로서의 관점에서 위의 마지막 수식을 더 자세히 살펴봅시다. 위의 수식은 TD($\\lambda$)와 많이 유사합니다. 그러나 TD($\\lambda$)는 value function를 estimator하고, 여기서는 advantage function을 estimator합니다.</p>\n<p>위의 수식에서 $\\lambda = 0$ and $\\lambda = 1$에 대해서는 특별한 case가 존재한다. 수식으로 표현하면 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ufglxzanhcnbi36/figure6.jpg?dl=1\" width=\"600\"> </center>\n\n<ul>\n<li>$GAE(\\gamma, 1)$은 $V$의 정확도와 관계없이 $\\gamma$-just입니다. 그러나 returns의 sum때문에 high variance합니다.</li>\n<li>$GAE(\\gamma, 0)$은 $V = V^{\\pi, \\gamma}$에 대해 $\\gamma$-just입니다. 그리고 bias하지만 일반적으로 훨씬 lower variance를 가집니다. </li>\n<li>$0 &lt; \\lambda &lt; 1$에 대해 GAE는 parameter $\\lambda$를 control하여 bias와 variance사이에 compromise를 만듭니다.</li>\n</ul>\n<p>두 가지 별개의 parameter $\\gamma$ and $\\lambda$를 가진 advantage estimator는 bias-variance tradeoff에 도움을 줍니다. 그러나 이 두 가지 parameter는 각각 다른 목적을 가지고 작동합니다.</p>\n<ul>\n<li>$\\gamma$는 가장 중요하게 value function $V^{\\pi, \\gamma}$ 의 scale을 결정합니다. 또한 $\\gamma$는 $\\lambda$에 의존하지 않습니다. $\\gamma &lt; 1$로 정하는 것은 policy gradient estimate에서 bias합니다.</li>\n<li>반면에, $\\lambda &lt; 1$는 유일하게 value function이 부정확할 때 bias합니다. 그리고 경험상, $\\lambda$의 best value는 $\\gamma$의 best value보다 훨씬 더 낮습니다. 왜냐하면 $\\lambda$가 정확한 value function에 대해 $\\gamma$보다 훨씬 덜 bias하기 때문입니다.</li>\n</ul>\n<p>GAE를 사용할 때, $g^\\gamma$의 biased estimator를 구성할 수 있습니다. 수식은 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ok57dsk52o7pl9r/Screen%20Shot%202018-08-22%20at%209.17.54%20PM.png?dl=1\" width=\"670\"> </center>\n\n<p>여기서 $\\lambda = 1$일 때 동일해집니다.</p>\n<p><br><br></p>\n<h1 id=\"5-Interpretation-as-Reward-Shaping\"><a href=\"#5-Interpretation-as-Reward-Shaping\" class=\"headerlink\" title=\"5. Interpretation as Reward Shaping\"></a>5. Interpretation as Reward Shaping</h1><p>이번 section에서는 앞서 다뤘던 수식 $\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V$를 modified reward function의 MDP의 관점으로 생각해봅시다. 조금 더 구체적으로 말하자면, MDP에서 reward shaping transformation을 실행한 후에 적용된 extra discounted factor로서 $\\lambda$를 어떻게 볼 것인지에 대해서 다룹니다.</p>\n<p>(개인적인 comment) 한 가지 먼저 언급하자면, 본래의 목적은 reward shaping이 아니라 variance reduction입니다. 이번 section은 그저 이전에 이러한 개념이 있었고, gae를 다른 관점에서 생각해보자라는 뜻에서 나온 section인 것 같습니다. ‘아~ 이러한 개념이 있구나~!’ 정도로만 알면 될 것 같습니다. ‘gae가 reward shaping의 효과까지 있어서 이러한 section을 넣은 것일까?’ 라는 생각도 해봤지만 아직 잘 모르겠습니다. 실험부분에도 딱히 하지 않은 걸로 봐서는.. 아닌 것 같기도 하고.. 아직까지는 그저 ‘reward shaping의 관점에서 봤을 때에도 gae로 만들어줄 수 있다.’ 정도만 생각하려고 합니다.</p>\n<p>Reward Shaping이란 개념 자체는 일반적인 알고리즘 분야(최단경로 문제 등)에도 있습니다. 하지만 이 개념이 머신러닝에 적용된 건 Andrew Y. Ng이 쓴 Policy Invariance under Reward Transformations Theory and Application to Reward Shaping(1999) 논문입니다. 논문에서 Reward Shaping을 설명하는 부분은 간략하게 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/0w9cuxjkhz2mbcy/figure7.jpg?dl=1\" width=\"400\"> </center><br><center> <img src=\"https://www.dropbox.com/s/xcp9eqvcbv3268g/figure8.jpg?dl=1\" width=\"400\"> </center>\n\n<p>위의 글만 보고 이해가 잘 안됩니다. 그래서 다른 자료들을 찾던 중에 다음의 그림을 찾게 되었습니다. (아래의 그림은 Youtube에 있는 <a href=\"https://www.youtube.com/watch?v=xManAGjbx2k&amp;t=95s\" target=\"_blank\" rel=\"noopener\">Udacity 영상</a>에서 있는 그림입니다.)</p>\n<center> <img src=\"https://www.dropbox.com/s/87kacngez9cl2e6/figure9.jpg?dl=1\" width=\"400\"> </center>\n\n<p>이 그림을 통해서 Reward Shaping을 이해해봅시다.</p>\n<p>Reward Shaping(보상 형성)을 통해서 뭘 하고 싶은지 목적부터 봅시다. reward space가 sparse 한 경우에 reward가 너무 드문드문 나옵니다. 따라서 이것을 꾸준히 reward를 받을 수 있도록 바꿉니다. potential-based shaping function인 $\\Phi$를 만들어서 더하고 빼줍니다. 저 $\\Phi$ 자리에는 대표적으로 state value function이 많이 들어간다고 합니다. (아직 목적이 이해가 안될 수 있습니다. Reward Shaping에 대해서 다 보고 나서 다시 한 번 읽어봅시다.)</p>\n<p>위의 그림은 돌고래가 점프하여 불구멍을 통과해야하는 환경입니다. 하지만 저 불구멍을 통과하기 위해서는 (1) 점프도 해야하고, (2) 불도 피해야하고, (3) 알맞게 착지까지 완료해야합니다. 이렇게 해야 reward를 +1 얻습니다. </p>\n<p>생각해봅시다. 어느 세월에 점프도 해야하고.. 불도 피해야하고.. 알맞게 착지까지해서 reward를 +1 얻겠습니까? 따라서 그 전에도 잘하고 있는 지, 못하고 있는 지를 판단하기 위해, 다시 말해 reward를 꾸준히 받도록 다음과 같이 transformed reward function $\\tilde{r}$을 정의합니다.</p>\n<p>$$\\tilde{r} (s, a, s’) = r(s, a, s’) + \\gamma \\Phi (s’) - \\Phi (s)$$</p>\n<ul>\n<li>여기서 $\\Phi : S \\rightarrow \\mathbb{R}$를 state space에서의 arbitrary scalar-valued function을 나타냅니다. 그리고 $\\Phi$자리에는 대표적으로 state value function이 들어간다고 생각합시다.</li>\n<li>형태가 TD residual term의 결과와 비슷하지만 의미와 의도가 다릅니다. reward shaping은 sparse reward 때문이고, 이전 section에서 봤던 gae는 variance reduction때문에 나온 것입니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/tkfpqniaazwfbld/figure10.jpg?dl=1\" width=\"600\"> </center>\n\n<p>이 transformation은 discounted advantage function $A^{\\pi, \\gamma}$으로도 둘 수 있습니다. state $s_t$를 시작하여 하나의 trajectory의 rewards의 discounted sum을 표현하면 다음과 같습니다.<br>$$\\sum_{l=0}^\\infty \\gamma^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty \\gamma^l r(s_{t+l}, a_{t+l}, s_{t+l+1}) - \\Phi(s_t)$$</p>\n<p>$\\tilde{Q}^{\\pi, \\gamma}, \\tilde{V}^{\\pi, \\gamma}, \\tilde{A}^{\\pi, \\gamma}$를  transformed MDP의 value function과 advantage function이라고 하면, 다음과 같은 수식이 나옵니다.<br>$$\\tilde{Q}^{\\pi, \\gamma} (s, a) = Q^{\\pi, \\gamma} (s, a) - \\Phi (s)$$<br>$$\\tilde{V}^{\\pi, \\gamma} (s, a) = V^{\\pi, \\gamma} (s, a) - \\Phi (s)$$<br>$$\\tilde{A}^{\\pi, \\gamma} (s, a) = (Q^{\\pi, \\gamma} (s, a) - \\Phi (s)) - (V^\\pi (s) - \\Phi (s)) = A^{\\pi, \\gamma} (s, a)$$</p>\n<p>이제 reward shaping의 idea를 가지고 어떻게 policy gradient estimate를 얻을 수 있는 지에 대해서 알아봅시다.</p>\n<ul>\n<li>$0 \\le \\lambda \\le 1$의 범위에 있는 steeper discount $\\gamma \\lambda$를 사용합니다.</li>\n<li>shaped reward $\\tilde{r}$는 Bellman residual term $\\delta^V$와 동일합니다.</li>\n<li>그리고 $\\Phi = V$와 같다고 보면 다음과 같은 수식이 나옵니다.</li>\n</ul>\n<p>$$\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V = \\hat{A}_t^{GAE(\\gamma, \\lambda)}$$<br>이렇게 shaped rewards의 $\\gamma \\lambda$-discounted sum을 고려함으로써 GAE를 얻을 수 있습니다. 더 정확하게 $\\lambda = 1$은 $g^\\gamma$의 unbiased estimate이고, 반면에 $\\lambda &lt; 1$은 biased estimate입니다.</p>\n<p>좀 더 나아가서, shaping transformation과 parameters $\\gamma$ and $\\lambda$의 결과를 보기 위해, response function $\\chi$를 이용하면 다음과 같은 수식이 나옵니다.<br>$$\\chi (l; s_t, a_t) = \\mathbb{E} [r_{t+l} | s_t, a_t] - \\mathbb{E} [r_{t+l} | s_t]$$<br>추가적으로 $A^{\\pi, \\gamma} (s, a) = \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$입니다.<br>그래서 discounted policy gradient estimator는 다음과 같이 쓸 수 있다.<br>$$\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) A^{\\pi, \\gamma} (s_t, a_t) = \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$$</p>\n<p><br><br></p>\n<h1 id=\"6-Value-Fuction-Estimation\"><a href=\"#6-Value-Fuction-Estimation\" class=\"headerlink\" title=\"6. Value Fuction Estimation\"></a>6. Value Fuction Estimation</h1><p>이번 section에서는 Trust Region Optimization Scheme에 따라 Value function을 Estimation합니다.</p>\n<p><br></p>\n<h2 id=\"6-1-Simplest-approach\"><a href=\"#6-1-Simplest-approach\" class=\"headerlink\" title=\"6.1 Simplest approach\"></a>6.1 Simplest approach</h2><p>$$minimize_{\\phi} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi}(s_n) - \\hat{V_n} \\vert\\vert^{2}$$</p>\n<ul>\n<li>위는 가장 간단하게 non-linear approximation으로 푸는 방법입니다.</li>\n<li>$\\hat{V_t} = \\sum_{l=0}^{\\infty}\\gamma^l r_{t+l}$은 reward 에 대한 discounted sum을 의미합니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"6-2-Trust-region-method-to-optimize-the-value-function\"><a href=\"#6-2-Trust-region-method-to-optimize-the-value-function\" class=\"headerlink\" title=\"6.2 Trust region method to optimize the value function\"></a>6.2 Trust region method to optimize the value function</h2><ul>\n<li>Value function을 최적화 하기 위해 trust region method를 사용합니다.</li>\n<li>Trust region은 최근 데이터에 대해 overfitting되는 것을 막아줍니다.</li>\n</ul>\n<p>Trust region문제를 풀기 위해서는 다음 스텝을 따릅니다.</p>\n<ul>\n<li>$\\sigma^2 = \\frac{1}{N} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi old}(s_n) - \\hat{V_n} \\vert\\vert^{2}$을 계산합니다.</li>\n<li>그 후에 다음과 같은 constrained opimization문제를 풉니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/d74cg3votxi2dbg/Screen%20Shot%202018-08-22%20at%209.13.28%20PM.png?dl=1\" width=\"300\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/jrkjax71t3j3vk4/Screen%20Shot%202018-08-22%20at%209.13.35%20PM.png?dl=1\" width=\"370\"> </center>\n\n<ul>\n<li>위의 수식은 사실 old Value function과 새로운 Value function KL distance가 $\\epsilon$ 다 작아야한다는 수식과 같습니다. Value function이 평균은 $V_{\\phi}(s)$이고 분산이 $\\sigma^2$인 conditional Gaussian distribution으로 parameterize되었을 뿐입니다.</li>\n</ul>\n<p><img width=\"400px\" src=\"https://www.dropbox.com/s/uw0v05feu8chqmc/Screenshot%202018-07-08%2010.04.38.png?dl=1\"> </p>\n<p>이 trust region문제의 답을 conjudate gradient algorithm을 이용하여 근사값을 구할 수 있습니다. 특히, 다음과 같은  quadratic program을 풀게됩니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/pawjkm8b9ycwsl5/Screen%20Shot%202018-08-22%20at%209.10.36%20PM.png?dl=1\" width=\"250\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/va7wms8uqxnw5rq/Screen%20Shot%202018-08-22%20at%209.10.40%20PM.png?dl=1\" width=\"370\"> </center>\n\n<ul>\n<li>여기서 $g$는 objective 의 gradient입니다.</li>\n<li>$j_n = \\nabla_{\\phi} V_{\\phi}(s_n)$일때, $H = \\frac{1}{N} \\sum_{n} j_n j^T_n$이며, $H$는 objective의 hessian에 대해서 gaussian newton method로 근사한 값입니다. 따라서, value function을 conditional probability로 해석한다면 Fisher information matrix가 됩니다.</li>\n<li>구현할때의 방법은 TRPO에서 사용한 방법과 모두 같습니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"7-Experiments\"><a href=\"#7-Experiments\" class=\"headerlink\" title=\"7.Experiments\"></a>7.Experiments</h1><p>실험은 다음 두 가지 물음에 대해서 디자인 되었습니다.</p>\n<ul>\n<li>GAE에 따라서 episodic total reward를 최적화 할때, $\\lambda$와 $\\gamma$가 변함에 따라서 어떤 경험적인 효과를 볼 수 있는지?</li>\n<li>GAE와 trust region alogorithm을 policy와 value function 모두에 함께 사용했을 때 어려운 문제에 적용되는 큰 뉴럴넷을 최적화 할 수 있을지?</li>\n</ul>\n<p><br></p>\n<h2 id=\"7-1-Policy-Optimization-Algorithm\"><a href=\"#7-1-Policy-Optimization-Algorithm\" class=\"headerlink\" title=\"7.1 Policy Optimization Algorithm\"></a>7.1 Policy Optimization Algorithm</h2><p>Policy update는 TRPO를 사용합니다. TRPO에 대한 설명은 여기서는 생략하겠습니다. TRPO 포스트를 보고 돌아와주세요!</p>\n<ul>\n<li><p>이전 TRPO에서 이미 TRPO와 다른 많은 알고리즘들을 비교하였기 때문에, 여기서는 똑같은 짓을 반복하지 않고 $\\lambda$, $\\gamma$가 변함에 따라 어떤 영향이 있는 지에 대한 실험에 집중하겠다고 합니다. (귀찮았던거죠..)</p>\n</li>\n<li><p>TRPO를 적용한 GAE의 최종 알고리즘은 다음과 같습니다.</p>\n</li>\n</ul>\n<center> <img width=\"600px\" src=\"https://www.dropbox.com/s/b1klz11f2frrvg4/Screenshot%202018-07-08%2010.33.54.png?dl=1\"> </center><br><center> <img width=\"300px\" src=\"https://www.dropbox.com/s/35fxte05pqtiel7/Screenshot%202018-07-08%2010.35.22.png?dl=1\"> </center><br><center> <img width=\"300px\" src=\"https://www.dropbox.com/s/u35pjow3w50bvz1/Screenshot%202018-07-08%2010.36.03.png?dl=1\"> </center>\n\n<ul>\n<li>여기서 주의할 점은 Policy update($\\theta_i \\rightarrow \\theta_{i+1}$)에서 $V_{\\phi_i}$를 사용했다는 점입니다.</li>\n<li>만약 Value function을 먼저 update하게 된다면 추가적인 bias가 발생합니다.</li>\n<li>극단적으로 생각해보아서, 우리가 Value function을 완벽하게 overfit을 해낸다면 Bellman residual($r_t + \\gamma V(s_{t+1}) - V(S_t)$)은 0이 됩니다. 그럼 Policy gradient의 estimation도 거의 0이 될 것입니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"7-2-Expermint-details\"><a href=\"#7-2-Expermint-details\" class=\"headerlink\" title=\"7.2 Expermint details\"></a>7.2 Expermint details</h2><h3 id=\"7-2-1-Environment\"><a href=\"#7-2-1-Environment\" class=\"headerlink\" title=\"7.2.1 Environment\"></a>7.2.1 Environment</h3><p>실험에서 사용된 환경은 다음 네 가지 입니다.</p>\n<ol>\n<li>classic cart-pole (x 3D)</li>\n<li>bipedal locomotion</li>\n<li>quadrupedal locomotion</li>\n<li>dynamically standing up for the biped</li>\n</ol>\n<h3 id=\"7-2-2-Architecture\"><a href=\"#7-2-2-Architecture\" class=\"headerlink\" title=\"7.2.2 Architecture\"></a>7.2.2 Architecture</h3><ul>\n<li>3D robot task에 대해서는 같은 모델을 사용하였습니다.<ul>\n<li>layers  = [100, 50, 25] 각각 tanh 사용. (Policy와 Value 네트워크 모두)</li>\n<li>Final output layer은 linear</li>\n</ul>\n</li>\n<li>Cartpole에 대해서는 1개의 layer 안에 20개의 hidden unit만 있는 linear policy를 사용했다고 합니다.</li>\n</ul>\n<h3 id=\"7-2-3-Task\"><a href=\"#7-2-3-Task\" class=\"headerlink\" title=\"7.2.3 Task\"></a>7.2.3 Task</h3><ul>\n<li>Cartpole<ul>\n<li>한 배치당 20 개의 trajectory를 모았고, maximum length는 1000입니다.</li>\n</ul>\n</li>\n<li>3D biped locomotion<ul>\n<li>33 dim state , 10 dim action</li>\n<li>50000 time step per batch</li>\n</ul>\n</li>\n<li>3D quadruped locomotion<ul>\n<li>29 dim state, 8 dim action</li>\n<li>200000 time step per batch</li>\n</ul>\n</li>\n<li>3D biped locomotion Standing<ul>\n<li>33 dim state , 10 dim action</li>\n<li>200000 time step per batch</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"7-2-3-results\"><a href=\"#7-2-3-results\" class=\"headerlink\" title=\"7.2.3 results\"></a>7.2.3 results</h3><p>cost의 관점에서 결과를 나타내었다고 합니다. Cost는 negative reward와 이것이 최소화 되었는가로 정의되었습니다.</p>\n<h4 id=\"7-2-3-1-Cartpole\"><a href=\"#7-2-3-1-Cartpole\" class=\"headerlink\" title=\"7.2.3.1 Cartpole\"></a>7.2.3.1 Cartpole</h4><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/x9pbms1wvg38lda/Screenshot%202018-07-08%2011.08.22.png?dl=1\"> </center>\n\n<ul>\n<li>왼쪽 그림은 $\\gamma$를 0.99로 고정시켜놓은 상태에서 $\\lambda$를 변화시킴에 따라서 cost를 측정한 것입니다. </li>\n<li>오른쪽은 $\\gamma$와 $\\lambda$를 둘 다 변화 시키면서 성능을 그림으로 나타낸 표입니다. 흰색에 가까울 수록 좋은 성능입니다. </li>\n</ul>\n<h4 id=\"7-2-3-2-3D-BIPEDAL-LOCOMOTINO\"><a href=\"#7-2-3-2-3D-BIPEDAL-LOCOMOTINO\" class=\"headerlink\" title=\"7.2.3.2 3D BIPEDAL LOCOMOTINO\"></a>7.2.3.2 3D BIPEDAL LOCOMOTINO</h4><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/i9wj4p6ijojsy82/Screenshot%202018-07-08%2011.25.08.png?dl=1\"> </center>\n\n\n<ul>\n<li>다른 random seed로 부터 9번 씩 시도한 결과를 mean을 취해서 사용합니다.</li>\n<li>Best performance는   $\\gamma \\in [0.99, 0.995]$ 그리고  $\\lambda \\in [0.96, 0.99]$일때. </li>\n<li>1000 iteration 후에 빠르고 부드럽고 안정적인 걸음거이가 나옵니다.</li>\n<li>실제로 걸린 시간은 0.01(타입스텝당 시간) <em> 50000(배치당 타임스텝) </em> 1000(배치) <em> 3600(초-&gt;시간) </em> 24 = 5.8일 정도가 걸렸습니다.</li>\n</ul>\n<h4 id=\"7-2-3-3-다른-ROBOT-TASKS\"><a href=\"#7-2-3-3-다른-ROBOT-TASKS\" class=\"headerlink\" title=\"7.2.3.3 다른 ROBOT TASKS\"></a>7.2.3.3 다른 ROBOT TASKS</h4><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/fuuat65we52quht/Screenshot%202018-07-08%2011.33.11.png?dl=1\"> </center>\n\n<ul>\n<li>다른 로봇 TASK에 대해서는 아주 제한적인 실험만 진행합니다.(시간이 부족했던듯 하네요..)</li>\n<li>Quadruped에 대해서는 $\\gamma = 0.995$로 fix, $\\lambda \\in {0, 0.96}$</li>\n<li>Standingup에 대해서는 $\\gamma = 0.99$로 fix, $\\lambda \\in {0, 0.96}$</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"8-Discussion\"><a href=\"#8-Discussion\" class=\"headerlink\" title=\"8. Discussion\"></a>8. Discussion</h1><p><br></p>\n<h2 id=\"8-1-Main-discussion\"><a href=\"#8-1-Main-discussion\" class=\"headerlink\" title=\"8.1 Main discussion\"></a>8.1 Main discussion</h2><p>지금까지 복잡하고 어려운 control problem에서 Reinforcement Learning(RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 “variance reduction”에 대해 연구하였습니다.</p>\n<p>“Generalized Advantage Estimator(GAE)”라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.<br>또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는 지를 보였습니다.</p>\n<p>이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.</p>\n<p>GAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.</p>\n<p><br></p>\n<h2 id=\"8-2-Future-work\"><a href=\"#8-2-Future-work\" class=\"headerlink\" title=\"8.2 Future work\"></a>8.2 Future work</h2><p>Value function estimation error와 Policy gradient estimation error사이의 관계를 알아낸다면, 우리는 Value function fitting에 더 잘 맞는 error metric(policy gradient estimation 의 정확성과 더 잘 맞는 value function)을 사용할 수 있습니다. 여기서 Policy와 Value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.</p>\n<p>추가적으로 DDPG는 별로라고 합니다. 그리고 TD(0)는 bias가 너무 크고, poor performance로 이끈다고 합니다. 특히나 이 논문에서는 low-dimention의 쉬운 문제들만 해결했습니다.</p>\n<center> <img width=\"500px\" src=\"https://www.dropbox.com/s/nhc7t9psul5lr3x/Screenshot%202018-07-08%2011.45.15.png?dl=1\"> </center>\n\n<p><br></p>\n<h2 id=\"8-3-FAQ\"><a href=\"#8-3-FAQ\" class=\"headerlink\" title=\"8.3 FAQ\"></a>8.3 FAQ</h2><ul>\n<li>Compatible features와는 무슨 관계?<ul>\n<li>Compatible features는 value function을 이용하는 policy gradient 알고리즘들과 함께 자주 언급됩니다.</li>\n<li>Actor Critic의 저자는 policy의 제한된 representation power때문에, policy gradient는 단지 advantage function space의 subspace에만 의존하게 됩니다.</li>\n<li>이 subspace는 compatible features에 의해 span됩니다.</li>\n<li>이 이론은 현재 문제 구조를 어떻게 이용해야 advantage function에 대해 더 나은 estimation을 할 수 있는 지에 대한 지침을 주지 않습니다. GAE 논문의 idea와 orthogonal합니다.</li>\n</ul>\n</li>\n<li>왜 Q function을 사용하지 않는가?<ul>\n<li>먼저 state-value function이 더 낮은 차원의 input을 가진다. 그래서 Q function보다 더 학습하기가 쉽습니다.</li>\n<li>두 번째로 이 논문에서 제안하는 방법으로는 high bias estimator에서 low bias estimator로 $\\lambda$를 통해서 부드럽게 interpolate를 할 수 있습니다.</li>\n<li>반면에 Q를 사용하면 단지 high-bias estimator 밖에 사용할 수 없습니다.</li>\n<li>특히나 return에 대한 one-step estimation은 엄두를 못낼 정도로 bias가 큽니다.</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"TRPO-여행하기\"><a href=\"#TRPO-여행하기\" class=\"headerlink\" title=\"TRPO 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/\">TRPO 여행하기</a></h2><h2 id=\"TRPO-Code\"><a href=\"#TRPO-Code\" class=\"headerlink\" title=\"TRPO Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"PPO-여행하기\"><a href=\"#PPO-여행하기\" class=\"headerlink\" title=\"PPO 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/\">PPO 여행하기</a></h2>"},{"title":"High-Dimensional Continuous Control using Generalized Advantage Estimation","date":"2018-06-23T10:18:45.000Z","author":"양혁렬, 이동민","subtitle":"피지여행 6번째 논문","_content":"\n<center> <img src=\"https://www.dropbox.com/s/p8gfpyo6xf9wm5w/Screen%20Shot%202018-07-18%20at%201.25.53%20AM.png?dl=1\" width=\"700\"> </center>\n\n논문 저자 : John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan and Pieter Abbeel\n논문 링크 : https://arxiv.org/pdf/1506.02438.pdf\nProceeding : International Conference of Learning Representations (ICLR) 2016\n정리 : 양혁렬, 이동민\n\n---\n\n# 1. 들어가며...\n\n현존하는 Policy Gradient Method들의 목적은 누적되는 reward들을 optimization하는 것입니다. 하지만 학습할 때에 많은 양의 sample이 필요로 하고, 들어오는 data가 nonstationarity임에도 불구하고 stable and steady improvement가 어렵습니다.\n\n그래서 이 논문에서는 다음과 같은 방법을 제시합니다.\n\n- TD($\\lambda$)와 유사한 advantage function의 exponentially-weighted estimator를 사용하여 policy gradient estimate의 variance를 줄이는 것 \n- policy와 value function에 대한 Trust Region Optimization 사용하는 것\n\n3D locomotion tasks에 대한 empirical results는 다음과 같습니다.\n\n- bipedal and quadrupedal simulated robots의 달리는 자세를 학습\n- bipedal 사람이 땅에 누워있다가 일어서는 것을 학습\n\n※ TD($\\lambda$)를 혹시 모르실 경우도 있을 것 같아 간략하게 정리하였습니다. http://dongminlee.tistory.com/10 를 먼저 보시고 아래의 내용을 봐주시기 바랍니다!\n\n<br><br>\n\n# 2. Introduction\n\n기본적으로 \"parameterized stochastic policy\"를 가정합니다. 이 때 expected total returns의 gradient에 대한 unbiased estimate를 얻을 수 있는데 이것을 REINFORCE라고 부릅니다. 하지만 하나의 action의 결과가 과거와 미래의 action의 결과로 혼동되기 때문에 gradient estimator의 high variance는 시간에 따라 scaling됩니다.\n\n또 다른 방법은 Actor-Critic이 있습니다. 이 방법은 empirical returns보다 하나의 value function을 사용합니다. 또한 bias하고 lower variance를 가진 estimator입니다. 구체적으로 말하자면, high variance하다면 더 sampling을 하면 되는 반면에 bias는 매우 치명적입니다. 다시 말해 bias는 algorithm이 converge하는 데에 실패하거나 또는 local optimum이 아닌 poor solution에 converge하도록 만듭니다.\n\n따라서 이 논문에서는 $\\gamma\\in [0,1]$ and $\\lambda\\in [0,1]$에 의해 parameterized estimation scheme인 Generalized Advantage Estimator(GAE)를 다룹니다.\n\n이 논문을 대략적으로 요약하자면 다음과 같습니다.\n\n- 효과적인 variance reduction scheme인 GAE를 다룹니다. 또한 실험할 때 batch trust region algorithm에 더하여 다양한 algorithm들에 적용됩니다.\n- value function에 대해 trust region optimization method를 사용합니다. 이렇게 함으로서 더 robust하고 efficient한 방법이 됩니다.\n- A와 B를 합쳐서, 실험적으로 control task에 neural network policies를 learning하는 데에 있어서 효과적인 algorithm을 얻습니다. 이러한 결과는 high-demensional continuous control에 RL을 사용함으로서 state of the art로 확장되었습니다.\n\n<br><br>\n\n# 3. Preliminaries\n\n먼저 policy optimization의 \"undiscounted formulation\"을 가정합니다. (undiscounted formulation에 주목합시다.)\n\n- initial state $s_0$는 distribution $\\rho_0$으로부터 sampling된 것입니다.\n- 하나의 trajectory ($s_0, a_0, s_1, a_1, ...$)는 terminal state에 도달될 때 까지 policy $a_t$ ~ $\\pi(a_t | s_t)$에 따라서 action을 sampling하고, dynamics $s_{t+1}$ ~ $P(s_{t+1} | s_t, a_t)$에 따라서 state를 sampling함으로써 생성됩니다.\n- reward $r_t = r(s_t, a_t, s_{t+1})$은 매 time step마다 받아집니다.\n- 목표는 모든 policies에 대해 finite하다고 가정됨으로서 expected total reward $\\sum_{t=0}^{\\infty} r_t$를 maximize하는 것입니다.\n\n여기서 중요한 점은 $\\gamma$를 discount의 parameter로 사용하는 것이 아니라 \"bias-variance tradeoff를 조절하는 parameter로 사용한다\" 는 것입니다.\n\npolicy gradient method는 gradient $g := \\nabla_\\theta \\mathbb{E} [\\sum_{t=0}^\\infty r_t]$를 반복적으로 estimate함으로써 expected total reward를 maximize하는 것인데, policy gradient에는 여러 다른 표현들이 있습니다.\n$$g = \\mathbb{E} [\\sum_{t=0}^\\infty \\Phi_t \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$\n\n여기서 $\\Phi_t$는 아래 중의 하나일 수 있습니다.\n\n1. $\\sum_{t=0}^\\infty r_t$: trajectory의 total reward\n2. $\\sum_{t'=t}^\\infty r_t'$: action $a_t$ 후의 reward\n3. $\\sum_{t'=t}^\\infty r_t' - b(s_t)$: 2의 baselined version\n4. $Q^\\pi (s_t, a_t)$: state-action value function\n5. $A^\\pi (s_t, a_t)$: advantage function\n6. $r_t + V^\\pi (s_{t+1}) - V^\\pi (s_t)$: TD residual\n\n위의 번호 중 4, 5, 6의 수식들은 다음의 정의를 사용합니다.\n\n- $V^\\pi (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty r_{t+1}]$\n- $Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty r_{t+l}]$\n- $A^\\pi (s_t, a_t) := Q^\\pi (s_t, a_t) - V^\\pi (s_t)$, (Advantage function)\n\n추가적으로 colon notation $a : b$는 포괄적인 범위 $(a, a+1, ... , b)$입니다. (잘 기억해둡시다. 뒤에 계속해서 colon notation이 나옵니다.)\n\n여기서부터 parameter $\\gamma$에 대해 좀 더 자세히 알아봅시다. parameter $\\gamma$는 bias하면서 동시에 reward를 downweighting함으로써 variance를 줄입니다. 다시 말해 MDP의 discounted formulation에서 사용된 discounted factor와 일치하지만, 이 논문에서는 \"$\\gamma$를 undiscounted MDP에서 variance reduction parameter\"로 다룹니다. -> 결과는 같지만, 의미와 의도가 다릅니다.\n\ndiscounted value function들은 다음과 같습니다.\n\n- $V^{\\pi, \\gamma} (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$ \n- $Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$\n- $A^{\\pi, \\gamma} (s_t, a_t) := Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)$\n\n따라서 policy gradient에서의 discounted approximation은 다음과 같이 나타낼 수 있습니다.\n$$g^\\gamma := \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\sum_{t=0}^\\infty A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$\n뒤이어 나오는 section에서는 위의 수식에서 $A^{\\pi, \\gamma}$에 대해 biased (but not too biased) estimator를 얻는 방법에 대해서 나옵니다.\n\n다음으로 advantage function에서 새롭게 접하는 \"$\\gamma$-just estimator\"의 notation에 대해 알아봅시다.\n\n- 먼저 $\\gamma$-just estimator는 $g^\\gamma$ ${}^1$를 estimate하기 위해 위의 수식에서 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 사용할 때, bias하지 않은 estimator라고 합시다. 그리고 이 $\\gamma$-just advantage estimator를 <img src=\"https://www.dropbox.com/s/7g2glqx2kbfynaa/Screen%20Shot%202018-08-22%20at%209.03.46%20PM.png?dl=1\" width=\"100\"> 라고 하고, 전체의 trajectory에 대한 하나의 function이라고 합시다.\n- ${}^1$에서 이 논문의 저자가 하고 싶은 말이 있는 것 같습니다. 개인적으로 $\\gamma$-just estimator를 이해하는 데에 있어서 중요한 주석이라 정확히 해석하고자 합니다.\n    - $A^\\pi$를 $A^{\\pi, \\gamma}$로 사용함으로써 이미 bias하다라고 말했지만, \"이 논문에서 하고자 하는 것은 $g^\\gamma$에 대해 unbiased estimate를 얻고 싶은 것입니다.\" 하지만 undiscounted MDP의 policy gradient에 대해서는 당연히 $\\gamma$를 사용하기 때문에 biased estimate를 얻습니다. 개인적으로 이것은 일단 무시하고 $\\gamma$를 사용할 때 어떻게 unbiased estimate를 얻을 지에 대해 좀 더 포커스를 맞추고 있는 것 같습니다.\n    - 그러니까 저 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 바꿔줌으로써 unbiased estimate를 하고 싶다는 것이 뒤이어 나오는 정의와 명제의 핵심이라고 할 수 있습니다.\n\nDefinition 1.\n먼저 가정을 합니다. (가정을 바탕으로 이루어지는 정의라는 것을 주목합시다.) 만약\n<center ><img src=\"https://www.dropbox.com/s/aia9f1l99wyxgah/Screen%20Shot%202018-08-22%20at%209.07.05%20PM.png?dl=1\" width=\"700\"> </center>\n두 수식이 같다면, estimator $\\hat{A}_t$는 $\\gamma$-just입니다.\n\n그리고 만약 모든 t에 대해서 $\\hat{A}_t$이 $\\gamma$-just이라면, 다음과 같이 표현할 수 있습니다.\n\n<center ><img src=\"https://www.dropbox.com/s/r4m6ktw8geeu7pe/Screen%20Shot%202018-08-22%20at%209.29.18%20PM.png?dl=1\" width=\"420\"> </center>\n\n위의 수식이 바로 unbiased estimate입니다.\n\n$\\gamma$-just인 $\\hat{A}_t$에 대한 한가지 조건은 $\\hat{A}_t$이 두 가지 function $Q_t$ and $b_t$로 나뉠 수 있다는 것입니다.\n\n- $Q_t$는 $\\gamma$-discounted Q-function의 unbiased estimator입니다.\n- $b_t$는 action $a_t$전에 sampling된 states and actions의 arbitrary function이다. \n\nProposition 1.\n모든 $(s_t, a_t)$에 대해,\n$$\\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty | s_t, a_t} [Q_t (s_{t:\\infty}, a_{t:\\infty})] = Q^{\\pi, \\gamma} (s_t, a_t)$$\n로 인하여 $\\hat{A}_t$이 \n\n<center> <img src=\"https://www.dropbox.com/s/59ydsav6djwyevl/Screen%20Shot%202018-08-22%20at%209.27.13%20PM.png?dl=1\" width=\"380\"> </center>\n\n형태라고 가정합시다. (가정을 바탕으로 이루어지는 명제라는 점을 주목합시다.)\n\n그 때, $\\hat{A}_t$은 $\\gamma$-just입니다.\n\n이 명제에 대한 증명은 Appendix B에 있습니. 그리고 $\\hat{A}_t$에 대한 $\\gamma$-just advantage estimator들은 다음과 같습니다.\n\n- $\\sum_{l=0}^\\infty \\gamma^l r_{t+1}$\n- $A^{\\pi, \\gamma} (s_t, a_t)$\n- $Q^{\\pi, \\gamma} (s_t, a_t)$\n- $r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)$\n\n증명을 보기 전에 먼저 Advantage function에 대해서 먼저 살펴봅시다. 아래의 내용은 Sutton이 쓴 논문인 Policy Gradient Methods for Reinforcement Learning with Function Approximation(2000)에서 나온 내용을 리뷰한 것입니다. \n\n<center> <img src=\"https://www.dropbox.com/s/x737yq97ut6gp1a/Screen%20Shot%202018-07-15%20at%201.16.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n여기서는 \"이 수식은 $\\sum_a \\frac{\\partial \\pi (s,a)}{\\partial \\theta} = 0$이기 때문에 가능해진다.\", \"즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없다\"라는 두 문장을 기억해둡시다.\n\n이제 증명을 보면 됩니다.\n\n<center> <img src=\"https://www.dropbox.com/s/e7sj2fwcm1f7hof/figure2.jpg?dl=1\" width=\"600\"> </center>\n<center> <img src=\"https://www.dropbox.com/s/kyk4jes1202az4m/figure3.jpg?dl=1\" width=\"600\"> </center>\n\n위의 증명 수식에서 Q와 b의 각각 세번째 수식과 마지막 수식을 자세히 봅시다.\n\n1. $Q$에 대한 증명\n    - 빨간색 부분\n        - $\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [Q_t (s_{0:\\infty}, a_{0:\\infty})]$를 쉽게 말하자면 \"모든(과거, 현재, 미래) state와 action에 대해서 expectation을 $t+1$부터 $\\infty$까지 하겠다.\"라는 말입니다. 이렇게 함으로써 원래는 $Q^\\pi (s_t, a_t)$가 나와야 맞는 건데, 앞에서 살펴봤다시피 4번째 수식 밑줄 친 자리에는 advantage 역할을 하는 함수들을 넣어도 결과값에 아무런 영향을 끼치지 않기 때문에 의도적으로 $A^\\pi (s_t, a_t)$로 바꾼 것입니다. 또한 이 증명을 바탕으로 $\\hat{A}$이 $\\gamma$-just라는 것을 표현하고 싶기 때문이라고 봐도 됩니다.\n\n    - 파란색 부분\n        - 또한 $a_{0:t}$에서 $a_{0:t-1}$로 변한 것은 $A^\\pi (s_t, a_t)$에 baseline이 들어가기 때문에 바꿔준 것입니다.\n\n2. $b$에 대한 증명\n    - 빨간색 부분\n        - $\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$가 0으로 바뀌는 것은 위에서 설명했듯이 $\\nabla_\\theta$자체가 $\\log \\pi_\\theta$만 gradient하기 때문에 이것을 expectation을 취하면 0이 됩니다.\n\n<br><br>\n\n# 4. Advantage Function Estimation\n\n이번 section에는 discounted advantage function $A^{\\pi, \\gamma} (s_t, a_t)$의 accurate estimate $\\hat{A}_t$에 대해서 살펴봅시다. 이에 따른 수식은 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/sn16g5iyzzflju0/Screen%20Shot%202018-08-22%20at%209.22.49%20PM.png?dl=1\" width=\"300\"> </center>\n\n여기서 n은 a batch of episodes에 대하여 index한 것입니다.\n\n$V$를 approximate value function이라고 합시다. 그리고 $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$이라고 합시다.\n\n만약 (이전과 마찬가지로 가정부터 합니다.) correct value function $V = V^{\\pi, \\gamma}$가 있다고 한다면, 이것은 $\\gamma$-just advantage estimator입니다. 실제로, $A^{\\pi, \\gamma}$의 unbiased estimator는 다음과 같습니다.\n$$\\mathbb{E}_{s_{t+1}} [\\delta_t^{V^{\\pi, \\gamma}}] = \\mathbb{E}_{s_{t+1}} [r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)]$$\n$$= \\mathbb{E}_{s_{t+1}} [Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)] = A^{\\pi, \\gamma} (s_t, a_t)$$\n그러나, \"이 estimator는 유일하게 $V = V^{\\pi, \\gamma}$에 대한 $\\gamma$-just입니다.\" 다른 경우라면 이것은 biased policy gradient estimate일 것입니다. (우리가 하고 싶은 것은 $V$에 대해서만 unbiased estimator가 아니라 advantage function에 대해서 일반화된 unbiased estimator를 얻고 싶은 것입니다. 그래서 아래에서도 나오겠지만, $\\gamma$와 함께 $\\lambda$를 이용한 estimator가 나옵니다.ㅏ)\n\n그래서 $\\delta$에 대해 $k$의 sum으로 생각해봅시다. 이것을 $\\hat{A}_t^{(k)}$라고 하자. 그러면 아래와 같이 표현할 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ra7hxksveg2hz45/figure4.jpg?dl=1\" width=\"600\"> </center>\n\n$\\hat{A}_t^{(k)}$은 returns의 $k$-step estimate와 연관지을 수 있고, $\\delta_t^V = \\hat{A}_t^{(1)}$의 case와 유사하게도 $\\hat{A}_t^{(k)}$를 advantage function의 estimator로 생각할 수 있습니다.\n\n여기서 $k \\rightarrow \\infty$로 생각해보면 bias가 일반적으로 점점 더 작아집니다. 왜냐하면 $\\gamma^k V(s_{t+k})$가 점점 많이 discounted되서 $-V(s_t)$가 bias에 영향을 미치지 못하기 때문입니다. $k \\rightarrow \\infty$를 취하면 다음과 같은 수식이 나옵니다.\n\n<center> <img src=\"https://www.dropbox.com/s/13fn9wcup8pfh9u/Screen%20Shot%202018-08-22%20at%209.19.52%20PM.png?dl=1\" width=\"320\"> </center>\n\n우변의 수식과 같이 empirical returns에서 value function baseline을 뺀 것으로 나타낼 수 있습니다.\n\nGeneralized Advantage Estimator GAE($\\gamma, \\lambda$)는 $k$-step estimators의 exponentially-weighted average로 나타낼 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/yg1ybmfkep3towu/figure5.jpg?dl=1\" width=\"600\"> </center>\n\n(TD($\\lambda$)를 떠올려주세요..!)\n\n위의 마지막 수식에서도 알 수 있듯이, advantage estimator는 Bellman residual terms의 discounted sum과 관련있는 간단한 수식입니다. 다음 section에서 modified reward function을 가진 MDP에 returns로서의 관점에서 위의 마지막 수식을 더 자세히 살펴봅시다. 위의 수식은 TD($\\lambda$)와 많이 유사합니다. 그러나 TD($\\lambda$)는 value function를 estimator하고, 여기서는 advantage function을 estimator합니다.\n\n위의 수식에서 $\\lambda = 0$ and $\\lambda = 1$에 대해서는 특별한 case가 존재한다. 수식으로 표현하면 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ufglxzanhcnbi36/figure6.jpg?dl=1\" width=\"600\"> </center>\n\n- $GAE(\\gamma, 1)$은 $V$의 정확도와 관계없이 $\\gamma$-just입니다. 그러나 returns의 sum때문에 high variance합니다.\n- $GAE(\\gamma, 0)$은 $V = V^{\\pi, \\gamma}$에 대해 $\\gamma$-just입니다. 그리고 bias하지만 일반적으로 훨씬 lower variance를 가집니다. \n- $0 < \\lambda < 1$에 대해 GAE는 parameter $\\lambda$를 control하여 bias와 variance사이에 compromise를 만듭니다.\n\n두 가지 별개의 parameter $\\gamma$ and $\\lambda$를 가진 advantage estimator는 bias-variance tradeoff에 도움을 줍니다. 그러나 이 두 가지 parameter는 각각 다른 목적을 가지고 작동합니다.\n\n- $\\gamma$는 가장 중요하게 value function $V^{\\pi, \\gamma}$ 의 scale을 결정합니다. 또한 $\\gamma$는 $\\lambda$에 의존하지 않습니다. $\\gamma < 1$로 정하는 것은 policy gradient estimate에서 bias합니다.\n- 반면에, $\\lambda < 1$는 유일하게 value function이 부정확할 때 bias합니다. 그리고 경험상, $\\lambda$의 best value는 $\\gamma$의 best value보다 훨씬 더 낮습니다. 왜냐하면 $\\lambda$가 정확한 value function에 대해 $\\gamma$보다 훨씬 덜 bias하기 때문입니다.\n\nGAE를 사용할 때, $g^\\gamma$의 biased estimator를 구성할 수 있습니다. 수식은 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ok57dsk52o7pl9r/Screen%20Shot%202018-08-22%20at%209.17.54%20PM.png?dl=1\" width=\"670\"> </center>\n\n여기서 $\\lambda = 1$일 때 동일해집니다.\n\n<br><br>\n\n# 5. Interpretation as Reward Shaping\n\n이번 section에서는 앞서 다뤘던 수식 $\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V$를 modified reward function의 MDP의 관점으로 생각해봅시다. 조금 더 구체적으로 말하자면, MDP에서 reward shaping transformation을 실행한 후에 적용된 extra discounted factor로서 $\\lambda$를 어떻게 볼 것인지에 대해서 다룹니다.\n\n(개인적인 comment) 한 가지 먼저 언급하자면, 본래의 목적은 reward shaping이 아니라 variance reduction입니다. 이번 section은 그저 이전에 이러한 개념이 있었고, gae를 다른 관점에서 생각해보자라는 뜻에서 나온 section인 것 같습니다. '아~ 이러한 개념이 있구나~!' 정도로만 알면 될 것 같습니다. 'gae가 reward shaping의 효과까지 있어서 이러한 section을 넣은 것일까?' 라는 생각도 해봤지만 아직 잘 모르겠습니다. 실험부분에도 딱히 하지 않은 걸로 봐서는.. 아닌 것 같기도 하고.. 아직까지는 그저 'reward shaping의 관점에서 봤을 때에도 gae로 만들어줄 수 있다.' 정도만 생각하려고 합니다.\n\nReward Shaping이란 개념 자체는 일반적인 알고리즘 분야(최단경로 문제 등)에도 있습니다. 하지만 이 개념이 머신러닝에 적용된 건 Andrew Y. Ng이 쓴 Policy Invariance under Reward Transformations Theory and Application to Reward Shaping(1999) 논문입니다. 논문에서 Reward Shaping을 설명하는 부분은 간략하게 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/0w9cuxjkhz2mbcy/figure7.jpg?dl=1\" width=\"400\"> </center>\n<center> <img src=\"https://www.dropbox.com/s/xcp9eqvcbv3268g/figure8.jpg?dl=1\" width=\"400\"> </center>\n\n위의 글만 보고 이해가 잘 안됩니다. 그래서 다른 자료들을 찾던 중에 다음의 그림을 찾게 되었습니다. (아래의 그림은 Youtube에 있는 [Udacity 영상](https://www.youtube.com/watch?v=xManAGjbx2k&t=95s)에서 있는 그림입니다.)\n\n<center> <img src=\"https://www.dropbox.com/s/87kacngez9cl2e6/figure9.jpg?dl=1\" width=\"400\"> </center>\n\n이 그림을 통해서 Reward Shaping을 이해해봅시다.\n\nReward Shaping(보상 형성)을 통해서 뭘 하고 싶은지 목적부터 봅시다. reward space가 sparse 한 경우에 reward가 너무 드문드문 나옵니다. 따라서 이것을 꾸준히 reward를 받을 수 있도록 바꿉니다. potential-based shaping function인 $\\Phi$를 만들어서 더하고 빼줍니다. 저 $\\Phi$ 자리에는 대표적으로 state value function이 많이 들어간다고 합니다. (아직 목적이 이해가 안될 수 있습니다. Reward Shaping에 대해서 다 보고 나서 다시 한 번 읽어봅시다.)\n\n위의 그림은 돌고래가 점프하여 불구멍을 통과해야하는 환경입니다. 하지만 저 불구멍을 통과하기 위해서는 (1) 점프도 해야하고, (2) 불도 피해야하고, (3) 알맞게 착지까지 완료해야합니다. 이렇게 해야 reward를 +1 얻습니다. \n\n생각해봅시다. 어느 세월에 점프도 해야하고.. 불도 피해야하고.. 알맞게 착지까지해서 reward를 +1 얻겠습니까? 따라서 그 전에도 잘하고 있는 지, 못하고 있는 지를 판단하기 위해, 다시 말해 reward를 꾸준히 받도록 다음과 같이 transformed reward function $\\tilde{r}$을 정의합니다.\n\n$$\\tilde{r} (s, a, s') = r(s, a, s') + \\gamma \\Phi (s') - \\Phi (s)$$\n\n- 여기서 $\\Phi : S \\rightarrow \\mathbb{R}$를 state space에서의 arbitrary scalar-valued function을 나타냅니다. 그리고 $\\Phi$자리에는 대표적으로 state value function이 들어간다고 생각합시다.\n- 형태가 TD residual term의 결과와 비슷하지만 의미와 의도가 다릅니다. reward shaping은 sparse reward 때문이고, 이전 section에서 봤던 gae는 variance reduction때문에 나온 것입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/tkfpqniaazwfbld/figure10.jpg?dl=1\" width=\"600\"> </center>\n\n이 transformation은 discounted advantage function $A^{\\pi, \\gamma}$으로도 둘 수 있습니다. state $s_t$를 시작하여 하나의 trajectory의 rewards의 discounted sum을 표현하면 다음과 같습니다.\n$$\\sum_{l=0}^\\infty \\gamma^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty \\gamma^l r(s_{t+l}, a_{t+l}, s_{t+l+1}) - \\Phi(s_t)$$\n\n$\\tilde{Q}^{\\pi, \\gamma}, \\tilde{V}^{\\pi, \\gamma}, \\tilde{A}^{\\pi, \\gamma}$를  transformed MDP의 value function과 advantage function이라고 하면, 다음과 같은 수식이 나옵니다.\n$$\\tilde{Q}^{\\pi, \\gamma} (s, a) = Q^{\\pi, \\gamma} (s, a) - \\Phi (s)$$\n$$\\tilde{V}^{\\pi, \\gamma} (s, a) = V^{\\pi, \\gamma} (s, a) - \\Phi (s)$$\n$$\\tilde{A}^{\\pi, \\gamma} (s, a) = (Q^{\\pi, \\gamma} (s, a) - \\Phi (s)) - (V^\\pi (s) - \\Phi (s)) = A^{\\pi, \\gamma} (s, a)$$\n\n이제 reward shaping의 idea를 가지고 어떻게 policy gradient estimate를 얻을 수 있는 지에 대해서 알아봅시다.\n\n- $0 \\le \\lambda \\le 1$의 범위에 있는 steeper discount $\\gamma \\lambda$를 사용합니다.\n- shaped reward $\\tilde{r}$는 Bellman residual term $\\delta^V$와 동일합니다.\n- 그리고 $\\Phi = V$와 같다고 보면 다음과 같은 수식이 나옵니다.\n\n$$\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V = \\hat{A}_t^{GAE(\\gamma, \\lambda)}$$\n이렇게 shaped rewards의 $\\gamma \\lambda$-discounted sum을 고려함으로써 GAE를 얻을 수 있습니다. 더 정확하게 $\\lambda = 1$은 $g^\\gamma$의 unbiased estimate이고, 반면에 $\\lambda < 1$은 biased estimate입니다.\n\n좀 더 나아가서, shaping transformation과 parameters $\\gamma$ and $\\lambda$의 결과를 보기 위해, response function $\\chi$를 이용하면 다음과 같은 수식이 나옵니다.\n$$\\chi (l; s_t, a_t) = \\mathbb{E} [r_{t+l} | s_t, a_t] - \\mathbb{E} [r_{t+l} | s_t]$$\n추가적으로 $A^{\\pi, \\gamma} (s, a) = \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$입니다.\n그래서 discounted policy gradient estimator는 다음과 같이 쓸 수 있다.\n$$\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) A^{\\pi, \\gamma} (s_t, a_t) = \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$$\n\n<br><br>\n\n# 6. Value Fuction Estimation\n\n이번 section에서는 Trust Region Optimization Scheme에 따라 Value function을 Estimation합니다.\n\n<br>\n## 6.1 Simplest approach\n\n$$minimize_{\\phi} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi}(s_n) - \\hat{V_n} \\vert\\vert^{2}$$\n\n- 위는 가장 간단하게 non-linear approximation으로 푸는 방법입니다.\n- $\\hat{V_t} = \\sum_{l=0}^{\\infty}\\gamma^l r_{t+l}$은 reward 에 대한 discounted sum을 의미합니다.\n\n<br>\n## 6.2 Trust region method to optimize the value function\n\n- Value function을 최적화 하기 위해 trust region method를 사용합니다.\n- Trust region은 최근 데이터에 대해 overfitting되는 것을 막아줍니다.\n\nTrust region문제를 풀기 위해서는 다음 스텝을 따릅니다.\n\n- $\\sigma^2 = \\frac{1}{N} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi old}(s_n) - \\hat{V_n} \\vert\\vert^{2}$을 계산합니다.\n- 그 후에 다음과 같은 constrained opimization문제를 풉니다.\n\n<center> <img src=\"https://www.dropbox.com/s/d74cg3votxi2dbg/Screen%20Shot%202018-08-22%20at%209.13.28%20PM.png?dl=1\" width=\"300\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/jrkjax71t3j3vk4/Screen%20Shot%202018-08-22%20at%209.13.35%20PM.png?dl=1\" width=\"370\"> </center>\n\n- 위의 수식은 사실 old Value function과 새로운 Value function KL distance가 $\\epsilon$ 다 작아야한다는 수식과 같습니다. Value function이 평균은 $V_{\\phi}(s)$이고 분산이 $\\sigma^2$인 conditional Gaussian distribution으로 parameterize되었을 뿐입니다.\n\n<img width =\"400px\" src=\"https://www.dropbox.com/s/uw0v05feu8chqmc/Screenshot%202018-07-08%2010.04.38.png?dl=1\"> \n\n이 trust region문제의 답을 conjudate gradient algorithm을 이용하여 근사값을 구할 수 있습니다. 특히, 다음과 같은  quadratic program을 풀게됩니다.\n\n<center> <img src=\"https://www.dropbox.com/s/pawjkm8b9ycwsl5/Screen%20Shot%202018-08-22%20at%209.10.36%20PM.png?dl=1\" width=\"250\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/va7wms8uqxnw5rq/Screen%20Shot%202018-08-22%20at%209.10.40%20PM.png?dl=1\" width=\"370\"> </center>\n\n- 여기서 $g$는 objective 의 gradient입니다.\n- $j_n = \\nabla_{\\phi} V_{\\phi}(s_n)$일때, $H = \\frac{1}{N} \\sum_{n} j_n j^T_n$이며, $H$는 objective의 hessian에 대해서 gaussian newton method로 근사한 값입니다. 따라서, value function을 conditional probability로 해석한다면 Fisher information matrix가 됩니다.\n- 구현할때의 방법은 TRPO에서 사용한 방법과 모두 같습니다.\n\n<br><br>\n\n# 7.Experiments\n\n실험은 다음 두 가지 물음에 대해서 디자인 되었습니다.\n\n- GAE에 따라서 episodic total reward를 최적화 할때, $\\lambda$와 $\\gamma$가 변함에 따라서 어떤 경험적인 효과를 볼 수 있는지?\n- GAE와 trust region alogorithm을 policy와 value function 모두에 함께 사용했을 때 어려운 문제에 적용되는 큰 뉴럴넷을 최적화 할 수 있을지?\n\n<br>\n## 7.1 Policy Optimization Algorithm\n\nPolicy update는 TRPO를 사용합니다. TRPO에 대한 설명은 여기서는 생략하겠습니다. TRPO 포스트를 보고 돌아와주세요!\n\n- 이전 TRPO에서 이미 TRPO와 다른 많은 알고리즘들을 비교하였기 때문에, 여기서는 똑같은 짓을 반복하지 않고 $\\lambda$, $\\gamma$가 변함에 따라 어떤 영향이 있는 지에 대한 실험에 집중하겠다고 합니다. (귀찮았던거죠..)\n\n- TRPO를 적용한 GAE의 최종 알고리즘은 다음과 같습니다.\n\n<center> <img width = \"600px\" src=\"https://www.dropbox.com/s/b1klz11f2frrvg4/Screenshot%202018-07-08%2010.33.54.png?dl=1\"> </center>\n<center> <img width = \"300px\" src=\"https://www.dropbox.com/s/35fxte05pqtiel7/Screenshot%202018-07-08%2010.35.22.png?dl=1\"> </center>\n<center> <img width = \"300px\" src=\"https://www.dropbox.com/s/u35pjow3w50bvz1/Screenshot%202018-07-08%2010.36.03.png?dl=1\"> </center>\n\n- 여기서 주의할 점은 Policy update($\\theta_i \\rightarrow \\theta_{i+1}$)에서 $V_{\\phi_i}$를 사용했다는 점입니다.\n- 만약 Value function을 먼저 update하게 된다면 추가적인 bias가 발생합니다.\n- 극단적으로 생각해보아서, 우리가 Value function을 완벽하게 overfit을 해낸다면 Bellman residual($r_t + \\gamma V(s_{t+1}) - V(S_t)$)은 0이 됩니다. 그럼 Policy gradient의 estimation도 거의 0이 될 것입니다.\n\n<br>\n## 7.2 Expermint details\n\n### 7.2.1 Environment \n실험에서 사용된 환경은 다음 네 가지 입니다.\n\n1. classic cart-pole (x 3D)\n2. bipedal locomotion\n3. quadrupedal locomotion\n4. dynamically standing up for the biped\n\n### 7.2.2 Architecture\n\n- 3D robot task에 대해서는 같은 모델을 사용하였습니다.\n    - layers  = [100, 50, 25] 각각 tanh 사용. (Policy와 Value 네트워크 모두)\n    - Final output layer은 linear\n- Cartpole에 대해서는 1개의 layer 안에 20개의 hidden unit만 있는 linear policy를 사용했다고 합니다.\n\n### 7.2.3 Task\n\n- Cartpole\n    - 한 배치당 20 개의 trajectory를 모았고, maximum length는 1000입니다.\n- 3D biped locomotion\n    - 33 dim state , 10 dim action\n    - 50000 time step per batch\n- 3D quadruped locomotion\n    - 29 dim state, 8 dim action\n    - 200000 time step per batch\n- 3D biped locomotion Standing\n    - 33 dim state , 10 dim action\n    - 200000 time step per batch\n\n\n### 7.2.3 results\ncost의 관점에서 결과를 나타내었다고 합니다. Cost는 negative reward와 이것이 최소화 되었는가로 정의되었습니다.\n\n#### 7.2.3.1 Cartpole\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/x9pbms1wvg38lda/Screenshot%202018-07-08%2011.08.22.png?dl=1\"> </center>\n\n- 왼쪽 그림은 $\\gamma$를 0.99로 고정시켜놓은 상태에서 $\\lambda$를 변화시킴에 따라서 cost를 측정한 것입니다. \n- 오른쪽은 $\\gamma$와 $\\lambda$를 둘 다 변화 시키면서 성능을 그림으로 나타낸 표입니다. 흰색에 가까울 수록 좋은 성능입니다. \n\n#### 7.2.3.2 3D BIPEDAL LOCOMOTINO\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/i9wj4p6ijojsy82/Screenshot%202018-07-08%2011.25.08.png?dl=1\"> </center>\n\n\n- 다른 random seed로 부터 9번 씩 시도한 결과를 mean을 취해서 사용합니다.\n- Best performance는   $\\gamma \\in [0.99, 0.995]$ 그리고  $\\lambda \\in [0.96, 0.99]$일때. \n- 1000 iteration 후에 빠르고 부드럽고 안정적인 걸음거이가 나옵니다.\n- 실제로 걸린 시간은 0.01(타입스텝당 시간) * 50000(배치당 타임스텝) * 1000(배치) * 3600(초->시간) * 24 = 5.8일 정도가 걸렸습니다.\n\n#### 7.2.3.3 다른 ROBOT TASKS\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/fuuat65we52quht/Screenshot%202018-07-08%2011.33.11.png?dl=1\"> </center>\n\n- 다른 로봇 TASK에 대해서는 아주 제한적인 실험만 진행합니다.(시간이 부족했던듯 하네요..)\n- Quadruped에 대해서는 $\\gamma = 0.995$로 fix, $\\lambda \\in {0, 0.96}$\n- Standingup에 대해서는 $\\gamma = 0.99$로 fix, $\\lambda \\in {0, 0.96}$\n\n<br><br>\n\n# 8. Discussion\n\n<br>\n## 8.1 Main discussion\n\n지금까지 복잡하고 어려운 control problem에서 Reinforcement Learning(RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 \"variance reduction\"에 대해 연구하였습니다.\n\n\"Generalized Advantage Estimator(GAE)\"라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.\n또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는 지를 보였습니다.\n\n이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.\n\nGAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.\n\n<br>\n## 8.2 Future work\n\nValue function estimation error와 Policy gradient estimation error사이의 관계를 알아낸다면, 우리는 Value function fitting에 더 잘 맞는 error metric(policy gradient estimation 의 정확성과 더 잘 맞는 value function)을 사용할 수 있습니다. 여기서 Policy와 Value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.\n\n추가적으로 DDPG는 별로라고 합니다. 그리고 TD(0)는 bias가 너무 크고, poor performance로 이끈다고 합니다. 특히나 이 논문에서는 low-dimention의 쉬운 문제들만 해결했습니다.\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/nhc7t9psul5lr3x/Screenshot%202018-07-08%2011.45.15.png?dl=1\"> </center>\n\n<br>\n## 8.3 FAQ\n\n- Compatible features와는 무슨 관계?\n     - Compatible features는 value function을 이용하는 policy gradient 알고리즘들과 함께 자주 언급됩니다.\n     - Actor Critic의 저자는 policy의 제한된 representation power때문에, policy gradient는 단지 advantage function space의 subspace에만 의존하게 됩니다.\n     - 이 subspace는 compatible features에 의해 span됩니다.\n     - 이 이론은 현재 문제 구조를 어떻게 이용해야 advantage function에 대해 더 나은 estimation을 할 수 있는 지에 대한 지침을 주지 않습니다. GAE 논문의 idea와 orthogonal합니다.\n- 왜 Q function을 사용하지 않는가?\n     - 먼저 state-value function이 더 낮은 차원의 input을 가진다. 그래서 Q function보다 더 학습하기가 쉽습니다.\n     - 두 번째로 이 논문에서 제안하는 방법으로는 high bias estimator에서 low bias estimator로 $\\lambda$를 통해서 부드럽게 interpolate를 할 수 있습니다.\n     - 반면에 Q를 사용하면 단지 high-bias estimator 밖에 사용할 수 없습니다.\n     - 특히나 return에 대한 one-step estimation은 엄두를 못낼 정도로 bias가 큽니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [TRPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/)\n\n## [TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py)\n\n<br>\n\n# 다음으로\n\n## [PPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/)\n","source":"_posts/6_gae.md","raw":"---\ntitle: High-Dimensional Continuous Control using Generalized Advantage Estimation\ndate: 2018-06-23 19:18:45\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 양혁렬, 이동민\nsubtitle: 피지여행 6번째 논문\n---\n\n<center> <img src=\"https://www.dropbox.com/s/p8gfpyo6xf9wm5w/Screen%20Shot%202018-07-18%20at%201.25.53%20AM.png?dl=1\" width=\"700\"> </center>\n\n논문 저자 : John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan and Pieter Abbeel\n논문 링크 : https://arxiv.org/pdf/1506.02438.pdf\nProceeding : International Conference of Learning Representations (ICLR) 2016\n정리 : 양혁렬, 이동민\n\n---\n\n# 1. 들어가며...\n\n현존하는 Policy Gradient Method들의 목적은 누적되는 reward들을 optimization하는 것입니다. 하지만 학습할 때에 많은 양의 sample이 필요로 하고, 들어오는 data가 nonstationarity임에도 불구하고 stable and steady improvement가 어렵습니다.\n\n그래서 이 논문에서는 다음과 같은 방법을 제시합니다.\n\n- TD($\\lambda$)와 유사한 advantage function의 exponentially-weighted estimator를 사용하여 policy gradient estimate의 variance를 줄이는 것 \n- policy와 value function에 대한 Trust Region Optimization 사용하는 것\n\n3D locomotion tasks에 대한 empirical results는 다음과 같습니다.\n\n- bipedal and quadrupedal simulated robots의 달리는 자세를 학습\n- bipedal 사람이 땅에 누워있다가 일어서는 것을 학습\n\n※ TD($\\lambda$)를 혹시 모르실 경우도 있을 것 같아 간략하게 정리하였습니다. http://dongminlee.tistory.com/10 를 먼저 보시고 아래의 내용을 봐주시기 바랍니다!\n\n<br><br>\n\n# 2. Introduction\n\n기본적으로 \"parameterized stochastic policy\"를 가정합니다. 이 때 expected total returns의 gradient에 대한 unbiased estimate를 얻을 수 있는데 이것을 REINFORCE라고 부릅니다. 하지만 하나의 action의 결과가 과거와 미래의 action의 결과로 혼동되기 때문에 gradient estimator의 high variance는 시간에 따라 scaling됩니다.\n\n또 다른 방법은 Actor-Critic이 있습니다. 이 방법은 empirical returns보다 하나의 value function을 사용합니다. 또한 bias하고 lower variance를 가진 estimator입니다. 구체적으로 말하자면, high variance하다면 더 sampling을 하면 되는 반면에 bias는 매우 치명적입니다. 다시 말해 bias는 algorithm이 converge하는 데에 실패하거나 또는 local optimum이 아닌 poor solution에 converge하도록 만듭니다.\n\n따라서 이 논문에서는 $\\gamma\\in [0,1]$ and $\\lambda\\in [0,1]$에 의해 parameterized estimation scheme인 Generalized Advantage Estimator(GAE)를 다룹니다.\n\n이 논문을 대략적으로 요약하자면 다음과 같습니다.\n\n- 효과적인 variance reduction scheme인 GAE를 다룹니다. 또한 실험할 때 batch trust region algorithm에 더하여 다양한 algorithm들에 적용됩니다.\n- value function에 대해 trust region optimization method를 사용합니다. 이렇게 함으로서 더 robust하고 efficient한 방법이 됩니다.\n- A와 B를 합쳐서, 실험적으로 control task에 neural network policies를 learning하는 데에 있어서 효과적인 algorithm을 얻습니다. 이러한 결과는 high-demensional continuous control에 RL을 사용함으로서 state of the art로 확장되었습니다.\n\n<br><br>\n\n# 3. Preliminaries\n\n먼저 policy optimization의 \"undiscounted formulation\"을 가정합니다. (undiscounted formulation에 주목합시다.)\n\n- initial state $s_0$는 distribution $\\rho_0$으로부터 sampling된 것입니다.\n- 하나의 trajectory ($s_0, a_0, s_1, a_1, ...$)는 terminal state에 도달될 때 까지 policy $a_t$ ~ $\\pi(a_t | s_t)$에 따라서 action을 sampling하고, dynamics $s_{t+1}$ ~ $P(s_{t+1} | s_t, a_t)$에 따라서 state를 sampling함으로써 생성됩니다.\n- reward $r_t = r(s_t, a_t, s_{t+1})$은 매 time step마다 받아집니다.\n- 목표는 모든 policies에 대해 finite하다고 가정됨으로서 expected total reward $\\sum_{t=0}^{\\infty} r_t$를 maximize하는 것입니다.\n\n여기서 중요한 점은 $\\gamma$를 discount의 parameter로 사용하는 것이 아니라 \"bias-variance tradeoff를 조절하는 parameter로 사용한다\" 는 것입니다.\n\npolicy gradient method는 gradient $g := \\nabla_\\theta \\mathbb{E} [\\sum_{t=0}^\\infty r_t]$를 반복적으로 estimate함으로써 expected total reward를 maximize하는 것인데, policy gradient에는 여러 다른 표현들이 있습니다.\n$$g = \\mathbb{E} [\\sum_{t=0}^\\infty \\Phi_t \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$\n\n여기서 $\\Phi_t$는 아래 중의 하나일 수 있습니다.\n\n1. $\\sum_{t=0}^\\infty r_t$: trajectory의 total reward\n2. $\\sum_{t'=t}^\\infty r_t'$: action $a_t$ 후의 reward\n3. $\\sum_{t'=t}^\\infty r_t' - b(s_t)$: 2의 baselined version\n4. $Q^\\pi (s_t, a_t)$: state-action value function\n5. $A^\\pi (s_t, a_t)$: advantage function\n6. $r_t + V^\\pi (s_{t+1}) - V^\\pi (s_t)$: TD residual\n\n위의 번호 중 4, 5, 6의 수식들은 다음의 정의를 사용합니다.\n\n- $V^\\pi (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty r_{t+1}]$\n- $Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty r_{t+l}]$\n- $A^\\pi (s_t, a_t) := Q^\\pi (s_t, a_t) - V^\\pi (s_t)$, (Advantage function)\n\n추가적으로 colon notation $a : b$는 포괄적인 범위 $(a, a+1, ... , b)$입니다. (잘 기억해둡시다. 뒤에 계속해서 colon notation이 나옵니다.)\n\n여기서부터 parameter $\\gamma$에 대해 좀 더 자세히 알아봅시다. parameter $\\gamma$는 bias하면서 동시에 reward를 downweighting함으로써 variance를 줄입니다. 다시 말해 MDP의 discounted formulation에서 사용된 discounted factor와 일치하지만, 이 논문에서는 \"$\\gamma$를 undiscounted MDP에서 variance reduction parameter\"로 다룹니다. -> 결과는 같지만, 의미와 의도가 다릅니다.\n\ndiscounted value function들은 다음과 같습니다.\n\n- $V^{\\pi, \\gamma} (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$ \n- $Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$\n- $A^{\\pi, \\gamma} (s_t, a_t) := Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)$\n\n따라서 policy gradient에서의 discounted approximation은 다음과 같이 나타낼 수 있습니다.\n$$g^\\gamma := \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\sum_{t=0}^\\infty A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$\n뒤이어 나오는 section에서는 위의 수식에서 $A^{\\pi, \\gamma}$에 대해 biased (but not too biased) estimator를 얻는 방법에 대해서 나옵니다.\n\n다음으로 advantage function에서 새롭게 접하는 \"$\\gamma$-just estimator\"의 notation에 대해 알아봅시다.\n\n- 먼저 $\\gamma$-just estimator는 $g^\\gamma$ ${}^1$를 estimate하기 위해 위의 수식에서 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 사용할 때, bias하지 않은 estimator라고 합시다. 그리고 이 $\\gamma$-just advantage estimator를 <img src=\"https://www.dropbox.com/s/7g2glqx2kbfynaa/Screen%20Shot%202018-08-22%20at%209.03.46%20PM.png?dl=1\" width=\"100\"> 라고 하고, 전체의 trajectory에 대한 하나의 function이라고 합시다.\n- ${}^1$에서 이 논문의 저자가 하고 싶은 말이 있는 것 같습니다. 개인적으로 $\\gamma$-just estimator를 이해하는 데에 있어서 중요한 주석이라 정확히 해석하고자 합니다.\n    - $A^\\pi$를 $A^{\\pi, \\gamma}$로 사용함으로써 이미 bias하다라고 말했지만, \"이 논문에서 하고자 하는 것은 $g^\\gamma$에 대해 unbiased estimate를 얻고 싶은 것입니다.\" 하지만 undiscounted MDP의 policy gradient에 대해서는 당연히 $\\gamma$를 사용하기 때문에 biased estimate를 얻습니다. 개인적으로 이것은 일단 무시하고 $\\gamma$를 사용할 때 어떻게 unbiased estimate를 얻을 지에 대해 좀 더 포커스를 맞추고 있는 것 같습니다.\n    - 그러니까 저 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 바꿔줌으로써 unbiased estimate를 하고 싶다는 것이 뒤이어 나오는 정의와 명제의 핵심이라고 할 수 있습니다.\n\nDefinition 1.\n먼저 가정을 합니다. (가정을 바탕으로 이루어지는 정의라는 것을 주목합시다.) 만약\n<center ><img src=\"https://www.dropbox.com/s/aia9f1l99wyxgah/Screen%20Shot%202018-08-22%20at%209.07.05%20PM.png?dl=1\" width=\"700\"> </center>\n두 수식이 같다면, estimator $\\hat{A}_t$는 $\\gamma$-just입니다.\n\n그리고 만약 모든 t에 대해서 $\\hat{A}_t$이 $\\gamma$-just이라면, 다음과 같이 표현할 수 있습니다.\n\n<center ><img src=\"https://www.dropbox.com/s/r4m6ktw8geeu7pe/Screen%20Shot%202018-08-22%20at%209.29.18%20PM.png?dl=1\" width=\"420\"> </center>\n\n위의 수식이 바로 unbiased estimate입니다.\n\n$\\gamma$-just인 $\\hat{A}_t$에 대한 한가지 조건은 $\\hat{A}_t$이 두 가지 function $Q_t$ and $b_t$로 나뉠 수 있다는 것입니다.\n\n- $Q_t$는 $\\gamma$-discounted Q-function의 unbiased estimator입니다.\n- $b_t$는 action $a_t$전에 sampling된 states and actions의 arbitrary function이다. \n\nProposition 1.\n모든 $(s_t, a_t)$에 대해,\n$$\\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty | s_t, a_t} [Q_t (s_{t:\\infty}, a_{t:\\infty})] = Q^{\\pi, \\gamma} (s_t, a_t)$$\n로 인하여 $\\hat{A}_t$이 \n\n<center> <img src=\"https://www.dropbox.com/s/59ydsav6djwyevl/Screen%20Shot%202018-08-22%20at%209.27.13%20PM.png?dl=1\" width=\"380\"> </center>\n\n형태라고 가정합시다. (가정을 바탕으로 이루어지는 명제라는 점을 주목합시다.)\n\n그 때, $\\hat{A}_t$은 $\\gamma$-just입니다.\n\n이 명제에 대한 증명은 Appendix B에 있습니. 그리고 $\\hat{A}_t$에 대한 $\\gamma$-just advantage estimator들은 다음과 같습니다.\n\n- $\\sum_{l=0}^\\infty \\gamma^l r_{t+1}$\n- $A^{\\pi, \\gamma} (s_t, a_t)$\n- $Q^{\\pi, \\gamma} (s_t, a_t)$\n- $r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)$\n\n증명을 보기 전에 먼저 Advantage function에 대해서 먼저 살펴봅시다. 아래의 내용은 Sutton이 쓴 논문인 Policy Gradient Methods for Reinforcement Learning with Function Approximation(2000)에서 나온 내용을 리뷰한 것입니다. \n\n<center> <img src=\"https://www.dropbox.com/s/x737yq97ut6gp1a/Screen%20Shot%202018-07-15%20at%201.16.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n여기서는 \"이 수식은 $\\sum_a \\frac{\\partial \\pi (s,a)}{\\partial \\theta} = 0$이기 때문에 가능해진다.\", \"즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없다\"라는 두 문장을 기억해둡시다.\n\n이제 증명을 보면 됩니다.\n\n<center> <img src=\"https://www.dropbox.com/s/e7sj2fwcm1f7hof/figure2.jpg?dl=1\" width=\"600\"> </center>\n<center> <img src=\"https://www.dropbox.com/s/kyk4jes1202az4m/figure3.jpg?dl=1\" width=\"600\"> </center>\n\n위의 증명 수식에서 Q와 b의 각각 세번째 수식과 마지막 수식을 자세히 봅시다.\n\n1. $Q$에 대한 증명\n    - 빨간색 부분\n        - $\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [Q_t (s_{0:\\infty}, a_{0:\\infty})]$를 쉽게 말하자면 \"모든(과거, 현재, 미래) state와 action에 대해서 expectation을 $t+1$부터 $\\infty$까지 하겠다.\"라는 말입니다. 이렇게 함으로써 원래는 $Q^\\pi (s_t, a_t)$가 나와야 맞는 건데, 앞에서 살펴봤다시피 4번째 수식 밑줄 친 자리에는 advantage 역할을 하는 함수들을 넣어도 결과값에 아무런 영향을 끼치지 않기 때문에 의도적으로 $A^\\pi (s_t, a_t)$로 바꾼 것입니다. 또한 이 증명을 바탕으로 $\\hat{A}$이 $\\gamma$-just라는 것을 표현하고 싶기 때문이라고 봐도 됩니다.\n\n    - 파란색 부분\n        - 또한 $a_{0:t}$에서 $a_{0:t-1}$로 변한 것은 $A^\\pi (s_t, a_t)$에 baseline이 들어가기 때문에 바꿔준 것입니다.\n\n2. $b$에 대한 증명\n    - 빨간색 부분\n        - $\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$가 0으로 바뀌는 것은 위에서 설명했듯이 $\\nabla_\\theta$자체가 $\\log \\pi_\\theta$만 gradient하기 때문에 이것을 expectation을 취하면 0이 됩니다.\n\n<br><br>\n\n# 4. Advantage Function Estimation\n\n이번 section에는 discounted advantage function $A^{\\pi, \\gamma} (s_t, a_t)$의 accurate estimate $\\hat{A}_t$에 대해서 살펴봅시다. 이에 따른 수식은 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/sn16g5iyzzflju0/Screen%20Shot%202018-08-22%20at%209.22.49%20PM.png?dl=1\" width=\"300\"> </center>\n\n여기서 n은 a batch of episodes에 대하여 index한 것입니다.\n\n$V$를 approximate value function이라고 합시다. 그리고 $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$이라고 합시다.\n\n만약 (이전과 마찬가지로 가정부터 합니다.) correct value function $V = V^{\\pi, \\gamma}$가 있다고 한다면, 이것은 $\\gamma$-just advantage estimator입니다. 실제로, $A^{\\pi, \\gamma}$의 unbiased estimator는 다음과 같습니다.\n$$\\mathbb{E}_{s_{t+1}} [\\delta_t^{V^{\\pi, \\gamma}}] = \\mathbb{E}_{s_{t+1}} [r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)]$$\n$$= \\mathbb{E}_{s_{t+1}} [Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)] = A^{\\pi, \\gamma} (s_t, a_t)$$\n그러나, \"이 estimator는 유일하게 $V = V^{\\pi, \\gamma}$에 대한 $\\gamma$-just입니다.\" 다른 경우라면 이것은 biased policy gradient estimate일 것입니다. (우리가 하고 싶은 것은 $V$에 대해서만 unbiased estimator가 아니라 advantage function에 대해서 일반화된 unbiased estimator를 얻고 싶은 것입니다. 그래서 아래에서도 나오겠지만, $\\gamma$와 함께 $\\lambda$를 이용한 estimator가 나옵니다.ㅏ)\n\n그래서 $\\delta$에 대해 $k$의 sum으로 생각해봅시다. 이것을 $\\hat{A}_t^{(k)}$라고 하자. 그러면 아래와 같이 표현할 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ra7hxksveg2hz45/figure4.jpg?dl=1\" width=\"600\"> </center>\n\n$\\hat{A}_t^{(k)}$은 returns의 $k$-step estimate와 연관지을 수 있고, $\\delta_t^V = \\hat{A}_t^{(1)}$의 case와 유사하게도 $\\hat{A}_t^{(k)}$를 advantage function의 estimator로 생각할 수 있습니다.\n\n여기서 $k \\rightarrow \\infty$로 생각해보면 bias가 일반적으로 점점 더 작아집니다. 왜냐하면 $\\gamma^k V(s_{t+k})$가 점점 많이 discounted되서 $-V(s_t)$가 bias에 영향을 미치지 못하기 때문입니다. $k \\rightarrow \\infty$를 취하면 다음과 같은 수식이 나옵니다.\n\n<center> <img src=\"https://www.dropbox.com/s/13fn9wcup8pfh9u/Screen%20Shot%202018-08-22%20at%209.19.52%20PM.png?dl=1\" width=\"320\"> </center>\n\n우변의 수식과 같이 empirical returns에서 value function baseline을 뺀 것으로 나타낼 수 있습니다.\n\nGeneralized Advantage Estimator GAE($\\gamma, \\lambda$)는 $k$-step estimators의 exponentially-weighted average로 나타낼 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/yg1ybmfkep3towu/figure5.jpg?dl=1\" width=\"600\"> </center>\n\n(TD($\\lambda$)를 떠올려주세요..!)\n\n위의 마지막 수식에서도 알 수 있듯이, advantage estimator는 Bellman residual terms의 discounted sum과 관련있는 간단한 수식입니다. 다음 section에서 modified reward function을 가진 MDP에 returns로서의 관점에서 위의 마지막 수식을 더 자세히 살펴봅시다. 위의 수식은 TD($\\lambda$)와 많이 유사합니다. 그러나 TD($\\lambda$)는 value function를 estimator하고, 여기서는 advantage function을 estimator합니다.\n\n위의 수식에서 $\\lambda = 0$ and $\\lambda = 1$에 대해서는 특별한 case가 존재한다. 수식으로 표현하면 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ufglxzanhcnbi36/figure6.jpg?dl=1\" width=\"600\"> </center>\n\n- $GAE(\\gamma, 1)$은 $V$의 정확도와 관계없이 $\\gamma$-just입니다. 그러나 returns의 sum때문에 high variance합니다.\n- $GAE(\\gamma, 0)$은 $V = V^{\\pi, \\gamma}$에 대해 $\\gamma$-just입니다. 그리고 bias하지만 일반적으로 훨씬 lower variance를 가집니다. \n- $0 < \\lambda < 1$에 대해 GAE는 parameter $\\lambda$를 control하여 bias와 variance사이에 compromise를 만듭니다.\n\n두 가지 별개의 parameter $\\gamma$ and $\\lambda$를 가진 advantage estimator는 bias-variance tradeoff에 도움을 줍니다. 그러나 이 두 가지 parameter는 각각 다른 목적을 가지고 작동합니다.\n\n- $\\gamma$는 가장 중요하게 value function $V^{\\pi, \\gamma}$ 의 scale을 결정합니다. 또한 $\\gamma$는 $\\lambda$에 의존하지 않습니다. $\\gamma < 1$로 정하는 것은 policy gradient estimate에서 bias합니다.\n- 반면에, $\\lambda < 1$는 유일하게 value function이 부정확할 때 bias합니다. 그리고 경험상, $\\lambda$의 best value는 $\\gamma$의 best value보다 훨씬 더 낮습니다. 왜냐하면 $\\lambda$가 정확한 value function에 대해 $\\gamma$보다 훨씬 덜 bias하기 때문입니다.\n\nGAE를 사용할 때, $g^\\gamma$의 biased estimator를 구성할 수 있습니다. 수식은 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/ok57dsk52o7pl9r/Screen%20Shot%202018-08-22%20at%209.17.54%20PM.png?dl=1\" width=\"670\"> </center>\n\n여기서 $\\lambda = 1$일 때 동일해집니다.\n\n<br><br>\n\n# 5. Interpretation as Reward Shaping\n\n이번 section에서는 앞서 다뤘던 수식 $\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V$를 modified reward function의 MDP의 관점으로 생각해봅시다. 조금 더 구체적으로 말하자면, MDP에서 reward shaping transformation을 실행한 후에 적용된 extra discounted factor로서 $\\lambda$를 어떻게 볼 것인지에 대해서 다룹니다.\n\n(개인적인 comment) 한 가지 먼저 언급하자면, 본래의 목적은 reward shaping이 아니라 variance reduction입니다. 이번 section은 그저 이전에 이러한 개념이 있었고, gae를 다른 관점에서 생각해보자라는 뜻에서 나온 section인 것 같습니다. '아~ 이러한 개념이 있구나~!' 정도로만 알면 될 것 같습니다. 'gae가 reward shaping의 효과까지 있어서 이러한 section을 넣은 것일까?' 라는 생각도 해봤지만 아직 잘 모르겠습니다. 실험부분에도 딱히 하지 않은 걸로 봐서는.. 아닌 것 같기도 하고.. 아직까지는 그저 'reward shaping의 관점에서 봤을 때에도 gae로 만들어줄 수 있다.' 정도만 생각하려고 합니다.\n\nReward Shaping이란 개념 자체는 일반적인 알고리즘 분야(최단경로 문제 등)에도 있습니다. 하지만 이 개념이 머신러닝에 적용된 건 Andrew Y. Ng이 쓴 Policy Invariance under Reward Transformations Theory and Application to Reward Shaping(1999) 논문입니다. 논문에서 Reward Shaping을 설명하는 부분은 간략하게 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/0w9cuxjkhz2mbcy/figure7.jpg?dl=1\" width=\"400\"> </center>\n<center> <img src=\"https://www.dropbox.com/s/xcp9eqvcbv3268g/figure8.jpg?dl=1\" width=\"400\"> </center>\n\n위의 글만 보고 이해가 잘 안됩니다. 그래서 다른 자료들을 찾던 중에 다음의 그림을 찾게 되었습니다. (아래의 그림은 Youtube에 있는 [Udacity 영상](https://www.youtube.com/watch?v=xManAGjbx2k&t=95s)에서 있는 그림입니다.)\n\n<center> <img src=\"https://www.dropbox.com/s/87kacngez9cl2e6/figure9.jpg?dl=1\" width=\"400\"> </center>\n\n이 그림을 통해서 Reward Shaping을 이해해봅시다.\n\nReward Shaping(보상 형성)을 통해서 뭘 하고 싶은지 목적부터 봅시다. reward space가 sparse 한 경우에 reward가 너무 드문드문 나옵니다. 따라서 이것을 꾸준히 reward를 받을 수 있도록 바꿉니다. potential-based shaping function인 $\\Phi$를 만들어서 더하고 빼줍니다. 저 $\\Phi$ 자리에는 대표적으로 state value function이 많이 들어간다고 합니다. (아직 목적이 이해가 안될 수 있습니다. Reward Shaping에 대해서 다 보고 나서 다시 한 번 읽어봅시다.)\n\n위의 그림은 돌고래가 점프하여 불구멍을 통과해야하는 환경입니다. 하지만 저 불구멍을 통과하기 위해서는 (1) 점프도 해야하고, (2) 불도 피해야하고, (3) 알맞게 착지까지 완료해야합니다. 이렇게 해야 reward를 +1 얻습니다. \n\n생각해봅시다. 어느 세월에 점프도 해야하고.. 불도 피해야하고.. 알맞게 착지까지해서 reward를 +1 얻겠습니까? 따라서 그 전에도 잘하고 있는 지, 못하고 있는 지를 판단하기 위해, 다시 말해 reward를 꾸준히 받도록 다음과 같이 transformed reward function $\\tilde{r}$을 정의합니다.\n\n$$\\tilde{r} (s, a, s') = r(s, a, s') + \\gamma \\Phi (s') - \\Phi (s)$$\n\n- 여기서 $\\Phi : S \\rightarrow \\mathbb{R}$를 state space에서의 arbitrary scalar-valued function을 나타냅니다. 그리고 $\\Phi$자리에는 대표적으로 state value function이 들어간다고 생각합시다.\n- 형태가 TD residual term의 결과와 비슷하지만 의미와 의도가 다릅니다. reward shaping은 sparse reward 때문이고, 이전 section에서 봤던 gae는 variance reduction때문에 나온 것입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/tkfpqniaazwfbld/figure10.jpg?dl=1\" width=\"600\"> </center>\n\n이 transformation은 discounted advantage function $A^{\\pi, \\gamma}$으로도 둘 수 있습니다. state $s_t$를 시작하여 하나의 trajectory의 rewards의 discounted sum을 표현하면 다음과 같습니다.\n$$\\sum_{l=0}^\\infty \\gamma^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty \\gamma^l r(s_{t+l}, a_{t+l}, s_{t+l+1}) - \\Phi(s_t)$$\n\n$\\tilde{Q}^{\\pi, \\gamma}, \\tilde{V}^{\\pi, \\gamma}, \\tilde{A}^{\\pi, \\gamma}$를  transformed MDP의 value function과 advantage function이라고 하면, 다음과 같은 수식이 나옵니다.\n$$\\tilde{Q}^{\\pi, \\gamma} (s, a) = Q^{\\pi, \\gamma} (s, a) - \\Phi (s)$$\n$$\\tilde{V}^{\\pi, \\gamma} (s, a) = V^{\\pi, \\gamma} (s, a) - \\Phi (s)$$\n$$\\tilde{A}^{\\pi, \\gamma} (s, a) = (Q^{\\pi, \\gamma} (s, a) - \\Phi (s)) - (V^\\pi (s) - \\Phi (s)) = A^{\\pi, \\gamma} (s, a)$$\n\n이제 reward shaping의 idea를 가지고 어떻게 policy gradient estimate를 얻을 수 있는 지에 대해서 알아봅시다.\n\n- $0 \\le \\lambda \\le 1$의 범위에 있는 steeper discount $\\gamma \\lambda$를 사용합니다.\n- shaped reward $\\tilde{r}$는 Bellman residual term $\\delta^V$와 동일합니다.\n- 그리고 $\\Phi = V$와 같다고 보면 다음과 같은 수식이 나옵니다.\n\n$$\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V = \\hat{A}_t^{GAE(\\gamma, \\lambda)}$$\n이렇게 shaped rewards의 $\\gamma \\lambda$-discounted sum을 고려함으로써 GAE를 얻을 수 있습니다. 더 정확하게 $\\lambda = 1$은 $g^\\gamma$의 unbiased estimate이고, 반면에 $\\lambda < 1$은 biased estimate입니다.\n\n좀 더 나아가서, shaping transformation과 parameters $\\gamma$ and $\\lambda$의 결과를 보기 위해, response function $\\chi$를 이용하면 다음과 같은 수식이 나옵니다.\n$$\\chi (l; s_t, a_t) = \\mathbb{E} [r_{t+l} | s_t, a_t] - \\mathbb{E} [r_{t+l} | s_t]$$\n추가적으로 $A^{\\pi, \\gamma} (s, a) = \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$입니다.\n그래서 discounted policy gradient estimator는 다음과 같이 쓸 수 있다.\n$$\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) A^{\\pi, \\gamma} (s_t, a_t) = \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$$\n\n<br><br>\n\n# 6. Value Fuction Estimation\n\n이번 section에서는 Trust Region Optimization Scheme에 따라 Value function을 Estimation합니다.\n\n<br>\n## 6.1 Simplest approach\n\n$$minimize_{\\phi} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi}(s_n) - \\hat{V_n} \\vert\\vert^{2}$$\n\n- 위는 가장 간단하게 non-linear approximation으로 푸는 방법입니다.\n- $\\hat{V_t} = \\sum_{l=0}^{\\infty}\\gamma^l r_{t+l}$은 reward 에 대한 discounted sum을 의미합니다.\n\n<br>\n## 6.2 Trust region method to optimize the value function\n\n- Value function을 최적화 하기 위해 trust region method를 사용합니다.\n- Trust region은 최근 데이터에 대해 overfitting되는 것을 막아줍니다.\n\nTrust region문제를 풀기 위해서는 다음 스텝을 따릅니다.\n\n- $\\sigma^2 = \\frac{1}{N} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi old}(s_n) - \\hat{V_n} \\vert\\vert^{2}$을 계산합니다.\n- 그 후에 다음과 같은 constrained opimization문제를 풉니다.\n\n<center> <img src=\"https://www.dropbox.com/s/d74cg3votxi2dbg/Screen%20Shot%202018-08-22%20at%209.13.28%20PM.png?dl=1\" width=\"300\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/jrkjax71t3j3vk4/Screen%20Shot%202018-08-22%20at%209.13.35%20PM.png?dl=1\" width=\"370\"> </center>\n\n- 위의 수식은 사실 old Value function과 새로운 Value function KL distance가 $\\epsilon$ 다 작아야한다는 수식과 같습니다. Value function이 평균은 $V_{\\phi}(s)$이고 분산이 $\\sigma^2$인 conditional Gaussian distribution으로 parameterize되었을 뿐입니다.\n\n<img width =\"400px\" src=\"https://www.dropbox.com/s/uw0v05feu8chqmc/Screenshot%202018-07-08%2010.04.38.png?dl=1\"> \n\n이 trust region문제의 답을 conjudate gradient algorithm을 이용하여 근사값을 구할 수 있습니다. 특히, 다음과 같은  quadratic program을 풀게됩니다.\n\n<center> <img src=\"https://www.dropbox.com/s/pawjkm8b9ycwsl5/Screen%20Shot%202018-08-22%20at%209.10.36%20PM.png?dl=1\" width=\"250\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/va7wms8uqxnw5rq/Screen%20Shot%202018-08-22%20at%209.10.40%20PM.png?dl=1\" width=\"370\"> </center>\n\n- 여기서 $g$는 objective 의 gradient입니다.\n- $j_n = \\nabla_{\\phi} V_{\\phi}(s_n)$일때, $H = \\frac{1}{N} \\sum_{n} j_n j^T_n$이며, $H$는 objective의 hessian에 대해서 gaussian newton method로 근사한 값입니다. 따라서, value function을 conditional probability로 해석한다면 Fisher information matrix가 됩니다.\n- 구현할때의 방법은 TRPO에서 사용한 방법과 모두 같습니다.\n\n<br><br>\n\n# 7.Experiments\n\n실험은 다음 두 가지 물음에 대해서 디자인 되었습니다.\n\n- GAE에 따라서 episodic total reward를 최적화 할때, $\\lambda$와 $\\gamma$가 변함에 따라서 어떤 경험적인 효과를 볼 수 있는지?\n- GAE와 trust region alogorithm을 policy와 value function 모두에 함께 사용했을 때 어려운 문제에 적용되는 큰 뉴럴넷을 최적화 할 수 있을지?\n\n<br>\n## 7.1 Policy Optimization Algorithm\n\nPolicy update는 TRPO를 사용합니다. TRPO에 대한 설명은 여기서는 생략하겠습니다. TRPO 포스트를 보고 돌아와주세요!\n\n- 이전 TRPO에서 이미 TRPO와 다른 많은 알고리즘들을 비교하였기 때문에, 여기서는 똑같은 짓을 반복하지 않고 $\\lambda$, $\\gamma$가 변함에 따라 어떤 영향이 있는 지에 대한 실험에 집중하겠다고 합니다. (귀찮았던거죠..)\n\n- TRPO를 적용한 GAE의 최종 알고리즘은 다음과 같습니다.\n\n<center> <img width = \"600px\" src=\"https://www.dropbox.com/s/b1klz11f2frrvg4/Screenshot%202018-07-08%2010.33.54.png?dl=1\"> </center>\n<center> <img width = \"300px\" src=\"https://www.dropbox.com/s/35fxte05pqtiel7/Screenshot%202018-07-08%2010.35.22.png?dl=1\"> </center>\n<center> <img width = \"300px\" src=\"https://www.dropbox.com/s/u35pjow3w50bvz1/Screenshot%202018-07-08%2010.36.03.png?dl=1\"> </center>\n\n- 여기서 주의할 점은 Policy update($\\theta_i \\rightarrow \\theta_{i+1}$)에서 $V_{\\phi_i}$를 사용했다는 점입니다.\n- 만약 Value function을 먼저 update하게 된다면 추가적인 bias가 발생합니다.\n- 극단적으로 생각해보아서, 우리가 Value function을 완벽하게 overfit을 해낸다면 Bellman residual($r_t + \\gamma V(s_{t+1}) - V(S_t)$)은 0이 됩니다. 그럼 Policy gradient의 estimation도 거의 0이 될 것입니다.\n\n<br>\n## 7.2 Expermint details\n\n### 7.2.1 Environment \n실험에서 사용된 환경은 다음 네 가지 입니다.\n\n1. classic cart-pole (x 3D)\n2. bipedal locomotion\n3. quadrupedal locomotion\n4. dynamically standing up for the biped\n\n### 7.2.2 Architecture\n\n- 3D robot task에 대해서는 같은 모델을 사용하였습니다.\n    - layers  = [100, 50, 25] 각각 tanh 사용. (Policy와 Value 네트워크 모두)\n    - Final output layer은 linear\n- Cartpole에 대해서는 1개의 layer 안에 20개의 hidden unit만 있는 linear policy를 사용했다고 합니다.\n\n### 7.2.3 Task\n\n- Cartpole\n    - 한 배치당 20 개의 trajectory를 모았고, maximum length는 1000입니다.\n- 3D biped locomotion\n    - 33 dim state , 10 dim action\n    - 50000 time step per batch\n- 3D quadruped locomotion\n    - 29 dim state, 8 dim action\n    - 200000 time step per batch\n- 3D biped locomotion Standing\n    - 33 dim state , 10 dim action\n    - 200000 time step per batch\n\n\n### 7.2.3 results\ncost의 관점에서 결과를 나타내었다고 합니다. Cost는 negative reward와 이것이 최소화 되었는가로 정의되었습니다.\n\n#### 7.2.3.1 Cartpole\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/x9pbms1wvg38lda/Screenshot%202018-07-08%2011.08.22.png?dl=1\"> </center>\n\n- 왼쪽 그림은 $\\gamma$를 0.99로 고정시켜놓은 상태에서 $\\lambda$를 변화시킴에 따라서 cost를 측정한 것입니다. \n- 오른쪽은 $\\gamma$와 $\\lambda$를 둘 다 변화 시키면서 성능을 그림으로 나타낸 표입니다. 흰색에 가까울 수록 좋은 성능입니다. \n\n#### 7.2.3.2 3D BIPEDAL LOCOMOTINO\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/i9wj4p6ijojsy82/Screenshot%202018-07-08%2011.25.08.png?dl=1\"> </center>\n\n\n- 다른 random seed로 부터 9번 씩 시도한 결과를 mean을 취해서 사용합니다.\n- Best performance는   $\\gamma \\in [0.99, 0.995]$ 그리고  $\\lambda \\in [0.96, 0.99]$일때. \n- 1000 iteration 후에 빠르고 부드럽고 안정적인 걸음거이가 나옵니다.\n- 실제로 걸린 시간은 0.01(타입스텝당 시간) * 50000(배치당 타임스텝) * 1000(배치) * 3600(초->시간) * 24 = 5.8일 정도가 걸렸습니다.\n\n#### 7.2.3.3 다른 ROBOT TASKS\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/fuuat65we52quht/Screenshot%202018-07-08%2011.33.11.png?dl=1\"> </center>\n\n- 다른 로봇 TASK에 대해서는 아주 제한적인 실험만 진행합니다.(시간이 부족했던듯 하네요..)\n- Quadruped에 대해서는 $\\gamma = 0.995$로 fix, $\\lambda \\in {0, 0.96}$\n- Standingup에 대해서는 $\\gamma = 0.99$로 fix, $\\lambda \\in {0, 0.96}$\n\n<br><br>\n\n# 8. Discussion\n\n<br>\n## 8.1 Main discussion\n\n지금까지 복잡하고 어려운 control problem에서 Reinforcement Learning(RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 \"variance reduction\"에 대해 연구하였습니다.\n\n\"Generalized Advantage Estimator(GAE)\"라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.\n또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는 지를 보였습니다.\n\n이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.\n\nGAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.\n\n<br>\n## 8.2 Future work\n\nValue function estimation error와 Policy gradient estimation error사이의 관계를 알아낸다면, 우리는 Value function fitting에 더 잘 맞는 error metric(policy gradient estimation 의 정확성과 더 잘 맞는 value function)을 사용할 수 있습니다. 여기서 Policy와 Value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.\n\n추가적으로 DDPG는 별로라고 합니다. 그리고 TD(0)는 bias가 너무 크고, poor performance로 이끈다고 합니다. 특히나 이 논문에서는 low-dimention의 쉬운 문제들만 해결했습니다.\n\n<center> <img width = \"500px\" src=\"https://www.dropbox.com/s/nhc7t9psul5lr3x/Screenshot%202018-07-08%2011.45.15.png?dl=1\"> </center>\n\n<br>\n## 8.3 FAQ\n\n- Compatible features와는 무슨 관계?\n     - Compatible features는 value function을 이용하는 policy gradient 알고리즘들과 함께 자주 언급됩니다.\n     - Actor Critic의 저자는 policy의 제한된 representation power때문에, policy gradient는 단지 advantage function space의 subspace에만 의존하게 됩니다.\n     - 이 subspace는 compatible features에 의해 span됩니다.\n     - 이 이론은 현재 문제 구조를 어떻게 이용해야 advantage function에 대해 더 나은 estimation을 할 수 있는 지에 대한 지침을 주지 않습니다. GAE 논문의 idea와 orthogonal합니다.\n- 왜 Q function을 사용하지 않는가?\n     - 먼저 state-value function이 더 낮은 차원의 input을 가진다. 그래서 Q function보다 더 학습하기가 쉽습니다.\n     - 두 번째로 이 논문에서 제안하는 방법으로는 high bias estimator에서 low bias estimator로 $\\lambda$를 통해서 부드럽게 interpolate를 할 수 있습니다.\n     - 반면에 Q를 사용하면 단지 high-bias estimator 밖에 사용할 수 없습니다.\n     - 특히나 return에 대한 one-step estimation은 엄두를 못낼 정도로 bias가 큽니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [TRPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/)\n\n## [TRPO Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py)\n\n<br>\n\n# 다음으로\n\n## [PPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/)\n","slug":"6_gae","published":1,"updated":"2019-02-07T11:21:31.963Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjrujlu1n00275wfeuhgo5dxo","content":"<center> <img src=\"https://www.dropbox.com/s/p8gfpyo6xf9wm5w/Screen%20Shot%202018-07-18%20at%201.25.53%20AM.png?dl=1\" width=\"700\"> </center>\n\n<p>논문 저자 : John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan and Pieter Abbeel<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1506.02438.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1506.02438.pdf</a><br>Proceeding : International Conference of Learning Representations (ICLR) 2016<br>정리 : 양혁렬, 이동민</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p>현존하는 Policy Gradient Method들의 목적은 누적되는 reward들을 optimization하는 것입니다. 하지만 학습할 때에 많은 양의 sample이 필요로 하고, 들어오는 data가 nonstationarity임에도 불구하고 stable and steady improvement가 어렵습니다.</p>\n<p>그래서 이 논문에서는 다음과 같은 방법을 제시합니다.</p>\n<ul>\n<li>TD($\\lambda$)와 유사한 advantage function의 exponentially-weighted estimator를 사용하여 policy gradient estimate의 variance를 줄이는 것 </li>\n<li>policy와 value function에 대한 Trust Region Optimization 사용하는 것</li>\n</ul>\n<p>3D locomotion tasks에 대한 empirical results는 다음과 같습니다.</p>\n<ul>\n<li>bipedal and quadrupedal simulated robots의 달리는 자세를 학습</li>\n<li>bipedal 사람이 땅에 누워있다가 일어서는 것을 학습</li>\n</ul>\n<p>※ TD($\\lambda$)를 혹시 모르실 경우도 있을 것 같아 간략하게 정리하였습니다. <a href=\"http://dongminlee.tistory.com/10\" target=\"_blank\" rel=\"noopener\">http://dongminlee.tistory.com/10</a> 를 먼저 보시고 아래의 내용을 봐주시기 바랍니다!</p>\n<p><br><br></p>\n<h1 id=\"2-Introduction\"><a href=\"#2-Introduction\" class=\"headerlink\" title=\"2. Introduction\"></a>2. Introduction</h1><p>기본적으로 “parameterized stochastic policy”를 가정합니다. 이 때 expected total returns의 gradient에 대한 unbiased estimate를 얻을 수 있는데 이것을 REINFORCE라고 부릅니다. 하지만 하나의 action의 결과가 과거와 미래의 action의 결과로 혼동되기 때문에 gradient estimator의 high variance는 시간에 따라 scaling됩니다.</p>\n<p>또 다른 방법은 Actor-Critic이 있습니다. 이 방법은 empirical returns보다 하나의 value function을 사용합니다. 또한 bias하고 lower variance를 가진 estimator입니다. 구체적으로 말하자면, high variance하다면 더 sampling을 하면 되는 반면에 bias는 매우 치명적입니다. 다시 말해 bias는 algorithm이 converge하는 데에 실패하거나 또는 local optimum이 아닌 poor solution에 converge하도록 만듭니다.</p>\n<p>따라서 이 논문에서는 $\\gamma\\in [0,1]$ and $\\lambda\\in [0,1]$에 의해 parameterized estimation scheme인 Generalized Advantage Estimator(GAE)를 다룹니다.</p>\n<p>이 논문을 대략적으로 요약하자면 다음과 같습니다.</p>\n<ul>\n<li>효과적인 variance reduction scheme인 GAE를 다룹니다. 또한 실험할 때 batch trust region algorithm에 더하여 다양한 algorithm들에 적용됩니다.</li>\n<li>value function에 대해 trust region optimization method를 사용합니다. 이렇게 함으로서 더 robust하고 efficient한 방법이 됩니다.</li>\n<li>A와 B를 합쳐서, 실험적으로 control task에 neural network policies를 learning하는 데에 있어서 효과적인 algorithm을 얻습니다. 이러한 결과는 high-demensional continuous control에 RL을 사용함으로서 state of the art로 확장되었습니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"3-Preliminaries\"><a href=\"#3-Preliminaries\" class=\"headerlink\" title=\"3. Preliminaries\"></a>3. Preliminaries</h1><p>먼저 policy optimization의 “undiscounted formulation”을 가정합니다. (undiscounted formulation에 주목합시다.)</p>\n<ul>\n<li>initial state $s_0$는 distribution $\\rho_0$으로부터 sampling된 것입니다.</li>\n<li>하나의 trajectory ($s_0, a_0, s_1, a_1, …$)는 terminal state에 도달될 때 까지 policy $a_t$ ~ $\\pi(a_t | s_t)$에 따라서 action을 sampling하고, dynamics $s_{t+1}$ ~ $P(s_{t+1} | s_t, a_t)$에 따라서 state를 sampling함으로써 생성됩니다.</li>\n<li>reward $r_t = r(s_t, a_t, s_{t+1})$은 매 time step마다 받아집니다.</li>\n<li>목표는 모든 policies에 대해 finite하다고 가정됨으로서 expected total reward $\\sum_{t=0}^{\\infty} r_t$를 maximize하는 것입니다.</li>\n</ul>\n<p>여기서 중요한 점은 $\\gamma$를 discount의 parameter로 사용하는 것이 아니라 “bias-variance tradeoff를 조절하는 parameter로 사용한다” 는 것입니다.</p>\n<p>policy gradient method는 gradient $g := \\nabla_\\theta \\mathbb{E} [\\sum_{t=0}^\\infty r_t]$를 반복적으로 estimate함으로써 expected total reward를 maximize하는 것인데, policy gradient에는 여러 다른 표현들이 있습니다.<br>$$g = \\mathbb{E} [\\sum_{t=0}^\\infty \\Phi_t \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$</p>\n<p>여기서 $\\Phi_t$는 아래 중의 하나일 수 있습니다.</p>\n<ol>\n<li>$\\sum_{t=0}^\\infty r_t$: trajectory의 total reward</li>\n<li>$\\sum_{t’=t}^\\infty r_t’$: action $a_t$ 후의 reward</li>\n<li>$\\sum_{t’=t}^\\infty r_t’ - b(s_t)$: 2의 baselined version</li>\n<li>$Q^\\pi (s_t, a_t)$: state-action value function</li>\n<li>$A^\\pi (s_t, a_t)$: advantage function</li>\n<li>$r_t + V^\\pi (s_{t+1}) - V^\\pi (s_t)$: TD residual</li>\n</ol>\n<p>위의 번호 중 4, 5, 6의 수식들은 다음의 정의를 사용합니다.</p>\n<ul>\n<li>$V^\\pi (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty r_{t+1}]$</li>\n<li>$Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty r_{t+l}]$</li>\n<li>$A^\\pi (s_t, a_t) := Q^\\pi (s_t, a_t) - V^\\pi (s_t)$, (Advantage function)</li>\n</ul>\n<p>추가적으로 colon notation $a : b$는 포괄적인 범위 $(a, a+1, … , b)$입니다. (잘 기억해둡시다. 뒤에 계속해서 colon notation이 나옵니다.)</p>\n<p>여기서부터 parameter $\\gamma$에 대해 좀 더 자세히 알아봅시다. parameter $\\gamma$는 bias하면서 동시에 reward를 downweighting함으로써 variance를 줄입니다. 다시 말해 MDP의 discounted formulation에서 사용된 discounted factor와 일치하지만, 이 논문에서는 “$\\gamma$를 undiscounted MDP에서 variance reduction parameter”로 다룹니다. -&gt; 결과는 같지만, 의미와 의도가 다릅니다.</p>\n<p>discounted value function들은 다음과 같습니다.</p>\n<ul>\n<li>$V^{\\pi, \\gamma} (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$ </li>\n<li>$Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$</li>\n<li>$A^{\\pi, \\gamma} (s_t, a_t) := Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)$</li>\n</ul>\n<p>따라서 policy gradient에서의 discounted approximation은 다음과 같이 나타낼 수 있습니다.<br>$$g^\\gamma := \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\sum_{t=0}^\\infty A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$<br>뒤이어 나오는 section에서는 위의 수식에서 $A^{\\pi, \\gamma}$에 대해 biased (but not too biased) estimator를 얻는 방법에 대해서 나옵니다.</p>\n<p>다음으로 advantage function에서 새롭게 접하는 “$\\gamma$-just estimator”의 notation에 대해 알아봅시다.</p>\n<ul>\n<li>먼저 $\\gamma$-just estimator는 $g^\\gamma$ ${}^1$를 estimate하기 위해 위의 수식에서 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 사용할 때, bias하지 않은 estimator라고 합시다. 그리고 이 $\\gamma$-just advantage estimator를 <img src=\"https://www.dropbox.com/s/7g2glqx2kbfynaa/Screen%20Shot%202018-08-22%20at%209.03.46%20PM.png?dl=1\" width=\"100\"> 라고 하고, 전체의 trajectory에 대한 하나의 function이라고 합시다.</li>\n<li>${}^1$에서 이 논문의 저자가 하고 싶은 말이 있는 것 같습니다. 개인적으로 $\\gamma$-just estimator를 이해하는 데에 있어서 중요한 주석이라 정확히 해석하고자 합니다.<ul>\n<li>$A^\\pi$를 $A^{\\pi, \\gamma}$로 사용함으로써 이미 bias하다라고 말했지만, “이 논문에서 하고자 하는 것은 $g^\\gamma$에 대해 unbiased estimate를 얻고 싶은 것입니다.” 하지만 undiscounted MDP의 policy gradient에 대해서는 당연히 $\\gamma$를 사용하기 때문에 biased estimate를 얻습니다. 개인적으로 이것은 일단 무시하고 $\\gamma$를 사용할 때 어떻게 unbiased estimate를 얻을 지에 대해 좀 더 포커스를 맞추고 있는 것 같습니다.</li>\n<li>그러니까 저 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 바꿔줌으로써 unbiased estimate를 하고 싶다는 것이 뒤이어 나오는 정의와 명제의 핵심이라고 할 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n<p>Definition 1.<br>먼저 가정을 합니다. (가정을 바탕으로 이루어지는 정의라는 것을 주목합시다.) 만약</p>\n<center><img src=\"https://www.dropbox.com/s/aia9f1l99wyxgah/Screen%20Shot%202018-08-22%20at%209.07.05%20PM.png?dl=1\" width=\"700\"> </center><br>두 수식이 같다면, estimator $\\hat{A}_t$는 $\\gamma$-just입니다.<br><br>그리고 만약 모든 t에 대해서 $\\hat{A}_t$이 $\\gamma$-just이라면, 다음과 같이 표현할 수 있습니다.<br><br><center><img src=\"https://www.dropbox.com/s/r4m6ktw8geeu7pe/Screen%20Shot%202018-08-22%20at%209.29.18%20PM.png?dl=1\" width=\"420\"> </center>\n\n<p>위의 수식이 바로 unbiased estimate입니다.</p>\n<p>$\\gamma$-just인 $\\hat{A}_t$에 대한 한가지 조건은 $\\hat{A}_t$이 두 가지 function $Q_t$ and $b_t$로 나뉠 수 있다는 것입니다.</p>\n<ul>\n<li>$Q_t$는 $\\gamma$-discounted Q-function의 unbiased estimator입니다.</li>\n<li>$b_t$는 action $a_t$전에 sampling된 states and actions의 arbitrary function이다. </li>\n</ul>\n<p>Proposition 1.<br>모든 $(s_t, a_t)$에 대해,<br>$$\\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty | s_t, a_t} [Q_t (s_{t:\\infty}, a_{t:\\infty})] = Q^{\\pi, \\gamma} (s_t, a_t)$$<br>로 인하여 $\\hat{A}_t$이 </p>\n<center> <img src=\"https://www.dropbox.com/s/59ydsav6djwyevl/Screen%20Shot%202018-08-22%20at%209.27.13%20PM.png?dl=1\" width=\"380\"> </center>\n\n<p>형태라고 가정합시다. (가정을 바탕으로 이루어지는 명제라는 점을 주목합시다.)</p>\n<p>그 때, $\\hat{A}_t$은 $\\gamma$-just입니다.</p>\n<p>이 명제에 대한 증명은 Appendix B에 있습니. 그리고 $\\hat{A}_t$에 대한 $\\gamma$-just advantage estimator들은 다음과 같습니다.</p>\n<ul>\n<li>$\\sum_{l=0}^\\infty \\gamma^l r_{t+1}$</li>\n<li>$A^{\\pi, \\gamma} (s_t, a_t)$</li>\n<li>$Q^{\\pi, \\gamma} (s_t, a_t)$</li>\n<li>$r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)$</li>\n</ul>\n<p>증명을 보기 전에 먼저 Advantage function에 대해서 먼저 살펴봅시다. 아래의 내용은 Sutton이 쓴 논문인 Policy Gradient Methods for Reinforcement Learning with Function Approximation(2000)에서 나온 내용을 리뷰한 것입니다. </p>\n<center> <img src=\"https://www.dropbox.com/s/x737yq97ut6gp1a/Screen%20Shot%202018-07-15%20at%201.16.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n<p>여기서는 “이 수식은 $\\sum_a \\frac{\\partial \\pi (s,a)}{\\partial \\theta} = 0$이기 때문에 가능해진다.”, “즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없다”라는 두 문장을 기억해둡시다.</p>\n<p>이제 증명을 보면 됩니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/e7sj2fwcm1f7hof/figure2.jpg?dl=1\" width=\"600\"> </center><br><center> <img src=\"https://www.dropbox.com/s/kyk4jes1202az4m/figure3.jpg?dl=1\" width=\"600\"> </center>\n\n<p>위의 증명 수식에서 Q와 b의 각각 세번째 수식과 마지막 수식을 자세히 봅시다.</p>\n<ol>\n<li><p>$Q$에 대한 증명</p>\n<ul>\n<li><p>빨간색 부분</p>\n<ul>\n<li>$\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [Q_t (s_{0:\\infty}, a_{0:\\infty})]$를 쉽게 말하자면 “모든(과거, 현재, 미래) state와 action에 대해서 expectation을 $t+1$부터 $\\infty$까지 하겠다.”라는 말입니다. 이렇게 함으로써 원래는 $Q^\\pi (s_t, a_t)$가 나와야 맞는 건데, 앞에서 살펴봤다시피 4번째 수식 밑줄 친 자리에는 advantage 역할을 하는 함수들을 넣어도 결과값에 아무런 영향을 끼치지 않기 때문에 의도적으로 $A^\\pi (s_t, a_t)$로 바꾼 것입니다. 또한 이 증명을 바탕으로 $\\hat{A}$이 $\\gamma$-just라는 것을 표현하고 싶기 때문이라고 봐도 됩니다.</li>\n</ul>\n</li>\n<li><p>파란색 부분</p>\n<ul>\n<li>또한 $a_{0:t}$에서 $a_{0:t-1}$로 변한 것은 $A^\\pi (s_t, a_t)$에 baseline이 들어가기 때문에 바꿔준 것입니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>$b$에 대한 증명</p>\n<ul>\n<li>빨간색 부분<ul>\n<li>$\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$가 0으로 바뀌는 것은 위에서 설명했듯이 $\\nabla_\\theta$자체가 $\\log \\pi_\\theta$만 gradient하기 때문에 이것을 expectation을 취하면 0이 됩니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p><br><br></p>\n<h1 id=\"4-Advantage-Function-Estimation\"><a href=\"#4-Advantage-Function-Estimation\" class=\"headerlink\" title=\"4. Advantage Function Estimation\"></a>4. Advantage Function Estimation</h1><p>이번 section에는 discounted advantage function $A^{\\pi, \\gamma} (s_t, a_t)$의 accurate estimate $\\hat{A}_t$에 대해서 살펴봅시다. 이에 따른 수식은 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/sn16g5iyzzflju0/Screen%20Shot%202018-08-22%20at%209.22.49%20PM.png?dl=1\" width=\"300\"> </center>\n\n<p>여기서 n은 a batch of episodes에 대하여 index한 것입니다.</p>\n<p>$V$를 approximate value function이라고 합시다. 그리고 $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$이라고 합시다.</p>\n<p>만약 (이전과 마찬가지로 가정부터 합니다.) correct value function $V = V^{\\pi, \\gamma}$가 있다고 한다면, 이것은 $\\gamma$-just advantage estimator입니다. 실제로, $A^{\\pi, \\gamma}$의 unbiased estimator는 다음과 같습니다.<br>$$\\mathbb{E}_{s_{t+1}} [\\delta_t^{V^{\\pi, \\gamma}}] = \\mathbb{E}_{s_{t+1}} [r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)]$$<br>$$= \\mathbb{E}_{s_{t+1}} [Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)] = A^{\\pi, \\gamma} (s_t, a_t)$$<br>그러나, “이 estimator는 유일하게 $V = V^{\\pi, \\gamma}$에 대한 $\\gamma$-just입니다.” 다른 경우라면 이것은 biased policy gradient estimate일 것입니다. (우리가 하고 싶은 것은 $V$에 대해서만 unbiased estimator가 아니라 advantage function에 대해서 일반화된 unbiased estimator를 얻고 싶은 것입니다. 그래서 아래에서도 나오겠지만, $\\gamma$와 함께 $\\lambda$를 이용한 estimator가 나옵니다.ㅏ)</p>\n<p>그래서 $\\delta$에 대해 $k$의 sum으로 생각해봅시다. 이것을 $\\hat{A}_t^{(k)}$라고 하자. 그러면 아래와 같이 표현할 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ra7hxksveg2hz45/figure4.jpg?dl=1\" width=\"600\"> </center>\n\n<p>$\\hat{A}_t^{(k)}$은 returns의 $k$-step estimate와 연관지을 수 있고, $\\delta_t^V = \\hat{A}_t^{(1)}$의 case와 유사하게도 $\\hat{A}_t^{(k)}$를 advantage function의 estimator로 생각할 수 있습니다.</p>\n<p>여기서 $k \\rightarrow \\infty$로 생각해보면 bias가 일반적으로 점점 더 작아집니다. 왜냐하면 $\\gamma^k V(s_{t+k})$가 점점 많이 discounted되서 $-V(s_t)$가 bias에 영향을 미치지 못하기 때문입니다. $k \\rightarrow \\infty$를 취하면 다음과 같은 수식이 나옵니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/13fn9wcup8pfh9u/Screen%20Shot%202018-08-22%20at%209.19.52%20PM.png?dl=1\" width=\"320\"> </center>\n\n<p>우변의 수식과 같이 empirical returns에서 value function baseline을 뺀 것으로 나타낼 수 있습니다.</p>\n<p>Generalized Advantage Estimator GAE($\\gamma, \\lambda$)는 $k$-step estimators의 exponentially-weighted average로 나타낼 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/yg1ybmfkep3towu/figure5.jpg?dl=1\" width=\"600\"> </center>\n\n<p>(TD($\\lambda$)를 떠올려주세요..!)</p>\n<p>위의 마지막 수식에서도 알 수 있듯이, advantage estimator는 Bellman residual terms의 discounted sum과 관련있는 간단한 수식입니다. 다음 section에서 modified reward function을 가진 MDP에 returns로서의 관점에서 위의 마지막 수식을 더 자세히 살펴봅시다. 위의 수식은 TD($\\lambda$)와 많이 유사합니다. 그러나 TD($\\lambda$)는 value function를 estimator하고, 여기서는 advantage function을 estimator합니다.</p>\n<p>위의 수식에서 $\\lambda = 0$ and $\\lambda = 1$에 대해서는 특별한 case가 존재한다. 수식으로 표현하면 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ufglxzanhcnbi36/figure6.jpg?dl=1\" width=\"600\"> </center>\n\n<ul>\n<li>$GAE(\\gamma, 1)$은 $V$의 정확도와 관계없이 $\\gamma$-just입니다. 그러나 returns의 sum때문에 high variance합니다.</li>\n<li>$GAE(\\gamma, 0)$은 $V = V^{\\pi, \\gamma}$에 대해 $\\gamma$-just입니다. 그리고 bias하지만 일반적으로 훨씬 lower variance를 가집니다. </li>\n<li>$0 &lt; \\lambda &lt; 1$에 대해 GAE는 parameter $\\lambda$를 control하여 bias와 variance사이에 compromise를 만듭니다.</li>\n</ul>\n<p>두 가지 별개의 parameter $\\gamma$ and $\\lambda$를 가진 advantage estimator는 bias-variance tradeoff에 도움을 줍니다. 그러나 이 두 가지 parameter는 각각 다른 목적을 가지고 작동합니다.</p>\n<ul>\n<li>$\\gamma$는 가장 중요하게 value function $V^{\\pi, \\gamma}$ 의 scale을 결정합니다. 또한 $\\gamma$는 $\\lambda$에 의존하지 않습니다. $\\gamma &lt; 1$로 정하는 것은 policy gradient estimate에서 bias합니다.</li>\n<li>반면에, $\\lambda &lt; 1$는 유일하게 value function이 부정확할 때 bias합니다. 그리고 경험상, $\\lambda$의 best value는 $\\gamma$의 best value보다 훨씬 더 낮습니다. 왜냐하면 $\\lambda$가 정확한 value function에 대해 $\\gamma$보다 훨씬 덜 bias하기 때문입니다.</li>\n</ul>\n<p>GAE를 사용할 때, $g^\\gamma$의 biased estimator를 구성할 수 있습니다. 수식은 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ok57dsk52o7pl9r/Screen%20Shot%202018-08-22%20at%209.17.54%20PM.png?dl=1\" width=\"670\"> </center>\n\n<p>여기서 $\\lambda = 1$일 때 동일해집니다.</p>\n<p><br><br></p>\n<h1 id=\"5-Interpretation-as-Reward-Shaping\"><a href=\"#5-Interpretation-as-Reward-Shaping\" class=\"headerlink\" title=\"5. Interpretation as Reward Shaping\"></a>5. Interpretation as Reward Shaping</h1><p>이번 section에서는 앞서 다뤘던 수식 $\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V$를 modified reward function의 MDP의 관점으로 생각해봅시다. 조금 더 구체적으로 말하자면, MDP에서 reward shaping transformation을 실행한 후에 적용된 extra discounted factor로서 $\\lambda$를 어떻게 볼 것인지에 대해서 다룹니다.</p>\n<p>(개인적인 comment) 한 가지 먼저 언급하자면, 본래의 목적은 reward shaping이 아니라 variance reduction입니다. 이번 section은 그저 이전에 이러한 개념이 있었고, gae를 다른 관점에서 생각해보자라는 뜻에서 나온 section인 것 같습니다. ‘아~ 이러한 개념이 있구나~!’ 정도로만 알면 될 것 같습니다. ‘gae가 reward shaping의 효과까지 있어서 이러한 section을 넣은 것일까?’ 라는 생각도 해봤지만 아직 잘 모르겠습니다. 실험부분에도 딱히 하지 않은 걸로 봐서는.. 아닌 것 같기도 하고.. 아직까지는 그저 ‘reward shaping의 관점에서 봤을 때에도 gae로 만들어줄 수 있다.’ 정도만 생각하려고 합니다.</p>\n<p>Reward Shaping이란 개념 자체는 일반적인 알고리즘 분야(최단경로 문제 등)에도 있습니다. 하지만 이 개념이 머신러닝에 적용된 건 Andrew Y. Ng이 쓴 Policy Invariance under Reward Transformations Theory and Application to Reward Shaping(1999) 논문입니다. 논문에서 Reward Shaping을 설명하는 부분은 간략하게 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/0w9cuxjkhz2mbcy/figure7.jpg?dl=1\" width=\"400\"> </center><br><center> <img src=\"https://www.dropbox.com/s/xcp9eqvcbv3268g/figure8.jpg?dl=1\" width=\"400\"> </center>\n\n<p>위의 글만 보고 이해가 잘 안됩니다. 그래서 다른 자료들을 찾던 중에 다음의 그림을 찾게 되었습니다. (아래의 그림은 Youtube에 있는 <a href=\"https://www.youtube.com/watch?v=xManAGjbx2k&amp;t=95s\" target=\"_blank\" rel=\"noopener\">Udacity 영상</a>에서 있는 그림입니다.)</p>\n<center> <img src=\"https://www.dropbox.com/s/87kacngez9cl2e6/figure9.jpg?dl=1\" width=\"400\"> </center>\n\n<p>이 그림을 통해서 Reward Shaping을 이해해봅시다.</p>\n<p>Reward Shaping(보상 형성)을 통해서 뭘 하고 싶은지 목적부터 봅시다. reward space가 sparse 한 경우에 reward가 너무 드문드문 나옵니다. 따라서 이것을 꾸준히 reward를 받을 수 있도록 바꿉니다. potential-based shaping function인 $\\Phi$를 만들어서 더하고 빼줍니다. 저 $\\Phi$ 자리에는 대표적으로 state value function이 많이 들어간다고 합니다. (아직 목적이 이해가 안될 수 있습니다. Reward Shaping에 대해서 다 보고 나서 다시 한 번 읽어봅시다.)</p>\n<p>위의 그림은 돌고래가 점프하여 불구멍을 통과해야하는 환경입니다. 하지만 저 불구멍을 통과하기 위해서는 (1) 점프도 해야하고, (2) 불도 피해야하고, (3) 알맞게 착지까지 완료해야합니다. 이렇게 해야 reward를 +1 얻습니다. </p>\n<p>생각해봅시다. 어느 세월에 점프도 해야하고.. 불도 피해야하고.. 알맞게 착지까지해서 reward를 +1 얻겠습니까? 따라서 그 전에도 잘하고 있는 지, 못하고 있는 지를 판단하기 위해, 다시 말해 reward를 꾸준히 받도록 다음과 같이 transformed reward function $\\tilde{r}$을 정의합니다.</p>\n<p>$$\\tilde{r} (s, a, s’) = r(s, a, s’) + \\gamma \\Phi (s’) - \\Phi (s)$$</p>\n<ul>\n<li>여기서 $\\Phi : S \\rightarrow \\mathbb{R}$를 state space에서의 arbitrary scalar-valued function을 나타냅니다. 그리고 $\\Phi$자리에는 대표적으로 state value function이 들어간다고 생각합시다.</li>\n<li>형태가 TD residual term의 결과와 비슷하지만 의미와 의도가 다릅니다. reward shaping은 sparse reward 때문이고, 이전 section에서 봤던 gae는 variance reduction때문에 나온 것입니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/tkfpqniaazwfbld/figure10.jpg?dl=1\" width=\"600\"> </center>\n\n<p>이 transformation은 discounted advantage function $A^{\\pi, \\gamma}$으로도 둘 수 있습니다. state $s_t$를 시작하여 하나의 trajectory의 rewards의 discounted sum을 표현하면 다음과 같습니다.<br>$$\\sum_{l=0}^\\infty \\gamma^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty \\gamma^l r(s_{t+l}, a_{t+l}, s_{t+l+1}) - \\Phi(s_t)$$</p>\n<p>$\\tilde{Q}^{\\pi, \\gamma}, \\tilde{V}^{\\pi, \\gamma}, \\tilde{A}^{\\pi, \\gamma}$를  transformed MDP의 value function과 advantage function이라고 하면, 다음과 같은 수식이 나옵니다.<br>$$\\tilde{Q}^{\\pi, \\gamma} (s, a) = Q^{\\pi, \\gamma} (s, a) - \\Phi (s)$$<br>$$\\tilde{V}^{\\pi, \\gamma} (s, a) = V^{\\pi, \\gamma} (s, a) - \\Phi (s)$$<br>$$\\tilde{A}^{\\pi, \\gamma} (s, a) = (Q^{\\pi, \\gamma} (s, a) - \\Phi (s)) - (V^\\pi (s) - \\Phi (s)) = A^{\\pi, \\gamma} (s, a)$$</p>\n<p>이제 reward shaping의 idea를 가지고 어떻게 policy gradient estimate를 얻을 수 있는 지에 대해서 알아봅시다.</p>\n<ul>\n<li>$0 \\le \\lambda \\le 1$의 범위에 있는 steeper discount $\\gamma \\lambda$를 사용합니다.</li>\n<li>shaped reward $\\tilde{r}$는 Bellman residual term $\\delta^V$와 동일합니다.</li>\n<li>그리고 $\\Phi = V$와 같다고 보면 다음과 같은 수식이 나옵니다.</li>\n</ul>\n<p>$$\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V = \\hat{A}_t^{GAE(\\gamma, \\lambda)}$$<br>이렇게 shaped rewards의 $\\gamma \\lambda$-discounted sum을 고려함으로써 GAE를 얻을 수 있습니다. 더 정확하게 $\\lambda = 1$은 $g^\\gamma$의 unbiased estimate이고, 반면에 $\\lambda &lt; 1$은 biased estimate입니다.</p>\n<p>좀 더 나아가서, shaping transformation과 parameters $\\gamma$ and $\\lambda$의 결과를 보기 위해, response function $\\chi$를 이용하면 다음과 같은 수식이 나옵니다.<br>$$\\chi (l; s_t, a_t) = \\mathbb{E} [r_{t+l} | s_t, a_t] - \\mathbb{E} [r_{t+l} | s_t]$$<br>추가적으로 $A^{\\pi, \\gamma} (s, a) = \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$입니다.<br>그래서 discounted policy gradient estimator는 다음과 같이 쓸 수 있다.<br>$$\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) A^{\\pi, \\gamma} (s_t, a_t) = \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$$</p>\n<p><br><br></p>\n<h1 id=\"6-Value-Fuction-Estimation\"><a href=\"#6-Value-Fuction-Estimation\" class=\"headerlink\" title=\"6. Value Fuction Estimation\"></a>6. Value Fuction Estimation</h1><p>이번 section에서는 Trust Region Optimization Scheme에 따라 Value function을 Estimation합니다.</p>\n<p><br></p>\n<h2 id=\"6-1-Simplest-approach\"><a href=\"#6-1-Simplest-approach\" class=\"headerlink\" title=\"6.1 Simplest approach\"></a>6.1 Simplest approach</h2><p>$$minimize_{\\phi} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi}(s_n) - \\hat{V_n} \\vert\\vert^{2}$$</p>\n<ul>\n<li>위는 가장 간단하게 non-linear approximation으로 푸는 방법입니다.</li>\n<li>$\\hat{V_t} = \\sum_{l=0}^{\\infty}\\gamma^l r_{t+l}$은 reward 에 대한 discounted sum을 의미합니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"6-2-Trust-region-method-to-optimize-the-value-function\"><a href=\"#6-2-Trust-region-method-to-optimize-the-value-function\" class=\"headerlink\" title=\"6.2 Trust region method to optimize the value function\"></a>6.2 Trust region method to optimize the value function</h2><ul>\n<li>Value function을 최적화 하기 위해 trust region method를 사용합니다.</li>\n<li>Trust region은 최근 데이터에 대해 overfitting되는 것을 막아줍니다.</li>\n</ul>\n<p>Trust region문제를 풀기 위해서는 다음 스텝을 따릅니다.</p>\n<ul>\n<li>$\\sigma^2 = \\frac{1}{N} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi old}(s_n) - \\hat{V_n} \\vert\\vert^{2}$을 계산합니다.</li>\n<li>그 후에 다음과 같은 constrained opimization문제를 풉니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/d74cg3votxi2dbg/Screen%20Shot%202018-08-22%20at%209.13.28%20PM.png?dl=1\" width=\"300\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/jrkjax71t3j3vk4/Screen%20Shot%202018-08-22%20at%209.13.35%20PM.png?dl=1\" width=\"370\"> </center>\n\n<ul>\n<li>위의 수식은 사실 old Value function과 새로운 Value function KL distance가 $\\epsilon$ 다 작아야한다는 수식과 같습니다. Value function이 평균은 $V_{\\phi}(s)$이고 분산이 $\\sigma^2$인 conditional Gaussian distribution으로 parameterize되었을 뿐입니다.</li>\n</ul>\n<p><img width=\"400px\" src=\"https://www.dropbox.com/s/uw0v05feu8chqmc/Screenshot%202018-07-08%2010.04.38.png?dl=1\"> </p>\n<p>이 trust region문제의 답을 conjudate gradient algorithm을 이용하여 근사값을 구할 수 있습니다. 특히, 다음과 같은  quadratic program을 풀게됩니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/pawjkm8b9ycwsl5/Screen%20Shot%202018-08-22%20at%209.10.36%20PM.png?dl=1\" width=\"250\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/va7wms8uqxnw5rq/Screen%20Shot%202018-08-22%20at%209.10.40%20PM.png?dl=1\" width=\"370\"> </center>\n\n<ul>\n<li>여기서 $g$는 objective 의 gradient입니다.</li>\n<li>$j_n = \\nabla_{\\phi} V_{\\phi}(s_n)$일때, $H = \\frac{1}{N} \\sum_{n} j_n j^T_n$이며, $H$는 objective의 hessian에 대해서 gaussian newton method로 근사한 값입니다. 따라서, value function을 conditional probability로 해석한다면 Fisher information matrix가 됩니다.</li>\n<li>구현할때의 방법은 TRPO에서 사용한 방법과 모두 같습니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"7-Experiments\"><a href=\"#7-Experiments\" class=\"headerlink\" title=\"7.Experiments\"></a>7.Experiments</h1><p>실험은 다음 두 가지 물음에 대해서 디자인 되었습니다.</p>\n<ul>\n<li>GAE에 따라서 episodic total reward를 최적화 할때, $\\lambda$와 $\\gamma$가 변함에 따라서 어떤 경험적인 효과를 볼 수 있는지?</li>\n<li>GAE와 trust region alogorithm을 policy와 value function 모두에 함께 사용했을 때 어려운 문제에 적용되는 큰 뉴럴넷을 최적화 할 수 있을지?</li>\n</ul>\n<p><br></p>\n<h2 id=\"7-1-Policy-Optimization-Algorithm\"><a href=\"#7-1-Policy-Optimization-Algorithm\" class=\"headerlink\" title=\"7.1 Policy Optimization Algorithm\"></a>7.1 Policy Optimization Algorithm</h2><p>Policy update는 TRPO를 사용합니다. TRPO에 대한 설명은 여기서는 생략하겠습니다. TRPO 포스트를 보고 돌아와주세요!</p>\n<ul>\n<li><p>이전 TRPO에서 이미 TRPO와 다른 많은 알고리즘들을 비교하였기 때문에, 여기서는 똑같은 짓을 반복하지 않고 $\\lambda$, $\\gamma$가 변함에 따라 어떤 영향이 있는 지에 대한 실험에 집중하겠다고 합니다. (귀찮았던거죠..)</p>\n</li>\n<li><p>TRPO를 적용한 GAE의 최종 알고리즘은 다음과 같습니다.</p>\n</li>\n</ul>\n<center> <img width=\"600px\" src=\"https://www.dropbox.com/s/b1klz11f2frrvg4/Screenshot%202018-07-08%2010.33.54.png?dl=1\"> </center><br><center> <img width=\"300px\" src=\"https://www.dropbox.com/s/35fxte05pqtiel7/Screenshot%202018-07-08%2010.35.22.png?dl=1\"> </center><br><center> <img width=\"300px\" src=\"https://www.dropbox.com/s/u35pjow3w50bvz1/Screenshot%202018-07-08%2010.36.03.png?dl=1\"> </center>\n\n<ul>\n<li>여기서 주의할 점은 Policy update($\\theta_i \\rightarrow \\theta_{i+1}$)에서 $V_{\\phi_i}$를 사용했다는 점입니다.</li>\n<li>만약 Value function을 먼저 update하게 된다면 추가적인 bias가 발생합니다.</li>\n<li>극단적으로 생각해보아서, 우리가 Value function을 완벽하게 overfit을 해낸다면 Bellman residual($r_t + \\gamma V(s_{t+1}) - V(S_t)$)은 0이 됩니다. 그럼 Policy gradient의 estimation도 거의 0이 될 것입니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"7-2-Expermint-details\"><a href=\"#7-2-Expermint-details\" class=\"headerlink\" title=\"7.2 Expermint details\"></a>7.2 Expermint details</h2><h3 id=\"7-2-1-Environment\"><a href=\"#7-2-1-Environment\" class=\"headerlink\" title=\"7.2.1 Environment\"></a>7.2.1 Environment</h3><p>실험에서 사용된 환경은 다음 네 가지 입니다.</p>\n<ol>\n<li>classic cart-pole (x 3D)</li>\n<li>bipedal locomotion</li>\n<li>quadrupedal locomotion</li>\n<li>dynamically standing up for the biped</li>\n</ol>\n<h3 id=\"7-2-2-Architecture\"><a href=\"#7-2-2-Architecture\" class=\"headerlink\" title=\"7.2.2 Architecture\"></a>7.2.2 Architecture</h3><ul>\n<li>3D robot task에 대해서는 같은 모델을 사용하였습니다.<ul>\n<li>layers  = [100, 50, 25] 각각 tanh 사용. (Policy와 Value 네트워크 모두)</li>\n<li>Final output layer은 linear</li>\n</ul>\n</li>\n<li>Cartpole에 대해서는 1개의 layer 안에 20개의 hidden unit만 있는 linear policy를 사용했다고 합니다.</li>\n</ul>\n<h3 id=\"7-2-3-Task\"><a href=\"#7-2-3-Task\" class=\"headerlink\" title=\"7.2.3 Task\"></a>7.2.3 Task</h3><ul>\n<li>Cartpole<ul>\n<li>한 배치당 20 개의 trajectory를 모았고, maximum length는 1000입니다.</li>\n</ul>\n</li>\n<li>3D biped locomotion<ul>\n<li>33 dim state , 10 dim action</li>\n<li>50000 time step per batch</li>\n</ul>\n</li>\n<li>3D quadruped locomotion<ul>\n<li>29 dim state, 8 dim action</li>\n<li>200000 time step per batch</li>\n</ul>\n</li>\n<li>3D biped locomotion Standing<ul>\n<li>33 dim state , 10 dim action</li>\n<li>200000 time step per batch</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"7-2-3-results\"><a href=\"#7-2-3-results\" class=\"headerlink\" title=\"7.2.3 results\"></a>7.2.3 results</h3><p>cost의 관점에서 결과를 나타내었다고 합니다. Cost는 negative reward와 이것이 최소화 되었는가로 정의되었습니다.</p>\n<h4 id=\"7-2-3-1-Cartpole\"><a href=\"#7-2-3-1-Cartpole\" class=\"headerlink\" title=\"7.2.3.1 Cartpole\"></a>7.2.3.1 Cartpole</h4><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/x9pbms1wvg38lda/Screenshot%202018-07-08%2011.08.22.png?dl=1\"> </center>\n\n<ul>\n<li>왼쪽 그림은 $\\gamma$를 0.99로 고정시켜놓은 상태에서 $\\lambda$를 변화시킴에 따라서 cost를 측정한 것입니다. </li>\n<li>오른쪽은 $\\gamma$와 $\\lambda$를 둘 다 변화 시키면서 성능을 그림으로 나타낸 표입니다. 흰색에 가까울 수록 좋은 성능입니다. </li>\n</ul>\n<h4 id=\"7-2-3-2-3D-BIPEDAL-LOCOMOTINO\"><a href=\"#7-2-3-2-3D-BIPEDAL-LOCOMOTINO\" class=\"headerlink\" title=\"7.2.3.2 3D BIPEDAL LOCOMOTINO\"></a>7.2.3.2 3D BIPEDAL LOCOMOTINO</h4><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/i9wj4p6ijojsy82/Screenshot%202018-07-08%2011.25.08.png?dl=1\"> </center>\n\n\n<ul>\n<li>다른 random seed로 부터 9번 씩 시도한 결과를 mean을 취해서 사용합니다.</li>\n<li>Best performance는   $\\gamma \\in [0.99, 0.995]$ 그리고  $\\lambda \\in [0.96, 0.99]$일때. </li>\n<li>1000 iteration 후에 빠르고 부드럽고 안정적인 걸음거이가 나옵니다.</li>\n<li>실제로 걸린 시간은 0.01(타입스텝당 시간) <em> 50000(배치당 타임스텝) </em> 1000(배치) <em> 3600(초-&gt;시간) </em> 24 = 5.8일 정도가 걸렸습니다.</li>\n</ul>\n<h4 id=\"7-2-3-3-다른-ROBOT-TASKS\"><a href=\"#7-2-3-3-다른-ROBOT-TASKS\" class=\"headerlink\" title=\"7.2.3.3 다른 ROBOT TASKS\"></a>7.2.3.3 다른 ROBOT TASKS</h4><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/fuuat65we52quht/Screenshot%202018-07-08%2011.33.11.png?dl=1\"> </center>\n\n<ul>\n<li>다른 로봇 TASK에 대해서는 아주 제한적인 실험만 진행합니다.(시간이 부족했던듯 하네요..)</li>\n<li>Quadruped에 대해서는 $\\gamma = 0.995$로 fix, $\\lambda \\in {0, 0.96}$</li>\n<li>Standingup에 대해서는 $\\gamma = 0.99$로 fix, $\\lambda \\in {0, 0.96}$</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"8-Discussion\"><a href=\"#8-Discussion\" class=\"headerlink\" title=\"8. Discussion\"></a>8. Discussion</h1><p><br></p>\n<h2 id=\"8-1-Main-discussion\"><a href=\"#8-1-Main-discussion\" class=\"headerlink\" title=\"8.1 Main discussion\"></a>8.1 Main discussion</h2><p>지금까지 복잡하고 어려운 control problem에서 Reinforcement Learning(RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 “variance reduction”에 대해 연구하였습니다.</p>\n<p>“Generalized Advantage Estimator(GAE)”라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.<br>또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는 지를 보였습니다.</p>\n<p>이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.</p>\n<p>GAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.</p>\n<p><br></p>\n<h2 id=\"8-2-Future-work\"><a href=\"#8-2-Future-work\" class=\"headerlink\" title=\"8.2 Future work\"></a>8.2 Future work</h2><p>Value function estimation error와 Policy gradient estimation error사이의 관계를 알아낸다면, 우리는 Value function fitting에 더 잘 맞는 error metric(policy gradient estimation 의 정확성과 더 잘 맞는 value function)을 사용할 수 있습니다. 여기서 Policy와 Value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.</p>\n<p>추가적으로 DDPG는 별로라고 합니다. 그리고 TD(0)는 bias가 너무 크고, poor performance로 이끈다고 합니다. 특히나 이 논문에서는 low-dimention의 쉬운 문제들만 해결했습니다.</p>\n<center> <img width=\"500px\" src=\"https://www.dropbox.com/s/nhc7t9psul5lr3x/Screenshot%202018-07-08%2011.45.15.png?dl=1\"> </center>\n\n<p><br></p>\n<h2 id=\"8-3-FAQ\"><a href=\"#8-3-FAQ\" class=\"headerlink\" title=\"8.3 FAQ\"></a>8.3 FAQ</h2><ul>\n<li>Compatible features와는 무슨 관계?<ul>\n<li>Compatible features는 value function을 이용하는 policy gradient 알고리즘들과 함께 자주 언급됩니다.</li>\n<li>Actor Critic의 저자는 policy의 제한된 representation power때문에, policy gradient는 단지 advantage function space의 subspace에만 의존하게 됩니다.</li>\n<li>이 subspace는 compatible features에 의해 span됩니다.</li>\n<li>이 이론은 현재 문제 구조를 어떻게 이용해야 advantage function에 대해 더 나은 estimation을 할 수 있는 지에 대한 지침을 주지 않습니다. GAE 논문의 idea와 orthogonal합니다.</li>\n</ul>\n</li>\n<li>왜 Q function을 사용하지 않는가?<ul>\n<li>먼저 state-value function이 더 낮은 차원의 input을 가진다. 그래서 Q function보다 더 학습하기가 쉽습니다.</li>\n<li>두 번째로 이 논문에서 제안하는 방법으로는 high bias estimator에서 low bias estimator로 $\\lambda$를 통해서 부드럽게 interpolate를 할 수 있습니다.</li>\n<li>반면에 Q를 사용하면 단지 high-bias estimator 밖에 사용할 수 없습니다.</li>\n<li>특히나 return에 대한 one-step estimation은 엄두를 못낼 정도로 bias가 큽니다.</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"TRPO-여행하기\"><a href=\"#TRPO-여행하기\" class=\"headerlink\" title=\"TRPO 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/\">TRPO 여행하기</a></h2><h2 id=\"TRPO-Code\"><a href=\"#TRPO-Code\" class=\"headerlink\" title=\"TRPO Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"PPO-여행하기\"><a href=\"#PPO-여행하기\" class=\"headerlink\" title=\"PPO 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/\">PPO 여행하기</a></h2>","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"https://www.dropbox.com/s/p8gfpyo6xf9wm5w/Screen%20Shot%202018-07-18%20at%201.25.53%20AM.png?dl=1\" width=\"700\"> </center>\n\n<p>논문 저자 : John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan and Pieter Abbeel<br>논문 링크 : <a href=\"https://arxiv.org/pdf/1506.02438.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1506.02438.pdf</a><br>Proceeding : International Conference of Learning Representations (ICLR) 2016<br>정리 : 양혁렬, 이동민</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p>현존하는 Policy Gradient Method들의 목적은 누적되는 reward들을 optimization하는 것입니다. 하지만 학습할 때에 많은 양의 sample이 필요로 하고, 들어오는 data가 nonstationarity임에도 불구하고 stable and steady improvement가 어렵습니다.</p>\n<p>그래서 이 논문에서는 다음과 같은 방법을 제시합니다.</p>\n<ul>\n<li>TD($\\lambda$)와 유사한 advantage function의 exponentially-weighted estimator를 사용하여 policy gradient estimate의 variance를 줄이는 것 </li>\n<li>policy와 value function에 대한 Trust Region Optimization 사용하는 것</li>\n</ul>\n<p>3D locomotion tasks에 대한 empirical results는 다음과 같습니다.</p>\n<ul>\n<li>bipedal and quadrupedal simulated robots의 달리는 자세를 학습</li>\n<li>bipedal 사람이 땅에 누워있다가 일어서는 것을 학습</li>\n</ul>\n<p>※ TD($\\lambda$)를 혹시 모르실 경우도 있을 것 같아 간략하게 정리하였습니다. <a href=\"http://dongminlee.tistory.com/10\" target=\"_blank\" rel=\"noopener\">http://dongminlee.tistory.com/10</a> 를 먼저 보시고 아래의 내용을 봐주시기 바랍니다!</p>\n<p><br><br></p>\n<h1 id=\"2-Introduction\"><a href=\"#2-Introduction\" class=\"headerlink\" title=\"2. Introduction\"></a>2. Introduction</h1><p>기본적으로 “parameterized stochastic policy”를 가정합니다. 이 때 expected total returns의 gradient에 대한 unbiased estimate를 얻을 수 있는데 이것을 REINFORCE라고 부릅니다. 하지만 하나의 action의 결과가 과거와 미래의 action의 결과로 혼동되기 때문에 gradient estimator의 high variance는 시간에 따라 scaling됩니다.</p>\n<p>또 다른 방법은 Actor-Critic이 있습니다. 이 방법은 empirical returns보다 하나의 value function을 사용합니다. 또한 bias하고 lower variance를 가진 estimator입니다. 구체적으로 말하자면, high variance하다면 더 sampling을 하면 되는 반면에 bias는 매우 치명적입니다. 다시 말해 bias는 algorithm이 converge하는 데에 실패하거나 또는 local optimum이 아닌 poor solution에 converge하도록 만듭니다.</p>\n<p>따라서 이 논문에서는 $\\gamma\\in [0,1]$ and $\\lambda\\in [0,1]$에 의해 parameterized estimation scheme인 Generalized Advantage Estimator(GAE)를 다룹니다.</p>\n<p>이 논문을 대략적으로 요약하자면 다음과 같습니다.</p>\n<ul>\n<li>효과적인 variance reduction scheme인 GAE를 다룹니다. 또한 실험할 때 batch trust region algorithm에 더하여 다양한 algorithm들에 적용됩니다.</li>\n<li>value function에 대해 trust region optimization method를 사용합니다. 이렇게 함으로서 더 robust하고 efficient한 방법이 됩니다.</li>\n<li>A와 B를 합쳐서, 실험적으로 control task에 neural network policies를 learning하는 데에 있어서 효과적인 algorithm을 얻습니다. 이러한 결과는 high-demensional continuous control에 RL을 사용함으로서 state of the art로 확장되었습니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"3-Preliminaries\"><a href=\"#3-Preliminaries\" class=\"headerlink\" title=\"3. Preliminaries\"></a>3. Preliminaries</h1><p>먼저 policy optimization의 “undiscounted formulation”을 가정합니다. (undiscounted formulation에 주목합시다.)</p>\n<ul>\n<li>initial state $s_0$는 distribution $\\rho_0$으로부터 sampling된 것입니다.</li>\n<li>하나의 trajectory ($s_0, a_0, s_1, a_1, …$)는 terminal state에 도달될 때 까지 policy $a_t$ ~ $\\pi(a_t | s_t)$에 따라서 action을 sampling하고, dynamics $s_{t+1}$ ~ $P(s_{t+1} | s_t, a_t)$에 따라서 state를 sampling함으로써 생성됩니다.</li>\n<li>reward $r_t = r(s_t, a_t, s_{t+1})$은 매 time step마다 받아집니다.</li>\n<li>목표는 모든 policies에 대해 finite하다고 가정됨으로서 expected total reward $\\sum_{t=0}^{\\infty} r_t$를 maximize하는 것입니다.</li>\n</ul>\n<p>여기서 중요한 점은 $\\gamma$를 discount의 parameter로 사용하는 것이 아니라 “bias-variance tradeoff를 조절하는 parameter로 사용한다” 는 것입니다.</p>\n<p>policy gradient method는 gradient $g := \\nabla_\\theta \\mathbb{E} [\\sum_{t=0}^\\infty r_t]$를 반복적으로 estimate함으로써 expected total reward를 maximize하는 것인데, policy gradient에는 여러 다른 표현들이 있습니다.<br>$$g = \\mathbb{E} [\\sum_{t=0}^\\infty \\Phi_t \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$</p>\n<p>여기서 $\\Phi_t$는 아래 중의 하나일 수 있습니다.</p>\n<ol>\n<li>$\\sum_{t=0}^\\infty r_t$: trajectory의 total reward</li>\n<li>$\\sum_{t’=t}^\\infty r_t’$: action $a_t$ 후의 reward</li>\n<li>$\\sum_{t’=t}^\\infty r_t’ - b(s_t)$: 2의 baselined version</li>\n<li>$Q^\\pi (s_t, a_t)$: state-action value function</li>\n<li>$A^\\pi (s_t, a_t)$: advantage function</li>\n<li>$r_t + V^\\pi (s_{t+1}) - V^\\pi (s_t)$: TD residual</li>\n</ol>\n<p>위의 번호 중 4, 5, 6의 수식들은 다음의 정의를 사용합니다.</p>\n<ul>\n<li>$V^\\pi (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty r_{t+1}]$</li>\n<li>$Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty r_{t+l}]$</li>\n<li>$A^\\pi (s_t, a_t) := Q^\\pi (s_t, a_t) - V^\\pi (s_t)$, (Advantage function)</li>\n</ul>\n<p>추가적으로 colon notation $a : b$는 포괄적인 범위 $(a, a+1, … , b)$입니다. (잘 기억해둡시다. 뒤에 계속해서 colon notation이 나옵니다.)</p>\n<p>여기서부터 parameter $\\gamma$에 대해 좀 더 자세히 알아봅시다. parameter $\\gamma$는 bias하면서 동시에 reward를 downweighting함으로써 variance를 줄입니다. 다시 말해 MDP의 discounted formulation에서 사용된 discounted factor와 일치하지만, 이 논문에서는 “$\\gamma$를 undiscounted MDP에서 variance reduction parameter”로 다룹니다. -&gt; 결과는 같지만, 의미와 의도가 다릅니다.</p>\n<p>discounted value function들은 다음과 같습니다.</p>\n<ul>\n<li>$V^{\\pi, \\gamma} (s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_t:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$ </li>\n<li>$Q^\\pi (s_t, a_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty} [\\sum_{l=0}^\\infty \\gamma^l r_{t+l}]$</li>\n<li>$A^{\\pi, \\gamma} (s_t, a_t) := Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)$</li>\n</ul>\n<p>따라서 policy gradient에서의 discounted approximation은 다음과 같이 나타낼 수 있습니다.<br>$$g^\\gamma := \\mathbb{E}_{s_{0:\\infty} a_{0:\\infty}} [\\sum_{t=0}^\\infty A^{\\pi, \\gamma} (s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$$<br>뒤이어 나오는 section에서는 위의 수식에서 $A^{\\pi, \\gamma}$에 대해 biased (but not too biased) estimator를 얻는 방법에 대해서 나옵니다.</p>\n<p>다음으로 advantage function에서 새롭게 접하는 “$\\gamma$-just estimator”의 notation에 대해 알아봅시다.</p>\n<ul>\n<li>먼저 $\\gamma$-just estimator는 $g^\\gamma$ ${}^1$를 estimate하기 위해 위의 수식에서 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 사용할 때, bias하지 않은 estimator라고 합시다. 그리고 이 $\\gamma$-just advantage estimator를 <img src=\"https://www.dropbox.com/s/7g2glqx2kbfynaa/Screen%20Shot%202018-08-22%20at%209.03.46%20PM.png?dl=1\" width=\"100\"> 라고 하고, 전체의 trajectory에 대한 하나의 function이라고 합시다.</li>\n<li>${}^1$에서 이 논문의 저자가 하고 싶은 말이 있는 것 같습니다. 개인적으로 $\\gamma$-just estimator를 이해하는 데에 있어서 중요한 주석이라 정확히 해석하고자 합니다.<ul>\n<li>$A^\\pi$를 $A^{\\pi, \\gamma}$로 사용함으로써 이미 bias하다라고 말했지만, “이 논문에서 하고자 하는 것은 $g^\\gamma$에 대해 unbiased estimate를 얻고 싶은 것입니다.” 하지만 undiscounted MDP의 policy gradient에 대해서는 당연히 $\\gamma$를 사용하기 때문에 biased estimate를 얻습니다. 개인적으로 이것은 일단 무시하고 $\\gamma$를 사용할 때 어떻게 unbiased estimate를 얻을 지에 대해 좀 더 포커스를 맞추고 있는 것 같습니다.</li>\n<li>그러니까 저 $A^{\\pi, \\gamma}$를 $\\gamma$-just estimator로 바꿔줌으로써 unbiased estimate를 하고 싶다는 것이 뒤이어 나오는 정의와 명제의 핵심이라고 할 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n<p>Definition 1.<br>먼저 가정을 합니다. (가정을 바탕으로 이루어지는 정의라는 것을 주목합시다.) 만약</p>\n<center><img src=\"https://www.dropbox.com/s/aia9f1l99wyxgah/Screen%20Shot%202018-08-22%20at%209.07.05%20PM.png?dl=1\" width=\"700\"> </center><br>두 수식이 같다면, estimator $\\hat{A}_t$는 $\\gamma$-just입니다.<br><br>그리고 만약 모든 t에 대해서 $\\hat{A}_t$이 $\\gamma$-just이라면, 다음과 같이 표현할 수 있습니다.<br><br><center><img src=\"https://www.dropbox.com/s/r4m6ktw8geeu7pe/Screen%20Shot%202018-08-22%20at%209.29.18%20PM.png?dl=1\" width=\"420\"> </center>\n\n<p>위의 수식이 바로 unbiased estimate입니다.</p>\n<p>$\\gamma$-just인 $\\hat{A}_t$에 대한 한가지 조건은 $\\hat{A}_t$이 두 가지 function $Q_t$ and $b_t$로 나뉠 수 있다는 것입니다.</p>\n<ul>\n<li>$Q_t$는 $\\gamma$-discounted Q-function의 unbiased estimator입니다.</li>\n<li>$b_t$는 action $a_t$전에 sampling된 states and actions의 arbitrary function이다. </li>\n</ul>\n<p>Proposition 1.<br>모든 $(s_t, a_t)$에 대해,<br>$$\\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty | s_t, a_t} [Q_t (s_{t:\\infty}, a_{t:\\infty})] = Q^{\\pi, \\gamma} (s_t, a_t)$$<br>로 인하여 $\\hat{A}_t$이 </p>\n<center> <img src=\"https://www.dropbox.com/s/59ydsav6djwyevl/Screen%20Shot%202018-08-22%20at%209.27.13%20PM.png?dl=1\" width=\"380\"> </center>\n\n<p>형태라고 가정합시다. (가정을 바탕으로 이루어지는 명제라는 점을 주목합시다.)</p>\n<p>그 때, $\\hat{A}_t$은 $\\gamma$-just입니다.</p>\n<p>이 명제에 대한 증명은 Appendix B에 있습니. 그리고 $\\hat{A}_t$에 대한 $\\gamma$-just advantage estimator들은 다음과 같습니다.</p>\n<ul>\n<li>$\\sum_{l=0}^\\infty \\gamma^l r_{t+1}$</li>\n<li>$A^{\\pi, \\gamma} (s_t, a_t)$</li>\n<li>$Q^{\\pi, \\gamma} (s_t, a_t)$</li>\n<li>$r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)$</li>\n</ul>\n<p>증명을 보기 전에 먼저 Advantage function에 대해서 먼저 살펴봅시다. 아래의 내용은 Sutton이 쓴 논문인 Policy Gradient Methods for Reinforcement Learning with Function Approximation(2000)에서 나온 내용을 리뷰한 것입니다. </p>\n<center> <img src=\"https://www.dropbox.com/s/x737yq97ut6gp1a/Screen%20Shot%202018-07-15%20at%201.16.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n<p>여기서는 “이 수식은 $\\sum_a \\frac{\\partial \\pi (s,a)}{\\partial \\theta} = 0$이기 때문에 가능해진다.”, “즉, 이 수식은 $\\pi(s,a)$의 gradient에만 dependent하기 때문에 advantage 역할을 하는 함수들을 넣어도 아무런 상관이 없다”라는 두 문장을 기억해둡시다.</p>\n<p>이제 증명을 보면 됩니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/e7sj2fwcm1f7hof/figure2.jpg?dl=1\" width=\"600\"> </center><br><center> <img src=\"https://www.dropbox.com/s/kyk4jes1202az4m/figure3.jpg?dl=1\" width=\"600\"> </center>\n\n<p>위의 증명 수식에서 Q와 b의 각각 세번째 수식과 마지막 수식을 자세히 봅시다.</p>\n<ol>\n<li><p>$Q$에 대한 증명</p>\n<ul>\n<li><p>빨간색 부분</p>\n<ul>\n<li>$\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [Q_t (s_{0:\\infty}, a_{0:\\infty})]$를 쉽게 말하자면 “모든(과거, 현재, 미래) state와 action에 대해서 expectation을 $t+1$부터 $\\infty$까지 하겠다.”라는 말입니다. 이렇게 함으로써 원래는 $Q^\\pi (s_t, a_t)$가 나와야 맞는 건데, 앞에서 살펴봤다시피 4번째 수식 밑줄 친 자리에는 advantage 역할을 하는 함수들을 넣어도 결과값에 아무런 영향을 끼치지 않기 때문에 의도적으로 $A^\\pi (s_t, a_t)$로 바꾼 것입니다. 또한 이 증명을 바탕으로 $\\hat{A}$이 $\\gamma$-just라는 것을 표현하고 싶기 때문이라고 봐도 됩니다.</li>\n</ul>\n</li>\n<li><p>파란색 부분</p>\n<ul>\n<li>또한 $a_{0:t}$에서 $a_{0:t-1}$로 변한 것은 $A^\\pi (s_t, a_t)$에 baseline이 들어가기 때문에 바꿔준 것입니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>$b$에 대한 증명</p>\n<ul>\n<li>빨간색 부분<ul>\n<li>$\\mathbb{E}_{s_{t+1:\\infty}, a_{t+1:\\infty}} [\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)]$가 0으로 바뀌는 것은 위에서 설명했듯이 $\\nabla_\\theta$자체가 $\\log \\pi_\\theta$만 gradient하기 때문에 이것을 expectation을 취하면 0이 됩니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p><br><br></p>\n<h1 id=\"4-Advantage-Function-Estimation\"><a href=\"#4-Advantage-Function-Estimation\" class=\"headerlink\" title=\"4. Advantage Function Estimation\"></a>4. Advantage Function Estimation</h1><p>이번 section에는 discounted advantage function $A^{\\pi, \\gamma} (s_t, a_t)$의 accurate estimate $\\hat{A}_t$에 대해서 살펴봅시다. 이에 따른 수식은 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/sn16g5iyzzflju0/Screen%20Shot%202018-08-22%20at%209.22.49%20PM.png?dl=1\" width=\"300\"> </center>\n\n<p>여기서 n은 a batch of episodes에 대하여 index한 것입니다.</p>\n<p>$V$를 approximate value function이라고 합시다. 그리고 $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$이라고 합시다.</p>\n<p>만약 (이전과 마찬가지로 가정부터 합니다.) correct value function $V = V^{\\pi, \\gamma}$가 있다고 한다면, 이것은 $\\gamma$-just advantage estimator입니다. 실제로, $A^{\\pi, \\gamma}$의 unbiased estimator는 다음과 같습니다.<br>$$\\mathbb{E}_{s_{t+1}} [\\delta_t^{V^{\\pi, \\gamma}}] = \\mathbb{E}_{s_{t+1}} [r_t + \\gamma V^{\\pi, \\gamma} (s_{t+1}) - V^{\\pi, \\gamma} (s_t)]$$<br>$$= \\mathbb{E}_{s_{t+1}} [Q^{\\pi, \\gamma} (s_t, a_t) - V^{\\pi, \\gamma} (s_t)] = A^{\\pi, \\gamma} (s_t, a_t)$$<br>그러나, “이 estimator는 유일하게 $V = V^{\\pi, \\gamma}$에 대한 $\\gamma$-just입니다.” 다른 경우라면 이것은 biased policy gradient estimate일 것입니다. (우리가 하고 싶은 것은 $V$에 대해서만 unbiased estimator가 아니라 advantage function에 대해서 일반화된 unbiased estimator를 얻고 싶은 것입니다. 그래서 아래에서도 나오겠지만, $\\gamma$와 함께 $\\lambda$를 이용한 estimator가 나옵니다.ㅏ)</p>\n<p>그래서 $\\delta$에 대해 $k$의 sum으로 생각해봅시다. 이것을 $\\hat{A}_t^{(k)}$라고 하자. 그러면 아래와 같이 표현할 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ra7hxksveg2hz45/figure4.jpg?dl=1\" width=\"600\"> </center>\n\n<p>$\\hat{A}_t^{(k)}$은 returns의 $k$-step estimate와 연관지을 수 있고, $\\delta_t^V = \\hat{A}_t^{(1)}$의 case와 유사하게도 $\\hat{A}_t^{(k)}$를 advantage function의 estimator로 생각할 수 있습니다.</p>\n<p>여기서 $k \\rightarrow \\infty$로 생각해보면 bias가 일반적으로 점점 더 작아집니다. 왜냐하면 $\\gamma^k V(s_{t+k})$가 점점 많이 discounted되서 $-V(s_t)$가 bias에 영향을 미치지 못하기 때문입니다. $k \\rightarrow \\infty$를 취하면 다음과 같은 수식이 나옵니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/13fn9wcup8pfh9u/Screen%20Shot%202018-08-22%20at%209.19.52%20PM.png?dl=1\" width=\"320\"> </center>\n\n<p>우변의 수식과 같이 empirical returns에서 value function baseline을 뺀 것으로 나타낼 수 있습니다.</p>\n<p>Generalized Advantage Estimator GAE($\\gamma, \\lambda$)는 $k$-step estimators의 exponentially-weighted average로 나타낼 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/yg1ybmfkep3towu/figure5.jpg?dl=1\" width=\"600\"> </center>\n\n<p>(TD($\\lambda$)를 떠올려주세요..!)</p>\n<p>위의 마지막 수식에서도 알 수 있듯이, advantage estimator는 Bellman residual terms의 discounted sum과 관련있는 간단한 수식입니다. 다음 section에서 modified reward function을 가진 MDP에 returns로서의 관점에서 위의 마지막 수식을 더 자세히 살펴봅시다. 위의 수식은 TD($\\lambda$)와 많이 유사합니다. 그러나 TD($\\lambda$)는 value function를 estimator하고, 여기서는 advantage function을 estimator합니다.</p>\n<p>위의 수식에서 $\\lambda = 0$ and $\\lambda = 1$에 대해서는 특별한 case가 존재한다. 수식으로 표현하면 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ufglxzanhcnbi36/figure6.jpg?dl=1\" width=\"600\"> </center>\n\n<ul>\n<li>$GAE(\\gamma, 1)$은 $V$의 정확도와 관계없이 $\\gamma$-just입니다. 그러나 returns의 sum때문에 high variance합니다.</li>\n<li>$GAE(\\gamma, 0)$은 $V = V^{\\pi, \\gamma}$에 대해 $\\gamma$-just입니다. 그리고 bias하지만 일반적으로 훨씬 lower variance를 가집니다. </li>\n<li>$0 &lt; \\lambda &lt; 1$에 대해 GAE는 parameter $\\lambda$를 control하여 bias와 variance사이에 compromise를 만듭니다.</li>\n</ul>\n<p>두 가지 별개의 parameter $\\gamma$ and $\\lambda$를 가진 advantage estimator는 bias-variance tradeoff에 도움을 줍니다. 그러나 이 두 가지 parameter는 각각 다른 목적을 가지고 작동합니다.</p>\n<ul>\n<li>$\\gamma$는 가장 중요하게 value function $V^{\\pi, \\gamma}$ 의 scale을 결정합니다. 또한 $\\gamma$는 $\\lambda$에 의존하지 않습니다. $\\gamma &lt; 1$로 정하는 것은 policy gradient estimate에서 bias합니다.</li>\n<li>반면에, $\\lambda &lt; 1$는 유일하게 value function이 부정확할 때 bias합니다. 그리고 경험상, $\\lambda$의 best value는 $\\gamma$의 best value보다 훨씬 더 낮습니다. 왜냐하면 $\\lambda$가 정확한 value function에 대해 $\\gamma$보다 훨씬 덜 bias하기 때문입니다.</li>\n</ul>\n<p>GAE를 사용할 때, $g^\\gamma$의 biased estimator를 구성할 수 있습니다. 수식은 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/ok57dsk52o7pl9r/Screen%20Shot%202018-08-22%20at%209.17.54%20PM.png?dl=1\" width=\"670\"> </center>\n\n<p>여기서 $\\lambda = 1$일 때 동일해집니다.</p>\n<p><br><br></p>\n<h1 id=\"5-Interpretation-as-Reward-Shaping\"><a href=\"#5-Interpretation-as-Reward-Shaping\" class=\"headerlink\" title=\"5. Interpretation as Reward Shaping\"></a>5. Interpretation as Reward Shaping</h1><p>이번 section에서는 앞서 다뤘던 수식 $\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V$를 modified reward function의 MDP의 관점으로 생각해봅시다. 조금 더 구체적으로 말하자면, MDP에서 reward shaping transformation을 실행한 후에 적용된 extra discounted factor로서 $\\lambda$를 어떻게 볼 것인지에 대해서 다룹니다.</p>\n<p>(개인적인 comment) 한 가지 먼저 언급하자면, 본래의 목적은 reward shaping이 아니라 variance reduction입니다. 이번 section은 그저 이전에 이러한 개념이 있었고, gae를 다른 관점에서 생각해보자라는 뜻에서 나온 section인 것 같습니다. ‘아~ 이러한 개념이 있구나~!’ 정도로만 알면 될 것 같습니다. ‘gae가 reward shaping의 효과까지 있어서 이러한 section을 넣은 것일까?’ 라는 생각도 해봤지만 아직 잘 모르겠습니다. 실험부분에도 딱히 하지 않은 걸로 봐서는.. 아닌 것 같기도 하고.. 아직까지는 그저 ‘reward shaping의 관점에서 봤을 때에도 gae로 만들어줄 수 있다.’ 정도만 생각하려고 합니다.</p>\n<p>Reward Shaping이란 개념 자체는 일반적인 알고리즘 분야(최단경로 문제 등)에도 있습니다. 하지만 이 개념이 머신러닝에 적용된 건 Andrew Y. Ng이 쓴 Policy Invariance under Reward Transformations Theory and Application to Reward Shaping(1999) 논문입니다. 논문에서 Reward Shaping을 설명하는 부분은 간략하게 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/0w9cuxjkhz2mbcy/figure7.jpg?dl=1\" width=\"400\"> </center><br><center> <img src=\"https://www.dropbox.com/s/xcp9eqvcbv3268g/figure8.jpg?dl=1\" width=\"400\"> </center>\n\n<p>위의 글만 보고 이해가 잘 안됩니다. 그래서 다른 자료들을 찾던 중에 다음의 그림을 찾게 되었습니다. (아래의 그림은 Youtube에 있는 <a href=\"https://www.youtube.com/watch?v=xManAGjbx2k&amp;t=95s\" target=\"_blank\" rel=\"noopener\">Udacity 영상</a>에서 있는 그림입니다.)</p>\n<center> <img src=\"https://www.dropbox.com/s/87kacngez9cl2e6/figure9.jpg?dl=1\" width=\"400\"> </center>\n\n<p>이 그림을 통해서 Reward Shaping을 이해해봅시다.</p>\n<p>Reward Shaping(보상 형성)을 통해서 뭘 하고 싶은지 목적부터 봅시다. reward space가 sparse 한 경우에 reward가 너무 드문드문 나옵니다. 따라서 이것을 꾸준히 reward를 받을 수 있도록 바꿉니다. potential-based shaping function인 $\\Phi$를 만들어서 더하고 빼줍니다. 저 $\\Phi$ 자리에는 대표적으로 state value function이 많이 들어간다고 합니다. (아직 목적이 이해가 안될 수 있습니다. Reward Shaping에 대해서 다 보고 나서 다시 한 번 읽어봅시다.)</p>\n<p>위의 그림은 돌고래가 점프하여 불구멍을 통과해야하는 환경입니다. 하지만 저 불구멍을 통과하기 위해서는 (1) 점프도 해야하고, (2) 불도 피해야하고, (3) 알맞게 착지까지 완료해야합니다. 이렇게 해야 reward를 +1 얻습니다. </p>\n<p>생각해봅시다. 어느 세월에 점프도 해야하고.. 불도 피해야하고.. 알맞게 착지까지해서 reward를 +1 얻겠습니까? 따라서 그 전에도 잘하고 있는 지, 못하고 있는 지를 판단하기 위해, 다시 말해 reward를 꾸준히 받도록 다음과 같이 transformed reward function $\\tilde{r}$을 정의합니다.</p>\n<p>$$\\tilde{r} (s, a, s’) = r(s, a, s’) + \\gamma \\Phi (s’) - \\Phi (s)$$</p>\n<ul>\n<li>여기서 $\\Phi : S \\rightarrow \\mathbb{R}$를 state space에서의 arbitrary scalar-valued function을 나타냅니다. 그리고 $\\Phi$자리에는 대표적으로 state value function이 들어간다고 생각합시다.</li>\n<li>형태가 TD residual term의 결과와 비슷하지만 의미와 의도가 다릅니다. reward shaping은 sparse reward 때문이고, 이전 section에서 봤던 gae는 variance reduction때문에 나온 것입니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/tkfpqniaazwfbld/figure10.jpg?dl=1\" width=\"600\"> </center>\n\n<p>이 transformation은 discounted advantage function $A^{\\pi, \\gamma}$으로도 둘 수 있습니다. state $s_t$를 시작하여 하나의 trajectory의 rewards의 discounted sum을 표현하면 다음과 같습니다.<br>$$\\sum_{l=0}^\\infty \\gamma^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty \\gamma^l r(s_{t+l}, a_{t+l}, s_{t+l+1}) - \\Phi(s_t)$$</p>\n<p>$\\tilde{Q}^{\\pi, \\gamma}, \\tilde{V}^{\\pi, \\gamma}, \\tilde{A}^{\\pi, \\gamma}$를  transformed MDP의 value function과 advantage function이라고 하면, 다음과 같은 수식이 나옵니다.<br>$$\\tilde{Q}^{\\pi, \\gamma} (s, a) = Q^{\\pi, \\gamma} (s, a) - \\Phi (s)$$<br>$$\\tilde{V}^{\\pi, \\gamma} (s, a) = V^{\\pi, \\gamma} (s, a) - \\Phi (s)$$<br>$$\\tilde{A}^{\\pi, \\gamma} (s, a) = (Q^{\\pi, \\gamma} (s, a) - \\Phi (s)) - (V^\\pi (s) - \\Phi (s)) = A^{\\pi, \\gamma} (s, a)$$</p>\n<p>이제 reward shaping의 idea를 가지고 어떻게 policy gradient estimate를 얻을 수 있는 지에 대해서 알아봅시다.</p>\n<ul>\n<li>$0 \\le \\lambda \\le 1$의 범위에 있는 steeper discount $\\gamma \\lambda$를 사용합니다.</li>\n<li>shaped reward $\\tilde{r}$는 Bellman residual term $\\delta^V$와 동일합니다.</li>\n<li>그리고 $\\Phi = V$와 같다고 보면 다음과 같은 수식이 나옵니다.</li>\n</ul>\n<p>$$\\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\tilde{r} (s_{t+l}, a_t, s_{t+l+1}) = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V = \\hat{A}_t^{GAE(\\gamma, \\lambda)}$$<br>이렇게 shaped rewards의 $\\gamma \\lambda$-discounted sum을 고려함으로써 GAE를 얻을 수 있습니다. 더 정확하게 $\\lambda = 1$은 $g^\\gamma$의 unbiased estimate이고, 반면에 $\\lambda &lt; 1$은 biased estimate입니다.</p>\n<p>좀 더 나아가서, shaping transformation과 parameters $\\gamma$ and $\\lambda$의 결과를 보기 위해, response function $\\chi$를 이용하면 다음과 같은 수식이 나옵니다.<br>$$\\chi (l; s_t, a_t) = \\mathbb{E} [r_{t+l} | s_t, a_t] - \\mathbb{E} [r_{t+l} | s_t]$$<br>추가적으로 $A^{\\pi, \\gamma} (s, a) = \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$입니다.<br>그래서 discounted policy gradient estimator는 다음과 같이 쓸 수 있다.<br>$$\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) A^{\\pi, \\gamma} (s_t, a_t) = \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) \\sum_{l=0}^\\infty \\gamma^l \\chi (l; s, a)$$</p>\n<p><br><br></p>\n<h1 id=\"6-Value-Fuction-Estimation\"><a href=\"#6-Value-Fuction-Estimation\" class=\"headerlink\" title=\"6. Value Fuction Estimation\"></a>6. Value Fuction Estimation</h1><p>이번 section에서는 Trust Region Optimization Scheme에 따라 Value function을 Estimation합니다.</p>\n<p><br></p>\n<h2 id=\"6-1-Simplest-approach\"><a href=\"#6-1-Simplest-approach\" class=\"headerlink\" title=\"6.1 Simplest approach\"></a>6.1 Simplest approach</h2><p>$$minimize_{\\phi} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi}(s_n) - \\hat{V_n} \\vert\\vert^{2}$$</p>\n<ul>\n<li>위는 가장 간단하게 non-linear approximation으로 푸는 방법입니다.</li>\n<li>$\\hat{V_t} = \\sum_{l=0}^{\\infty}\\gamma^l r_{t+l}$은 reward 에 대한 discounted sum을 의미합니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"6-2-Trust-region-method-to-optimize-the-value-function\"><a href=\"#6-2-Trust-region-method-to-optimize-the-value-function\" class=\"headerlink\" title=\"6.2 Trust region method to optimize the value function\"></a>6.2 Trust region method to optimize the value function</h2><ul>\n<li>Value function을 최적화 하기 위해 trust region method를 사용합니다.</li>\n<li>Trust region은 최근 데이터에 대해 overfitting되는 것을 막아줍니다.</li>\n</ul>\n<p>Trust region문제를 풀기 위해서는 다음 스텝을 따릅니다.</p>\n<ul>\n<li>$\\sigma^2 = \\frac{1}{N} \\sum_{n=1}^{N} \\vert\\vert V_{\\phi old}(s_n) - \\hat{V_n} \\vert\\vert^{2}$을 계산합니다.</li>\n<li>그 후에 다음과 같은 constrained opimization문제를 풉니다.</li>\n</ul>\n<center> <img src=\"https://www.dropbox.com/s/d74cg3votxi2dbg/Screen%20Shot%202018-08-22%20at%209.13.28%20PM.png?dl=1\" width=\"300\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/jrkjax71t3j3vk4/Screen%20Shot%202018-08-22%20at%209.13.35%20PM.png?dl=1\" width=\"370\"> </center>\n\n<ul>\n<li>위의 수식은 사실 old Value function과 새로운 Value function KL distance가 $\\epsilon$ 다 작아야한다는 수식과 같습니다. Value function이 평균은 $V_{\\phi}(s)$이고 분산이 $\\sigma^2$인 conditional Gaussian distribution으로 parameterize되었을 뿐입니다.</li>\n</ul>\n<p><img width=\"400px\" src=\"https://www.dropbox.com/s/uw0v05feu8chqmc/Screenshot%202018-07-08%2010.04.38.png?dl=1\"> </p>\n<p>이 trust region문제의 답을 conjudate gradient algorithm을 이용하여 근사값을 구할 수 있습니다. 특히, 다음과 같은  quadratic program을 풀게됩니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/pawjkm8b9ycwsl5/Screen%20Shot%202018-08-22%20at%209.10.36%20PM.png?dl=1\" width=\"250\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/va7wms8uqxnw5rq/Screen%20Shot%202018-08-22%20at%209.10.40%20PM.png?dl=1\" width=\"370\"> </center>\n\n<ul>\n<li>여기서 $g$는 objective 의 gradient입니다.</li>\n<li>$j_n = \\nabla_{\\phi} V_{\\phi}(s_n)$일때, $H = \\frac{1}{N} \\sum_{n} j_n j^T_n$이며, $H$는 objective의 hessian에 대해서 gaussian newton method로 근사한 값입니다. 따라서, value function을 conditional probability로 해석한다면 Fisher information matrix가 됩니다.</li>\n<li>구현할때의 방법은 TRPO에서 사용한 방법과 모두 같습니다.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"7-Experiments\"><a href=\"#7-Experiments\" class=\"headerlink\" title=\"7.Experiments\"></a>7.Experiments</h1><p>실험은 다음 두 가지 물음에 대해서 디자인 되었습니다.</p>\n<ul>\n<li>GAE에 따라서 episodic total reward를 최적화 할때, $\\lambda$와 $\\gamma$가 변함에 따라서 어떤 경험적인 효과를 볼 수 있는지?</li>\n<li>GAE와 trust region alogorithm을 policy와 value function 모두에 함께 사용했을 때 어려운 문제에 적용되는 큰 뉴럴넷을 최적화 할 수 있을지?</li>\n</ul>\n<p><br></p>\n<h2 id=\"7-1-Policy-Optimization-Algorithm\"><a href=\"#7-1-Policy-Optimization-Algorithm\" class=\"headerlink\" title=\"7.1 Policy Optimization Algorithm\"></a>7.1 Policy Optimization Algorithm</h2><p>Policy update는 TRPO를 사용합니다. TRPO에 대한 설명은 여기서는 생략하겠습니다. TRPO 포스트를 보고 돌아와주세요!</p>\n<ul>\n<li><p>이전 TRPO에서 이미 TRPO와 다른 많은 알고리즘들을 비교하였기 때문에, 여기서는 똑같은 짓을 반복하지 않고 $\\lambda$, $\\gamma$가 변함에 따라 어떤 영향이 있는 지에 대한 실험에 집중하겠다고 합니다. (귀찮았던거죠..)</p>\n</li>\n<li><p>TRPO를 적용한 GAE의 최종 알고리즘은 다음과 같습니다.</p>\n</li>\n</ul>\n<center> <img width=\"600px\" src=\"https://www.dropbox.com/s/b1klz11f2frrvg4/Screenshot%202018-07-08%2010.33.54.png?dl=1\"> </center><br><center> <img width=\"300px\" src=\"https://www.dropbox.com/s/35fxte05pqtiel7/Screenshot%202018-07-08%2010.35.22.png?dl=1\"> </center><br><center> <img width=\"300px\" src=\"https://www.dropbox.com/s/u35pjow3w50bvz1/Screenshot%202018-07-08%2010.36.03.png?dl=1\"> </center>\n\n<ul>\n<li>여기서 주의할 점은 Policy update($\\theta_i \\rightarrow \\theta_{i+1}$)에서 $V_{\\phi_i}$를 사용했다는 점입니다.</li>\n<li>만약 Value function을 먼저 update하게 된다면 추가적인 bias가 발생합니다.</li>\n<li>극단적으로 생각해보아서, 우리가 Value function을 완벽하게 overfit을 해낸다면 Bellman residual($r_t + \\gamma V(s_{t+1}) - V(S_t)$)은 0이 됩니다. 그럼 Policy gradient의 estimation도 거의 0이 될 것입니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"7-2-Expermint-details\"><a href=\"#7-2-Expermint-details\" class=\"headerlink\" title=\"7.2 Expermint details\"></a>7.2 Expermint details</h2><h3 id=\"7-2-1-Environment\"><a href=\"#7-2-1-Environment\" class=\"headerlink\" title=\"7.2.1 Environment\"></a>7.2.1 Environment</h3><p>실험에서 사용된 환경은 다음 네 가지 입니다.</p>\n<ol>\n<li>classic cart-pole (x 3D)</li>\n<li>bipedal locomotion</li>\n<li>quadrupedal locomotion</li>\n<li>dynamically standing up for the biped</li>\n</ol>\n<h3 id=\"7-2-2-Architecture\"><a href=\"#7-2-2-Architecture\" class=\"headerlink\" title=\"7.2.2 Architecture\"></a>7.2.2 Architecture</h3><ul>\n<li>3D robot task에 대해서는 같은 모델을 사용하였습니다.<ul>\n<li>layers  = [100, 50, 25] 각각 tanh 사용. (Policy와 Value 네트워크 모두)</li>\n<li>Final output layer은 linear</li>\n</ul>\n</li>\n<li>Cartpole에 대해서는 1개의 layer 안에 20개의 hidden unit만 있는 linear policy를 사용했다고 합니다.</li>\n</ul>\n<h3 id=\"7-2-3-Task\"><a href=\"#7-2-3-Task\" class=\"headerlink\" title=\"7.2.3 Task\"></a>7.2.3 Task</h3><ul>\n<li>Cartpole<ul>\n<li>한 배치당 20 개의 trajectory를 모았고, maximum length는 1000입니다.</li>\n</ul>\n</li>\n<li>3D biped locomotion<ul>\n<li>33 dim state , 10 dim action</li>\n<li>50000 time step per batch</li>\n</ul>\n</li>\n<li>3D quadruped locomotion<ul>\n<li>29 dim state, 8 dim action</li>\n<li>200000 time step per batch</li>\n</ul>\n</li>\n<li>3D biped locomotion Standing<ul>\n<li>33 dim state , 10 dim action</li>\n<li>200000 time step per batch</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"7-2-3-results\"><a href=\"#7-2-3-results\" class=\"headerlink\" title=\"7.2.3 results\"></a>7.2.3 results</h3><p>cost의 관점에서 결과를 나타내었다고 합니다. Cost는 negative reward와 이것이 최소화 되었는가로 정의되었습니다.</p>\n<h4 id=\"7-2-3-1-Cartpole\"><a href=\"#7-2-3-1-Cartpole\" class=\"headerlink\" title=\"7.2.3.1 Cartpole\"></a>7.2.3.1 Cartpole</h4><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/x9pbms1wvg38lda/Screenshot%202018-07-08%2011.08.22.png?dl=1\"> </center>\n\n<ul>\n<li>왼쪽 그림은 $\\gamma$를 0.99로 고정시켜놓은 상태에서 $\\lambda$를 변화시킴에 따라서 cost를 측정한 것입니다. </li>\n<li>오른쪽은 $\\gamma$와 $\\lambda$를 둘 다 변화 시키면서 성능을 그림으로 나타낸 표입니다. 흰색에 가까울 수록 좋은 성능입니다. </li>\n</ul>\n<h4 id=\"7-2-3-2-3D-BIPEDAL-LOCOMOTINO\"><a href=\"#7-2-3-2-3D-BIPEDAL-LOCOMOTINO\" class=\"headerlink\" title=\"7.2.3.2 3D BIPEDAL LOCOMOTINO\"></a>7.2.3.2 3D BIPEDAL LOCOMOTINO</h4><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/i9wj4p6ijojsy82/Screenshot%202018-07-08%2011.25.08.png?dl=1\"> </center>\n\n\n<ul>\n<li>다른 random seed로 부터 9번 씩 시도한 결과를 mean을 취해서 사용합니다.</li>\n<li>Best performance는   $\\gamma \\in [0.99, 0.995]$ 그리고  $\\lambda \\in [0.96, 0.99]$일때. </li>\n<li>1000 iteration 후에 빠르고 부드럽고 안정적인 걸음거이가 나옵니다.</li>\n<li>실제로 걸린 시간은 0.01(타입스텝당 시간) <em> 50000(배치당 타임스텝) </em> 1000(배치) <em> 3600(초-&gt;시간) </em> 24 = 5.8일 정도가 걸렸습니다.</li>\n</ul>\n<h4 id=\"7-2-3-3-다른-ROBOT-TASKS\"><a href=\"#7-2-3-3-다른-ROBOT-TASKS\" class=\"headerlink\" title=\"7.2.3.3 다른 ROBOT TASKS\"></a>7.2.3.3 다른 ROBOT TASKS</h4><center> <img width=\"500px\" src=\"https://www.dropbox.com/s/fuuat65we52quht/Screenshot%202018-07-08%2011.33.11.png?dl=1\"> </center>\n\n<ul>\n<li>다른 로봇 TASK에 대해서는 아주 제한적인 실험만 진행합니다.(시간이 부족했던듯 하네요..)</li>\n<li>Quadruped에 대해서는 $\\gamma = 0.995$로 fix, $\\lambda \\in {0, 0.96}$</li>\n<li>Standingup에 대해서는 $\\gamma = 0.99$로 fix, $\\lambda \\in {0, 0.96}$</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"8-Discussion\"><a href=\"#8-Discussion\" class=\"headerlink\" title=\"8. Discussion\"></a>8. Discussion</h1><p><br></p>\n<h2 id=\"8-1-Main-discussion\"><a href=\"#8-1-Main-discussion\" class=\"headerlink\" title=\"8.1 Main discussion\"></a>8.1 Main discussion</h2><p>지금까지 복잡하고 어려운 control problem에서 Reinforcement Learning(RL)은 high sample complexity 때문에 제한이 되어왔습니다. 따라서 이 논문에서 그 제한을 풀고자 advantage function의 good estimate를 얻는 “variance reduction”에 대해 연구하였습니다.</p>\n<p>“Generalized Advantage Estimator(GAE)”라는 것을 제안했고, 이것은 bias-variance tradeoff를 조절하는 두 개의 parameter $\\gamma,\\lambda$를 가집니다.<br>또한 어떻게 Trust Region Policy Optimization과 value function을 optimize하는 Trust Region Algorithm의 idea를 합치는 지를 보였습니다.</p>\n<p>이렇게 함으로써 보다 더 복잡하고 어려운 control task들을 해결할 수 있었습니다.</p>\n<p>GAE의 실험적인 입증으로는 robotic locomotion을 simulation하는 domain입니다. 실험에서도 보여준 것처럼 [0.9, 0.99]의 범위에서 $\\lambda$의 적절한 중간의 값을 통해 best performance를 얻습니다. 좀 더 나아가 연구되어야할 점은 adaptive or automatic하도록 estimator parameter $\\gamma,\\lambda$를 조절하는 방법입니다.</p>\n<p><br></p>\n<h2 id=\"8-2-Future-work\"><a href=\"#8-2-Future-work\" class=\"headerlink\" title=\"8.2 Future work\"></a>8.2 Future work</h2><p>Value function estimation error와 Policy gradient estimation error사이의 관계를 알아낸다면, 우리는 Value function fitting에 더 잘 맞는 error metric(policy gradient estimation 의 정확성과 더 잘 맞는 value function)을 사용할 수 있습니다. 여기서 Policy와 Value function의 파라미터를 공유하는 모델을 만드는 것은 아주 흥미롭고 이점이 많습니다. 하지만 수렴을 보장하도록 적절한 numerical optimization을 제시해야 할 것입니다.</p>\n<p>추가적으로 DDPG는 별로라고 합니다. 그리고 TD(0)는 bias가 너무 크고, poor performance로 이끈다고 합니다. 특히나 이 논문에서는 low-dimention의 쉬운 문제들만 해결했습니다.</p>\n<center> <img width=\"500px\" src=\"https://www.dropbox.com/s/nhc7t9psul5lr3x/Screenshot%202018-07-08%2011.45.15.png?dl=1\"> </center>\n\n<p><br></p>\n<h2 id=\"8-3-FAQ\"><a href=\"#8-3-FAQ\" class=\"headerlink\" title=\"8.3 FAQ\"></a>8.3 FAQ</h2><ul>\n<li>Compatible features와는 무슨 관계?<ul>\n<li>Compatible features는 value function을 이용하는 policy gradient 알고리즘들과 함께 자주 언급됩니다.</li>\n<li>Actor Critic의 저자는 policy의 제한된 representation power때문에, policy gradient는 단지 advantage function space의 subspace에만 의존하게 됩니다.</li>\n<li>이 subspace는 compatible features에 의해 span됩니다.</li>\n<li>이 이론은 현재 문제 구조를 어떻게 이용해야 advantage function에 대해 더 나은 estimation을 할 수 있는 지에 대한 지침을 주지 않습니다. GAE 논문의 idea와 orthogonal합니다.</li>\n</ul>\n</li>\n<li>왜 Q function을 사용하지 않는가?<ul>\n<li>먼저 state-value function이 더 낮은 차원의 input을 가진다. 그래서 Q function보다 더 학습하기가 쉽습니다.</li>\n<li>두 번째로 이 논문에서 제안하는 방법으로는 high bias estimator에서 low bias estimator로 $\\lambda$를 통해서 부드럽게 interpolate를 할 수 있습니다.</li>\n<li>반면에 Q를 사용하면 단지 high-bias estimator 밖에 사용할 수 없습니다.</li>\n<li>특히나 return에 대한 one-step estimation은 엄두를 못낼 정도로 bias가 큽니다.</li>\n</ul>\n</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"TRPO-여행하기\"><a href=\"#TRPO-여행하기\" class=\"headerlink\" title=\"TRPO 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/\">TRPO 여행하기</a></h2><h2 id=\"TRPO-Code\"><a href=\"#TRPO-Code\" class=\"headerlink\" title=\"TRPO Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/trpo_gae.py\" target=\"_blank\" rel=\"noopener\">TRPO Code</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"PPO-여행하기\"><a href=\"#PPO-여행하기\" class=\"headerlink\" title=\"PPO 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/\">PPO 여행하기</a></h2>"},{"title":"Natural Policy Gradient","date":"2018-06-25T02:36:45.000Z","author":"김동민, 이동민, 이웅원, 차금강","subtitle":"피지여행 4번째 논문","_content":"\n<center> <img src=\"https://www.dropbox.com/s/yd0x14ljrhpnj1b/Screen%20Shot%202018-07-18%20at%201.08.05%20AM.png?dl=1\" width=\"600\"> </center>\n\n논문 저자 : Sham Kakade\n논문 링크 : https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf\nProceeding : Advances in Neural Information Processing Systems (NIPS) 2002\n정리 : 김동민, 이동민, 이웅원, 차금강\n\n---\n\n# 1. 들어가며...\n\n이 논문이 발표된 2002년 당시에도 많은 연구자들이 objective function의 gradient 값을 따라서 좋은 policy $\\pi$를 찾고자 하였습니다. 하지만 기존의 우리가 알던 gradient descent method는 steepest descent direction이 아닐 수 있기 때문에(쉽게 말해 가장 가파른 방향을 따라서 내려가야 하는데 그러지 못할 수도 있다는 것입니다.) 이 논문에서 steepest descent direction를 나타내는 natural gradient method를 policy gradient에 적용하여 좋은 policy $\\pi$를 찾습니다. \n\n<br>\n## 1.1 NPG 흐름 잡기\n\n### 1.1.1 매니폴드(manifold)\n\n<center> <img src=\"https://www.dropbox.com/s/cstjgemqpby4ysr/Screen%20Shot%202018-08-12%20at%208.28.29%20PM.png?dl=1\" width=\"300\"> </center>\n\n매니폴드는 간단하게 말해 위의 그림에서의 점들을 아우르는 subspace입니다. 그래서 NPG 공부하기 전에 매니폴드를 왜 배울까라는 의문이 들으실텐데 그 이유는 위에서도 언급했듯이 natural gradient method는 어떠한 파라미터 공간에서의 steepest descent direction을 강조하기 때문입니다. 여기서 파라미터 공간이 바로 리만 매니폴드입니다. 리만 매니폴드는 매니폴드가 각지지 않고 미분가능하게 부드럽게 곡률을 가진 면이라고 생각하면 편합니다.\n\nNeural Network(NN)을 사용할 경우 NN의 parameter space가 우리가 보통 생각하는 직선으로 쭉쭉 뻗어있는 유클리디안 공간(Euclidean space)가 아닙니다. 좀 더 일반적으로는 구의 표면과 같이 휘어져있는 공간 즉, 리만 공간(Riemannian space)로 표현할 수 있습니다. 다음 그림들을 보겠습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/jnlm6he18ar64yc/Screen%20Shot%202018-08-12%20at%208.14.13%20PM.png?dl=1\" width=\"400\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/tufjqmaeaz29kaz/Screen%20Shot%202018-08-12%20at%208.15.53%20PM.png?dl=1\" width=\"400\"> </center>\n\n아래의 그림처럼 어떠한 확률 분포가 있다고 해봅시다.\n\n<center> <img src=\"https://www.dropbox.com/s/mpoop19eyu1vp0a/Screen%20Shot%202018-08-13%20at%2012.52.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n어떠한 공간의 확률 분포에 있는 한 점은 다른 공간의 확률 분포에서의 한 점이 될 것입니다. 이렇게 위의 그림들과 같이 유클리디안 공간의 확률 분포에서의 한 점이 리만 공간의 확률 분포의 한 점이 되는 것이고, 곡률의 일차 근사가 유클리디안 공간에서는 일차 근사가 아닌 이차 근사이고, 리만 공간에서는 휘어진 공간이기 때문에 곡률을 일차 근사라고 보는 것입니다. 추가적으로 일차 근사와 이차 근사의 차이는 [다크 프로그래머님의 블로그](http://darkpgmr.tistory.com/149)를 참고해주시기 바랍니다. (꼭 보세요!) \n\n일차 근사와 이차 근사의 차이점과 각각의 장단점, 그리고 추가적으로 Line Search까지 알고 난 후에 이 논문을 보시는 것을 권장해드립니다.\n\n### 1.1.2 Natural Gradient + Policy Gradient\n\n먼저 아래의 그림들을 보여드리겠습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/65hra43zadsubff/Screen%20Shot%202018-08-13%20at%2012.51.45%20PM.png?dl=1\" width=\"500\"> <img src=\"https://www.dropbox.com/s/8g332zceeordqwv/Screen%20Shot%202018-08-13%20at%2012.51.51%20PM.png?dl=1\" width=\"400\"> </center>\n\n위에서도 언급했듯이 natural gradient가 steepest direction이 된다는 연구가 이뤄지고 있었습니다. 강화학습의 policy gradient은 objective function의 gradient를 따라 policy를 업데이트를 합니다. 이 때 policy는 parameterization되는데 이 경우에도 gradient 대신에 natural gradient가 좋다는 것을 실험해보는 논문이 지금 다루고 있는 논문입니다.\n\ngradient가 non-covariant(1차 근사)해서 생기는 문제는 간단히 말하자면 다음과 같습니다. policy가 parameterized된 상황에서는 같은 policy라도 다른 parameter를 가질 수 있습니다. 이 때, steepest direction은 두 경우에 같은 방향을 가리켜야 하는데 non-covariant한 경우 그렇지 못합니다. 이러한 부분이 바로 결국 느린 학습으로 연결이 되는 것입니다.\n\n논문에서 2차미분 방법론들과 짧게 비교합니다. 하지만 개인적인 의견으로 2차미분을 이용한 다른 방법들과의 비교가 생각보다 없는 점이 부족해 보였습니다.(Hessian을 이용한다거나 conjugate gradient method를 이용한다거나). 실험을 통해 Fisher Information Matrix(FIM)가 hessian에 수렴안하는 거라던지 Hessian 방법론이 local maxima 부근에서 상당히 느리다던지의 결과를 보여줬었으면 좋았을 것 같습니다.\n\n또한 natural gradient의 단점으로 natural gradient 만으로 업데이트하면 policy의 improvement 보장이 안될 수 있습니다. policy의 improvement를 보장하기 위해 line search도 써야하는데 line search를 어떻게 쓰는지에 대한 자세한 언급이 없습니다. 다시 말해 자세한 algorithm 설명이 없다는 것입니다.\n\nnatural policy gradient 논문은 natural gradient + policy gradient를 처음 적용했다는데 의의가 있습니다. 하지만 이 논문이 문제 삼은 gradient는 non-covariant하다라는 문제를 natural gradient를 통해 해결하지는 못했습니다(Experiment를 통해 covariant gradient가 되지 못했다는 것이 보입니다). NPG의 뒤를 잇는 논문이 “covariant policy search”와 “natural actor-critic”에서 covariant(2차 근사)하지 못하다는 것을 해결하기 위해 Fisher Information Matrix를 sample 하나 하나에 대해서 구하는 것이 아니라 trajectory 전체에 대해서 구합니다.\n\n또한 논문은 pg의 두 가지 세팅 중에 average-reward setting(infinite horizon)에서만 NPG를 다룹니다. “covariant policy search” 논문에서는 average-reward setting과 start-state setting 모두에 대해서 npg를 적용합니다.\n\nnatural gradient + policy gradient를 처음 제시했다는 것은 좋지만 npg 학습의 과정을 자세하게 설명하지 않았고 다른 2차 미분 방법들과 비교를 많이 하지 않은 점이 아쉬운 논문이었습니다.\n\n<br><br>\n\n# 2. Introduction\n\n소개는 앞에서 다 했기 때문에 간략하게 다시 한 번 정리하겠습니다. direct policy gradient method는 future reward의 gradient를 따라 policy를 update합니다. 하지만 gradient descent는 non-covariant입니다. 따라서 이 논문에서는 covarient gradient를 제시합니다. 바로 \"Natural Gradient\" 입니다. \n\n또한 natural gradient와 policy iteration의 연관성을 설명합니다. natural policy gradient is moving toward choosing a greedy optimal action (이런 연결점은 아마도 step-size를 덜 신경쓰고 싶어서 그런게 아닌가 싶습니다)\n\n논문의 Introduction 부분에 다음 멘트가 있습니다. 이 글만 봐서는 이해가 안갔는데 Mackay 논문에 좀 더 자세히 나와있었습니다.\n<center> <img src=\"https://www.dropbox.com/s/41xhhr7lgfk24a1/Screenshot%202018-06-10%2011.45.18.png?dl=1\"> </center>\n\n[Mackay](http://www.inference.org.uk/mackay/ica.pdf)논문에서는 다음과 같이 언급하고 있습니다. Back-propagation을 사용할 경우에 learning rate를 dimension에 1/n로 사용하면 수렴한다는 것이 증명됐습니다. 하지만 너무 느리다고 합니다.\n<center> <img src=\"https://www.dropbox.com/s/us9ezc7vxgrkez6/Screenshot%202018-06-10%2011.47.21.png?dl=1\"> </center>\n\n<br><br>\n\n# 3. A Natural Gradient\n\n<br>\n## 3.1 Notation\n\n이 논문에서 제시하는 Notation은 다음과 같습니다.\n\n- MDP : tuple $(S, s_0, A, R, P)$\n- $S$ : a finite set of states\n- $s_0$ : a start state\n- $A$ : a finite set of actions\n- $R$ : reward function $R: S \\times A -> [0, R_{max}]$\n- $\\pi(a;s, \\theta)$ : stochastic policy parameterized by $\\theta$\n- 모든 정책 $\\pi$는 ergodic : stationary distribution $\\rho^{\\pi}$이 잘 정의되어 있다고 봅니다.\n- 이 논문에서는 sutton의 pg 논문의 두 세팅(start-state formulation, average-reward formulation) 중에 두 번째인 average-reward formulation을 가정합니다.\n- performance or average reward : $\\eta(\\pi)=\\sum_{s,a}\\rho^{\\pi}(s)\\pi(a;s)R(s,a)$\n- state-action value : $Q^{\\pi}(s,a)=E_{\\pi}[\\sum_{t=0}^{\\infty}R(s_t, a_t)-\\eta(\\pi)\\vert s_0=s, a_0=a]$ ([Sutton PG](https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/) 를 참고해주시기 바랍니다.)\n- 정책이 $\\theta$로 parameterize되어있으므로 performance는 $\\eta(\\pi_{\\theta})$인데 $\\eta(\\theta)$로 씁니다.\n\n<br>\n## 3.2 Natural Gradient\n\nSutton PG 논문의 policy gradient theorem에 따라 exact gradient of the average reward는 다음과 같습니다. 다음 수식이 어떻게 유도되었는지, 어떤 의미인지 모른다면 [Sutton PG](https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/)을 통해 제대로 이해하는 것이 좋습니다. Euclidean space에서는 objective function의 일반적인 policy gradient는 Sutton PG에서 논의한 바와 같이 다음과 같이 구할 수 있습니다.\n\n$$\\nabla\\eta(\\pi_\\theta) = \\Sigma_{s,a}\\rho^\\pi(s)\\nabla\\pi(a;s,\\theta)Q^\\pi(s,a)$$\n\n여기서 steepest descent direction of $\\eta(\\theta)$는 $\\eta(\\theta + d\\theta)$를 최소화하는 $d\\theta$로 정의됩니다. 이 때, $\\vert d\\theta \\vert^2$가 일정 크기 이하인 것으로 제약조건을 주는데(held to small constant) Euclidian space에서는 $\\eta(\\theta)$가 steepest direction이지만 Riemannian space에서는 natural gradient가 steepest direction입니다.\n\n추가적으로 결국 우리가 원하는 건 ‘policy 최적화를 좀 더 스마트하게 해 보자!’입니다. Policy 최적화를 잘한다는 것은 Policy를 어느 방향으로 얼만큼 업데이트 하느냐에 따라 달라집니다.\n\n### 3.2.1 Natural gradient 증명\n\nRiemannian space에서 거리는 다음과 같이 정의됩니다. 여기서 $G(\\theta)$는 특정한 양수로 이루어진 matrix입니다. 자세한 내용은 [양의 정부호 행렬(positive definite matrix)이란?](http://bskyvision.com/205)을 참고해주시기 바랍니다.\n\n$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$\n\n이 수식은 [Natural gradient works in efficiently in learning](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&rep=rep1&type=pdf) 논문에서 증명되어 있습니다. 다음은 natural gradient 증명입니다.\n\nsteepest direction을 구할 때 $\\theta$의 크기를 제약조건으로 줍니다. 제약조건은 다음과 같습니다.\n\n$$\\vert d\\theta \\vert^2 = \\epsilon^2$$\n\n그리고 steepest vector인 $d\\theta$는 다음과 같이 정의할 수 있습니다.\n\n$$d\\theta = \\epsilon a$$\n\n$$\\vert a \\vert^2=a^TG(\\theta)a = 1$$\n\n이 때, $a$가 steepest direction unit vector이 되려면 다음 수식을 최소로 만들어야 합니다.\n\n$$\\eta(\\theta + d\\theta) = \\eta(\\theta) + \\epsilon\\nabla\\eta(\\theta)^Ta$$\n\n위 수식이 제약조건 아래 최소가 되는 $a$를 구하기 위해 Lagrangian method를 사용합니다. Lagrangian method를 모른다면 [위키피디아](https://en.wikipedia.org/wiki/Lagrange_multiplier)를 참고하는 것을 추천합니다. 위 수식이 최소라는 것은 $\\nabla\\eta(\\theta)^Ta$가 최소라는 것입니다.\n\n$$\\frac{\\partial}{\\partial a_i}(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$$\n\n따라서 $(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$는 상수입니다. 상수를 미분하면 0이므로 이 식을 $a$로 미분합니다. 그러면 다음과 같이 steepest direction을 구한 것입니다.\n\n$$\\nabla\\eta(\\theta) = 2 \\lambda G(\\theta)a$$\n\n$$a=\\frac{1}{2\\lambda}G^{-1}\\nabla\\eta(\\theta)$$\n\n이 때, 다음 식을 natural gradient라고 정의합니다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = G^{-1}\\nabla\\eta(\\theta)$$\n\nnatural gradient를 이용한 업데이트는 다음과 같습니다.\n\n$$\\theta_{t+1}=\\theta_t - \\alpha_tG^{-1}\\nabla\\eta(\\theta)$$\n\n여기까지는 natural gradient의 증명이었습니다. 이 natural gradient를 policy gradient에 적용한 것이 natural policy gradient입니다. natural policy gradient는 다음과 같이 정의됩니다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$\n\n여기서 $G$대신 $F$를 사용했는데 $F$는 Fisher information matix입니다. 수식은 다음과 같습니다.\n\n$$F(\\theta) = E_{\\rho^\\pi(s)}[F_s(\\theta)]$$\n\n$$F_s(\\theta)=E_{\\pi(a;s,\\theta)}[\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial \\theta_i}\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial\\theta_j}]$$\n\n### 3.2.2 Fisher Information Matrix에 정의된 Metric\n\n추가적으로 Fisher Information Matrix(FIM)에 대해서 설명하겠습니다. 위의 문제를 우리가 생각하기 쉬운 Neural Network(NN)으로 구성하고 해결할 수 있다고 해봅시다. NN은 여러가지 parameter set들로 구성될 수 있습니다. 게다가 다른 parameter set을 가지지만 같은 policy를 가질 수도 있습니다. 이 경우 steepest direction은 같은 policy이기 때문에 같은 방향을 가리키고 있어야 하는데 non-covariant한 경우 그렇지 못합니다. 떄문에 학습이 느려지고, 이러한 문제를 해결하기 위해 단순히 Positive-Definite Matrix $G(\\theta)$를 사용하지 않고 FIM $F_s(\\theta)$를 사용합니다. 어떠한 확률 변수 $X$가 임의의 매개변수 $\\theta$에 의해 정의되는 분포를 따른다고 하면 $X=x$일때 FIM은 다음과 같이 정의됩니다.\n\n$$F_x(\\theta)=E\\left[\\left(\\dfrac{\\partial}{\\partial\\theta}\\log\\Pr(x|\\theta)\\right)^2\\right]$$\n\n강화학습 관점에서 생각해보면 정보 $x$는 에피소드에 의해 관측된 상태값 $s$이며 매개변수 $\\theta$에 의해 선택될 수 있는 행동에 대한 분포가 나오게 됩니다. 이에 의해 위의 FIM는 다음과 같이 표현할 수 있습니다.\n\n$$F_s(\\theta) \\equiv E_{\\pi(a;s,\\theta)}\\left[\\left(\\dfrac{\\partial}{\\partial\\theta}\\log\\pi(a;s,\\theta)\\right)^2\\right] =E_{\\pi(a;s,\\theta)}\\left[\\dfrac{\\partial \\log\\pi(a;s,\\theta)}{\\partial \\theta_i}\\dfrac{\\partial \\log\\pi(a;s,\\theta)}{\\partial\\theta_j}\\right]$$\n\n그리고 위의 식들을 이용하여 objective function을 정리하면 아래의 식과 같이 표현됩니다.\n\n$$F(\\theta) = E_{\\rho^{\\pi}(s)}[F_s(\\theta)]$$\n\n또한 이 Fisher Information Matrix에 정의된 이 metric은 다음과 같은 성질을 가지고 있습니다.\n\n1. 업데이트되는 파라미터에 의해 구성되는 매니폴드에 기반한 metric입니다.\n2. 확률분포($\\pi(a;s,\\theta)$)를 구성하는 파라미터($\\theta$)의 변화에 독립적입니다.\n3. 마지막으로 positive-definite한 값을 가집니다. 이렇기 때문에 steepest gradient에서 objective function의 방향을 알기 위해 사용한 방법과 같은 방법으로 natural gradient direction을 다음과 같이 구할 수 있는 것입니다.\n\n$$\\bar{\\nabla}\\eta(\\theta) \\equiv F(\\theta)^{-1}\\nabla\\eta(\\theta)$$\n\n<br><br>\n\n# 4. The Natural Gradient and Policy Iteration\n\n4장에서는 Natural gradient를 통한 policy iteration을 수행하여 실제로 정책의 향상이 있는지를 증명합니다. 여기서 $Q^\\pi(s,a)$는 compatible function approximator $f^\\pi(s,a;w)$로 근사됩니다. ([Sutton PG](https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/)를 참고해주시기 바랍니다.)\n\n<br>\n## 4.1 Theorem 1: Compatible Function Approximation\n\napproximate하는 함수 $f^{\\pi}(s,a;w)$는 다음과 같습니다.(compatible value function)\n\n$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$\n\n$$\\psi^{\\pi}(s,a) = \\nabla log\\pi(a;s,\\theta)$$\n\n여기서 $[\\nabla \\log\\pi(a;s,\\theta)]_i=\\partial \\log\\pi(a;s,\\theta)/\\partial\\theta_i$입니다. $w$는 원래 approximate하는 함수 $Q$와 $f$의 차이를 줄이도록 학습합니다(mean square error). 수렴한 local minima의 $w$를 $\\bar{w}$라고 가정 하겠습니다. 에러는 다음과 같은 수식으로 나타낼 수 있습니다.\n\n$$\\epsilon(w,\\pi)\\equiv\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)(f^{\\pi}(s,a;w)-Q^{\\pi}(s,a))^2$$\n\n그 때, $\\bar{\\omega} = \\bar{\\nabla}\\eta(\\theta)$이면 function approximator의 gradient 방향과 정책의 gradient 방향이 같다는 것을 의미합니다.\n\n아래의 내용은 위의 Theorem에 대한 증명입니다.\n\n위의 수식이 local minima이면 미분값이 0이 됩니다. $w$에 대해서 미분하면 다음과 같습니다.\n\n$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)(\\psi^{\\pi}(s,a)^T\\bar{w}-Q^{\\pi}(s,a))=0$$\n\n$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)^T\\bar{w}=\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)Q^{\\pi}(s,a)$$\n\n이 때, 위 식의 우변은 $\\psi$의 정의에 의해 policy gradient가 됩니다. 또한 왼쪽 항에서는 Fisher information matrix가 나옵니다.\n\n$$F(\\theta)=\\sum_{s,a}\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)=E_{\\rho^\\pi(s)}[F_s(\\theta)]$$\n\n따라서 다음과 같이 쓸 수 있습니다.\n\n$$F(\\theta)\\bar{w}=\\nabla\\eta(\\theta)$$\n\n$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$\n\n위의 수식은 natural gradient식과 동일합니다. 위의 수식은 policy가 update 될 때, value function approximator의 parameter 방향으로 이동한다는 것을 의미합니다. function approximation이 정확하다면 그 parameter의 natural policy gradient와 inner product가 커야합니다.\n\n<br>\n## 4.2 Theorem 2: Greedy Policy Improvement\n\n이번 장에서는 natrual gradient는 다른 policy iteration 방법처럼 단순히 더 좋은 행동을 고르도록 학습하는 것이 아니라 가장 좋은(greedy) 행동을 고르도록 학습한다는 것을 증명하는 부분입니다. 이것을 일반적인 형태의 policy에 대해서 증명하기 전에 exponential 형태의 policy에 대해서 증명하는 것이 Theorem 2입니다. 특수한 정책을 가지는 상황안에서 학습속도($\\alpha$)를 무한대로 가져감으로서 어떤 action을 선택하는지를 알아봅니다.\n\npolicy를 다음과 같이 정의합니다.\n\n$$\\pi(a;s,\\theta) \\propto \\exp(\\theta^T\\phi_{sa})$$\n\n여기서 $\\bar{\\nabla}\\eta(\\theta)$가 0이 아니고 $\\bar{w}$는 approximation error를 최소화된 $w$라고 가정합니다. 이 상태에서 natural gradient update를 생각해봅시다. 그리고 policy gradient는 gradient ascent임을 기억합시다.\n\n$$\\theta_{t+1}=\\theta_t + \\alpha_t\\bar{\\nabla}\\eta(\\theta)$$\n\n이 때 $\\alpha$가 learning rate로 parameter를 얼마나 업데이트하는지를 결정합니다. 이 값을 무한대로 늘렸을 때 policy가 어떻게 업데이트되는지 생각해봅시다.\n\n$$\\pi_{\\infty}(a;s)=lim_{\\alpha\\rightarrow\\infty}\\pi(a;s,\\theta+\\alpha\\bar{\\nabla}\\eta(\\theta))-(1)$$\n\n이 때,\n\n$$\\pi_{\\infty}=0 \\, \\, \\, if \\, and \\, only \\, if(필요충분조건) \\, \\, \\, a \\notin argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n이라고 말할 수 있습니다.\n\n아래의 내용은 위의 Theorem에 대한 증명입니다.\n\n먼저 function approximator는 앞서 다뤘듯이 아래와 같습니다.\n\n$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$\n\n여기서 function approximator는 Theorem 1에 의해 아래와 같이 쓸 수 있습니다.\n\n$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T\\psi^{\\pi}(s,a)$$\n\n$\\theta$의 정의에 의해 $\\psi$는 다음과 같습니다.\n\n$$\\psi^{\\pi}(s,a)=\\phi_{sa}-E_{\\pi(a';s,\\theta)}[\\phi_{sa'}]$$\n\n따라서 function approximator는 다음과 같이 다시 쓸 수 있습니다.\n\n$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T(\\phi_{sa}-E_{\\pi(a';s,\\theta)}[\\phi_{sa'}])$$\n\ngreedy policy improvement가 Q function 값 중 가장 큰 값을 가지는 action을 선택하듯이 여기서도 function approximator의 값이 가장 큰 action을 선택하는 상황을 가정해봅시다. 이 때 function approximator의 argmax는 다음과 같이 쓸 수 있습니다.\n\n$$argmax_{a'}f^{\\pi}(s,a)=argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n(1) 식을 다시 살펴봅시다. 그러면 policy의 정의에 따라 다음과 같이 쓸 수 있습니다.\n\n$$\\pi(a;s,\\theta + \\alpha\\bar{\\nabla}\\eta(\\theta)) \\propto exp(\\theta^T\\phi_{sa} + \\alpha\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa})$$\n\n$\\bar{\\nabla}\\eta(\\theta) \\neq 0$이고 $\\alpha\\rightarrow\\infty$이면 exp안의 항 중에서 뒤의 항이 dominate하게 됩니다. 여러 행동 중에 $\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa}$가 가장 큰 행동이 있다면 이 행동의 policy probability가 1이 되고 나머지는 0이 됩니다. 따라서 다음이 성립합니다.\n\n$$\\pi_{\\infty}=0 \\, \\, \\, if \\, and \\, only \\, if \\, \\, \\, a \\notin argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n이 결과로부터 natural policy gradient는 단지 더 좋은 action이 아니라 best action을 고르도록 학습이 됩니다. 반면에 non-covariant gradient(1차미분)에서는 그저 더 좋은 action을 고르도록 학습이 됩니다. 이 natural policy gradient에 대한 결과는 infinite learning rate 세팅에서만 성립합니다. 좀 더 일반적인 경우에 대해서 살펴봅시다.\n\n<br>\n## 4.3 Theorem 3: General Parameterized Policy\n\nTheorem 2에서와는 달리 일반적인 policy를 가정해봅시다(general parameterized policy). Theorem 3는 이 상황에서 natural gradient를 통한 업데이트가 best action를 고르는 방향으로 학습이 된다는 것을 보여줍니다.\n\nnatural gradien에 따른 policy parameter의 업데이트는 다음과 같습니다. $\\bar{w}$는 approximation error를 minimize하는 $w$입니다.\n\n$$\\delta\\theta = \\theta' - \\theta = \\alpha\\bar{\\nabla}\\eta(\\theta)=\\alpha\\bar{w}$$\n\npolicy에 대해서 1차근사를 하면 다음과 같습니다.\n\n$$\\pi(a;s,\\theta')=\\pi(a;s,\\theta)+\\frac{\\partial\\pi(a;s,\\theta)^T}{\\partial\\theta}\\delta\\theta + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\psi(s,a)^T\\delta\\theta) + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\alpha\\psi(s,a)^T\\bar{w}) + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\alpha f^{\\pi}(s,a;\\bar{w})) + O(\\delta\\theta^2)$$\n\npolicy 자체가 function approximator의 크기대로 업데이트가 되므로 local하게 best action의 probability는 커지고 다른 probability의 크기는 작아질 것입니다. 하지만 만약 greedy improvement가 된다하더라도 그게 performance의 improvement를 보장하는 것은 아닙니다. 하지만 line search와 함께 사용할 경우 improvement를 보장할 수 있습니다. 왜 그런지는 처음에 말씀드린 블로그 [다크 프로그래머님의 블로그](http://darkpgmr.tistory.com/149)를 참고해주시기 바랍니다.\n\n<br><br>\n\n# 5. Metrics and Curvatures\n\n$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$\n\n이 파트에서는 FIM과 다른 metric 사이의 관계를 다룹니다.\n\n위의 식에 해당하는 G는 Fisher Information Matrix만 사용할 수 있는 것이 아닙니다. Positive-Definite Matrix인 FIM이외의 다른 Matrix도 사용할 수 있습니다. 또한 다양한 파라미터 추정에서 FIM은 Hessian Matrix에 수렴하지 않을 수 있다고 합니다. 이 말은 2nd order(2차 근사) 수렴이 보장되지 않는다는 말입니다.\n\n논문에 있는 내용들을 조금 더 추가적으로 보면 아래와 같이 나옵니다.\n\nIn the different setting of parameter estimation, the Fisher information converges to the ```Hessian```, so it is [asymptotically efficient](https://en.wikipedia.org/wiki/Efficiency_(statistics)) 이지만,\n\n이 논문의 경우, 아마리 논문의 'blind separation case'와 유사한데 이 때는 꼭 asymtotically efficient하지 않다고 말합니다. 이 말은 즉 2nd order 수렴이 보장되지 않는다는 것이다.\n\n[Mackay](http://www.inference.org.uk/mackay/ica.pdf) 논문에서는 Hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했습니다. 그래서 performance를 2번 미분해보면 아래의 수식과 같습니다. 하지만 다음 식에서는 모든 항이 data dependent합니다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있습니다.\n\n$$\\nabla^2\\eta(\\theta)=\\sum_{sa}\\rho^{\\pi}(s)(\\nabla^2\\pi(a;s)Q^{\\pi}(s,a)+\\nabla\\pi(a;s)\\nabla Q^{\\pi}(s,a)^T+\\nabla Q^{\\pi}(s,a)\\nabla\\pi(a;s)^T)$$\n\nhessian은 보통 positive definite가 아닐수도 있습니다. 따라서 local maxima가 될 때까지 Hessian이 사용하기 별로 안좋습니다. 그리고 local maxima에서는 Hessian보다는 Conjugate methods가 더 효율적이라고 합니다.\n\n사실 이 파트에서는 무엇을 말하고 있는지 알기가 어렵습니다. FIM과 Hessian이 관련이 있다는 것을 알겠는데 asymtotically efficient와 같은 내용을 모르므로 내용의 이해가 어려웠습니다.\n\nMackay 논문에서 해당 부분은 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/x4n6z6pdyi7xtb9/Screenshot%202018-06-10%2012.04.13.png?dl=1\"> </center>\n\n<br>\n## 5.1 Fisher Information Matrix(FIM) vs. Hessian\n\nFIM과 Hessian에 대해서 추가적인 설명을 하고자 합니다.\n\n- FIM\n\n일단 정의되려면 space 자체가 stochastic한 성질을 가지고 있어야 합니다. 모든 parameter를 표현하는 함수가 deterministic한 함수가 아니라 확률로 나타낸 분포입니다. (probability distribution)\n\n결국은 FIM에서도 hessian처럼 비슷한 과정을 취하고 싶은 것입니다. 따라서 확률 변수이기 때문에 어떠한 특정 sample을 취하면 그게 항상 다른값이 됩니다. 그래서 FIM에다가 expectation을 취한 것입니다. expectation을 취함으로써 hessian같은 성질을 가집니다. 왜냐하면 expectation을 취함으로써 constant한 값이 되기 때문입니다.\n\n- Hessian\n\n그냥 deterministic한 함수에서 정의됩니다. 그냥 함수를 두 번 미분 한 것이라고 보면 됩니다.\n\n<br>\n## 5.2 Conjugate Gradient Method\n\n추가적으로 Conjugate Gradient Method(CGM)에 대해서 다루고자 합니다.\n\n- 참고자료\n    - https://www.quora.com/What-is-an-intuitive-explanation-of-what-the-conjugate-gradient-method-is\n    - https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf\n    - https://en.wikipedia.org/wiki/Conjugate_gradient_method\n\n### $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해 구하기\n\n$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해를 구하는 문제를 생각해봅시다. $\\mathbf{A}$의 역행렬을 구해서 양변에 곱해주면 $\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$가 되어 쉽게 해를 구할 수 있습니다. 하지만 $\\mathbf{A}^{-1}$은 계산이 많이 필요 자원소모가 큰 연산입니다. 역행렬을 취하지 않고 해를 구할 수 있는 방법이 있을까요?\n\n위의 방정식에서 $\\mathbf{b}$를 이항시키면 $\\mathbf{A}\\mathbf{x} - \\mathbf{b} = 0$이 됩니다. 최적화문제는 많은 경우 1차 미분이 0이 되는 지점이 해일 확률이 높습니다. $\\mathbf{A}\\mathbf{x} - \\mathbf{b} = 0$을 1차 미분으로 가지는 함수는 무엇일까요?\n\n$$f( \\mathbf{x} ) = \\frac{1}{2}\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x} - \\mathbf{x}^\\mathrm{T}\\mathbf{b}$$\n\n위의 함수는 $\\mathbf{A}\\mathbf{x} - \\mathbf{b}$를 1차 미분값으로 가집니다. 이 때 $\\mathbf{A}$는 symmetric positive definite해야 합니다.\n\n- symmetric: $\\mathbf{A}=\\mathbf{A}^\\mathrm{T}$\n- positive definite: $\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}>0,\\quad\\forall\\mathbf{x}$ or all eigenvalues of $\\mathbf{A}$ are positive\n\nsymmetric positive definite한 성질은 $\\mathbf{x}$를 strict convex function이 되게 만들어서 unique solution이 존재하게 합니다. $\\mathbf{A}$는 $\\mathbf{x}$의 Hessian이기도 합니다.\n\n자, 우리는 위의 방정식의 해 $\\mathbf{x}^*$를 iterative한 방식으로 찾고자 합니다. $\\mathbf{x}_0$을 초기값이라고 합시다.\n\n우리는 <img src=\"https://www.dropbox.com/s/g91yqajf72jzjoc/Screen%20Shot%202018-08-14%20at%208.43.28%20AM.png?dl=1\" width=\"80\">가 되는 <img src=\"https://www.dropbox.com/s/aj3j5fjtiyewlo1/Screen%20Shot%202018-08-14%20at%208.43.37%20AM.png?dl=1\" width=\"30\">를 찾고 싶은데 이를 위한 현재의 추정값은 $\\mathbf{A} \\mathbf{x}_0$입니다. 최적의 값을 찾았을 때와 비교하면 현재의 오차는 $\\mathbf{b} - \\mathbf{A}\\mathbf{x}_0 = \\mathbf{r}_0$입니다. 이것을 residual이라고 합니다. 어떻게 효율적으로 값을 변화시켜서 오차를 0으로 만들 수 있을까요? 일단 매 iteration마다 residual이 작아지는 방향으로 나아가야겠죠?\n\n가장 널리 알려진 방법은 gradient descent 방향입니다. gradient가 증가하면 반대로 가고 gradient가 감소하면 그 방향으로 가는 것입니다. 그리고 어떤 방향으로든 gradient가 0이면 그 지점에 멈추는 것이죠. 이 방법은 수렴은 하지만 zigzag하게 움직여서 느립니다. 이것보다 더 좋은 방법이 없을까요?\n\n### Conjugate Gradient Method\n\n#### Gram-Schmidt orthgonalization\n\n다음과 같은 vector들의 집합 <img src=\"https://www.dropbox.com/s/csgdsfofe19q5r6/Screen%20Shot%202018-08-14%20at%208.56.04%20AM.png?dl=1\" width=\"135\">이 있다고 해봅시다. 이 vector들이 서로 linearly independent하다면 이 vector들은 $\\mathbb{R}^n$ 공간 상의 basis입니다. 이 vector들을 이용해서 orthogonal한 vector들을 만들어보겠습니다. 새로운 vector들을 <img src=\"https://www.dropbox.com/s/7xg7pciar1ndu44/Screen%20Shot%202018-08-14%20at%208.58.25%20AM.png?dl=1\" width=\"130\">라고 한다면 아래와 같은 과정을 통해 만들 수 있습니다. 이 방법을 Gram-Schmidt orthogonalization이라고 합니다.\n\n- $\\mathbf{d}_1=\\mathbf{v}_1$\n- $\\mathbf{d}_2=\\mathbf{v}_2 - \\left\\langle\\mathbf{v}_2,\\frac{\\mathbf{d}_1}{\\parallel\\mathbf{d}_1\\parallel}\\right\\rangle\\frac{\\mathbf{d}_1}{\\parallel\\mathbf{d}_1\\parallel}$\n...\n- <img src=\"https://www.dropbox.com/s/arzc5cez8az0j7h/Screen%20Shot%202018-08-14%20at%209.00.04%20AM.png?dl=1\" width=\"290\">\n\n간단히 설명을 하면, 첫 vector는 basis와 같은 방향으로 출발합니다. 그 다음 vector는 그 다음 basis를 이전 vector로 projection한 다음 해당 basis로부터 빼줍니다. 그 다음 vector를 만들 때는 이전에 만든 모든 vector들에 대해서 projection을 취한 다음 모두 더한 것을 해당 basis에서 빼줍니다. \n\n#### $A$-conjugate\n\n이 방법을 이용하여 matrix $A$에 관하여 orthogonal한 새로운 vector들의 집합을 만들어보겠습니다. 일반적인 inner product와 약간 다른 다음과 같은 inner product를 정의할 수 있습니다.\n\n$$<\\mathbf{x},\\mathbf{y}>_\\mathbf{A} = \\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{y}$$\n\n이 때 $<\\mathbf{x},\\mathbf{y}>_\\mathbf{A} = 0$이면 $\\mathbf{x}와 \\mathbf{y}$는 $\\mathbf{A}$-orthogonal 또는 $\\mathbf{A}$-conjugate하다라고 합니다. vector norm $\\parallel\\cdot\\parallel$도 일반적인 vector norm이 아닌 A-norm을 다음과 같이 정의합니다.\n\n$$\\parallel\\mathbf{x}\\parallel_\\mathbf{A}^2=\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}$$\n\nGram-Schmidt orthogonalization을 또 이용해볼까요? 아래와 같습니다.\n\n* $\\mathbf{d}_1=\\mathbf{v}_1$\n* <img src=\"https://www.dropbox.com/s/i8wnvoccefgg7t8/Screen%20Shot%202018-08-14%20at%209.08.42%20AM.png?dl=1\" width=\"400\">\n...\n* <img src=\"https://www.dropbox.com/s/045x2v5sx7s0wot/Screen%20Shot%202018-08-14%20at%209.09.03%20AM.png?dl=1\" width=\"500\">\n\n그런데 이 Gram-Schmidt 방법은 단점이 하나 있습니다. 새로운 vector를 만들어내기 위해서 이전에 만든 vector들을 모두 가지고 있어야 하고 projection도 다시 계산해야 한다는 점입니다. 그런데 마법같은 이유($\\approx$ 여기서 설명하지 않는 이유)로 인해서 오직 마지막으로 만들어낸 vector만 가지고 있어도 기존의 모든 vector들이 성질을 표현해낼 수 있습니다. 그러면 우리가 만들어낸 vector는 다음과 같이 간단해집니다.\n\n<center> <img src=\"https://www.dropbox.com/s/wlyu7ax6qt0aw9w/Screen%20Shot%202018-08-14%20at%209.09.43%20AM.png?dl=1\" width=\"550\"> </center>\n\n이 방향이 우리가 업데이트시키고 싶은 방향입니다. 즉 우리는 다음 수식처럼 움직이고자 합니다.\n\n<center> <img src=\"https://www.dropbox.com/s/053o4ocvxsgxfl9/Screen%20Shot%202018-08-14%20at%209.09.56%20AM.png?dl=1\" width=\"180\"> </center>\n\n$d_k$는 방향이고 $\\alpha_{k}$는 step size입니다. 이러한 방법을 일반적으로 line search method라고 부릅니다. 방향은 위에서 구한 conjugate 방향을 이용합니다. 그래서 이 line search를 conjugate gradient method라고 부릅니다. 이 방법의 한가지 중요한 장점은 업데이트가 무조건 n번만 일어난다는 것입니다! n-dimensional space에서는 basis가 n개 밖에 없거든요. 그렇다면 얼마만큼 많이 이 방향으로 움직여야 할까요? 더 이상 $f(\\mathbf{x})$가 감소하지 않을 때까지 움직이는게 좋지 않을까요? 이것은 다시 말하면 gradient가 0이 될 때까지 움직인다는 뜻입니다. 위에서 설명한 residual과도 관계있을 것 같지 않나요?\n \n$\\nabla f(\\mathbf{x})=\\mathbf{A}\\mathbf{x} - \\mathbf{b}$에 $x_k + \\alpha_{k} d_k$를 대입해 봅시다. 아래와 같이 step size를 구할 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/xikpfgcix64ngmp/Screen%20Shot%202018-08-14%20at%209.13.20%20AM.png?dl=1\" width=\"300\"> </center>\n\n네, 이제 우리는 방향과 step size를 모두 알았습니다. 이제 update만 하면 해를 찾을 수 있겠군요!\n\n<center> <img src=\"https://www.dropbox.com/s/avjw4kbzd4ormv5/conjugate_gradient_wikipedia.png?dl=1\" width=\"200\"> </center>\n\n위키피디아에서 가져온 위의 그림을 봅시다. 그림의 녹색선이 gradient descent이고 빨간선이 conjugate gradient입니다. conjugate gradient는 gradient descent보다 빨리 수렴합니다. 녹색선은 항상 이전 이동 방향과 직각으로 이동합니다. 빨간선은 보다 빠르게 더 낮은 값이 있는 곳으로 이동합니다.\n\n위에서 설명을 생략했지만 왜 conjugate gradient는 빠르게 수렴(n-step)하며 Gram-Schmidt orthogonalization을 할 때도 메모리가 적게 필요할까요? 수학적으로 엄밀하게 설명하기 보다는 개념적으로 설명을 하겠습니다. 그 비밀은 바로 $\\mathbf{A}$-orthogonal 또는 $\\mathbf{A}$-conjugate vector들을 이용한데 있습니다. 이 vector들은 basis라고 했습니다. 즉, 처음에 우리가 구하고자 했던 해 $\\mathbf{x}^*$를 이 vector들을 이용해서 표현할 수 있습니다. 또한 iterative하게 찾아가기 위한 초기 vector $\\mathbf{x}_0$도 이 vector들로 표현할 수 있습니다. 그렇다면 이 둘 간의 오차도 이 vector들도 표현할 수 있게 됩니다. 다음 수식처럼 말입니다.\n\n$$\\mathbf{x}^\\* - \\mathbf{x}_0 = \\epsilon_1\\mathbf{d}_1 + \\epsilon_2\\mathbf{d}_2 + \\cdots + \\epsilon_n\\mathbf{d}_n$$\n\n우리의 목표는 이 오차를 줄이는 것입니다. 각각의 basis마다 오차가 있으며 이들은 서로 independent합니다. 즉, 특정 iteration에서 특정 basis에 대한 오차를 0으로 만들면 다음 번 iteration에서는 이 오차는 영향을 받지 않습니다! 이러한 이유로 n번의 iteration만으로 해를 찾을 수 있고 다른 vector들을 저장할 필요도 없는 것입니다. 선형대수의 아름다움이 느껴지지 않으시나요? 오래되었지만 참 잘 디자인된 기법이라는 생각이 듭니다.\n\n<br><br>\n\n# 6. Experiment\n\n이 논문에서는 natural gradient를 simple MDP와 tetris MDP에 대해서 실험을 진행했습니다. FIM은 다음과 같은 식으로 업데이트합니다.\n\n$$f\\leftarrow f+\\nabla \\log \\pi(a_t; s_t, \\theta)\\nabla \\log \\pi(a_t; s_t, \\theta)^T$$\n\n$T$ length trajectory에 대해서 $f/T$를 이용해 $F$의 기대값($E$)를 구합니다.\n\n<br>\n## 6.1 LQR(Linear Quadratic Regulator)\n\nAgent를 실험할 환경은 다음과 같은 dynamics를 가지고 있습니다.\n\n$x(t+1) = 0.7x(t)+u(t)+\\epsilon(t)$\n\n$u(t)$는 control 신호로서 에이전트의 행동입니다. $\\epsilon$은 noise distribution으로 환경에 가해지는 노이즈입니다. 에이전트의 목표는 적절한 $u(t)$를 통해 $x(t)$를 0으로 유지하는 것입니다. $x(t)$를 0으로 유지하기 위해서 필요한 소모값(cost)는 $x(t)^2$로 정의하며 cost를 최소화하도록 학습합니다. 이 논문에서는 실험할 때 복잡성을 더 해주기 위해 noise distribution인 $\\epsilon$을 dynamics에 추가하였습니다.\n\n이 실험에서 policy는 다음과 같이 설정하였습니다. 파라미터가 2개 밖에 없는 간단한 policy입니다.\n\n$\\pi(u;x,\\theta)\\propto \\exp(\\theta_1s_1x^2+\\theta_2s_2x)$\n\n이 policy를 간단히 그래프로 그려보면 다음과 같습니다. 가로축은 $x$, 세로축은 cost입니다. $\\theta_1$과 $\\theta_2$를 (0.5, 0.5), (1, 0), (0, 1)로 설정하고 $s_1$, $s_2$는, 1로 두었습니다. $x$는 -1~1사이의 범위로 그렸습니다. \n\n<center> <img src='https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1' width='500px'> </center>\n\n아래의 그림은 LQR을 학습한 그래프입니다. cost가 $x^2$이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있습니다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과입니다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 곡선입니다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습합니다(time 축이 log scale인 것을 감안하면 차이가 많이 납니다.). \n\n하지만 문제가 있습니다. NPG를 학습한 세 개의 곡선은 $\\theta$를 rescale 한 것입니다. $\\theta$앞에 곱해지는 숫자에 따라 학습의 과정이 다릅니다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것입니다. 즉, covariant gradient가 아니라는 뜻입니다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1\" width=\"300px\"> </center>\n\nnatural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문입니다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 $\\rho_s$가 곱해지기 때문입니다. 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것입니다!\n\n<br>\n## 6.2 Simple 2-state MDP\n\n이제 다른 예제에서 NPG를 테스트합니다. 2개의 state만 가지는 MDP를 고려해봅시다. [그림출처](http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&context=robotics). 그림으로보면 다음과 같습니다. $x=0$ 상태와 $x=1$ 상태 두 개가 존재합니다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있습니다. 상태 $x=0$에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 $x=1$에서 자기 자신으로 돌아오면 2의 보상을 받습니다. 결국 optimal policy는 상태 $x=1$에서 계속 자기 자신으로 돌아오는 행동을 취하는 것입니다. \n\n<img src=\"https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1\">\n\n문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정합니다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것입니다.\n\n$$\\rho(x=0)=0.8,  \\rho(x=1)=0.2$$\n\n일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 됩니다. 이 때, $\\rho(s)$가 gradient에 곱해지므로 상대적으로 상태 0에서의 gradient 값이 커지게 됩니다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update합니다. 따라서 아래 그림의 첫번째 그림에서처럼 reward가 1에서 오랜 시간동안 머무릅니다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것입니다.\n\n$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$\n\n<center><img src=\"https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1\" width=\"300px\"></center>\n\n하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달합니다. gradient 방법이 $1.7\\times 10^7$정도의 시간만에 2에 도달한 반면 NPG는 2의 시간만에 도달합니다. 또한 $\\rho(x=0)$가 $10^{-5}$이하로 떨어지지 않습니다.\n\n한 가지 그래프를 더 살펴봅시다. 다음 그래프는 parameter $\\theta$가 업데이트 되는 과정을 보여줍니다. 이 그래프에서는 parameter가 2개 있습니다. 일반적인 gradient가 아래 그래프에서 실선에 해당합니다. 이 실선의 그래프는 보면 처음부터 중반까지 $\\theta_i$만 거의 업데이트하는 것을 볼 수 있습니다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있습니다. \n\n<center><img src=\"https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1\" width=\"300px\"></center>\n\npolicy가 $\\pi(a;s,\\theta)\\propto \\exp(\\theta_{sa})$일 때, 다음과 같이 $F_{-1}$이 gradient 앞에 weight로 곱해지는데 이게 $\\rho$와는 달리 각 parameter에 대해 균등합니다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것입니다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$\n\n<br>\n## 6.3 Tetris\n\nNPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어 있습니다. 다음 그림은 tetris 예제를 보여줍니다. 보통 그림에서와 같이 state의 feature를 정해줍니다. [그림 출처](http://slideplayer.com/slide/5215520/)\n\n<img src=\"https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1\">\n\n이 예제에서도 exponential family로 policy를 표현합니다. $\\pi(a;s,\\theta) \\propto \\exp(\\theta^T\\phi_{sa})$ 로 표현합니다.\n\ntetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있습니다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우입니다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법입니다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷합니다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지합니다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있습니다.\n\n<img src=\"https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1\">\n\n<br><br>\n\n# 7. Discussion\n\nNatural Gradient Method는  Policy Iteration에서와 같이 Greedy Action을 선택하도록 학습합니다. Line search 기법과 함께 사용하면 더 Policy Iteration과 비슷해집니다. Greedy Policy Iteration에서와 비교하면 Performance Improvement가 보장됩니다. 하지만 FIM이 asymtotically Hessian으로 수렴하지 않습니다. 그렇기 때문에 Conjugate Gradient Method로 구하는 방법이 더 좋을수 있습니다.\n\n살펴봤듯이 본 논문의 NPG는 완벽하지 않습니다. 위의 몇가지 문제점을 극복하기 위한 추가적인 연구가 필요하다고 할 수 있습니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [DDPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/)\n\n<br>\n\n# 다음으로\n\n## [NPG Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py)\n\n## [TRPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/)","source":"_posts/4_npg.md","raw":"---\ntitle: Natural Policy Gradient\ndate: 2018-06-25 11:36:45\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 김동민, 이동민, 이웅원, 차금강\nsubtitle: 피지여행 4번째 논문\n---\n\n<center> <img src=\"https://www.dropbox.com/s/yd0x14ljrhpnj1b/Screen%20Shot%202018-07-18%20at%201.08.05%20AM.png?dl=1\" width=\"600\"> </center>\n\n논문 저자 : Sham Kakade\n논문 링크 : https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf\nProceeding : Advances in Neural Information Processing Systems (NIPS) 2002\n정리 : 김동민, 이동민, 이웅원, 차금강\n\n---\n\n# 1. 들어가며...\n\n이 논문이 발표된 2002년 당시에도 많은 연구자들이 objective function의 gradient 값을 따라서 좋은 policy $\\pi$를 찾고자 하였습니다. 하지만 기존의 우리가 알던 gradient descent method는 steepest descent direction이 아닐 수 있기 때문에(쉽게 말해 가장 가파른 방향을 따라서 내려가야 하는데 그러지 못할 수도 있다는 것입니다.) 이 논문에서 steepest descent direction를 나타내는 natural gradient method를 policy gradient에 적용하여 좋은 policy $\\pi$를 찾습니다. \n\n<br>\n## 1.1 NPG 흐름 잡기\n\n### 1.1.1 매니폴드(manifold)\n\n<center> <img src=\"https://www.dropbox.com/s/cstjgemqpby4ysr/Screen%20Shot%202018-08-12%20at%208.28.29%20PM.png?dl=1\" width=\"300\"> </center>\n\n매니폴드는 간단하게 말해 위의 그림에서의 점들을 아우르는 subspace입니다. 그래서 NPG 공부하기 전에 매니폴드를 왜 배울까라는 의문이 들으실텐데 그 이유는 위에서도 언급했듯이 natural gradient method는 어떠한 파라미터 공간에서의 steepest descent direction을 강조하기 때문입니다. 여기서 파라미터 공간이 바로 리만 매니폴드입니다. 리만 매니폴드는 매니폴드가 각지지 않고 미분가능하게 부드럽게 곡률을 가진 면이라고 생각하면 편합니다.\n\nNeural Network(NN)을 사용할 경우 NN의 parameter space가 우리가 보통 생각하는 직선으로 쭉쭉 뻗어있는 유클리디안 공간(Euclidean space)가 아닙니다. 좀 더 일반적으로는 구의 표면과 같이 휘어져있는 공간 즉, 리만 공간(Riemannian space)로 표현할 수 있습니다. 다음 그림들을 보겠습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/jnlm6he18ar64yc/Screen%20Shot%202018-08-12%20at%208.14.13%20PM.png?dl=1\" width=\"400\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/tufjqmaeaz29kaz/Screen%20Shot%202018-08-12%20at%208.15.53%20PM.png?dl=1\" width=\"400\"> </center>\n\n아래의 그림처럼 어떠한 확률 분포가 있다고 해봅시다.\n\n<center> <img src=\"https://www.dropbox.com/s/mpoop19eyu1vp0a/Screen%20Shot%202018-08-13%20at%2012.52.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n어떠한 공간의 확률 분포에 있는 한 점은 다른 공간의 확률 분포에서의 한 점이 될 것입니다. 이렇게 위의 그림들과 같이 유클리디안 공간의 확률 분포에서의 한 점이 리만 공간의 확률 분포의 한 점이 되는 것이고, 곡률의 일차 근사가 유클리디안 공간에서는 일차 근사가 아닌 이차 근사이고, 리만 공간에서는 휘어진 공간이기 때문에 곡률을 일차 근사라고 보는 것입니다. 추가적으로 일차 근사와 이차 근사의 차이는 [다크 프로그래머님의 블로그](http://darkpgmr.tistory.com/149)를 참고해주시기 바랍니다. (꼭 보세요!) \n\n일차 근사와 이차 근사의 차이점과 각각의 장단점, 그리고 추가적으로 Line Search까지 알고 난 후에 이 논문을 보시는 것을 권장해드립니다.\n\n### 1.1.2 Natural Gradient + Policy Gradient\n\n먼저 아래의 그림들을 보여드리겠습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/65hra43zadsubff/Screen%20Shot%202018-08-13%20at%2012.51.45%20PM.png?dl=1\" width=\"500\"> <img src=\"https://www.dropbox.com/s/8g332zceeordqwv/Screen%20Shot%202018-08-13%20at%2012.51.51%20PM.png?dl=1\" width=\"400\"> </center>\n\n위에서도 언급했듯이 natural gradient가 steepest direction이 된다는 연구가 이뤄지고 있었습니다. 강화학습의 policy gradient은 objective function의 gradient를 따라 policy를 업데이트를 합니다. 이 때 policy는 parameterization되는데 이 경우에도 gradient 대신에 natural gradient가 좋다는 것을 실험해보는 논문이 지금 다루고 있는 논문입니다.\n\ngradient가 non-covariant(1차 근사)해서 생기는 문제는 간단히 말하자면 다음과 같습니다. policy가 parameterized된 상황에서는 같은 policy라도 다른 parameter를 가질 수 있습니다. 이 때, steepest direction은 두 경우에 같은 방향을 가리켜야 하는데 non-covariant한 경우 그렇지 못합니다. 이러한 부분이 바로 결국 느린 학습으로 연결이 되는 것입니다.\n\n논문에서 2차미분 방법론들과 짧게 비교합니다. 하지만 개인적인 의견으로 2차미분을 이용한 다른 방법들과의 비교가 생각보다 없는 점이 부족해 보였습니다.(Hessian을 이용한다거나 conjugate gradient method를 이용한다거나). 실험을 통해 Fisher Information Matrix(FIM)가 hessian에 수렴안하는 거라던지 Hessian 방법론이 local maxima 부근에서 상당히 느리다던지의 결과를 보여줬었으면 좋았을 것 같습니다.\n\n또한 natural gradient의 단점으로 natural gradient 만으로 업데이트하면 policy의 improvement 보장이 안될 수 있습니다. policy의 improvement를 보장하기 위해 line search도 써야하는데 line search를 어떻게 쓰는지에 대한 자세한 언급이 없습니다. 다시 말해 자세한 algorithm 설명이 없다는 것입니다.\n\nnatural policy gradient 논문은 natural gradient + policy gradient를 처음 적용했다는데 의의가 있습니다. 하지만 이 논문이 문제 삼은 gradient는 non-covariant하다라는 문제를 natural gradient를 통해 해결하지는 못했습니다(Experiment를 통해 covariant gradient가 되지 못했다는 것이 보입니다). NPG의 뒤를 잇는 논문이 “covariant policy search”와 “natural actor-critic”에서 covariant(2차 근사)하지 못하다는 것을 해결하기 위해 Fisher Information Matrix를 sample 하나 하나에 대해서 구하는 것이 아니라 trajectory 전체에 대해서 구합니다.\n\n또한 논문은 pg의 두 가지 세팅 중에 average-reward setting(infinite horizon)에서만 NPG를 다룹니다. “covariant policy search” 논문에서는 average-reward setting과 start-state setting 모두에 대해서 npg를 적용합니다.\n\nnatural gradient + policy gradient를 처음 제시했다는 것은 좋지만 npg 학습의 과정을 자세하게 설명하지 않았고 다른 2차 미분 방법들과 비교를 많이 하지 않은 점이 아쉬운 논문이었습니다.\n\n<br><br>\n\n# 2. Introduction\n\n소개는 앞에서 다 했기 때문에 간략하게 다시 한 번 정리하겠습니다. direct policy gradient method는 future reward의 gradient를 따라 policy를 update합니다. 하지만 gradient descent는 non-covariant입니다. 따라서 이 논문에서는 covarient gradient를 제시합니다. 바로 \"Natural Gradient\" 입니다. \n\n또한 natural gradient와 policy iteration의 연관성을 설명합니다. natural policy gradient is moving toward choosing a greedy optimal action (이런 연결점은 아마도 step-size를 덜 신경쓰고 싶어서 그런게 아닌가 싶습니다)\n\n논문의 Introduction 부분에 다음 멘트가 있습니다. 이 글만 봐서는 이해가 안갔는데 Mackay 논문에 좀 더 자세히 나와있었습니다.\n<center> <img src=\"https://www.dropbox.com/s/41xhhr7lgfk24a1/Screenshot%202018-06-10%2011.45.18.png?dl=1\"> </center>\n\n[Mackay](http://www.inference.org.uk/mackay/ica.pdf)논문에서는 다음과 같이 언급하고 있습니다. Back-propagation을 사용할 경우에 learning rate를 dimension에 1/n로 사용하면 수렴한다는 것이 증명됐습니다. 하지만 너무 느리다고 합니다.\n<center> <img src=\"https://www.dropbox.com/s/us9ezc7vxgrkez6/Screenshot%202018-06-10%2011.47.21.png?dl=1\"> </center>\n\n<br><br>\n\n# 3. A Natural Gradient\n\n<br>\n## 3.1 Notation\n\n이 논문에서 제시하는 Notation은 다음과 같습니다.\n\n- MDP : tuple $(S, s_0, A, R, P)$\n- $S$ : a finite set of states\n- $s_0$ : a start state\n- $A$ : a finite set of actions\n- $R$ : reward function $R: S \\times A -> [0, R_{max}]$\n- $\\pi(a;s, \\theta)$ : stochastic policy parameterized by $\\theta$\n- 모든 정책 $\\pi$는 ergodic : stationary distribution $\\rho^{\\pi}$이 잘 정의되어 있다고 봅니다.\n- 이 논문에서는 sutton의 pg 논문의 두 세팅(start-state formulation, average-reward formulation) 중에 두 번째인 average-reward formulation을 가정합니다.\n- performance or average reward : $\\eta(\\pi)=\\sum_{s,a}\\rho^{\\pi}(s)\\pi(a;s)R(s,a)$\n- state-action value : $Q^{\\pi}(s,a)=E_{\\pi}[\\sum_{t=0}^{\\infty}R(s_t, a_t)-\\eta(\\pi)\\vert s_0=s, a_0=a]$ ([Sutton PG](https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/) 를 참고해주시기 바랍니다.)\n- 정책이 $\\theta$로 parameterize되어있으므로 performance는 $\\eta(\\pi_{\\theta})$인데 $\\eta(\\theta)$로 씁니다.\n\n<br>\n## 3.2 Natural Gradient\n\nSutton PG 논문의 policy gradient theorem에 따라 exact gradient of the average reward는 다음과 같습니다. 다음 수식이 어떻게 유도되었는지, 어떤 의미인지 모른다면 [Sutton PG](https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/)을 통해 제대로 이해하는 것이 좋습니다. Euclidean space에서는 objective function의 일반적인 policy gradient는 Sutton PG에서 논의한 바와 같이 다음과 같이 구할 수 있습니다.\n\n$$\\nabla\\eta(\\pi_\\theta) = \\Sigma_{s,a}\\rho^\\pi(s)\\nabla\\pi(a;s,\\theta)Q^\\pi(s,a)$$\n\n여기서 steepest descent direction of $\\eta(\\theta)$는 $\\eta(\\theta + d\\theta)$를 최소화하는 $d\\theta$로 정의됩니다. 이 때, $\\vert d\\theta \\vert^2$가 일정 크기 이하인 것으로 제약조건을 주는데(held to small constant) Euclidian space에서는 $\\eta(\\theta)$가 steepest direction이지만 Riemannian space에서는 natural gradient가 steepest direction입니다.\n\n추가적으로 결국 우리가 원하는 건 ‘policy 최적화를 좀 더 스마트하게 해 보자!’입니다. Policy 최적화를 잘한다는 것은 Policy를 어느 방향으로 얼만큼 업데이트 하느냐에 따라 달라집니다.\n\n### 3.2.1 Natural gradient 증명\n\nRiemannian space에서 거리는 다음과 같이 정의됩니다. 여기서 $G(\\theta)$는 특정한 양수로 이루어진 matrix입니다. 자세한 내용은 [양의 정부호 행렬(positive definite matrix)이란?](http://bskyvision.com/205)을 참고해주시기 바랍니다.\n\n$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$\n\n이 수식은 [Natural gradient works in efficiently in learning](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&rep=rep1&type=pdf) 논문에서 증명되어 있습니다. 다음은 natural gradient 증명입니다.\n\nsteepest direction을 구할 때 $\\theta$의 크기를 제약조건으로 줍니다. 제약조건은 다음과 같습니다.\n\n$$\\vert d\\theta \\vert^2 = \\epsilon^2$$\n\n그리고 steepest vector인 $d\\theta$는 다음과 같이 정의할 수 있습니다.\n\n$$d\\theta = \\epsilon a$$\n\n$$\\vert a \\vert^2=a^TG(\\theta)a = 1$$\n\n이 때, $a$가 steepest direction unit vector이 되려면 다음 수식을 최소로 만들어야 합니다.\n\n$$\\eta(\\theta + d\\theta) = \\eta(\\theta) + \\epsilon\\nabla\\eta(\\theta)^Ta$$\n\n위 수식이 제약조건 아래 최소가 되는 $a$를 구하기 위해 Lagrangian method를 사용합니다. Lagrangian method를 모른다면 [위키피디아](https://en.wikipedia.org/wiki/Lagrange_multiplier)를 참고하는 것을 추천합니다. 위 수식이 최소라는 것은 $\\nabla\\eta(\\theta)^Ta$가 최소라는 것입니다.\n\n$$\\frac{\\partial}{\\partial a_i}(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$$\n\n따라서 $(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$는 상수입니다. 상수를 미분하면 0이므로 이 식을 $a$로 미분합니다. 그러면 다음과 같이 steepest direction을 구한 것입니다.\n\n$$\\nabla\\eta(\\theta) = 2 \\lambda G(\\theta)a$$\n\n$$a=\\frac{1}{2\\lambda}G^{-1}\\nabla\\eta(\\theta)$$\n\n이 때, 다음 식을 natural gradient라고 정의합니다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = G^{-1}\\nabla\\eta(\\theta)$$\n\nnatural gradient를 이용한 업데이트는 다음과 같습니다.\n\n$$\\theta_{t+1}=\\theta_t - \\alpha_tG^{-1}\\nabla\\eta(\\theta)$$\n\n여기까지는 natural gradient의 증명이었습니다. 이 natural gradient를 policy gradient에 적용한 것이 natural policy gradient입니다. natural policy gradient는 다음과 같이 정의됩니다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$\n\n여기서 $G$대신 $F$를 사용했는데 $F$는 Fisher information matix입니다. 수식은 다음과 같습니다.\n\n$$F(\\theta) = E_{\\rho^\\pi(s)}[F_s(\\theta)]$$\n\n$$F_s(\\theta)=E_{\\pi(a;s,\\theta)}[\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial \\theta_i}\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial\\theta_j}]$$\n\n### 3.2.2 Fisher Information Matrix에 정의된 Metric\n\n추가적으로 Fisher Information Matrix(FIM)에 대해서 설명하겠습니다. 위의 문제를 우리가 생각하기 쉬운 Neural Network(NN)으로 구성하고 해결할 수 있다고 해봅시다. NN은 여러가지 parameter set들로 구성될 수 있습니다. 게다가 다른 parameter set을 가지지만 같은 policy를 가질 수도 있습니다. 이 경우 steepest direction은 같은 policy이기 때문에 같은 방향을 가리키고 있어야 하는데 non-covariant한 경우 그렇지 못합니다. 떄문에 학습이 느려지고, 이러한 문제를 해결하기 위해 단순히 Positive-Definite Matrix $G(\\theta)$를 사용하지 않고 FIM $F_s(\\theta)$를 사용합니다. 어떠한 확률 변수 $X$가 임의의 매개변수 $\\theta$에 의해 정의되는 분포를 따른다고 하면 $X=x$일때 FIM은 다음과 같이 정의됩니다.\n\n$$F_x(\\theta)=E\\left[\\left(\\dfrac{\\partial}{\\partial\\theta}\\log\\Pr(x|\\theta)\\right)^2\\right]$$\n\n강화학습 관점에서 생각해보면 정보 $x$는 에피소드에 의해 관측된 상태값 $s$이며 매개변수 $\\theta$에 의해 선택될 수 있는 행동에 대한 분포가 나오게 됩니다. 이에 의해 위의 FIM는 다음과 같이 표현할 수 있습니다.\n\n$$F_s(\\theta) \\equiv E_{\\pi(a;s,\\theta)}\\left[\\left(\\dfrac{\\partial}{\\partial\\theta}\\log\\pi(a;s,\\theta)\\right)^2\\right] =E_{\\pi(a;s,\\theta)}\\left[\\dfrac{\\partial \\log\\pi(a;s,\\theta)}{\\partial \\theta_i}\\dfrac{\\partial \\log\\pi(a;s,\\theta)}{\\partial\\theta_j}\\right]$$\n\n그리고 위의 식들을 이용하여 objective function을 정리하면 아래의 식과 같이 표현됩니다.\n\n$$F(\\theta) = E_{\\rho^{\\pi}(s)}[F_s(\\theta)]$$\n\n또한 이 Fisher Information Matrix에 정의된 이 metric은 다음과 같은 성질을 가지고 있습니다.\n\n1. 업데이트되는 파라미터에 의해 구성되는 매니폴드에 기반한 metric입니다.\n2. 확률분포($\\pi(a;s,\\theta)$)를 구성하는 파라미터($\\theta$)의 변화에 독립적입니다.\n3. 마지막으로 positive-definite한 값을 가집니다. 이렇기 때문에 steepest gradient에서 objective function의 방향을 알기 위해 사용한 방법과 같은 방법으로 natural gradient direction을 다음과 같이 구할 수 있는 것입니다.\n\n$$\\bar{\\nabla}\\eta(\\theta) \\equiv F(\\theta)^{-1}\\nabla\\eta(\\theta)$$\n\n<br><br>\n\n# 4. The Natural Gradient and Policy Iteration\n\n4장에서는 Natural gradient를 통한 policy iteration을 수행하여 실제로 정책의 향상이 있는지를 증명합니다. 여기서 $Q^\\pi(s,a)$는 compatible function approximator $f^\\pi(s,a;w)$로 근사됩니다. ([Sutton PG](https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/)를 참고해주시기 바랍니다.)\n\n<br>\n## 4.1 Theorem 1: Compatible Function Approximation\n\napproximate하는 함수 $f^{\\pi}(s,a;w)$는 다음과 같습니다.(compatible value function)\n\n$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$\n\n$$\\psi^{\\pi}(s,a) = \\nabla log\\pi(a;s,\\theta)$$\n\n여기서 $[\\nabla \\log\\pi(a;s,\\theta)]_i=\\partial \\log\\pi(a;s,\\theta)/\\partial\\theta_i$입니다. $w$는 원래 approximate하는 함수 $Q$와 $f$의 차이를 줄이도록 학습합니다(mean square error). 수렴한 local minima의 $w$를 $\\bar{w}$라고 가정 하겠습니다. 에러는 다음과 같은 수식으로 나타낼 수 있습니다.\n\n$$\\epsilon(w,\\pi)\\equiv\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)(f^{\\pi}(s,a;w)-Q^{\\pi}(s,a))^2$$\n\n그 때, $\\bar{\\omega} = \\bar{\\nabla}\\eta(\\theta)$이면 function approximator의 gradient 방향과 정책의 gradient 방향이 같다는 것을 의미합니다.\n\n아래의 내용은 위의 Theorem에 대한 증명입니다.\n\n위의 수식이 local minima이면 미분값이 0이 됩니다. $w$에 대해서 미분하면 다음과 같습니다.\n\n$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)(\\psi^{\\pi}(s,a)^T\\bar{w}-Q^{\\pi}(s,a))=0$$\n\n$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)^T\\bar{w}=\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)Q^{\\pi}(s,a)$$\n\n이 때, 위 식의 우변은 $\\psi$의 정의에 의해 policy gradient가 됩니다. 또한 왼쪽 항에서는 Fisher information matrix가 나옵니다.\n\n$$F(\\theta)=\\sum_{s,a}\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)=E_{\\rho^\\pi(s)}[F_s(\\theta)]$$\n\n따라서 다음과 같이 쓸 수 있습니다.\n\n$$F(\\theta)\\bar{w}=\\nabla\\eta(\\theta)$$\n\n$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$\n\n위의 수식은 natural gradient식과 동일합니다. 위의 수식은 policy가 update 될 때, value function approximator의 parameter 방향으로 이동한다는 것을 의미합니다. function approximation이 정확하다면 그 parameter의 natural policy gradient와 inner product가 커야합니다.\n\n<br>\n## 4.2 Theorem 2: Greedy Policy Improvement\n\n이번 장에서는 natrual gradient는 다른 policy iteration 방법처럼 단순히 더 좋은 행동을 고르도록 학습하는 것이 아니라 가장 좋은(greedy) 행동을 고르도록 학습한다는 것을 증명하는 부분입니다. 이것을 일반적인 형태의 policy에 대해서 증명하기 전에 exponential 형태의 policy에 대해서 증명하는 것이 Theorem 2입니다. 특수한 정책을 가지는 상황안에서 학습속도($\\alpha$)를 무한대로 가져감으로서 어떤 action을 선택하는지를 알아봅니다.\n\npolicy를 다음과 같이 정의합니다.\n\n$$\\pi(a;s,\\theta) \\propto \\exp(\\theta^T\\phi_{sa})$$\n\n여기서 $\\bar{\\nabla}\\eta(\\theta)$가 0이 아니고 $\\bar{w}$는 approximation error를 최소화된 $w$라고 가정합니다. 이 상태에서 natural gradient update를 생각해봅시다. 그리고 policy gradient는 gradient ascent임을 기억합시다.\n\n$$\\theta_{t+1}=\\theta_t + \\alpha_t\\bar{\\nabla}\\eta(\\theta)$$\n\n이 때 $\\alpha$가 learning rate로 parameter를 얼마나 업데이트하는지를 결정합니다. 이 값을 무한대로 늘렸을 때 policy가 어떻게 업데이트되는지 생각해봅시다.\n\n$$\\pi_{\\infty}(a;s)=lim_{\\alpha\\rightarrow\\infty}\\pi(a;s,\\theta+\\alpha\\bar{\\nabla}\\eta(\\theta))-(1)$$\n\n이 때,\n\n$$\\pi_{\\infty}=0 \\, \\, \\, if \\, and \\, only \\, if(필요충분조건) \\, \\, \\, a \\notin argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n이라고 말할 수 있습니다.\n\n아래의 내용은 위의 Theorem에 대한 증명입니다.\n\n먼저 function approximator는 앞서 다뤘듯이 아래와 같습니다.\n\n$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$\n\n여기서 function approximator는 Theorem 1에 의해 아래와 같이 쓸 수 있습니다.\n\n$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T\\psi^{\\pi}(s,a)$$\n\n$\\theta$의 정의에 의해 $\\psi$는 다음과 같습니다.\n\n$$\\psi^{\\pi}(s,a)=\\phi_{sa}-E_{\\pi(a';s,\\theta)}[\\phi_{sa'}]$$\n\n따라서 function approximator는 다음과 같이 다시 쓸 수 있습니다.\n\n$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T(\\phi_{sa}-E_{\\pi(a';s,\\theta)}[\\phi_{sa'}])$$\n\ngreedy policy improvement가 Q function 값 중 가장 큰 값을 가지는 action을 선택하듯이 여기서도 function approximator의 값이 가장 큰 action을 선택하는 상황을 가정해봅시다. 이 때 function approximator의 argmax는 다음과 같이 쓸 수 있습니다.\n\n$$argmax_{a'}f^{\\pi}(s,a)=argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n(1) 식을 다시 살펴봅시다. 그러면 policy의 정의에 따라 다음과 같이 쓸 수 있습니다.\n\n$$\\pi(a;s,\\theta + \\alpha\\bar{\\nabla}\\eta(\\theta)) \\propto exp(\\theta^T\\phi_{sa} + \\alpha\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa})$$\n\n$\\bar{\\nabla}\\eta(\\theta) \\neq 0$이고 $\\alpha\\rightarrow\\infty$이면 exp안의 항 중에서 뒤의 항이 dominate하게 됩니다. 여러 행동 중에 $\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa}$가 가장 큰 행동이 있다면 이 행동의 policy probability가 1이 되고 나머지는 0이 됩니다. 따라서 다음이 성립합니다.\n\n$$\\pi_{\\infty}=0 \\, \\, \\, if \\, and \\, only \\, if \\, \\, \\, a \\notin argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n이 결과로부터 natural policy gradient는 단지 더 좋은 action이 아니라 best action을 고르도록 학습이 됩니다. 반면에 non-covariant gradient(1차미분)에서는 그저 더 좋은 action을 고르도록 학습이 됩니다. 이 natural policy gradient에 대한 결과는 infinite learning rate 세팅에서만 성립합니다. 좀 더 일반적인 경우에 대해서 살펴봅시다.\n\n<br>\n## 4.3 Theorem 3: General Parameterized Policy\n\nTheorem 2에서와는 달리 일반적인 policy를 가정해봅시다(general parameterized policy). Theorem 3는 이 상황에서 natural gradient를 통한 업데이트가 best action를 고르는 방향으로 학습이 된다는 것을 보여줍니다.\n\nnatural gradien에 따른 policy parameter의 업데이트는 다음과 같습니다. $\\bar{w}$는 approximation error를 minimize하는 $w$입니다.\n\n$$\\delta\\theta = \\theta' - \\theta = \\alpha\\bar{\\nabla}\\eta(\\theta)=\\alpha\\bar{w}$$\n\npolicy에 대해서 1차근사를 하면 다음과 같습니다.\n\n$$\\pi(a;s,\\theta')=\\pi(a;s,\\theta)+\\frac{\\partial\\pi(a;s,\\theta)^T}{\\partial\\theta}\\delta\\theta + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\psi(s,a)^T\\delta\\theta) + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\alpha\\psi(s,a)^T\\bar{w}) + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\alpha f^{\\pi}(s,a;\\bar{w})) + O(\\delta\\theta^2)$$\n\npolicy 자체가 function approximator의 크기대로 업데이트가 되므로 local하게 best action의 probability는 커지고 다른 probability의 크기는 작아질 것입니다. 하지만 만약 greedy improvement가 된다하더라도 그게 performance의 improvement를 보장하는 것은 아닙니다. 하지만 line search와 함께 사용할 경우 improvement를 보장할 수 있습니다. 왜 그런지는 처음에 말씀드린 블로그 [다크 프로그래머님의 블로그](http://darkpgmr.tistory.com/149)를 참고해주시기 바랍니다.\n\n<br><br>\n\n# 5. Metrics and Curvatures\n\n$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$\n\n이 파트에서는 FIM과 다른 metric 사이의 관계를 다룹니다.\n\n위의 식에 해당하는 G는 Fisher Information Matrix만 사용할 수 있는 것이 아닙니다. Positive-Definite Matrix인 FIM이외의 다른 Matrix도 사용할 수 있습니다. 또한 다양한 파라미터 추정에서 FIM은 Hessian Matrix에 수렴하지 않을 수 있다고 합니다. 이 말은 2nd order(2차 근사) 수렴이 보장되지 않는다는 말입니다.\n\n논문에 있는 내용들을 조금 더 추가적으로 보면 아래와 같이 나옵니다.\n\nIn the different setting of parameter estimation, the Fisher information converges to the ```Hessian```, so it is [asymptotically efficient](https://en.wikipedia.org/wiki/Efficiency_(statistics)) 이지만,\n\n이 논문의 경우, 아마리 논문의 'blind separation case'와 유사한데 이 때는 꼭 asymtotically efficient하지 않다고 말합니다. 이 말은 즉 2nd order 수렴이 보장되지 않는다는 것이다.\n\n[Mackay](http://www.inference.org.uk/mackay/ica.pdf) 논문에서는 Hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했습니다. 그래서 performance를 2번 미분해보면 아래의 수식과 같습니다. 하지만 다음 식에서는 모든 항이 data dependent합니다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있습니다.\n\n$$\\nabla^2\\eta(\\theta)=\\sum_{sa}\\rho^{\\pi}(s)(\\nabla^2\\pi(a;s)Q^{\\pi}(s,a)+\\nabla\\pi(a;s)\\nabla Q^{\\pi}(s,a)^T+\\nabla Q^{\\pi}(s,a)\\nabla\\pi(a;s)^T)$$\n\nhessian은 보통 positive definite가 아닐수도 있습니다. 따라서 local maxima가 될 때까지 Hessian이 사용하기 별로 안좋습니다. 그리고 local maxima에서는 Hessian보다는 Conjugate methods가 더 효율적이라고 합니다.\n\n사실 이 파트에서는 무엇을 말하고 있는지 알기가 어렵습니다. FIM과 Hessian이 관련이 있다는 것을 알겠는데 asymtotically efficient와 같은 내용을 모르므로 내용의 이해가 어려웠습니다.\n\nMackay 논문에서 해당 부분은 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/x4n6z6pdyi7xtb9/Screenshot%202018-06-10%2012.04.13.png?dl=1\"> </center>\n\n<br>\n## 5.1 Fisher Information Matrix(FIM) vs. Hessian\n\nFIM과 Hessian에 대해서 추가적인 설명을 하고자 합니다.\n\n- FIM\n\n일단 정의되려면 space 자체가 stochastic한 성질을 가지고 있어야 합니다. 모든 parameter를 표현하는 함수가 deterministic한 함수가 아니라 확률로 나타낸 분포입니다. (probability distribution)\n\n결국은 FIM에서도 hessian처럼 비슷한 과정을 취하고 싶은 것입니다. 따라서 확률 변수이기 때문에 어떠한 특정 sample을 취하면 그게 항상 다른값이 됩니다. 그래서 FIM에다가 expectation을 취한 것입니다. expectation을 취함으로써 hessian같은 성질을 가집니다. 왜냐하면 expectation을 취함으로써 constant한 값이 되기 때문입니다.\n\n- Hessian\n\n그냥 deterministic한 함수에서 정의됩니다. 그냥 함수를 두 번 미분 한 것이라고 보면 됩니다.\n\n<br>\n## 5.2 Conjugate Gradient Method\n\n추가적으로 Conjugate Gradient Method(CGM)에 대해서 다루고자 합니다.\n\n- 참고자료\n    - https://www.quora.com/What-is-an-intuitive-explanation-of-what-the-conjugate-gradient-method-is\n    - https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf\n    - https://en.wikipedia.org/wiki/Conjugate_gradient_method\n\n### $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해 구하기\n\n$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해를 구하는 문제를 생각해봅시다. $\\mathbf{A}$의 역행렬을 구해서 양변에 곱해주면 $\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$가 되어 쉽게 해를 구할 수 있습니다. 하지만 $\\mathbf{A}^{-1}$은 계산이 많이 필요 자원소모가 큰 연산입니다. 역행렬을 취하지 않고 해를 구할 수 있는 방법이 있을까요?\n\n위의 방정식에서 $\\mathbf{b}$를 이항시키면 $\\mathbf{A}\\mathbf{x} - \\mathbf{b} = 0$이 됩니다. 최적화문제는 많은 경우 1차 미분이 0이 되는 지점이 해일 확률이 높습니다. $\\mathbf{A}\\mathbf{x} - \\mathbf{b} = 0$을 1차 미분으로 가지는 함수는 무엇일까요?\n\n$$f( \\mathbf{x} ) = \\frac{1}{2}\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x} - \\mathbf{x}^\\mathrm{T}\\mathbf{b}$$\n\n위의 함수는 $\\mathbf{A}\\mathbf{x} - \\mathbf{b}$를 1차 미분값으로 가집니다. 이 때 $\\mathbf{A}$는 symmetric positive definite해야 합니다.\n\n- symmetric: $\\mathbf{A}=\\mathbf{A}^\\mathrm{T}$\n- positive definite: $\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}>0,\\quad\\forall\\mathbf{x}$ or all eigenvalues of $\\mathbf{A}$ are positive\n\nsymmetric positive definite한 성질은 $\\mathbf{x}$를 strict convex function이 되게 만들어서 unique solution이 존재하게 합니다. $\\mathbf{A}$는 $\\mathbf{x}$의 Hessian이기도 합니다.\n\n자, 우리는 위의 방정식의 해 $\\mathbf{x}^*$를 iterative한 방식으로 찾고자 합니다. $\\mathbf{x}_0$을 초기값이라고 합시다.\n\n우리는 <img src=\"https://www.dropbox.com/s/g91yqajf72jzjoc/Screen%20Shot%202018-08-14%20at%208.43.28%20AM.png?dl=1\" width=\"80\">가 되는 <img src=\"https://www.dropbox.com/s/aj3j5fjtiyewlo1/Screen%20Shot%202018-08-14%20at%208.43.37%20AM.png?dl=1\" width=\"30\">를 찾고 싶은데 이를 위한 현재의 추정값은 $\\mathbf{A} \\mathbf{x}_0$입니다. 최적의 값을 찾았을 때와 비교하면 현재의 오차는 $\\mathbf{b} - \\mathbf{A}\\mathbf{x}_0 = \\mathbf{r}_0$입니다. 이것을 residual이라고 합니다. 어떻게 효율적으로 값을 변화시켜서 오차를 0으로 만들 수 있을까요? 일단 매 iteration마다 residual이 작아지는 방향으로 나아가야겠죠?\n\n가장 널리 알려진 방법은 gradient descent 방향입니다. gradient가 증가하면 반대로 가고 gradient가 감소하면 그 방향으로 가는 것입니다. 그리고 어떤 방향으로든 gradient가 0이면 그 지점에 멈추는 것이죠. 이 방법은 수렴은 하지만 zigzag하게 움직여서 느립니다. 이것보다 더 좋은 방법이 없을까요?\n\n### Conjugate Gradient Method\n\n#### Gram-Schmidt orthgonalization\n\n다음과 같은 vector들의 집합 <img src=\"https://www.dropbox.com/s/csgdsfofe19q5r6/Screen%20Shot%202018-08-14%20at%208.56.04%20AM.png?dl=1\" width=\"135\">이 있다고 해봅시다. 이 vector들이 서로 linearly independent하다면 이 vector들은 $\\mathbb{R}^n$ 공간 상의 basis입니다. 이 vector들을 이용해서 orthogonal한 vector들을 만들어보겠습니다. 새로운 vector들을 <img src=\"https://www.dropbox.com/s/7xg7pciar1ndu44/Screen%20Shot%202018-08-14%20at%208.58.25%20AM.png?dl=1\" width=\"130\">라고 한다면 아래와 같은 과정을 통해 만들 수 있습니다. 이 방법을 Gram-Schmidt orthogonalization이라고 합니다.\n\n- $\\mathbf{d}_1=\\mathbf{v}_1$\n- $\\mathbf{d}_2=\\mathbf{v}_2 - \\left\\langle\\mathbf{v}_2,\\frac{\\mathbf{d}_1}{\\parallel\\mathbf{d}_1\\parallel}\\right\\rangle\\frac{\\mathbf{d}_1}{\\parallel\\mathbf{d}_1\\parallel}$\n...\n- <img src=\"https://www.dropbox.com/s/arzc5cez8az0j7h/Screen%20Shot%202018-08-14%20at%209.00.04%20AM.png?dl=1\" width=\"290\">\n\n간단히 설명을 하면, 첫 vector는 basis와 같은 방향으로 출발합니다. 그 다음 vector는 그 다음 basis를 이전 vector로 projection한 다음 해당 basis로부터 빼줍니다. 그 다음 vector를 만들 때는 이전에 만든 모든 vector들에 대해서 projection을 취한 다음 모두 더한 것을 해당 basis에서 빼줍니다. \n\n#### $A$-conjugate\n\n이 방법을 이용하여 matrix $A$에 관하여 orthogonal한 새로운 vector들의 집합을 만들어보겠습니다. 일반적인 inner product와 약간 다른 다음과 같은 inner product를 정의할 수 있습니다.\n\n$$<\\mathbf{x},\\mathbf{y}>_\\mathbf{A} = \\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{y}$$\n\n이 때 $<\\mathbf{x},\\mathbf{y}>_\\mathbf{A} = 0$이면 $\\mathbf{x}와 \\mathbf{y}$는 $\\mathbf{A}$-orthogonal 또는 $\\mathbf{A}$-conjugate하다라고 합니다. vector norm $\\parallel\\cdot\\parallel$도 일반적인 vector norm이 아닌 A-norm을 다음과 같이 정의합니다.\n\n$$\\parallel\\mathbf{x}\\parallel_\\mathbf{A}^2=\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}$$\n\nGram-Schmidt orthogonalization을 또 이용해볼까요? 아래와 같습니다.\n\n* $\\mathbf{d}_1=\\mathbf{v}_1$\n* <img src=\"https://www.dropbox.com/s/i8wnvoccefgg7t8/Screen%20Shot%202018-08-14%20at%209.08.42%20AM.png?dl=1\" width=\"400\">\n...\n* <img src=\"https://www.dropbox.com/s/045x2v5sx7s0wot/Screen%20Shot%202018-08-14%20at%209.09.03%20AM.png?dl=1\" width=\"500\">\n\n그런데 이 Gram-Schmidt 방법은 단점이 하나 있습니다. 새로운 vector를 만들어내기 위해서 이전에 만든 vector들을 모두 가지고 있어야 하고 projection도 다시 계산해야 한다는 점입니다. 그런데 마법같은 이유($\\approx$ 여기서 설명하지 않는 이유)로 인해서 오직 마지막으로 만들어낸 vector만 가지고 있어도 기존의 모든 vector들이 성질을 표현해낼 수 있습니다. 그러면 우리가 만들어낸 vector는 다음과 같이 간단해집니다.\n\n<center> <img src=\"https://www.dropbox.com/s/wlyu7ax6qt0aw9w/Screen%20Shot%202018-08-14%20at%209.09.43%20AM.png?dl=1\" width=\"550\"> </center>\n\n이 방향이 우리가 업데이트시키고 싶은 방향입니다. 즉 우리는 다음 수식처럼 움직이고자 합니다.\n\n<center> <img src=\"https://www.dropbox.com/s/053o4ocvxsgxfl9/Screen%20Shot%202018-08-14%20at%209.09.56%20AM.png?dl=1\" width=\"180\"> </center>\n\n$d_k$는 방향이고 $\\alpha_{k}$는 step size입니다. 이러한 방법을 일반적으로 line search method라고 부릅니다. 방향은 위에서 구한 conjugate 방향을 이용합니다. 그래서 이 line search를 conjugate gradient method라고 부릅니다. 이 방법의 한가지 중요한 장점은 업데이트가 무조건 n번만 일어난다는 것입니다! n-dimensional space에서는 basis가 n개 밖에 없거든요. 그렇다면 얼마만큼 많이 이 방향으로 움직여야 할까요? 더 이상 $f(\\mathbf{x})$가 감소하지 않을 때까지 움직이는게 좋지 않을까요? 이것은 다시 말하면 gradient가 0이 될 때까지 움직인다는 뜻입니다. 위에서 설명한 residual과도 관계있을 것 같지 않나요?\n \n$\\nabla f(\\mathbf{x})=\\mathbf{A}\\mathbf{x} - \\mathbf{b}$에 $x_k + \\alpha_{k} d_k$를 대입해 봅시다. 아래와 같이 step size를 구할 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/xikpfgcix64ngmp/Screen%20Shot%202018-08-14%20at%209.13.20%20AM.png?dl=1\" width=\"300\"> </center>\n\n네, 이제 우리는 방향과 step size를 모두 알았습니다. 이제 update만 하면 해를 찾을 수 있겠군요!\n\n<center> <img src=\"https://www.dropbox.com/s/avjw4kbzd4ormv5/conjugate_gradient_wikipedia.png?dl=1\" width=\"200\"> </center>\n\n위키피디아에서 가져온 위의 그림을 봅시다. 그림의 녹색선이 gradient descent이고 빨간선이 conjugate gradient입니다. conjugate gradient는 gradient descent보다 빨리 수렴합니다. 녹색선은 항상 이전 이동 방향과 직각으로 이동합니다. 빨간선은 보다 빠르게 더 낮은 값이 있는 곳으로 이동합니다.\n\n위에서 설명을 생략했지만 왜 conjugate gradient는 빠르게 수렴(n-step)하며 Gram-Schmidt orthogonalization을 할 때도 메모리가 적게 필요할까요? 수학적으로 엄밀하게 설명하기 보다는 개념적으로 설명을 하겠습니다. 그 비밀은 바로 $\\mathbf{A}$-orthogonal 또는 $\\mathbf{A}$-conjugate vector들을 이용한데 있습니다. 이 vector들은 basis라고 했습니다. 즉, 처음에 우리가 구하고자 했던 해 $\\mathbf{x}^*$를 이 vector들을 이용해서 표현할 수 있습니다. 또한 iterative하게 찾아가기 위한 초기 vector $\\mathbf{x}_0$도 이 vector들로 표현할 수 있습니다. 그렇다면 이 둘 간의 오차도 이 vector들도 표현할 수 있게 됩니다. 다음 수식처럼 말입니다.\n\n$$\\mathbf{x}^\\* - \\mathbf{x}_0 = \\epsilon_1\\mathbf{d}_1 + \\epsilon_2\\mathbf{d}_2 + \\cdots + \\epsilon_n\\mathbf{d}_n$$\n\n우리의 목표는 이 오차를 줄이는 것입니다. 각각의 basis마다 오차가 있으며 이들은 서로 independent합니다. 즉, 특정 iteration에서 특정 basis에 대한 오차를 0으로 만들면 다음 번 iteration에서는 이 오차는 영향을 받지 않습니다! 이러한 이유로 n번의 iteration만으로 해를 찾을 수 있고 다른 vector들을 저장할 필요도 없는 것입니다. 선형대수의 아름다움이 느껴지지 않으시나요? 오래되었지만 참 잘 디자인된 기법이라는 생각이 듭니다.\n\n<br><br>\n\n# 6. Experiment\n\n이 논문에서는 natural gradient를 simple MDP와 tetris MDP에 대해서 실험을 진행했습니다. FIM은 다음과 같은 식으로 업데이트합니다.\n\n$$f\\leftarrow f+\\nabla \\log \\pi(a_t; s_t, \\theta)\\nabla \\log \\pi(a_t; s_t, \\theta)^T$$\n\n$T$ length trajectory에 대해서 $f/T$를 이용해 $F$의 기대값($E$)를 구합니다.\n\n<br>\n## 6.1 LQR(Linear Quadratic Regulator)\n\nAgent를 실험할 환경은 다음과 같은 dynamics를 가지고 있습니다.\n\n$x(t+1) = 0.7x(t)+u(t)+\\epsilon(t)$\n\n$u(t)$는 control 신호로서 에이전트의 행동입니다. $\\epsilon$은 noise distribution으로 환경에 가해지는 노이즈입니다. 에이전트의 목표는 적절한 $u(t)$를 통해 $x(t)$를 0으로 유지하는 것입니다. $x(t)$를 0으로 유지하기 위해서 필요한 소모값(cost)는 $x(t)^2$로 정의하며 cost를 최소화하도록 학습합니다. 이 논문에서는 실험할 때 복잡성을 더 해주기 위해 noise distribution인 $\\epsilon$을 dynamics에 추가하였습니다.\n\n이 실험에서 policy는 다음과 같이 설정하였습니다. 파라미터가 2개 밖에 없는 간단한 policy입니다.\n\n$\\pi(u;x,\\theta)\\propto \\exp(\\theta_1s_1x^2+\\theta_2s_2x)$\n\n이 policy를 간단히 그래프로 그려보면 다음과 같습니다. 가로축은 $x$, 세로축은 cost입니다. $\\theta_1$과 $\\theta_2$를 (0.5, 0.5), (1, 0), (0, 1)로 설정하고 $s_1$, $s_2$는, 1로 두었습니다. $x$는 -1~1사이의 범위로 그렸습니다. \n\n<center> <img src='https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1' width='500px'> </center>\n\n아래의 그림은 LQR을 학습한 그래프입니다. cost가 $x^2$이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있습니다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과입니다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 곡선입니다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습합니다(time 축이 log scale인 것을 감안하면 차이가 많이 납니다.). \n\n하지만 문제가 있습니다. NPG를 학습한 세 개의 곡선은 $\\theta$를 rescale 한 것입니다. $\\theta$앞에 곱해지는 숫자에 따라 학습의 과정이 다릅니다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것입니다. 즉, covariant gradient가 아니라는 뜻입니다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1\" width=\"300px\"> </center>\n\nnatural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문입니다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 $\\rho_s$가 곱해지기 때문입니다. 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것입니다!\n\n<br>\n## 6.2 Simple 2-state MDP\n\n이제 다른 예제에서 NPG를 테스트합니다. 2개의 state만 가지는 MDP를 고려해봅시다. [그림출처](http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&context=robotics). 그림으로보면 다음과 같습니다. $x=0$ 상태와 $x=1$ 상태 두 개가 존재합니다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있습니다. 상태 $x=0$에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 $x=1$에서 자기 자신으로 돌아오면 2의 보상을 받습니다. 결국 optimal policy는 상태 $x=1$에서 계속 자기 자신으로 돌아오는 행동을 취하는 것입니다. \n\n<img src=\"https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1\">\n\n문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정합니다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것입니다.\n\n$$\\rho(x=0)=0.8,  \\rho(x=1)=0.2$$\n\n일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 됩니다. 이 때, $\\rho(s)$가 gradient에 곱해지므로 상대적으로 상태 0에서의 gradient 값이 커지게 됩니다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update합니다. 따라서 아래 그림의 첫번째 그림에서처럼 reward가 1에서 오랜 시간동안 머무릅니다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것입니다.\n\n$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$\n\n<center><img src=\"https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1\" width=\"300px\"></center>\n\n하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달합니다. gradient 방법이 $1.7\\times 10^7$정도의 시간만에 2에 도달한 반면 NPG는 2의 시간만에 도달합니다. 또한 $\\rho(x=0)$가 $10^{-5}$이하로 떨어지지 않습니다.\n\n한 가지 그래프를 더 살펴봅시다. 다음 그래프는 parameter $\\theta$가 업데이트 되는 과정을 보여줍니다. 이 그래프에서는 parameter가 2개 있습니다. 일반적인 gradient가 아래 그래프에서 실선에 해당합니다. 이 실선의 그래프는 보면 처음부터 중반까지 $\\theta_i$만 거의 업데이트하는 것을 볼 수 있습니다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있습니다. \n\n<center><img src=\"https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1\" width=\"300px\"></center>\n\npolicy가 $\\pi(a;s,\\theta)\\propto \\exp(\\theta_{sa})$일 때, 다음과 같이 $F_{-1}$이 gradient 앞에 weight로 곱해지는데 이게 $\\rho$와는 달리 각 parameter에 대해 균등합니다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것입니다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$\n\n<br>\n## 6.3 Tetris\n\nNPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어 있습니다. 다음 그림은 tetris 예제를 보여줍니다. 보통 그림에서와 같이 state의 feature를 정해줍니다. [그림 출처](http://slideplayer.com/slide/5215520/)\n\n<img src=\"https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1\">\n\n이 예제에서도 exponential family로 policy를 표현합니다. $\\pi(a;s,\\theta) \\propto \\exp(\\theta^T\\phi_{sa})$ 로 표현합니다.\n\ntetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있습니다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우입니다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법입니다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷합니다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지합니다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있습니다.\n\n<img src=\"https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1\">\n\n<br><br>\n\n# 7. Discussion\n\nNatural Gradient Method는  Policy Iteration에서와 같이 Greedy Action을 선택하도록 학습합니다. Line search 기법과 함께 사용하면 더 Policy Iteration과 비슷해집니다. Greedy Policy Iteration에서와 비교하면 Performance Improvement가 보장됩니다. 하지만 FIM이 asymtotically Hessian으로 수렴하지 않습니다. 그렇기 때문에 Conjugate Gradient Method로 구하는 방법이 더 좋을수 있습니다.\n\n살펴봤듯이 본 논문의 NPG는 완벽하지 않습니다. 위의 몇가지 문제점을 극복하기 위한 추가적인 연구가 필요하다고 할 수 있습니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [DDPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/)\n\n<br>\n\n# 다음으로\n\n## [NPG Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py)\n\n## [TRPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/)","slug":"4_npg","published":1,"updated":"2019-02-07T11:21:31.947Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjrujlu1y002j5wfef2g0snts","content":"<center> <img src=\"https://www.dropbox.com/s/yd0x14ljrhpnj1b/Screen%20Shot%202018-07-18%20at%201.08.05%20AM.png?dl=1\" width=\"600\"> </center>\n\n<p>논문 저자 : Sham Kakade<br>논문 링크 : <a href=\"https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf\" target=\"_blank\" rel=\"noopener\">https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf</a><br>Proceeding : Advances in Neural Information Processing Systems (NIPS) 2002<br>정리 : 김동민, 이동민, 이웅원, 차금강</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p>이 논문이 발표된 2002년 당시에도 많은 연구자들이 objective function의 gradient 값을 따라서 좋은 policy $\\pi$를 찾고자 하였습니다. 하지만 기존의 우리가 알던 gradient descent method는 steepest descent direction이 아닐 수 있기 때문에(쉽게 말해 가장 가파른 방향을 따라서 내려가야 하는데 그러지 못할 수도 있다는 것입니다.) 이 논문에서 steepest descent direction를 나타내는 natural gradient method를 policy gradient에 적용하여 좋은 policy $\\pi$를 찾습니다. </p>\n<p><br></p>\n<h2 id=\"1-1-NPG-흐름-잡기\"><a href=\"#1-1-NPG-흐름-잡기\" class=\"headerlink\" title=\"1.1 NPG 흐름 잡기\"></a>1.1 NPG 흐름 잡기</h2><h3 id=\"1-1-1-매니폴드-manifold\"><a href=\"#1-1-1-매니폴드-manifold\" class=\"headerlink\" title=\"1.1.1 매니폴드(manifold)\"></a>1.1.1 매니폴드(manifold)</h3><center> <img src=\"https://www.dropbox.com/s/cstjgemqpby4ysr/Screen%20Shot%202018-08-12%20at%208.28.29%20PM.png?dl=1\" width=\"300\"> </center>\n\n<p>매니폴드는 간단하게 말해 위의 그림에서의 점들을 아우르는 subspace입니다. 그래서 NPG 공부하기 전에 매니폴드를 왜 배울까라는 의문이 들으실텐데 그 이유는 위에서도 언급했듯이 natural gradient method는 어떠한 파라미터 공간에서의 steepest descent direction을 강조하기 때문입니다. 여기서 파라미터 공간이 바로 리만 매니폴드입니다. 리만 매니폴드는 매니폴드가 각지지 않고 미분가능하게 부드럽게 곡률을 가진 면이라고 생각하면 편합니다.</p>\n<p>Neural Network(NN)을 사용할 경우 NN의 parameter space가 우리가 보통 생각하는 직선으로 쭉쭉 뻗어있는 유클리디안 공간(Euclidean space)가 아닙니다. 좀 더 일반적으로는 구의 표면과 같이 휘어져있는 공간 즉, 리만 공간(Riemannian space)로 표현할 수 있습니다. 다음 그림들을 보겠습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/jnlm6he18ar64yc/Screen%20Shot%202018-08-12%20at%208.14.13%20PM.png?dl=1\" width=\"400\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/tufjqmaeaz29kaz/Screen%20Shot%202018-08-12%20at%208.15.53%20PM.png?dl=1\" width=\"400\"> </center>\n\n<p>아래의 그림처럼 어떠한 확률 분포가 있다고 해봅시다.</p>\n<center> <img src=\"https://www.dropbox.com/s/mpoop19eyu1vp0a/Screen%20Shot%202018-08-13%20at%2012.52.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n<p>어떠한 공간의 확률 분포에 있는 한 점은 다른 공간의 확률 분포에서의 한 점이 될 것입니다. 이렇게 위의 그림들과 같이 유클리디안 공간의 확률 분포에서의 한 점이 리만 공간의 확률 분포의 한 점이 되는 것이고, 곡률의 일차 근사가 유클리디안 공간에서는 일차 근사가 아닌 이차 근사이고, 리만 공간에서는 휘어진 공간이기 때문에 곡률을 일차 근사라고 보는 것입니다. 추가적으로 일차 근사와 이차 근사의 차이는 <a href=\"http://darkpgmr.tistory.com/149\" target=\"_blank\" rel=\"noopener\">다크 프로그래머님의 블로그</a>를 참고해주시기 바랍니다. (꼭 보세요!) </p>\n<p>일차 근사와 이차 근사의 차이점과 각각의 장단점, 그리고 추가적으로 Line Search까지 알고 난 후에 이 논문을 보시는 것을 권장해드립니다.</p>\n<h3 id=\"1-1-2-Natural-Gradient-Policy-Gradient\"><a href=\"#1-1-2-Natural-Gradient-Policy-Gradient\" class=\"headerlink\" title=\"1.1.2 Natural Gradient + Policy Gradient\"></a>1.1.2 Natural Gradient + Policy Gradient</h3><p>먼저 아래의 그림들을 보여드리겠습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/65hra43zadsubff/Screen%20Shot%202018-08-13%20at%2012.51.45%20PM.png?dl=1\" width=\"500\"> <img src=\"https://www.dropbox.com/s/8g332zceeordqwv/Screen%20Shot%202018-08-13%20at%2012.51.51%20PM.png?dl=1\" width=\"400\"> </center>\n\n<p>위에서도 언급했듯이 natural gradient가 steepest direction이 된다는 연구가 이뤄지고 있었습니다. 강화학습의 policy gradient은 objective function의 gradient를 따라 policy를 업데이트를 합니다. 이 때 policy는 parameterization되는데 이 경우에도 gradient 대신에 natural gradient가 좋다는 것을 실험해보는 논문이 지금 다루고 있는 논문입니다.</p>\n<p>gradient가 non-covariant(1차 근사)해서 생기는 문제는 간단히 말하자면 다음과 같습니다. policy가 parameterized된 상황에서는 같은 policy라도 다른 parameter를 가질 수 있습니다. 이 때, steepest direction은 두 경우에 같은 방향을 가리켜야 하는데 non-covariant한 경우 그렇지 못합니다. 이러한 부분이 바로 결국 느린 학습으로 연결이 되는 것입니다.</p>\n<p>논문에서 2차미분 방법론들과 짧게 비교합니다. 하지만 개인적인 의견으로 2차미분을 이용한 다른 방법들과의 비교가 생각보다 없는 점이 부족해 보였습니다.(Hessian을 이용한다거나 conjugate gradient method를 이용한다거나). 실험을 통해 Fisher Information Matrix(FIM)가 hessian에 수렴안하는 거라던지 Hessian 방법론이 local maxima 부근에서 상당히 느리다던지의 결과를 보여줬었으면 좋았을 것 같습니다.</p>\n<p>또한 natural gradient의 단점으로 natural gradient 만으로 업데이트하면 policy의 improvement 보장이 안될 수 있습니다. policy의 improvement를 보장하기 위해 line search도 써야하는데 line search를 어떻게 쓰는지에 대한 자세한 언급이 없습니다. 다시 말해 자세한 algorithm 설명이 없다는 것입니다.</p>\n<p>natural policy gradient 논문은 natural gradient + policy gradient를 처음 적용했다는데 의의가 있습니다. 하지만 이 논문이 문제 삼은 gradient는 non-covariant하다라는 문제를 natural gradient를 통해 해결하지는 못했습니다(Experiment를 통해 covariant gradient가 되지 못했다는 것이 보입니다). NPG의 뒤를 잇는 논문이 “covariant policy search”와 “natural actor-critic”에서 covariant(2차 근사)하지 못하다는 것을 해결하기 위해 Fisher Information Matrix를 sample 하나 하나에 대해서 구하는 것이 아니라 trajectory 전체에 대해서 구합니다.</p>\n<p>또한 논문은 pg의 두 가지 세팅 중에 average-reward setting(infinite horizon)에서만 NPG를 다룹니다. “covariant policy search” 논문에서는 average-reward setting과 start-state setting 모두에 대해서 npg를 적용합니다.</p>\n<p>natural gradient + policy gradient를 처음 제시했다는 것은 좋지만 npg 학습의 과정을 자세하게 설명하지 않았고 다른 2차 미분 방법들과 비교를 많이 하지 않은 점이 아쉬운 논문이었습니다.</p>\n<p><br><br></p>\n<h1 id=\"2-Introduction\"><a href=\"#2-Introduction\" class=\"headerlink\" title=\"2. Introduction\"></a>2. Introduction</h1><p>소개는 앞에서 다 했기 때문에 간략하게 다시 한 번 정리하겠습니다. direct policy gradient method는 future reward의 gradient를 따라 policy를 update합니다. 하지만 gradient descent는 non-covariant입니다. 따라서 이 논문에서는 covarient gradient를 제시합니다. 바로 “Natural Gradient” 입니다. </p>\n<p>또한 natural gradient와 policy iteration의 연관성을 설명합니다. natural policy gradient is moving toward choosing a greedy optimal action (이런 연결점은 아마도 step-size를 덜 신경쓰고 싶어서 그런게 아닌가 싶습니다)</p>\n<p>논문의 Introduction 부분에 다음 멘트가 있습니다. 이 글만 봐서는 이해가 안갔는데 Mackay 논문에 좀 더 자세히 나와있었습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/41xhhr7lgfk24a1/Screenshot%202018-06-10%2011.45.18.png?dl=1\"> </center>\n\n<p><a href=\"http://www.inference.org.uk/mackay/ica.pdf\" target=\"_blank\" rel=\"noopener\">Mackay</a>논문에서는 다음과 같이 언급하고 있습니다. Back-propagation을 사용할 경우에 learning rate를 dimension에 1/n로 사용하면 수렴한다는 것이 증명됐습니다. 하지만 너무 느리다고 합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/us9ezc7vxgrkez6/Screenshot%202018-06-10%2011.47.21.png?dl=1\"> </center>\n\n<p><br><br></p>\n<h1 id=\"3-A-Natural-Gradient\"><a href=\"#3-A-Natural-Gradient\" class=\"headerlink\" title=\"3. A Natural Gradient\"></a>3. A Natural Gradient</h1><p><br></p>\n<h2 id=\"3-1-Notation\"><a href=\"#3-1-Notation\" class=\"headerlink\" title=\"3.1 Notation\"></a>3.1 Notation</h2><p>이 논문에서 제시하는 Notation은 다음과 같습니다.</p>\n<ul>\n<li>MDP : tuple $(S, s_0, A, R, P)$</li>\n<li>$S$ : a finite set of states</li>\n<li>$s_0$ : a start state</li>\n<li>$A$ : a finite set of actions</li>\n<li>$R$ : reward function $R: S \\times A -&gt; [0, R_{max}]$</li>\n<li>$\\pi(a;s, \\theta)$ : stochastic policy parameterized by $\\theta$</li>\n<li>모든 정책 $\\pi$는 ergodic : stationary distribution $\\rho^{\\pi}$이 잘 정의되어 있다고 봅니다.</li>\n<li>이 논문에서는 sutton의 pg 논문의 두 세팅(start-state formulation, average-reward formulation) 중에 두 번째인 average-reward formulation을 가정합니다.</li>\n<li>performance or average reward : $\\eta(\\pi)=\\sum_{s,a}\\rho^{\\pi}(s)\\pi(a;s)R(s,a)$</li>\n<li>state-action value : $Q^{\\pi}(s,a)=E_{\\pi}[\\sum_{t=0}^{\\infty}R(s_t, a_t)-\\eta(\\pi)\\vert s_0=s, a_0=a]$ (<a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/\">Sutton PG</a> 를 참고해주시기 바랍니다.)</li>\n<li>정책이 $\\theta$로 parameterize되어있으므로 performance는 $\\eta(\\pi_{\\theta})$인데 $\\eta(\\theta)$로 씁니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-2-Natural-Gradient\"><a href=\"#3-2-Natural-Gradient\" class=\"headerlink\" title=\"3.2 Natural Gradient\"></a>3.2 Natural Gradient</h2><p>Sutton PG 논문의 policy gradient theorem에 따라 exact gradient of the average reward는 다음과 같습니다. 다음 수식이 어떻게 유도되었는지, 어떤 의미인지 모른다면 <a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/\">Sutton PG</a>을 통해 제대로 이해하는 것이 좋습니다. Euclidean space에서는 objective function의 일반적인 policy gradient는 Sutton PG에서 논의한 바와 같이 다음과 같이 구할 수 있습니다.</p>\n<p>$$\\nabla\\eta(\\pi_\\theta) = \\Sigma_{s,a}\\rho^\\pi(s)\\nabla\\pi(a;s,\\theta)Q^\\pi(s,a)$$</p>\n<p>여기서 steepest descent direction of $\\eta(\\theta)$는 $\\eta(\\theta + d\\theta)$를 최소화하는 $d\\theta$로 정의됩니다. 이 때, $\\vert d\\theta \\vert^2$가 일정 크기 이하인 것으로 제약조건을 주는데(held to small constant) Euclidian space에서는 $\\eta(\\theta)$가 steepest direction이지만 Riemannian space에서는 natural gradient가 steepest direction입니다.</p>\n<p>추가적으로 결국 우리가 원하는 건 ‘policy 최적화를 좀 더 스마트하게 해 보자!’입니다. Policy 최적화를 잘한다는 것은 Policy를 어느 방향으로 얼만큼 업데이트 하느냐에 따라 달라집니다.</p>\n<h3 id=\"3-2-1-Natural-gradient-증명\"><a href=\"#3-2-1-Natural-gradient-증명\" class=\"headerlink\" title=\"3.2.1 Natural gradient 증명\"></a>3.2.1 Natural gradient 증명</h3><p>Riemannian space에서 거리는 다음과 같이 정의됩니다. 여기서 $G(\\theta)$는 특정한 양수로 이루어진 matrix입니다. 자세한 내용은 <a href=\"http://bskyvision.com/205\" target=\"_blank\" rel=\"noopener\">양의 정부호 행렬(positive definite matrix)이란?</a>을 참고해주시기 바랍니다.</p>\n<p>$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$</p>\n<p>이 수식은 <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&amp;rep=rep1&amp;type=pdf\" target=\"_blank\" rel=\"noopener\">Natural gradient works in efficiently in learning</a> 논문에서 증명되어 있습니다. 다음은 natural gradient 증명입니다.</p>\n<p>steepest direction을 구할 때 $\\theta$의 크기를 제약조건으로 줍니다. 제약조건은 다음과 같습니다.</p>\n<p>$$\\vert d\\theta \\vert^2 = \\epsilon^2$$</p>\n<p>그리고 steepest vector인 $d\\theta$는 다음과 같이 정의할 수 있습니다.</p>\n<p>$$d\\theta = \\epsilon a$$</p>\n<p>$$\\vert a \\vert^2=a^TG(\\theta)a = 1$$</p>\n<p>이 때, $a$가 steepest direction unit vector이 되려면 다음 수식을 최소로 만들어야 합니다.</p>\n<p>$$\\eta(\\theta + d\\theta) = \\eta(\\theta) + \\epsilon\\nabla\\eta(\\theta)^Ta$$</p>\n<p>위 수식이 제약조건 아래 최소가 되는 $a$를 구하기 위해 Lagrangian method를 사용합니다. Lagrangian method를 모른다면 <a href=\"https://en.wikipedia.org/wiki/Lagrange_multiplier\" target=\"_blank\" rel=\"noopener\">위키피디아</a>를 참고하는 것을 추천합니다. 위 수식이 최소라는 것은 $\\nabla\\eta(\\theta)^Ta$가 최소라는 것입니다.</p>\n<p>$$\\frac{\\partial}{\\partial a_i}(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$$</p>\n<p>따라서 $(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$는 상수입니다. 상수를 미분하면 0이므로 이 식을 $a$로 미분합니다. 그러면 다음과 같이 steepest direction을 구한 것입니다.</p>\n<p>$$\\nabla\\eta(\\theta) = 2 \\lambda G(\\theta)a$$</p>\n<p>$$a=\\frac{1}{2\\lambda}G^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>이 때, 다음 식을 natural gradient라고 정의합니다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = G^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>natural gradient를 이용한 업데이트는 다음과 같습니다.</p>\n<p>$$\\theta_{t+1}=\\theta_t - \\alpha_tG^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>여기까지는 natural gradient의 증명이었습니다. 이 natural gradient를 policy gradient에 적용한 것이 natural policy gradient입니다. natural policy gradient는 다음과 같이 정의됩니다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>여기서 $G$대신 $F$를 사용했는데 $F$는 Fisher information matix입니다. 수식은 다음과 같습니다.</p>\n<p>$$F(\\theta) = E_{\\rho^\\pi(s)}[F_s(\\theta)]$$</p>\n<p>$$F_s(\\theta)=E_{\\pi(a;s,\\theta)}[\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial \\theta_i}\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial\\theta_j}]$$</p>\n<h3 id=\"3-2-2-Fisher-Information-Matrix에-정의된-Metric\"><a href=\"#3-2-2-Fisher-Information-Matrix에-정의된-Metric\" class=\"headerlink\" title=\"3.2.2 Fisher Information Matrix에 정의된 Metric\"></a>3.2.2 Fisher Information Matrix에 정의된 Metric</h3><p>추가적으로 Fisher Information Matrix(FIM)에 대해서 설명하겠습니다. 위의 문제를 우리가 생각하기 쉬운 Neural Network(NN)으로 구성하고 해결할 수 있다고 해봅시다. NN은 여러가지 parameter set들로 구성될 수 있습니다. 게다가 다른 parameter set을 가지지만 같은 policy를 가질 수도 있습니다. 이 경우 steepest direction은 같은 policy이기 때문에 같은 방향을 가리키고 있어야 하는데 non-covariant한 경우 그렇지 못합니다. 떄문에 학습이 느려지고, 이러한 문제를 해결하기 위해 단순히 Positive-Definite Matrix $G(\\theta)$를 사용하지 않고 FIM $F_s(\\theta)$를 사용합니다. 어떠한 확률 변수 $X$가 임의의 매개변수 $\\theta$에 의해 정의되는 분포를 따른다고 하면 $X=x$일때 FIM은 다음과 같이 정의됩니다.</p>\n<p>$$F_x(\\theta)=E\\left[\\left(\\dfrac{\\partial}{\\partial\\theta}\\log\\Pr(x|\\theta)\\right)^2\\right]$$</p>\n<p>강화학습 관점에서 생각해보면 정보 $x$는 에피소드에 의해 관측된 상태값 $s$이며 매개변수 $\\theta$에 의해 선택될 수 있는 행동에 대한 분포가 나오게 됩니다. 이에 의해 위의 FIM는 다음과 같이 표현할 수 있습니다.</p>\n<p>$$F_s(\\theta) \\equiv E_{\\pi(a;s,\\theta)}\\left[\\left(\\dfrac{\\partial}{\\partial\\theta}\\log\\pi(a;s,\\theta)\\right)^2\\right] =E_{\\pi(a;s,\\theta)}\\left[\\dfrac{\\partial \\log\\pi(a;s,\\theta)}{\\partial \\theta_i}\\dfrac{\\partial \\log\\pi(a;s,\\theta)}{\\partial\\theta_j}\\right]$$</p>\n<p>그리고 위의 식들을 이용하여 objective function을 정리하면 아래의 식과 같이 표현됩니다.</p>\n<p>$$F(\\theta) = E_{\\rho^{\\pi}(s)}[F_s(\\theta)]$$</p>\n<p>또한 이 Fisher Information Matrix에 정의된 이 metric은 다음과 같은 성질을 가지고 있습니다.</p>\n<ol>\n<li>업데이트되는 파라미터에 의해 구성되는 매니폴드에 기반한 metric입니다.</li>\n<li>확률분포($\\pi(a;s,\\theta)$)를 구성하는 파라미터($\\theta$)의 변화에 독립적입니다.</li>\n<li>마지막으로 positive-definite한 값을 가집니다. 이렇기 때문에 steepest gradient에서 objective function의 방향을 알기 위해 사용한 방법과 같은 방법으로 natural gradient direction을 다음과 같이 구할 수 있는 것입니다.</li>\n</ol>\n<p>$$\\bar{\\nabla}\\eta(\\theta) \\equiv F(\\theta)^{-1}\\nabla\\eta(\\theta)$$</p>\n<p><br><br></p>\n<h1 id=\"4-The-Natural-Gradient-and-Policy-Iteration\"><a href=\"#4-The-Natural-Gradient-and-Policy-Iteration\" class=\"headerlink\" title=\"4. The Natural Gradient and Policy Iteration\"></a>4. The Natural Gradient and Policy Iteration</h1><p>4장에서는 Natural gradient를 통한 policy iteration을 수행하여 실제로 정책의 향상이 있는지를 증명합니다. 여기서 $Q^\\pi(s,a)$는 compatible function approximator $f^\\pi(s,a;w)$로 근사됩니다. (<a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/\">Sutton PG</a>를 참고해주시기 바랍니다.)</p>\n<p><br></p>\n<h2 id=\"4-1-Theorem-1-Compatible-Function-Approximation\"><a href=\"#4-1-Theorem-1-Compatible-Function-Approximation\" class=\"headerlink\" title=\"4.1 Theorem 1: Compatible Function Approximation\"></a>4.1 Theorem 1: Compatible Function Approximation</h2><p>approximate하는 함수 $f^{\\pi}(s,a;w)$는 다음과 같습니다.(compatible value function)</p>\n<p>$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$</p>\n<p>$$\\psi^{\\pi}(s,a) = \\nabla log\\pi(a;s,\\theta)$$</p>\n<p>여기서 $[\\nabla \\log\\pi(a;s,\\theta)]_i=\\partial \\log\\pi(a;s,\\theta)/\\partial\\theta_i$입니다. $w$는 원래 approximate하는 함수 $Q$와 $f$의 차이를 줄이도록 학습합니다(mean square error). 수렴한 local minima의 $w$를 $\\bar{w}$라고 가정 하겠습니다. 에러는 다음과 같은 수식으로 나타낼 수 있습니다.</p>\n<p>$$\\epsilon(w,\\pi)\\equiv\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)(f^{\\pi}(s,a;w)-Q^{\\pi}(s,a))^2$$</p>\n<p>그 때, $\\bar{\\omega} = \\bar{\\nabla}\\eta(\\theta)$이면 function approximator의 gradient 방향과 정책의 gradient 방향이 같다는 것을 의미합니다.</p>\n<p>아래의 내용은 위의 Theorem에 대한 증명입니다.</p>\n<p>위의 수식이 local minima이면 미분값이 0이 됩니다. $w$에 대해서 미분하면 다음과 같습니다.</p>\n<p>$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)(\\psi^{\\pi}(s,a)^T\\bar{w}-Q^{\\pi}(s,a))=0$$</p>\n<p>$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)^T\\bar{w}=\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)Q^{\\pi}(s,a)$$</p>\n<p>이 때, 위 식의 우변은 $\\psi$의 정의에 의해 policy gradient가 됩니다. 또한 왼쪽 항에서는 Fisher information matrix가 나옵니다.</p>\n<p>$$F(\\theta)=\\sum_{s,a}\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)=E_{\\rho^\\pi(s)}[F_s(\\theta)]$$</p>\n<p>따라서 다음과 같이 쓸 수 있습니다.</p>\n<p>$$F(\\theta)\\bar{w}=\\nabla\\eta(\\theta)$$</p>\n<p>$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>위의 수식은 natural gradient식과 동일합니다. 위의 수식은 policy가 update 될 때, value function approximator의 parameter 방향으로 이동한다는 것을 의미합니다. function approximation이 정확하다면 그 parameter의 natural policy gradient와 inner product가 커야합니다.</p>\n<p><br></p>\n<h2 id=\"4-2-Theorem-2-Greedy-Policy-Improvement\"><a href=\"#4-2-Theorem-2-Greedy-Policy-Improvement\" class=\"headerlink\" title=\"4.2 Theorem 2: Greedy Policy Improvement\"></a>4.2 Theorem 2: Greedy Policy Improvement</h2><p>이번 장에서는 natrual gradient는 다른 policy iteration 방법처럼 단순히 더 좋은 행동을 고르도록 학습하는 것이 아니라 가장 좋은(greedy) 행동을 고르도록 학습한다는 것을 증명하는 부분입니다. 이것을 일반적인 형태의 policy에 대해서 증명하기 전에 exponential 형태의 policy에 대해서 증명하는 것이 Theorem 2입니다. 특수한 정책을 가지는 상황안에서 학습속도($\\alpha$)를 무한대로 가져감으로서 어떤 action을 선택하는지를 알아봅니다.</p>\n<p>policy를 다음과 같이 정의합니다.</p>\n<p>$$\\pi(a;s,\\theta) \\propto \\exp(\\theta^T\\phi_{sa})$$</p>\n<p>여기서 $\\bar{\\nabla}\\eta(\\theta)$가 0이 아니고 $\\bar{w}$는 approximation error를 최소화된 $w$라고 가정합니다. 이 상태에서 natural gradient update를 생각해봅시다. 그리고 policy gradient는 gradient ascent임을 기억합시다.</p>\n<p>$$\\theta_{t+1}=\\theta_t + \\alpha_t\\bar{\\nabla}\\eta(\\theta)$$</p>\n<p>이 때 $\\alpha$가 learning rate로 parameter를 얼마나 업데이트하는지를 결정합니다. 이 값을 무한대로 늘렸을 때 policy가 어떻게 업데이트되는지 생각해봅시다.</p>\n<p>$$\\pi_{\\infty}(a;s)=lim_{\\alpha\\rightarrow\\infty}\\pi(a;s,\\theta+\\alpha\\bar{\\nabla}\\eta(\\theta))-(1)$$</p>\n<p>이 때,</p>\n<p>$$\\pi_{\\infty}=0 \\, \\, \\, if \\, and \\, only \\, if(필요충분조건) \\, \\, \\, a \\notin argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>이라고 말할 수 있습니다.</p>\n<p>아래의 내용은 위의 Theorem에 대한 증명입니다.</p>\n<p>먼저 function approximator는 앞서 다뤘듯이 아래와 같습니다.</p>\n<p>$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$</p>\n<p>여기서 function approximator는 Theorem 1에 의해 아래와 같이 쓸 수 있습니다.</p>\n<p>$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T\\psi^{\\pi}(s,a)$$</p>\n<p>$\\theta$의 정의에 의해 $\\psi$는 다음과 같습니다.</p>\n<p>$$\\psi^{\\pi}(s,a)=\\phi_{sa}-E_{\\pi(a’;s,\\theta)}[\\phi_{sa’}]$$</p>\n<p>따라서 function approximator는 다음과 같이 다시 쓸 수 있습니다.</p>\n<p>$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T(\\phi_{sa}-E_{\\pi(a’;s,\\theta)}[\\phi_{sa’}])$$</p>\n<p>greedy policy improvement가 Q function 값 중 가장 큰 값을 가지는 action을 선택하듯이 여기서도 function approximator의 값이 가장 큰 action을 선택하는 상황을 가정해봅시다. 이 때 function approximator의 argmax는 다음과 같이 쓸 수 있습니다.</p>\n<p>$$argmax_{a’}f^{\\pi}(s,a)=argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>(1) 식을 다시 살펴봅시다. 그러면 policy의 정의에 따라 다음과 같이 쓸 수 있습니다.</p>\n<p>$$\\pi(a;s,\\theta + \\alpha\\bar{\\nabla}\\eta(\\theta)) \\propto exp(\\theta^T\\phi_{sa} + \\alpha\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa})$$</p>\n<p>$\\bar{\\nabla}\\eta(\\theta) \\neq 0$이고 $\\alpha\\rightarrow\\infty$이면 exp안의 항 중에서 뒤의 항이 dominate하게 됩니다. 여러 행동 중에 $\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa}$가 가장 큰 행동이 있다면 이 행동의 policy probability가 1이 되고 나머지는 0이 됩니다. 따라서 다음이 성립합니다.</p>\n<p>$$\\pi_{\\infty}=0 \\, \\, \\, if \\, and \\, only \\, if \\, \\, \\, a \\notin argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>이 결과로부터 natural policy gradient는 단지 더 좋은 action이 아니라 best action을 고르도록 학습이 됩니다. 반면에 non-covariant gradient(1차미분)에서는 그저 더 좋은 action을 고르도록 학습이 됩니다. 이 natural policy gradient에 대한 결과는 infinite learning rate 세팅에서만 성립합니다. 좀 더 일반적인 경우에 대해서 살펴봅시다.</p>\n<p><br></p>\n<h2 id=\"4-3-Theorem-3-General-Parameterized-Policy\"><a href=\"#4-3-Theorem-3-General-Parameterized-Policy\" class=\"headerlink\" title=\"4.3 Theorem 3: General Parameterized Policy\"></a>4.3 Theorem 3: General Parameterized Policy</h2><p>Theorem 2에서와는 달리 일반적인 policy를 가정해봅시다(general parameterized policy). Theorem 3는 이 상황에서 natural gradient를 통한 업데이트가 best action를 고르는 방향으로 학습이 된다는 것을 보여줍니다.</p>\n<p>natural gradien에 따른 policy parameter의 업데이트는 다음과 같습니다. $\\bar{w}$는 approximation error를 minimize하는 $w$입니다.</p>\n<p>$$\\delta\\theta = \\theta’ - \\theta = \\alpha\\bar{\\nabla}\\eta(\\theta)=\\alpha\\bar{w}$$</p>\n<p>policy에 대해서 1차근사를 하면 다음과 같습니다.</p>\n<p>$$\\pi(a;s,\\theta’)=\\pi(a;s,\\theta)+\\frac{\\partial\\pi(a;s,\\theta)^T}{\\partial\\theta}\\delta\\theta + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\psi(s,a)^T\\delta\\theta) + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\alpha\\psi(s,a)^T\\bar{w}) + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\alpha f^{\\pi}(s,a;\\bar{w})) + O(\\delta\\theta^2)$$</p>\n<p>policy 자체가 function approximator의 크기대로 업데이트가 되므로 local하게 best action의 probability는 커지고 다른 probability의 크기는 작아질 것입니다. 하지만 만약 greedy improvement가 된다하더라도 그게 performance의 improvement를 보장하는 것은 아닙니다. 하지만 line search와 함께 사용할 경우 improvement를 보장할 수 있습니다. 왜 그런지는 처음에 말씀드린 블로그 <a href=\"http://darkpgmr.tistory.com/149\" target=\"_blank\" rel=\"noopener\">다크 프로그래머님의 블로그</a>를 참고해주시기 바랍니다.</p>\n<p><br><br></p>\n<h1 id=\"5-Metrics-and-Curvatures\"><a href=\"#5-Metrics-and-Curvatures\" class=\"headerlink\" title=\"5. Metrics and Curvatures\"></a>5. Metrics and Curvatures</h1><p>$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$</p>\n<p>이 파트에서는 FIM과 다른 metric 사이의 관계를 다룹니다.</p>\n<p>위의 식에 해당하는 G는 Fisher Information Matrix만 사용할 수 있는 것이 아닙니다. Positive-Definite Matrix인 FIM이외의 다른 Matrix도 사용할 수 있습니다. 또한 다양한 파라미터 추정에서 FIM은 Hessian Matrix에 수렴하지 않을 수 있다고 합니다. 이 말은 2nd order(2차 근사) 수렴이 보장되지 않는다는 말입니다.</p>\n<p>논문에 있는 내용들을 조금 더 추가적으로 보면 아래와 같이 나옵니다.</p>\n<p>In the different setting of parameter estimation, the Fisher information converges to the <code>Hessian</code>, so it is <a href=\"https://en.wikipedia.org/wiki/Efficiency_(statistics\" target=\"_blank\" rel=\"noopener\">asymptotically efficient</a>) 이지만,</p>\n<p>이 논문의 경우, 아마리 논문의 ‘blind separation case’와 유사한데 이 때는 꼭 asymtotically efficient하지 않다고 말합니다. 이 말은 즉 2nd order 수렴이 보장되지 않는다는 것이다.</p>\n<p><a href=\"http://www.inference.org.uk/mackay/ica.pdf\" target=\"_blank\" rel=\"noopener\">Mackay</a> 논문에서는 Hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했습니다. 그래서 performance를 2번 미분해보면 아래의 수식과 같습니다. 하지만 다음 식에서는 모든 항이 data dependent합니다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있습니다.</p>\n<p>$$\\nabla^2\\eta(\\theta)=\\sum_{sa}\\rho^{\\pi}(s)(\\nabla^2\\pi(a;s)Q^{\\pi}(s,a)+\\nabla\\pi(a;s)\\nabla Q^{\\pi}(s,a)^T+\\nabla Q^{\\pi}(s,a)\\nabla\\pi(a;s)^T)$$</p>\n<p>hessian은 보통 positive definite가 아닐수도 있습니다. 따라서 local maxima가 될 때까지 Hessian이 사용하기 별로 안좋습니다. 그리고 local maxima에서는 Hessian보다는 Conjugate methods가 더 효율적이라고 합니다.</p>\n<p>사실 이 파트에서는 무엇을 말하고 있는지 알기가 어렵습니다. FIM과 Hessian이 관련이 있다는 것을 알겠는데 asymtotically efficient와 같은 내용을 모르므로 내용의 이해가 어려웠습니다.</p>\n<p>Mackay 논문에서 해당 부분은 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/x4n6z6pdyi7xtb9/Screenshot%202018-06-10%2012.04.13.png?dl=1\"> </center>\n\n<p><br></p>\n<h2 id=\"5-1-Fisher-Information-Matrix-FIM-vs-Hessian\"><a href=\"#5-1-Fisher-Information-Matrix-FIM-vs-Hessian\" class=\"headerlink\" title=\"5.1 Fisher Information Matrix(FIM) vs. Hessian\"></a>5.1 Fisher Information Matrix(FIM) vs. Hessian</h2><p>FIM과 Hessian에 대해서 추가적인 설명을 하고자 합니다.</p>\n<ul>\n<li>FIM</li>\n</ul>\n<p>일단 정의되려면 space 자체가 stochastic한 성질을 가지고 있어야 합니다. 모든 parameter를 표현하는 함수가 deterministic한 함수가 아니라 확률로 나타낸 분포입니다. (probability distribution)</p>\n<p>결국은 FIM에서도 hessian처럼 비슷한 과정을 취하고 싶은 것입니다. 따라서 확률 변수이기 때문에 어떠한 특정 sample을 취하면 그게 항상 다른값이 됩니다. 그래서 FIM에다가 expectation을 취한 것입니다. expectation을 취함으로써 hessian같은 성질을 가집니다. 왜냐하면 expectation을 취함으로써 constant한 값이 되기 때문입니다.</p>\n<ul>\n<li>Hessian</li>\n</ul>\n<p>그냥 deterministic한 함수에서 정의됩니다. 그냥 함수를 두 번 미분 한 것이라고 보면 됩니다.</p>\n<p><br></p>\n<h2 id=\"5-2-Conjugate-Gradient-Method\"><a href=\"#5-2-Conjugate-Gradient-Method\" class=\"headerlink\" title=\"5.2 Conjugate Gradient Method\"></a>5.2 Conjugate Gradient Method</h2><p>추가적으로 Conjugate Gradient Method(CGM)에 대해서 다루고자 합니다.</p>\n<ul>\n<li>참고자료<ul>\n<li><a href=\"https://www.quora.com/What-is-an-intuitive-explanation-of-what-the-conjugate-gradient-method-is\" target=\"_blank\" rel=\"noopener\">https://www.quora.com/What-is-an-intuitive-explanation-of-what-the-conjugate-gradient-method-is</a></li>\n<li><a href=\"https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf\" target=\"_blank\" rel=\"noopener\">https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Conjugate_gradient_method\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Conjugate_gradient_method</a></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"mathbf-A-mathbf-x-mathbf-b-의-해-구하기\"><a href=\"#mathbf-A-mathbf-x-mathbf-b-의-해-구하기\" class=\"headerlink\" title=\"$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해 구하기\"></a>$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해 구하기</h3><p>$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해를 구하는 문제를 생각해봅시다. $\\mathbf{A}$의 역행렬을 구해서 양변에 곱해주면 $\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$가 되어 쉽게 해를 구할 수 있습니다. 하지만 $\\mathbf{A}^{-1}$은 계산이 많이 필요 자원소모가 큰 연산입니다. 역행렬을 취하지 않고 해를 구할 수 있는 방법이 있을까요?</p>\n<p>위의 방정식에서 $\\mathbf{b}$를 이항시키면 $\\mathbf{A}\\mathbf{x} - \\mathbf{b} = 0$이 됩니다. 최적화문제는 많은 경우 1차 미분이 0이 되는 지점이 해일 확률이 높습니다. $\\mathbf{A}\\mathbf{x} - \\mathbf{b} = 0$을 1차 미분으로 가지는 함수는 무엇일까요?</p>\n<p>$$f( \\mathbf{x} ) = \\frac{1}{2}\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x} - \\mathbf{x}^\\mathrm{T}\\mathbf{b}$$</p>\n<p>위의 함수는 $\\mathbf{A}\\mathbf{x} - \\mathbf{b}$를 1차 미분값으로 가집니다. 이 때 $\\mathbf{A}$는 symmetric positive definite해야 합니다.</p>\n<ul>\n<li>symmetric: $\\mathbf{A}=\\mathbf{A}^\\mathrm{T}$</li>\n<li>positive definite: $\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}&gt;0,\\quad\\forall\\mathbf{x}$ or all eigenvalues of $\\mathbf{A}$ are positive</li>\n</ul>\n<p>symmetric positive definite한 성질은 $\\mathbf{x}$를 strict convex function이 되게 만들어서 unique solution이 존재하게 합니다. $\\mathbf{A}$는 $\\mathbf{x}$의 Hessian이기도 합니다.</p>\n<p>자, 우리는 위의 방정식의 해 $\\mathbf{x}^*$를 iterative한 방식으로 찾고자 합니다. $\\mathbf{x}_0$을 초기값이라고 합시다.</p>\n<p>우리는 <img src=\"https://www.dropbox.com/s/g91yqajf72jzjoc/Screen%20Shot%202018-08-14%20at%208.43.28%20AM.png?dl=1\" width=\"80\">가 되는 <img src=\"https://www.dropbox.com/s/aj3j5fjtiyewlo1/Screen%20Shot%202018-08-14%20at%208.43.37%20AM.png?dl=1\" width=\"30\">를 찾고 싶은데 이를 위한 현재의 추정값은 $\\mathbf{A} \\mathbf{x}_0$입니다. 최적의 값을 찾았을 때와 비교하면 현재의 오차는 $\\mathbf{b} - \\mathbf{A}\\mathbf{x}_0 = \\mathbf{r}_0$입니다. 이것을 residual이라고 합니다. 어떻게 효율적으로 값을 변화시켜서 오차를 0으로 만들 수 있을까요? 일단 매 iteration마다 residual이 작아지는 방향으로 나아가야겠죠?</p>\n<p>가장 널리 알려진 방법은 gradient descent 방향입니다. gradient가 증가하면 반대로 가고 gradient가 감소하면 그 방향으로 가는 것입니다. 그리고 어떤 방향으로든 gradient가 0이면 그 지점에 멈추는 것이죠. 이 방법은 수렴은 하지만 zigzag하게 움직여서 느립니다. 이것보다 더 좋은 방법이 없을까요?</p>\n<h3 id=\"Conjugate-Gradient-Method\"><a href=\"#Conjugate-Gradient-Method\" class=\"headerlink\" title=\"Conjugate Gradient Method\"></a>Conjugate Gradient Method</h3><h4 id=\"Gram-Schmidt-orthgonalization\"><a href=\"#Gram-Schmidt-orthgonalization\" class=\"headerlink\" title=\"Gram-Schmidt orthgonalization\"></a>Gram-Schmidt orthgonalization</h4><p>다음과 같은 vector들의 집합 <img src=\"https://www.dropbox.com/s/csgdsfofe19q5r6/Screen%20Shot%202018-08-14%20at%208.56.04%20AM.png?dl=1\" width=\"135\">이 있다고 해봅시다. 이 vector들이 서로 linearly independent하다면 이 vector들은 $\\mathbb{R}^n$ 공간 상의 basis입니다. 이 vector들을 이용해서 orthogonal한 vector들을 만들어보겠습니다. 새로운 vector들을 <img src=\"https://www.dropbox.com/s/7xg7pciar1ndu44/Screen%20Shot%202018-08-14%20at%208.58.25%20AM.png?dl=1\" width=\"130\">라고 한다면 아래와 같은 과정을 통해 만들 수 있습니다. 이 방법을 Gram-Schmidt orthogonalization이라고 합니다.</p>\n<ul>\n<li>$\\mathbf{d}_1=\\mathbf{v}_1$</li>\n<li>$\\mathbf{d}_2=\\mathbf{v}_2 - \\left\\langle\\mathbf{v}_2,\\frac{\\mathbf{d}_1}{\\parallel\\mathbf{d}_1\\parallel}\\right\\rangle\\frac{\\mathbf{d}_1}{\\parallel\\mathbf{d}_1\\parallel}$<br>…</li>\n<li><img src=\"https://www.dropbox.com/s/arzc5cez8az0j7h/Screen%20Shot%202018-08-14%20at%209.00.04%20AM.png?dl=1\" width=\"290\"></li>\n</ul>\n<p>간단히 설명을 하면, 첫 vector는 basis와 같은 방향으로 출발합니다. 그 다음 vector는 그 다음 basis를 이전 vector로 projection한 다음 해당 basis로부터 빼줍니다. 그 다음 vector를 만들 때는 이전에 만든 모든 vector들에 대해서 projection을 취한 다음 모두 더한 것을 해당 basis에서 빼줍니다. </p>\n<h4 id=\"A-conjugate\"><a href=\"#A-conjugate\" class=\"headerlink\" title=\"$A$-conjugate\"></a>$A$-conjugate</h4><p>이 방법을 이용하여 matrix $A$에 관하여 orthogonal한 새로운 vector들의 집합을 만들어보겠습니다. 일반적인 inner product와 약간 다른 다음과 같은 inner product를 정의할 수 있습니다.</p>\n<p>$$&lt;\\mathbf{x},\\mathbf{y}&gt;_\\mathbf{A} = \\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{y}$$</p>\n<p>이 때 $&lt;\\mathbf{x},\\mathbf{y}&gt;_\\mathbf{A} = 0$이면 $\\mathbf{x}와 \\mathbf{y}$는 $\\mathbf{A}$-orthogonal 또는 $\\mathbf{A}$-conjugate하다라고 합니다. vector norm $\\parallel\\cdot\\parallel$도 일반적인 vector norm이 아닌 A-norm을 다음과 같이 정의합니다.</p>\n<p>$$\\parallel\\mathbf{x}\\parallel_\\mathbf{A}^2=\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}$$</p>\n<p>Gram-Schmidt orthogonalization을 또 이용해볼까요? 아래와 같습니다.</p>\n<ul>\n<li>$\\mathbf{d}_1=\\mathbf{v}_1$</li>\n<li><img src=\"https://www.dropbox.com/s/i8wnvoccefgg7t8/Screen%20Shot%202018-08-14%20at%209.08.42%20AM.png?dl=1\" width=\"400\"><br>…</li>\n<li><img src=\"https://www.dropbox.com/s/045x2v5sx7s0wot/Screen%20Shot%202018-08-14%20at%209.09.03%20AM.png?dl=1\" width=\"500\"></li>\n</ul>\n<p>그런데 이 Gram-Schmidt 방법은 단점이 하나 있습니다. 새로운 vector를 만들어내기 위해서 이전에 만든 vector들을 모두 가지고 있어야 하고 projection도 다시 계산해야 한다는 점입니다. 그런데 마법같은 이유($\\approx$ 여기서 설명하지 않는 이유)로 인해서 오직 마지막으로 만들어낸 vector만 가지고 있어도 기존의 모든 vector들이 성질을 표현해낼 수 있습니다. 그러면 우리가 만들어낸 vector는 다음과 같이 간단해집니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/wlyu7ax6qt0aw9w/Screen%20Shot%202018-08-14%20at%209.09.43%20AM.png?dl=1\" width=\"550\"> </center>\n\n<p>이 방향이 우리가 업데이트시키고 싶은 방향입니다. 즉 우리는 다음 수식처럼 움직이고자 합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/053o4ocvxsgxfl9/Screen%20Shot%202018-08-14%20at%209.09.56%20AM.png?dl=1\" width=\"180\"> </center>\n\n<p>$d_k$는 방향이고 $\\alpha_{k}$는 step size입니다. 이러한 방법을 일반적으로 line search method라고 부릅니다. 방향은 위에서 구한 conjugate 방향을 이용합니다. 그래서 이 line search를 conjugate gradient method라고 부릅니다. 이 방법의 한가지 중요한 장점은 업데이트가 무조건 n번만 일어난다는 것입니다! n-dimensional space에서는 basis가 n개 밖에 없거든요. 그렇다면 얼마만큼 많이 이 방향으로 움직여야 할까요? 더 이상 $f(\\mathbf{x})$가 감소하지 않을 때까지 움직이는게 좋지 않을까요? 이것은 다시 말하면 gradient가 0이 될 때까지 움직인다는 뜻입니다. 위에서 설명한 residual과도 관계있을 것 같지 않나요?</p>\n<p>$\\nabla f(\\mathbf{x})=\\mathbf{A}\\mathbf{x} - \\mathbf{b}$에 $x_k + \\alpha_{k} d_k$를 대입해 봅시다. 아래와 같이 step size를 구할 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/xikpfgcix64ngmp/Screen%20Shot%202018-08-14%20at%209.13.20%20AM.png?dl=1\" width=\"300\"> </center>\n\n<p>네, 이제 우리는 방향과 step size를 모두 알았습니다. 이제 update만 하면 해를 찾을 수 있겠군요!</p>\n<center> <img src=\"https://www.dropbox.com/s/avjw4kbzd4ormv5/conjugate_gradient_wikipedia.png?dl=1\" width=\"200\"> </center>\n\n<p>위키피디아에서 가져온 위의 그림을 봅시다. 그림의 녹색선이 gradient descent이고 빨간선이 conjugate gradient입니다. conjugate gradient는 gradient descent보다 빨리 수렴합니다. 녹색선은 항상 이전 이동 방향과 직각으로 이동합니다. 빨간선은 보다 빠르게 더 낮은 값이 있는 곳으로 이동합니다.</p>\n<p>위에서 설명을 생략했지만 왜 conjugate gradient는 빠르게 수렴(n-step)하며 Gram-Schmidt orthogonalization을 할 때도 메모리가 적게 필요할까요? 수학적으로 엄밀하게 설명하기 보다는 개념적으로 설명을 하겠습니다. 그 비밀은 바로 $\\mathbf{A}$-orthogonal 또는 $\\mathbf{A}$-conjugate vector들을 이용한데 있습니다. 이 vector들은 basis라고 했습니다. 즉, 처음에 우리가 구하고자 했던 해 $\\mathbf{x}^*$를 이 vector들을 이용해서 표현할 수 있습니다. 또한 iterative하게 찾아가기 위한 초기 vector $\\mathbf{x}_0$도 이 vector들로 표현할 수 있습니다. 그렇다면 이 둘 간의 오차도 이 vector들도 표현할 수 있게 됩니다. 다음 수식처럼 말입니다.</p>\n<p>$$\\mathbf{x}^* - \\mathbf{x}_0 = \\epsilon_1\\mathbf{d}_1 + \\epsilon_2\\mathbf{d}_2 + \\cdots + \\epsilon_n\\mathbf{d}_n$$</p>\n<p>우리의 목표는 이 오차를 줄이는 것입니다. 각각의 basis마다 오차가 있으며 이들은 서로 independent합니다. 즉, 특정 iteration에서 특정 basis에 대한 오차를 0으로 만들면 다음 번 iteration에서는 이 오차는 영향을 받지 않습니다! 이러한 이유로 n번의 iteration만으로 해를 찾을 수 있고 다른 vector들을 저장할 필요도 없는 것입니다. 선형대수의 아름다움이 느껴지지 않으시나요? 오래되었지만 참 잘 디자인된 기법이라는 생각이 듭니다.</p>\n<p><br><br></p>\n<h1 id=\"6-Experiment\"><a href=\"#6-Experiment\" class=\"headerlink\" title=\"6. Experiment\"></a>6. Experiment</h1><p>이 논문에서는 natural gradient를 simple MDP와 tetris MDP에 대해서 실험을 진행했습니다. FIM은 다음과 같은 식으로 업데이트합니다.</p>\n<p>$$f\\leftarrow f+\\nabla \\log \\pi(a_t; s_t, \\theta)\\nabla \\log \\pi(a_t; s_t, \\theta)^T$$</p>\n<p>$T$ length trajectory에 대해서 $f/T$를 이용해 $F$의 기대값($E$)를 구합니다.</p>\n<p><br></p>\n<h2 id=\"6-1-LQR-Linear-Quadratic-Regulator\"><a href=\"#6-1-LQR-Linear-Quadratic-Regulator\" class=\"headerlink\" title=\"6.1 LQR(Linear Quadratic Regulator)\"></a>6.1 LQR(Linear Quadratic Regulator)</h2><p>Agent를 실험할 환경은 다음과 같은 dynamics를 가지고 있습니다.</p>\n<p>$x(t+1) = 0.7x(t)+u(t)+\\epsilon(t)$</p>\n<p>$u(t)$는 control 신호로서 에이전트의 행동입니다. $\\epsilon$은 noise distribution으로 환경에 가해지는 노이즈입니다. 에이전트의 목표는 적절한 $u(t)$를 통해 $x(t)$를 0으로 유지하는 것입니다. $x(t)$를 0으로 유지하기 위해서 필요한 소모값(cost)는 $x(t)^2$로 정의하며 cost를 최소화하도록 학습합니다. 이 논문에서는 실험할 때 복잡성을 더 해주기 위해 noise distribution인 $\\epsilon$을 dynamics에 추가하였습니다.</p>\n<p>이 실험에서 policy는 다음과 같이 설정하였습니다. 파라미터가 2개 밖에 없는 간단한 policy입니다.</p>\n<p>$\\pi(u;x,\\theta)\\propto \\exp(\\theta_1s_1x^2+\\theta_2s_2x)$</p>\n<p>이 policy를 간단히 그래프로 그려보면 다음과 같습니다. 가로축은 $x$, 세로축은 cost입니다. $\\theta_1$과 $\\theta_2$를 (0.5, 0.5), (1, 0), (0, 1)로 설정하고 $s_1$, $s_2$는, 1로 두었습니다. $x$는 -1~1사이의 범위로 그렸습니다. </p>\n<center> <img src=\"https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1\" width=\"500px\"> </center>\n\n<p>아래의 그림은 LQR을 학습한 그래프입니다. cost가 $x^2$이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있습니다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과입니다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 곡선입니다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습합니다(time 축이 log scale인 것을 감안하면 차이가 많이 납니다.). </p>\n<p>하지만 문제가 있습니다. NPG를 학습한 세 개의 곡선은 $\\theta$를 rescale 한 것입니다. $\\theta$앞에 곱해지는 숫자에 따라 학습의 과정이 다릅니다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것입니다. 즉, covariant gradient가 아니라는 뜻입니다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것입니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1\" width=\"300px\"> </center>\n\n<p>natural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문입니다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 $\\rho_s$가 곱해지기 때문입니다. 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것입니다!</p>\n<p><br></p>\n<h2 id=\"6-2-Simple-2-state-MDP\"><a href=\"#6-2-Simple-2-state-MDP\" class=\"headerlink\" title=\"6.2 Simple 2-state MDP\"></a>6.2 Simple 2-state MDP</h2><p>이제 다른 예제에서 NPG를 테스트합니다. 2개의 state만 가지는 MDP를 고려해봅시다. <a href=\"http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&amp;context=robotics\" target=\"_blank\" rel=\"noopener\">그림출처</a>. 그림으로보면 다음과 같습니다. $x=0$ 상태와 $x=1$ 상태 두 개가 존재합니다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있습니다. 상태 $x=0$에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 $x=1$에서 자기 자신으로 돌아오면 2의 보상을 받습니다. 결국 optimal policy는 상태 $x=1$에서 계속 자기 자신으로 돌아오는 행동을 취하는 것입니다. </p>\n<p><img src=\"https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1\"></p>\n<p>문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정합니다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것입니다.</p>\n<p>$$\\rho(x=0)=0.8,  \\rho(x=1)=0.2$$</p>\n<p>일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 됩니다. 이 때, $\\rho(s)$가 gradient에 곱해지므로 상대적으로 상태 0에서의 gradient 값이 커지게 됩니다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update합니다. 따라서 아래 그림의 첫번째 그림에서처럼 reward가 1에서 오랜 시간동안 머무릅니다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것입니다.</p>\n<p>$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$</p>\n<center><img src=\"https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1\" width=\"300px\"></center>\n\n<p>하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달합니다. gradient 방법이 $1.7\\times 10^7$정도의 시간만에 2에 도달한 반면 NPG는 2의 시간만에 도달합니다. 또한 $\\rho(x=0)$가 $10^{-5}$이하로 떨어지지 않습니다.</p>\n<p>한 가지 그래프를 더 살펴봅시다. 다음 그래프는 parameter $\\theta$가 업데이트 되는 과정을 보여줍니다. 이 그래프에서는 parameter가 2개 있습니다. 일반적인 gradient가 아래 그래프에서 실선에 해당합니다. 이 실선의 그래프는 보면 처음부터 중반까지 $\\theta_i$만 거의 업데이트하는 것을 볼 수 있습니다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있습니다. </p>\n<center><img src=\"https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1\" width=\"300px\"></center>\n\n<p>policy가 $\\pi(a;s,\\theta)\\propto \\exp(\\theta_{sa})$일 때, 다음과 같이 $F_{-1}$이 gradient 앞에 weight로 곱해지는데 이게 $\\rho$와는 달리 각 parameter에 대해 균등합니다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것입니다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$</p>\n<p><br></p>\n<h2 id=\"6-3-Tetris\"><a href=\"#6-3-Tetris\" class=\"headerlink\" title=\"6.3 Tetris\"></a>6.3 Tetris</h2><p>NPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어 있습니다. 다음 그림은 tetris 예제를 보여줍니다. 보통 그림에서와 같이 state의 feature를 정해줍니다. <a href=\"http://slideplayer.com/slide/5215520/\" target=\"_blank\" rel=\"noopener\">그림 출처</a></p>\n<p><img src=\"https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1\"></p>\n<p>이 예제에서도 exponential family로 policy를 표현합니다. $\\pi(a;s,\\theta) \\propto \\exp(\\theta^T\\phi_{sa})$ 로 표현합니다.</p>\n<p>tetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있습니다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우입니다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법입니다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷합니다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지합니다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있습니다.</p>\n<p><img src=\"https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1\"></p>\n<p><br><br></p>\n<h1 id=\"7-Discussion\"><a href=\"#7-Discussion\" class=\"headerlink\" title=\"7. Discussion\"></a>7. Discussion</h1><p>Natural Gradient Method는  Policy Iteration에서와 같이 Greedy Action을 선택하도록 학습합니다. Line search 기법과 함께 사용하면 더 Policy Iteration과 비슷해집니다. Greedy Policy Iteration에서와 비교하면 Performance Improvement가 보장됩니다. 하지만 FIM이 asymtotically Hessian으로 수렴하지 않습니다. 그렇기 때문에 Conjugate Gradient Method로 구하는 방법이 더 좋을수 있습니다.</p>\n<p>살펴봤듯이 본 논문의 NPG는 완벽하지 않습니다. 위의 몇가지 문제점을 극복하기 위한 추가적인 연구가 필요하다고 할 수 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"DDPG-여행하기\"><a href=\"#DDPG-여행하기\" class=\"headerlink\" title=\"DDPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/\">DDPG 여행하기</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"NPG-Code\"><a href=\"#NPG-Code\" class=\"headerlink\" title=\"NPG Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py\" target=\"_blank\" rel=\"noopener\">NPG Code</a></h2><h2 id=\"TRPO-여행하기\"><a href=\"#TRPO-여행하기\" class=\"headerlink\" title=\"TRPO 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/\">TRPO 여행하기</a></h2>","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"https://www.dropbox.com/s/yd0x14ljrhpnj1b/Screen%20Shot%202018-07-18%20at%201.08.05%20AM.png?dl=1\" width=\"600\"> </center>\n\n<p>논문 저자 : Sham Kakade<br>논문 링크 : <a href=\"https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf\" target=\"_blank\" rel=\"noopener\">https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf</a><br>Proceeding : Advances in Neural Information Processing Systems (NIPS) 2002<br>정리 : 김동민, 이동민, 이웅원, 차금강</p>\n<hr>\n<h1 id=\"1-들어가며…\"><a href=\"#1-들어가며…\" class=\"headerlink\" title=\"1. 들어가며…\"></a>1. 들어가며…</h1><p>이 논문이 발표된 2002년 당시에도 많은 연구자들이 objective function의 gradient 값을 따라서 좋은 policy $\\pi$를 찾고자 하였습니다. 하지만 기존의 우리가 알던 gradient descent method는 steepest descent direction이 아닐 수 있기 때문에(쉽게 말해 가장 가파른 방향을 따라서 내려가야 하는데 그러지 못할 수도 있다는 것입니다.) 이 논문에서 steepest descent direction를 나타내는 natural gradient method를 policy gradient에 적용하여 좋은 policy $\\pi$를 찾습니다. </p>\n<p><br></p>\n<h2 id=\"1-1-NPG-흐름-잡기\"><a href=\"#1-1-NPG-흐름-잡기\" class=\"headerlink\" title=\"1.1 NPG 흐름 잡기\"></a>1.1 NPG 흐름 잡기</h2><h3 id=\"1-1-1-매니폴드-manifold\"><a href=\"#1-1-1-매니폴드-manifold\" class=\"headerlink\" title=\"1.1.1 매니폴드(manifold)\"></a>1.1.1 매니폴드(manifold)</h3><center> <img src=\"https://www.dropbox.com/s/cstjgemqpby4ysr/Screen%20Shot%202018-08-12%20at%208.28.29%20PM.png?dl=1\" width=\"300\"> </center>\n\n<p>매니폴드는 간단하게 말해 위의 그림에서의 점들을 아우르는 subspace입니다. 그래서 NPG 공부하기 전에 매니폴드를 왜 배울까라는 의문이 들으실텐데 그 이유는 위에서도 언급했듯이 natural gradient method는 어떠한 파라미터 공간에서의 steepest descent direction을 강조하기 때문입니다. 여기서 파라미터 공간이 바로 리만 매니폴드입니다. 리만 매니폴드는 매니폴드가 각지지 않고 미분가능하게 부드럽게 곡률을 가진 면이라고 생각하면 편합니다.</p>\n<p>Neural Network(NN)을 사용할 경우 NN의 parameter space가 우리가 보통 생각하는 직선으로 쭉쭉 뻗어있는 유클리디안 공간(Euclidean space)가 아닙니다. 좀 더 일반적으로는 구의 표면과 같이 휘어져있는 공간 즉, 리만 공간(Riemannian space)로 표현할 수 있습니다. 다음 그림들을 보겠습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/jnlm6he18ar64yc/Screen%20Shot%202018-08-12%20at%208.14.13%20PM.png?dl=1\" width=\"400\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/tufjqmaeaz29kaz/Screen%20Shot%202018-08-12%20at%208.15.53%20PM.png?dl=1\" width=\"400\"> </center>\n\n<p>아래의 그림처럼 어떠한 확률 분포가 있다고 해봅시다.</p>\n<center> <img src=\"https://www.dropbox.com/s/mpoop19eyu1vp0a/Screen%20Shot%202018-08-13%20at%2012.52.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n<p>어떠한 공간의 확률 분포에 있는 한 점은 다른 공간의 확률 분포에서의 한 점이 될 것입니다. 이렇게 위의 그림들과 같이 유클리디안 공간의 확률 분포에서의 한 점이 리만 공간의 확률 분포의 한 점이 되는 것이고, 곡률의 일차 근사가 유클리디안 공간에서는 일차 근사가 아닌 이차 근사이고, 리만 공간에서는 휘어진 공간이기 때문에 곡률을 일차 근사라고 보는 것입니다. 추가적으로 일차 근사와 이차 근사의 차이는 <a href=\"http://darkpgmr.tistory.com/149\" target=\"_blank\" rel=\"noopener\">다크 프로그래머님의 블로그</a>를 참고해주시기 바랍니다. (꼭 보세요!) </p>\n<p>일차 근사와 이차 근사의 차이점과 각각의 장단점, 그리고 추가적으로 Line Search까지 알고 난 후에 이 논문을 보시는 것을 권장해드립니다.</p>\n<h3 id=\"1-1-2-Natural-Gradient-Policy-Gradient\"><a href=\"#1-1-2-Natural-Gradient-Policy-Gradient\" class=\"headerlink\" title=\"1.1.2 Natural Gradient + Policy Gradient\"></a>1.1.2 Natural Gradient + Policy Gradient</h3><p>먼저 아래의 그림들을 보여드리겠습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/65hra43zadsubff/Screen%20Shot%202018-08-13%20at%2012.51.45%20PM.png?dl=1\" width=\"500\"> <img src=\"https://www.dropbox.com/s/8g332zceeordqwv/Screen%20Shot%202018-08-13%20at%2012.51.51%20PM.png?dl=1\" width=\"400\"> </center>\n\n<p>위에서도 언급했듯이 natural gradient가 steepest direction이 된다는 연구가 이뤄지고 있었습니다. 강화학습의 policy gradient은 objective function의 gradient를 따라 policy를 업데이트를 합니다. 이 때 policy는 parameterization되는데 이 경우에도 gradient 대신에 natural gradient가 좋다는 것을 실험해보는 논문이 지금 다루고 있는 논문입니다.</p>\n<p>gradient가 non-covariant(1차 근사)해서 생기는 문제는 간단히 말하자면 다음과 같습니다. policy가 parameterized된 상황에서는 같은 policy라도 다른 parameter를 가질 수 있습니다. 이 때, steepest direction은 두 경우에 같은 방향을 가리켜야 하는데 non-covariant한 경우 그렇지 못합니다. 이러한 부분이 바로 결국 느린 학습으로 연결이 되는 것입니다.</p>\n<p>논문에서 2차미분 방법론들과 짧게 비교합니다. 하지만 개인적인 의견으로 2차미분을 이용한 다른 방법들과의 비교가 생각보다 없는 점이 부족해 보였습니다.(Hessian을 이용한다거나 conjugate gradient method를 이용한다거나). 실험을 통해 Fisher Information Matrix(FIM)가 hessian에 수렴안하는 거라던지 Hessian 방법론이 local maxima 부근에서 상당히 느리다던지의 결과를 보여줬었으면 좋았을 것 같습니다.</p>\n<p>또한 natural gradient의 단점으로 natural gradient 만으로 업데이트하면 policy의 improvement 보장이 안될 수 있습니다. policy의 improvement를 보장하기 위해 line search도 써야하는데 line search를 어떻게 쓰는지에 대한 자세한 언급이 없습니다. 다시 말해 자세한 algorithm 설명이 없다는 것입니다.</p>\n<p>natural policy gradient 논문은 natural gradient + policy gradient를 처음 적용했다는데 의의가 있습니다. 하지만 이 논문이 문제 삼은 gradient는 non-covariant하다라는 문제를 natural gradient를 통해 해결하지는 못했습니다(Experiment를 통해 covariant gradient가 되지 못했다는 것이 보입니다). NPG의 뒤를 잇는 논문이 “covariant policy search”와 “natural actor-critic”에서 covariant(2차 근사)하지 못하다는 것을 해결하기 위해 Fisher Information Matrix를 sample 하나 하나에 대해서 구하는 것이 아니라 trajectory 전체에 대해서 구합니다.</p>\n<p>또한 논문은 pg의 두 가지 세팅 중에 average-reward setting(infinite horizon)에서만 NPG를 다룹니다. “covariant policy search” 논문에서는 average-reward setting과 start-state setting 모두에 대해서 npg를 적용합니다.</p>\n<p>natural gradient + policy gradient를 처음 제시했다는 것은 좋지만 npg 학습의 과정을 자세하게 설명하지 않았고 다른 2차 미분 방법들과 비교를 많이 하지 않은 점이 아쉬운 논문이었습니다.</p>\n<p><br><br></p>\n<h1 id=\"2-Introduction\"><a href=\"#2-Introduction\" class=\"headerlink\" title=\"2. Introduction\"></a>2. Introduction</h1><p>소개는 앞에서 다 했기 때문에 간략하게 다시 한 번 정리하겠습니다. direct policy gradient method는 future reward의 gradient를 따라 policy를 update합니다. 하지만 gradient descent는 non-covariant입니다. 따라서 이 논문에서는 covarient gradient를 제시합니다. 바로 “Natural Gradient” 입니다. </p>\n<p>또한 natural gradient와 policy iteration의 연관성을 설명합니다. natural policy gradient is moving toward choosing a greedy optimal action (이런 연결점은 아마도 step-size를 덜 신경쓰고 싶어서 그런게 아닌가 싶습니다)</p>\n<p>논문의 Introduction 부분에 다음 멘트가 있습니다. 이 글만 봐서는 이해가 안갔는데 Mackay 논문에 좀 더 자세히 나와있었습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/41xhhr7lgfk24a1/Screenshot%202018-06-10%2011.45.18.png?dl=1\"> </center>\n\n<p><a href=\"http://www.inference.org.uk/mackay/ica.pdf\" target=\"_blank\" rel=\"noopener\">Mackay</a>논문에서는 다음과 같이 언급하고 있습니다. Back-propagation을 사용할 경우에 learning rate를 dimension에 1/n로 사용하면 수렴한다는 것이 증명됐습니다. 하지만 너무 느리다고 합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/us9ezc7vxgrkez6/Screenshot%202018-06-10%2011.47.21.png?dl=1\"> </center>\n\n<p><br><br></p>\n<h1 id=\"3-A-Natural-Gradient\"><a href=\"#3-A-Natural-Gradient\" class=\"headerlink\" title=\"3. A Natural Gradient\"></a>3. A Natural Gradient</h1><p><br></p>\n<h2 id=\"3-1-Notation\"><a href=\"#3-1-Notation\" class=\"headerlink\" title=\"3.1 Notation\"></a>3.1 Notation</h2><p>이 논문에서 제시하는 Notation은 다음과 같습니다.</p>\n<ul>\n<li>MDP : tuple $(S, s_0, A, R, P)$</li>\n<li>$S$ : a finite set of states</li>\n<li>$s_0$ : a start state</li>\n<li>$A$ : a finite set of actions</li>\n<li>$R$ : reward function $R: S \\times A -&gt; [0, R_{max}]$</li>\n<li>$\\pi(a;s, \\theta)$ : stochastic policy parameterized by $\\theta$</li>\n<li>모든 정책 $\\pi$는 ergodic : stationary distribution $\\rho^{\\pi}$이 잘 정의되어 있다고 봅니다.</li>\n<li>이 논문에서는 sutton의 pg 논문의 두 세팅(start-state formulation, average-reward formulation) 중에 두 번째인 average-reward formulation을 가정합니다.</li>\n<li>performance or average reward : $\\eta(\\pi)=\\sum_{s,a}\\rho^{\\pi}(s)\\pi(a;s)R(s,a)$</li>\n<li>state-action value : $Q^{\\pi}(s,a)=E_{\\pi}[\\sum_{t=0}^{\\infty}R(s_t, a_t)-\\eta(\\pi)\\vert s_0=s, a_0=a]$ (<a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/\">Sutton PG</a> 를 참고해주시기 바랍니다.)</li>\n<li>정책이 $\\theta$로 parameterize되어있으므로 performance는 $\\eta(\\pi_{\\theta})$인데 $\\eta(\\theta)$로 씁니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-2-Natural-Gradient\"><a href=\"#3-2-Natural-Gradient\" class=\"headerlink\" title=\"3.2 Natural Gradient\"></a>3.2 Natural Gradient</h2><p>Sutton PG 논문의 policy gradient theorem에 따라 exact gradient of the average reward는 다음과 같습니다. 다음 수식이 어떻게 유도되었는지, 어떤 의미인지 모른다면 <a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/\">Sutton PG</a>을 통해 제대로 이해하는 것이 좋습니다. Euclidean space에서는 objective function의 일반적인 policy gradient는 Sutton PG에서 논의한 바와 같이 다음과 같이 구할 수 있습니다.</p>\n<p>$$\\nabla\\eta(\\pi_\\theta) = \\Sigma_{s,a}\\rho^\\pi(s)\\nabla\\pi(a;s,\\theta)Q^\\pi(s,a)$$</p>\n<p>여기서 steepest descent direction of $\\eta(\\theta)$는 $\\eta(\\theta + d\\theta)$를 최소화하는 $d\\theta$로 정의됩니다. 이 때, $\\vert d\\theta \\vert^2$가 일정 크기 이하인 것으로 제약조건을 주는데(held to small constant) Euclidian space에서는 $\\eta(\\theta)$가 steepest direction이지만 Riemannian space에서는 natural gradient가 steepest direction입니다.</p>\n<p>추가적으로 결국 우리가 원하는 건 ‘policy 최적화를 좀 더 스마트하게 해 보자!’입니다. Policy 최적화를 잘한다는 것은 Policy를 어느 방향으로 얼만큼 업데이트 하느냐에 따라 달라집니다.</p>\n<h3 id=\"3-2-1-Natural-gradient-증명\"><a href=\"#3-2-1-Natural-gradient-증명\" class=\"headerlink\" title=\"3.2.1 Natural gradient 증명\"></a>3.2.1 Natural gradient 증명</h3><p>Riemannian space에서 거리는 다음과 같이 정의됩니다. 여기서 $G(\\theta)$는 특정한 양수로 이루어진 matrix입니다. 자세한 내용은 <a href=\"http://bskyvision.com/205\" target=\"_blank\" rel=\"noopener\">양의 정부호 행렬(positive definite matrix)이란?</a>을 참고해주시기 바랍니다.</p>\n<p>$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$</p>\n<p>이 수식은 <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&amp;rep=rep1&amp;type=pdf\" target=\"_blank\" rel=\"noopener\">Natural gradient works in efficiently in learning</a> 논문에서 증명되어 있습니다. 다음은 natural gradient 증명입니다.</p>\n<p>steepest direction을 구할 때 $\\theta$의 크기를 제약조건으로 줍니다. 제약조건은 다음과 같습니다.</p>\n<p>$$\\vert d\\theta \\vert^2 = \\epsilon^2$$</p>\n<p>그리고 steepest vector인 $d\\theta$는 다음과 같이 정의할 수 있습니다.</p>\n<p>$$d\\theta = \\epsilon a$$</p>\n<p>$$\\vert a \\vert^2=a^TG(\\theta)a = 1$$</p>\n<p>이 때, $a$가 steepest direction unit vector이 되려면 다음 수식을 최소로 만들어야 합니다.</p>\n<p>$$\\eta(\\theta + d\\theta) = \\eta(\\theta) + \\epsilon\\nabla\\eta(\\theta)^Ta$$</p>\n<p>위 수식이 제약조건 아래 최소가 되는 $a$를 구하기 위해 Lagrangian method를 사용합니다. Lagrangian method를 모른다면 <a href=\"https://en.wikipedia.org/wiki/Lagrange_multiplier\" target=\"_blank\" rel=\"noopener\">위키피디아</a>를 참고하는 것을 추천합니다. 위 수식이 최소라는 것은 $\\nabla\\eta(\\theta)^Ta$가 최소라는 것입니다.</p>\n<p>$$\\frac{\\partial}{\\partial a_i}(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$$</p>\n<p>따라서 $(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$는 상수입니다. 상수를 미분하면 0이므로 이 식을 $a$로 미분합니다. 그러면 다음과 같이 steepest direction을 구한 것입니다.</p>\n<p>$$\\nabla\\eta(\\theta) = 2 \\lambda G(\\theta)a$$</p>\n<p>$$a=\\frac{1}{2\\lambda}G^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>이 때, 다음 식을 natural gradient라고 정의합니다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = G^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>natural gradient를 이용한 업데이트는 다음과 같습니다.</p>\n<p>$$\\theta_{t+1}=\\theta_t - \\alpha_tG^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>여기까지는 natural gradient의 증명이었습니다. 이 natural gradient를 policy gradient에 적용한 것이 natural policy gradient입니다. natural policy gradient는 다음과 같이 정의됩니다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>여기서 $G$대신 $F$를 사용했는데 $F$는 Fisher information matix입니다. 수식은 다음과 같습니다.</p>\n<p>$$F(\\theta) = E_{\\rho^\\pi(s)}[F_s(\\theta)]$$</p>\n<p>$$F_s(\\theta)=E_{\\pi(a;s,\\theta)}[\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial \\theta_i}\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial\\theta_j}]$$</p>\n<h3 id=\"3-2-2-Fisher-Information-Matrix에-정의된-Metric\"><a href=\"#3-2-2-Fisher-Information-Matrix에-정의된-Metric\" class=\"headerlink\" title=\"3.2.2 Fisher Information Matrix에 정의된 Metric\"></a>3.2.2 Fisher Information Matrix에 정의된 Metric</h3><p>추가적으로 Fisher Information Matrix(FIM)에 대해서 설명하겠습니다. 위의 문제를 우리가 생각하기 쉬운 Neural Network(NN)으로 구성하고 해결할 수 있다고 해봅시다. NN은 여러가지 parameter set들로 구성될 수 있습니다. 게다가 다른 parameter set을 가지지만 같은 policy를 가질 수도 있습니다. 이 경우 steepest direction은 같은 policy이기 때문에 같은 방향을 가리키고 있어야 하는데 non-covariant한 경우 그렇지 못합니다. 떄문에 학습이 느려지고, 이러한 문제를 해결하기 위해 단순히 Positive-Definite Matrix $G(\\theta)$를 사용하지 않고 FIM $F_s(\\theta)$를 사용합니다. 어떠한 확률 변수 $X$가 임의의 매개변수 $\\theta$에 의해 정의되는 분포를 따른다고 하면 $X=x$일때 FIM은 다음과 같이 정의됩니다.</p>\n<p>$$F_x(\\theta)=E\\left[\\left(\\dfrac{\\partial}{\\partial\\theta}\\log\\Pr(x|\\theta)\\right)^2\\right]$$</p>\n<p>강화학습 관점에서 생각해보면 정보 $x$는 에피소드에 의해 관측된 상태값 $s$이며 매개변수 $\\theta$에 의해 선택될 수 있는 행동에 대한 분포가 나오게 됩니다. 이에 의해 위의 FIM는 다음과 같이 표현할 수 있습니다.</p>\n<p>$$F_s(\\theta) \\equiv E_{\\pi(a;s,\\theta)}\\left[\\left(\\dfrac{\\partial}{\\partial\\theta}\\log\\pi(a;s,\\theta)\\right)^2\\right] =E_{\\pi(a;s,\\theta)}\\left[\\dfrac{\\partial \\log\\pi(a;s,\\theta)}{\\partial \\theta_i}\\dfrac{\\partial \\log\\pi(a;s,\\theta)}{\\partial\\theta_j}\\right]$$</p>\n<p>그리고 위의 식들을 이용하여 objective function을 정리하면 아래의 식과 같이 표현됩니다.</p>\n<p>$$F(\\theta) = E_{\\rho^{\\pi}(s)}[F_s(\\theta)]$$</p>\n<p>또한 이 Fisher Information Matrix에 정의된 이 metric은 다음과 같은 성질을 가지고 있습니다.</p>\n<ol>\n<li>업데이트되는 파라미터에 의해 구성되는 매니폴드에 기반한 metric입니다.</li>\n<li>확률분포($\\pi(a;s,\\theta)$)를 구성하는 파라미터($\\theta$)의 변화에 독립적입니다.</li>\n<li>마지막으로 positive-definite한 값을 가집니다. 이렇기 때문에 steepest gradient에서 objective function의 방향을 알기 위해 사용한 방법과 같은 방법으로 natural gradient direction을 다음과 같이 구할 수 있는 것입니다.</li>\n</ol>\n<p>$$\\bar{\\nabla}\\eta(\\theta) \\equiv F(\\theta)^{-1}\\nabla\\eta(\\theta)$$</p>\n<p><br><br></p>\n<h1 id=\"4-The-Natural-Gradient-and-Policy-Iteration\"><a href=\"#4-The-Natural-Gradient-and-Policy-Iteration\" class=\"headerlink\" title=\"4. The Natural Gradient and Policy Iteration\"></a>4. The Natural Gradient and Policy Iteration</h1><p>4장에서는 Natural gradient를 통한 policy iteration을 수행하여 실제로 정책의 향상이 있는지를 증명합니다. 여기서 $Q^\\pi(s,a)$는 compatible function approximator $f^\\pi(s,a;w)$로 근사됩니다. (<a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/\">Sutton PG</a>를 참고해주시기 바랍니다.)</p>\n<p><br></p>\n<h2 id=\"4-1-Theorem-1-Compatible-Function-Approximation\"><a href=\"#4-1-Theorem-1-Compatible-Function-Approximation\" class=\"headerlink\" title=\"4.1 Theorem 1: Compatible Function Approximation\"></a>4.1 Theorem 1: Compatible Function Approximation</h2><p>approximate하는 함수 $f^{\\pi}(s,a;w)$는 다음과 같습니다.(compatible value function)</p>\n<p>$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$</p>\n<p>$$\\psi^{\\pi}(s,a) = \\nabla log\\pi(a;s,\\theta)$$</p>\n<p>여기서 $[\\nabla \\log\\pi(a;s,\\theta)]_i=\\partial \\log\\pi(a;s,\\theta)/\\partial\\theta_i$입니다. $w$는 원래 approximate하는 함수 $Q$와 $f$의 차이를 줄이도록 학습합니다(mean square error). 수렴한 local minima의 $w$를 $\\bar{w}$라고 가정 하겠습니다. 에러는 다음과 같은 수식으로 나타낼 수 있습니다.</p>\n<p>$$\\epsilon(w,\\pi)\\equiv\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)(f^{\\pi}(s,a;w)-Q^{\\pi}(s,a))^2$$</p>\n<p>그 때, $\\bar{\\omega} = \\bar{\\nabla}\\eta(\\theta)$이면 function approximator의 gradient 방향과 정책의 gradient 방향이 같다는 것을 의미합니다.</p>\n<p>아래의 내용은 위의 Theorem에 대한 증명입니다.</p>\n<p>위의 수식이 local minima이면 미분값이 0이 됩니다. $w$에 대해서 미분하면 다음과 같습니다.</p>\n<p>$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)(\\psi^{\\pi}(s,a)^T\\bar{w}-Q^{\\pi}(s,a))=0$$</p>\n<p>$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)^T\\bar{w}=\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)Q^{\\pi}(s,a)$$</p>\n<p>이 때, 위 식의 우변은 $\\psi$의 정의에 의해 policy gradient가 됩니다. 또한 왼쪽 항에서는 Fisher information matrix가 나옵니다.</p>\n<p>$$F(\\theta)=\\sum_{s,a}\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)=E_{\\rho^\\pi(s)}[F_s(\\theta)]$$</p>\n<p>따라서 다음과 같이 쓸 수 있습니다.</p>\n<p>$$F(\\theta)\\bar{w}=\\nabla\\eta(\\theta)$$</p>\n<p>$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>위의 수식은 natural gradient식과 동일합니다. 위의 수식은 policy가 update 될 때, value function approximator의 parameter 방향으로 이동한다는 것을 의미합니다. function approximation이 정확하다면 그 parameter의 natural policy gradient와 inner product가 커야합니다.</p>\n<p><br></p>\n<h2 id=\"4-2-Theorem-2-Greedy-Policy-Improvement\"><a href=\"#4-2-Theorem-2-Greedy-Policy-Improvement\" class=\"headerlink\" title=\"4.2 Theorem 2: Greedy Policy Improvement\"></a>4.2 Theorem 2: Greedy Policy Improvement</h2><p>이번 장에서는 natrual gradient는 다른 policy iteration 방법처럼 단순히 더 좋은 행동을 고르도록 학습하는 것이 아니라 가장 좋은(greedy) 행동을 고르도록 학습한다는 것을 증명하는 부분입니다. 이것을 일반적인 형태의 policy에 대해서 증명하기 전에 exponential 형태의 policy에 대해서 증명하는 것이 Theorem 2입니다. 특수한 정책을 가지는 상황안에서 학습속도($\\alpha$)를 무한대로 가져감으로서 어떤 action을 선택하는지를 알아봅니다.</p>\n<p>policy를 다음과 같이 정의합니다.</p>\n<p>$$\\pi(a;s,\\theta) \\propto \\exp(\\theta^T\\phi_{sa})$$</p>\n<p>여기서 $\\bar{\\nabla}\\eta(\\theta)$가 0이 아니고 $\\bar{w}$는 approximation error를 최소화된 $w$라고 가정합니다. 이 상태에서 natural gradient update를 생각해봅시다. 그리고 policy gradient는 gradient ascent임을 기억합시다.</p>\n<p>$$\\theta_{t+1}=\\theta_t + \\alpha_t\\bar{\\nabla}\\eta(\\theta)$$</p>\n<p>이 때 $\\alpha$가 learning rate로 parameter를 얼마나 업데이트하는지를 결정합니다. 이 값을 무한대로 늘렸을 때 policy가 어떻게 업데이트되는지 생각해봅시다.</p>\n<p>$$\\pi_{\\infty}(a;s)=lim_{\\alpha\\rightarrow\\infty}\\pi(a;s,\\theta+\\alpha\\bar{\\nabla}\\eta(\\theta))-(1)$$</p>\n<p>이 때,</p>\n<p>$$\\pi_{\\infty}=0 \\, \\, \\, if \\, and \\, only \\, if(필요충분조건) \\, \\, \\, a \\notin argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>이라고 말할 수 있습니다.</p>\n<p>아래의 내용은 위의 Theorem에 대한 증명입니다.</p>\n<p>먼저 function approximator는 앞서 다뤘듯이 아래와 같습니다.</p>\n<p>$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$</p>\n<p>여기서 function approximator는 Theorem 1에 의해 아래와 같이 쓸 수 있습니다.</p>\n<p>$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T\\psi^{\\pi}(s,a)$$</p>\n<p>$\\theta$의 정의에 의해 $\\psi$는 다음과 같습니다.</p>\n<p>$$\\psi^{\\pi}(s,a)=\\phi_{sa}-E_{\\pi(a’;s,\\theta)}[\\phi_{sa’}]$$</p>\n<p>따라서 function approximator는 다음과 같이 다시 쓸 수 있습니다.</p>\n<p>$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T(\\phi_{sa}-E_{\\pi(a’;s,\\theta)}[\\phi_{sa’}])$$</p>\n<p>greedy policy improvement가 Q function 값 중 가장 큰 값을 가지는 action을 선택하듯이 여기서도 function approximator의 값이 가장 큰 action을 선택하는 상황을 가정해봅시다. 이 때 function approximator의 argmax는 다음과 같이 쓸 수 있습니다.</p>\n<p>$$argmax_{a’}f^{\\pi}(s,a)=argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>(1) 식을 다시 살펴봅시다. 그러면 policy의 정의에 따라 다음과 같이 쓸 수 있습니다.</p>\n<p>$$\\pi(a;s,\\theta + \\alpha\\bar{\\nabla}\\eta(\\theta)) \\propto exp(\\theta^T\\phi_{sa} + \\alpha\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa})$$</p>\n<p>$\\bar{\\nabla}\\eta(\\theta) \\neq 0$이고 $\\alpha\\rightarrow\\infty$이면 exp안의 항 중에서 뒤의 항이 dominate하게 됩니다. 여러 행동 중에 $\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa}$가 가장 큰 행동이 있다면 이 행동의 policy probability가 1이 되고 나머지는 0이 됩니다. 따라서 다음이 성립합니다.</p>\n<p>$$\\pi_{\\infty}=0 \\, \\, \\, if \\, and \\, only \\, if \\, \\, \\, a \\notin argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>이 결과로부터 natural policy gradient는 단지 더 좋은 action이 아니라 best action을 고르도록 학습이 됩니다. 반면에 non-covariant gradient(1차미분)에서는 그저 더 좋은 action을 고르도록 학습이 됩니다. 이 natural policy gradient에 대한 결과는 infinite learning rate 세팅에서만 성립합니다. 좀 더 일반적인 경우에 대해서 살펴봅시다.</p>\n<p><br></p>\n<h2 id=\"4-3-Theorem-3-General-Parameterized-Policy\"><a href=\"#4-3-Theorem-3-General-Parameterized-Policy\" class=\"headerlink\" title=\"4.3 Theorem 3: General Parameterized Policy\"></a>4.3 Theorem 3: General Parameterized Policy</h2><p>Theorem 2에서와는 달리 일반적인 policy를 가정해봅시다(general parameterized policy). Theorem 3는 이 상황에서 natural gradient를 통한 업데이트가 best action를 고르는 방향으로 학습이 된다는 것을 보여줍니다.</p>\n<p>natural gradien에 따른 policy parameter의 업데이트는 다음과 같습니다. $\\bar{w}$는 approximation error를 minimize하는 $w$입니다.</p>\n<p>$$\\delta\\theta = \\theta’ - \\theta = \\alpha\\bar{\\nabla}\\eta(\\theta)=\\alpha\\bar{w}$$</p>\n<p>policy에 대해서 1차근사를 하면 다음과 같습니다.</p>\n<p>$$\\pi(a;s,\\theta’)=\\pi(a;s,\\theta)+\\frac{\\partial\\pi(a;s,\\theta)^T}{\\partial\\theta}\\delta\\theta + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\psi(s,a)^T\\delta\\theta) + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\alpha\\psi(s,a)^T\\bar{w}) + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\alpha f^{\\pi}(s,a;\\bar{w})) + O(\\delta\\theta^2)$$</p>\n<p>policy 자체가 function approximator의 크기대로 업데이트가 되므로 local하게 best action의 probability는 커지고 다른 probability의 크기는 작아질 것입니다. 하지만 만약 greedy improvement가 된다하더라도 그게 performance의 improvement를 보장하는 것은 아닙니다. 하지만 line search와 함께 사용할 경우 improvement를 보장할 수 있습니다. 왜 그런지는 처음에 말씀드린 블로그 <a href=\"http://darkpgmr.tistory.com/149\" target=\"_blank\" rel=\"noopener\">다크 프로그래머님의 블로그</a>를 참고해주시기 바랍니다.</p>\n<p><br><br></p>\n<h1 id=\"5-Metrics-and-Curvatures\"><a href=\"#5-Metrics-and-Curvatures\" class=\"headerlink\" title=\"5. Metrics and Curvatures\"></a>5. Metrics and Curvatures</h1><p>$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$</p>\n<p>이 파트에서는 FIM과 다른 metric 사이의 관계를 다룹니다.</p>\n<p>위의 식에 해당하는 G는 Fisher Information Matrix만 사용할 수 있는 것이 아닙니다. Positive-Definite Matrix인 FIM이외의 다른 Matrix도 사용할 수 있습니다. 또한 다양한 파라미터 추정에서 FIM은 Hessian Matrix에 수렴하지 않을 수 있다고 합니다. 이 말은 2nd order(2차 근사) 수렴이 보장되지 않는다는 말입니다.</p>\n<p>논문에 있는 내용들을 조금 더 추가적으로 보면 아래와 같이 나옵니다.</p>\n<p>In the different setting of parameter estimation, the Fisher information converges to the <code>Hessian</code>, so it is <a href=\"https://en.wikipedia.org/wiki/Efficiency_(statistics\" target=\"_blank\" rel=\"noopener\">asymptotically efficient</a>) 이지만,</p>\n<p>이 논문의 경우, 아마리 논문의 ‘blind separation case’와 유사한데 이 때는 꼭 asymtotically efficient하지 않다고 말합니다. 이 말은 즉 2nd order 수렴이 보장되지 않는다는 것이다.</p>\n<p><a href=\"http://www.inference.org.uk/mackay/ica.pdf\" target=\"_blank\" rel=\"noopener\">Mackay</a> 논문에서는 Hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했습니다. 그래서 performance를 2번 미분해보면 아래의 수식과 같습니다. 하지만 다음 식에서는 모든 항이 data dependent합니다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있습니다.</p>\n<p>$$\\nabla^2\\eta(\\theta)=\\sum_{sa}\\rho^{\\pi}(s)(\\nabla^2\\pi(a;s)Q^{\\pi}(s,a)+\\nabla\\pi(a;s)\\nabla Q^{\\pi}(s,a)^T+\\nabla Q^{\\pi}(s,a)\\nabla\\pi(a;s)^T)$$</p>\n<p>hessian은 보통 positive definite가 아닐수도 있습니다. 따라서 local maxima가 될 때까지 Hessian이 사용하기 별로 안좋습니다. 그리고 local maxima에서는 Hessian보다는 Conjugate methods가 더 효율적이라고 합니다.</p>\n<p>사실 이 파트에서는 무엇을 말하고 있는지 알기가 어렵습니다. FIM과 Hessian이 관련이 있다는 것을 알겠는데 asymtotically efficient와 같은 내용을 모르므로 내용의 이해가 어려웠습니다.</p>\n<p>Mackay 논문에서 해당 부분은 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/x4n6z6pdyi7xtb9/Screenshot%202018-06-10%2012.04.13.png?dl=1\"> </center>\n\n<p><br></p>\n<h2 id=\"5-1-Fisher-Information-Matrix-FIM-vs-Hessian\"><a href=\"#5-1-Fisher-Information-Matrix-FIM-vs-Hessian\" class=\"headerlink\" title=\"5.1 Fisher Information Matrix(FIM) vs. Hessian\"></a>5.1 Fisher Information Matrix(FIM) vs. Hessian</h2><p>FIM과 Hessian에 대해서 추가적인 설명을 하고자 합니다.</p>\n<ul>\n<li>FIM</li>\n</ul>\n<p>일단 정의되려면 space 자체가 stochastic한 성질을 가지고 있어야 합니다. 모든 parameter를 표현하는 함수가 deterministic한 함수가 아니라 확률로 나타낸 분포입니다. (probability distribution)</p>\n<p>결국은 FIM에서도 hessian처럼 비슷한 과정을 취하고 싶은 것입니다. 따라서 확률 변수이기 때문에 어떠한 특정 sample을 취하면 그게 항상 다른값이 됩니다. 그래서 FIM에다가 expectation을 취한 것입니다. expectation을 취함으로써 hessian같은 성질을 가집니다. 왜냐하면 expectation을 취함으로써 constant한 값이 되기 때문입니다.</p>\n<ul>\n<li>Hessian</li>\n</ul>\n<p>그냥 deterministic한 함수에서 정의됩니다. 그냥 함수를 두 번 미분 한 것이라고 보면 됩니다.</p>\n<p><br></p>\n<h2 id=\"5-2-Conjugate-Gradient-Method\"><a href=\"#5-2-Conjugate-Gradient-Method\" class=\"headerlink\" title=\"5.2 Conjugate Gradient Method\"></a>5.2 Conjugate Gradient Method</h2><p>추가적으로 Conjugate Gradient Method(CGM)에 대해서 다루고자 합니다.</p>\n<ul>\n<li>참고자료<ul>\n<li><a href=\"https://www.quora.com/What-is-an-intuitive-explanation-of-what-the-conjugate-gradient-method-is\" target=\"_blank\" rel=\"noopener\">https://www.quora.com/What-is-an-intuitive-explanation-of-what-the-conjugate-gradient-method-is</a></li>\n<li><a href=\"https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf\" target=\"_blank\" rel=\"noopener\">https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Conjugate_gradient_method\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Conjugate_gradient_method</a></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"mathbf-A-mathbf-x-mathbf-b-의-해-구하기\"><a href=\"#mathbf-A-mathbf-x-mathbf-b-의-해-구하기\" class=\"headerlink\" title=\"$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해 구하기\"></a>$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해 구하기</h3><p>$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해를 구하는 문제를 생각해봅시다. $\\mathbf{A}$의 역행렬을 구해서 양변에 곱해주면 $\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$가 되어 쉽게 해를 구할 수 있습니다. 하지만 $\\mathbf{A}^{-1}$은 계산이 많이 필요 자원소모가 큰 연산입니다. 역행렬을 취하지 않고 해를 구할 수 있는 방법이 있을까요?</p>\n<p>위의 방정식에서 $\\mathbf{b}$를 이항시키면 $\\mathbf{A}\\mathbf{x} - \\mathbf{b} = 0$이 됩니다. 최적화문제는 많은 경우 1차 미분이 0이 되는 지점이 해일 확률이 높습니다. $\\mathbf{A}\\mathbf{x} - \\mathbf{b} = 0$을 1차 미분으로 가지는 함수는 무엇일까요?</p>\n<p>$$f( \\mathbf{x} ) = \\frac{1}{2}\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x} - \\mathbf{x}^\\mathrm{T}\\mathbf{b}$$</p>\n<p>위의 함수는 $\\mathbf{A}\\mathbf{x} - \\mathbf{b}$를 1차 미분값으로 가집니다. 이 때 $\\mathbf{A}$는 symmetric positive definite해야 합니다.</p>\n<ul>\n<li>symmetric: $\\mathbf{A}=\\mathbf{A}^\\mathrm{T}$</li>\n<li>positive definite: $\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}&gt;0,\\quad\\forall\\mathbf{x}$ or all eigenvalues of $\\mathbf{A}$ are positive</li>\n</ul>\n<p>symmetric positive definite한 성질은 $\\mathbf{x}$를 strict convex function이 되게 만들어서 unique solution이 존재하게 합니다. $\\mathbf{A}$는 $\\mathbf{x}$의 Hessian이기도 합니다.</p>\n<p>자, 우리는 위의 방정식의 해 $\\mathbf{x}^*$를 iterative한 방식으로 찾고자 합니다. $\\mathbf{x}_0$을 초기값이라고 합시다.</p>\n<p>우리는 <img src=\"https://www.dropbox.com/s/g91yqajf72jzjoc/Screen%20Shot%202018-08-14%20at%208.43.28%20AM.png?dl=1\" width=\"80\">가 되는 <img src=\"https://www.dropbox.com/s/aj3j5fjtiyewlo1/Screen%20Shot%202018-08-14%20at%208.43.37%20AM.png?dl=1\" width=\"30\">를 찾고 싶은데 이를 위한 현재의 추정값은 $\\mathbf{A} \\mathbf{x}_0$입니다. 최적의 값을 찾았을 때와 비교하면 현재의 오차는 $\\mathbf{b} - \\mathbf{A}\\mathbf{x}_0 = \\mathbf{r}_0$입니다. 이것을 residual이라고 합니다. 어떻게 효율적으로 값을 변화시켜서 오차를 0으로 만들 수 있을까요? 일단 매 iteration마다 residual이 작아지는 방향으로 나아가야겠죠?</p>\n<p>가장 널리 알려진 방법은 gradient descent 방향입니다. gradient가 증가하면 반대로 가고 gradient가 감소하면 그 방향으로 가는 것입니다. 그리고 어떤 방향으로든 gradient가 0이면 그 지점에 멈추는 것이죠. 이 방법은 수렴은 하지만 zigzag하게 움직여서 느립니다. 이것보다 더 좋은 방법이 없을까요?</p>\n<h3 id=\"Conjugate-Gradient-Method\"><a href=\"#Conjugate-Gradient-Method\" class=\"headerlink\" title=\"Conjugate Gradient Method\"></a>Conjugate Gradient Method</h3><h4 id=\"Gram-Schmidt-orthgonalization\"><a href=\"#Gram-Schmidt-orthgonalization\" class=\"headerlink\" title=\"Gram-Schmidt orthgonalization\"></a>Gram-Schmidt orthgonalization</h4><p>다음과 같은 vector들의 집합 <img src=\"https://www.dropbox.com/s/csgdsfofe19q5r6/Screen%20Shot%202018-08-14%20at%208.56.04%20AM.png?dl=1\" width=\"135\">이 있다고 해봅시다. 이 vector들이 서로 linearly independent하다면 이 vector들은 $\\mathbb{R}^n$ 공간 상의 basis입니다. 이 vector들을 이용해서 orthogonal한 vector들을 만들어보겠습니다. 새로운 vector들을 <img src=\"https://www.dropbox.com/s/7xg7pciar1ndu44/Screen%20Shot%202018-08-14%20at%208.58.25%20AM.png?dl=1\" width=\"130\">라고 한다면 아래와 같은 과정을 통해 만들 수 있습니다. 이 방법을 Gram-Schmidt orthogonalization이라고 합니다.</p>\n<ul>\n<li>$\\mathbf{d}_1=\\mathbf{v}_1$</li>\n<li>$\\mathbf{d}_2=\\mathbf{v}_2 - \\left\\langle\\mathbf{v}_2,\\frac{\\mathbf{d}_1}{\\parallel\\mathbf{d}_1\\parallel}\\right\\rangle\\frac{\\mathbf{d}_1}{\\parallel\\mathbf{d}_1\\parallel}$<br>…</li>\n<li><img src=\"https://www.dropbox.com/s/arzc5cez8az0j7h/Screen%20Shot%202018-08-14%20at%209.00.04%20AM.png?dl=1\" width=\"290\"></li>\n</ul>\n<p>간단히 설명을 하면, 첫 vector는 basis와 같은 방향으로 출발합니다. 그 다음 vector는 그 다음 basis를 이전 vector로 projection한 다음 해당 basis로부터 빼줍니다. 그 다음 vector를 만들 때는 이전에 만든 모든 vector들에 대해서 projection을 취한 다음 모두 더한 것을 해당 basis에서 빼줍니다. </p>\n<h4 id=\"A-conjugate\"><a href=\"#A-conjugate\" class=\"headerlink\" title=\"$A$-conjugate\"></a>$A$-conjugate</h4><p>이 방법을 이용하여 matrix $A$에 관하여 orthogonal한 새로운 vector들의 집합을 만들어보겠습니다. 일반적인 inner product와 약간 다른 다음과 같은 inner product를 정의할 수 있습니다.</p>\n<p>$$&lt;\\mathbf{x},\\mathbf{y}&gt;_\\mathbf{A} = \\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{y}$$</p>\n<p>이 때 $&lt;\\mathbf{x},\\mathbf{y}&gt;_\\mathbf{A} = 0$이면 $\\mathbf{x}와 \\mathbf{y}$는 $\\mathbf{A}$-orthogonal 또는 $\\mathbf{A}$-conjugate하다라고 합니다. vector norm $\\parallel\\cdot\\parallel$도 일반적인 vector norm이 아닌 A-norm을 다음과 같이 정의합니다.</p>\n<p>$$\\parallel\\mathbf{x}\\parallel_\\mathbf{A}^2=\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}$$</p>\n<p>Gram-Schmidt orthogonalization을 또 이용해볼까요? 아래와 같습니다.</p>\n<ul>\n<li>$\\mathbf{d}_1=\\mathbf{v}_1$</li>\n<li><img src=\"https://www.dropbox.com/s/i8wnvoccefgg7t8/Screen%20Shot%202018-08-14%20at%209.08.42%20AM.png?dl=1\" width=\"400\"><br>…</li>\n<li><img src=\"https://www.dropbox.com/s/045x2v5sx7s0wot/Screen%20Shot%202018-08-14%20at%209.09.03%20AM.png?dl=1\" width=\"500\"></li>\n</ul>\n<p>그런데 이 Gram-Schmidt 방법은 단점이 하나 있습니다. 새로운 vector를 만들어내기 위해서 이전에 만든 vector들을 모두 가지고 있어야 하고 projection도 다시 계산해야 한다는 점입니다. 그런데 마법같은 이유($\\approx$ 여기서 설명하지 않는 이유)로 인해서 오직 마지막으로 만들어낸 vector만 가지고 있어도 기존의 모든 vector들이 성질을 표현해낼 수 있습니다. 그러면 우리가 만들어낸 vector는 다음과 같이 간단해집니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/wlyu7ax6qt0aw9w/Screen%20Shot%202018-08-14%20at%209.09.43%20AM.png?dl=1\" width=\"550\"> </center>\n\n<p>이 방향이 우리가 업데이트시키고 싶은 방향입니다. 즉 우리는 다음 수식처럼 움직이고자 합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/053o4ocvxsgxfl9/Screen%20Shot%202018-08-14%20at%209.09.56%20AM.png?dl=1\" width=\"180\"> </center>\n\n<p>$d_k$는 방향이고 $\\alpha_{k}$는 step size입니다. 이러한 방법을 일반적으로 line search method라고 부릅니다. 방향은 위에서 구한 conjugate 방향을 이용합니다. 그래서 이 line search를 conjugate gradient method라고 부릅니다. 이 방법의 한가지 중요한 장점은 업데이트가 무조건 n번만 일어난다는 것입니다! n-dimensional space에서는 basis가 n개 밖에 없거든요. 그렇다면 얼마만큼 많이 이 방향으로 움직여야 할까요? 더 이상 $f(\\mathbf{x})$가 감소하지 않을 때까지 움직이는게 좋지 않을까요? 이것은 다시 말하면 gradient가 0이 될 때까지 움직인다는 뜻입니다. 위에서 설명한 residual과도 관계있을 것 같지 않나요?</p>\n<p>$\\nabla f(\\mathbf{x})=\\mathbf{A}\\mathbf{x} - \\mathbf{b}$에 $x_k + \\alpha_{k} d_k$를 대입해 봅시다. 아래와 같이 step size를 구할 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/xikpfgcix64ngmp/Screen%20Shot%202018-08-14%20at%209.13.20%20AM.png?dl=1\" width=\"300\"> </center>\n\n<p>네, 이제 우리는 방향과 step size를 모두 알았습니다. 이제 update만 하면 해를 찾을 수 있겠군요!</p>\n<center> <img src=\"https://www.dropbox.com/s/avjw4kbzd4ormv5/conjugate_gradient_wikipedia.png?dl=1\" width=\"200\"> </center>\n\n<p>위키피디아에서 가져온 위의 그림을 봅시다. 그림의 녹색선이 gradient descent이고 빨간선이 conjugate gradient입니다. conjugate gradient는 gradient descent보다 빨리 수렴합니다. 녹색선은 항상 이전 이동 방향과 직각으로 이동합니다. 빨간선은 보다 빠르게 더 낮은 값이 있는 곳으로 이동합니다.</p>\n<p>위에서 설명을 생략했지만 왜 conjugate gradient는 빠르게 수렴(n-step)하며 Gram-Schmidt orthogonalization을 할 때도 메모리가 적게 필요할까요? 수학적으로 엄밀하게 설명하기 보다는 개념적으로 설명을 하겠습니다. 그 비밀은 바로 $\\mathbf{A}$-orthogonal 또는 $\\mathbf{A}$-conjugate vector들을 이용한데 있습니다. 이 vector들은 basis라고 했습니다. 즉, 처음에 우리가 구하고자 했던 해 $\\mathbf{x}^*$를 이 vector들을 이용해서 표현할 수 있습니다. 또한 iterative하게 찾아가기 위한 초기 vector $\\mathbf{x}_0$도 이 vector들로 표현할 수 있습니다. 그렇다면 이 둘 간의 오차도 이 vector들도 표현할 수 있게 됩니다. 다음 수식처럼 말입니다.</p>\n<p>$$\\mathbf{x}^* - \\mathbf{x}_0 = \\epsilon_1\\mathbf{d}_1 + \\epsilon_2\\mathbf{d}_2 + \\cdots + \\epsilon_n\\mathbf{d}_n$$</p>\n<p>우리의 목표는 이 오차를 줄이는 것입니다. 각각의 basis마다 오차가 있으며 이들은 서로 independent합니다. 즉, 특정 iteration에서 특정 basis에 대한 오차를 0으로 만들면 다음 번 iteration에서는 이 오차는 영향을 받지 않습니다! 이러한 이유로 n번의 iteration만으로 해를 찾을 수 있고 다른 vector들을 저장할 필요도 없는 것입니다. 선형대수의 아름다움이 느껴지지 않으시나요? 오래되었지만 참 잘 디자인된 기법이라는 생각이 듭니다.</p>\n<p><br><br></p>\n<h1 id=\"6-Experiment\"><a href=\"#6-Experiment\" class=\"headerlink\" title=\"6. Experiment\"></a>6. Experiment</h1><p>이 논문에서는 natural gradient를 simple MDP와 tetris MDP에 대해서 실험을 진행했습니다. FIM은 다음과 같은 식으로 업데이트합니다.</p>\n<p>$$f\\leftarrow f+\\nabla \\log \\pi(a_t; s_t, \\theta)\\nabla \\log \\pi(a_t; s_t, \\theta)^T$$</p>\n<p>$T$ length trajectory에 대해서 $f/T$를 이용해 $F$의 기대값($E$)를 구합니다.</p>\n<p><br></p>\n<h2 id=\"6-1-LQR-Linear-Quadratic-Regulator\"><a href=\"#6-1-LQR-Linear-Quadratic-Regulator\" class=\"headerlink\" title=\"6.1 LQR(Linear Quadratic Regulator)\"></a>6.1 LQR(Linear Quadratic Regulator)</h2><p>Agent를 실험할 환경은 다음과 같은 dynamics를 가지고 있습니다.</p>\n<p>$x(t+1) = 0.7x(t)+u(t)+\\epsilon(t)$</p>\n<p>$u(t)$는 control 신호로서 에이전트의 행동입니다. $\\epsilon$은 noise distribution으로 환경에 가해지는 노이즈입니다. 에이전트의 목표는 적절한 $u(t)$를 통해 $x(t)$를 0으로 유지하는 것입니다. $x(t)$를 0으로 유지하기 위해서 필요한 소모값(cost)는 $x(t)^2$로 정의하며 cost를 최소화하도록 학습합니다. 이 논문에서는 실험할 때 복잡성을 더 해주기 위해 noise distribution인 $\\epsilon$을 dynamics에 추가하였습니다.</p>\n<p>이 실험에서 policy는 다음과 같이 설정하였습니다. 파라미터가 2개 밖에 없는 간단한 policy입니다.</p>\n<p>$\\pi(u;x,\\theta)\\propto \\exp(\\theta_1s_1x^2+\\theta_2s_2x)$</p>\n<p>이 policy를 간단히 그래프로 그려보면 다음과 같습니다. 가로축은 $x$, 세로축은 cost입니다. $\\theta_1$과 $\\theta_2$를 (0.5, 0.5), (1, 0), (0, 1)로 설정하고 $s_1$, $s_2$는, 1로 두었습니다. $x$는 -1~1사이의 범위로 그렸습니다. </p>\n<center> <img src=\"https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1\" width=\"500px\"> </center>\n\n<p>아래의 그림은 LQR을 학습한 그래프입니다. cost가 $x^2$이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있습니다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과입니다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 곡선입니다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습합니다(time 축이 log scale인 것을 감안하면 차이가 많이 납니다.). </p>\n<p>하지만 문제가 있습니다. NPG를 학습한 세 개의 곡선은 $\\theta$를 rescale 한 것입니다. $\\theta$앞에 곱해지는 숫자에 따라 학습의 과정이 다릅니다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것입니다. 즉, covariant gradient가 아니라는 뜻입니다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것입니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1\" width=\"300px\"> </center>\n\n<p>natural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문입니다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 $\\rho_s$가 곱해지기 때문입니다. 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것입니다!</p>\n<p><br></p>\n<h2 id=\"6-2-Simple-2-state-MDP\"><a href=\"#6-2-Simple-2-state-MDP\" class=\"headerlink\" title=\"6.2 Simple 2-state MDP\"></a>6.2 Simple 2-state MDP</h2><p>이제 다른 예제에서 NPG를 테스트합니다. 2개의 state만 가지는 MDP를 고려해봅시다. <a href=\"http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&amp;context=robotics\" target=\"_blank\" rel=\"noopener\">그림출처</a>. 그림으로보면 다음과 같습니다. $x=0$ 상태와 $x=1$ 상태 두 개가 존재합니다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있습니다. 상태 $x=0$에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 $x=1$에서 자기 자신으로 돌아오면 2의 보상을 받습니다. 결국 optimal policy는 상태 $x=1$에서 계속 자기 자신으로 돌아오는 행동을 취하는 것입니다. </p>\n<p><img src=\"https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1\"></p>\n<p>문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정합니다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것입니다.</p>\n<p>$$\\rho(x=0)=0.8,  \\rho(x=1)=0.2$$</p>\n<p>일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 됩니다. 이 때, $\\rho(s)$가 gradient에 곱해지므로 상대적으로 상태 0에서의 gradient 값이 커지게 됩니다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update합니다. 따라서 아래 그림의 첫번째 그림에서처럼 reward가 1에서 오랜 시간동안 머무릅니다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것입니다.</p>\n<p>$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$</p>\n<center><img src=\"https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1\" width=\"300px\"></center>\n\n<p>하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달합니다. gradient 방법이 $1.7\\times 10^7$정도의 시간만에 2에 도달한 반면 NPG는 2의 시간만에 도달합니다. 또한 $\\rho(x=0)$가 $10^{-5}$이하로 떨어지지 않습니다.</p>\n<p>한 가지 그래프를 더 살펴봅시다. 다음 그래프는 parameter $\\theta$가 업데이트 되는 과정을 보여줍니다. 이 그래프에서는 parameter가 2개 있습니다. 일반적인 gradient가 아래 그래프에서 실선에 해당합니다. 이 실선의 그래프는 보면 처음부터 중반까지 $\\theta_i$만 거의 업데이트하는 것을 볼 수 있습니다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있습니다. </p>\n<center><img src=\"https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1\" width=\"300px\"></center>\n\n<p>policy가 $\\pi(a;s,\\theta)\\propto \\exp(\\theta_{sa})$일 때, 다음과 같이 $F_{-1}$이 gradient 앞에 weight로 곱해지는데 이게 $\\rho$와는 달리 각 parameter에 대해 균등합니다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것입니다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$</p>\n<p><br></p>\n<h2 id=\"6-3-Tetris\"><a href=\"#6-3-Tetris\" class=\"headerlink\" title=\"6.3 Tetris\"></a>6.3 Tetris</h2><p>NPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어 있습니다. 다음 그림은 tetris 예제를 보여줍니다. 보통 그림에서와 같이 state의 feature를 정해줍니다. <a href=\"http://slideplayer.com/slide/5215520/\" target=\"_blank\" rel=\"noopener\">그림 출처</a></p>\n<p><img src=\"https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1\"></p>\n<p>이 예제에서도 exponential family로 policy를 표현합니다. $\\pi(a;s,\\theta) \\propto \\exp(\\theta^T\\phi_{sa})$ 로 표현합니다.</p>\n<p>tetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있습니다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우입니다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법입니다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷합니다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지합니다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있습니다.</p>\n<p><img src=\"https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1\"></p>\n<p><br><br></p>\n<h1 id=\"7-Discussion\"><a href=\"#7-Discussion\" class=\"headerlink\" title=\"7. Discussion\"></a>7. Discussion</h1><p>Natural Gradient Method는  Policy Iteration에서와 같이 Greedy Action을 선택하도록 학습합니다. Line search 기법과 함께 사용하면 더 Policy Iteration과 비슷해집니다. Greedy Policy Iteration에서와 비교하면 Performance Improvement가 보장됩니다. 하지만 FIM이 asymtotically Hessian으로 수렴하지 않습니다. 그렇기 때문에 Conjugate Gradient Method로 구하는 방법이 더 좋을수 있습니다.</p>\n<p>살펴봤듯이 본 논문의 NPG는 완벽하지 않습니다. 위의 몇가지 문제점을 극복하기 위한 추가적인 연구가 필요하다고 할 수 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"DDPG-여행하기\"><a href=\"#DDPG-여행하기\" class=\"headerlink\" title=\"DDPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/\">DDPG 여행하기</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"NPG-Code\"><a href=\"#NPG-Code\" class=\"headerlink\" title=\"NPG Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py\" target=\"_blank\" rel=\"noopener\">NPG Code</a></h2><h2 id=\"TRPO-여행하기\"><a href=\"#TRPO-여행하기\" class=\"headerlink\" title=\"TRPO 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/\">TRPO 여행하기</a></h2>"},{"title":"Maximum Entropy Inverse Reinforcement Learning","date":"2019-02-09T15:00:00.000Z","author":"이승현","subtitle":"Inverse RL 4번째 논문","_content":"\n<center> <img src=\"../../../../img/irl/maxent_1.png\" width=\"850\"> </center>\n\nAuthor : Brian D. Ziebart, Andrew Maas, J.Andrew Bagnell, Anind K. Dey\nPaper Link : http://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf\nProceeding : Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence 2008\n\n---\n\n# 0. Abstract\n\n이 논문이 발표된 2002년 당시에도 많은 연구자들이 objective function의 gradient 값을 따라서 좋은 policy $\\pi$를 찾고자 하였습니다. 하지만 기존의 우리가 알던 gradient descent method는 steepest descent direction이 아닐 수 있기 때문에(쉽게 말해 가장 가파른 방향을 따라서 내려가야 하는데 그러지 못할 수도 있다는 것입니다.) 이 논문에서 steepest descent direction를 나타내는 natural gradient method를 policy gradient에 적용하여 좋은 policy $\\pi$를 찾습니다. \n\n<br>\n## 1.1 NPG 흐름 잡기\n\n### 1.1.1 매니폴드(manifold)\n\n<center> <img src=\"https://www.dropbox.com/s/cstjgemqpby4ysr/Screen%20Shot%202018-08-12%20at%208.28.29%20PM.png?dl=1\" width=\"300\"> </center>\n\n매니폴드는 간단하게 말해 위의 그림에서의 점들을 아우르는 subspace입니다. 그래서 NPG 공부하기 전에 매니폴드를 왜 배울까라는 의문이 들으실텐데 그 이유는 위에서도 언급했듯이 natural gradient method는 어떠한 파라미터 공간에서의 steepest descent direction을 강조하기 때문입니다. 여기서 파라미터 공간이 바로 리만 매니폴드입니다. 리만 매니폴드는 매니폴드가 각지지 않고 미분가능하게 부드럽게 곡률을 가진 면이라고 생각하면 편합니다.\n\nNeural Network(NN)을 사용할 경우 NN의 parameter space가 우리가 보통 생각하는 직선으로 쭉쭉 뻗어있는 유클리디안 공간(Euclidean space)가 아닙니다. 좀 더 일반적으로는 구의 표면과 같이 휘어져있는 공간 즉, 리만 공간(Riemannian space)로 표현할 수 있습니다. 다음 그림들을 보겠습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/jnlm6he18ar64yc/Screen%20Shot%202018-08-12%20at%208.14.13%20PM.png?dl=1\" width=\"400\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/tufjqmaeaz29kaz/Screen%20Shot%202018-08-12%20at%208.15.53%20PM.png?dl=1\" width=\"400\"> </center>\n\n아래의 그림처럼 어떠한 확률 분포가 있다고 해봅시다.\n\n<center> <img src=\"https://www.dropbox.com/s/mpoop19eyu1vp0a/Screen%20Shot%202018-08-13%20at%2012.52.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n어떠한 공간의 확률 분포에 있는 한 점은 다른 공간의 확률 분포에서의 한 점이 될 것입니다. 이렇게 위의 그림들과 같이 유클리디안 공간의 확률 분포에서의 한 점이 리만 공간의 확률 분포의 한 점이 되는 것이고, 곡률의 일차 근사가 유클리디안 공간에서는 일차 근사가 아닌 이차 근사이고, 리만 공간에서는 휘어진 공간이기 때문에 곡률을 일차 근사라고 보는 것입니다. 추가적으로 일차 근사와 이차 근사의 차이는 [다크 프로그래머님의 블로그](http://darkpgmr.tistory.com/149)를 참고해주시기 바랍니다. (꼭 보세요!) \n\n일차 근사와 이차 근사의 차이점과 각각의 장단점, 그리고 추가적으로 Line Search까지 알고 난 후에 이 논문을 보시는 것을 권장해드립니다.\n\n### 1.1.2 Natural Gradient + Policy Gradient\n\n먼저 아래의 그림들을 보여드리겠습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/65hra43zadsubff/Screen%20Shot%202018-08-13%20at%2012.51.45%20PM.png?dl=1\" width=\"500\"> <img src=\"https://www.dropbox.com/s/8g332zceeordqwv/Screen%20Shot%202018-08-13%20at%2012.51.51%20PM.png?dl=1\" width=\"400\"> </center>\n\n위에서도 언급했듯이 natural gradient가 steepest direction이 된다는 연구가 이뤄지고 있었습니다. 강화학습의 policy gradient은 objective function의 gradient를 따라 policy를 업데이트를 합니다. 이 때 policy는 parameterization되는데 이 경우에도 gradient 대신에 natural gradient가 좋다는 것을 실험해보는 논문이 지금 다루고 있는 논문입니다.\n\ngradient가 non-covariant(1차 근사)해서 생기는 문제는 간단히 말하자면 다음과 같습니다. policy가 parameterized된 상황에서는 같은 policy라도 다른 parameter를 가질 수 있습니다. 이 때, steepest direction은 두 경우에 같은 방향을 가리켜야 하는데 non-covariant한 경우 그렇지 못합니다. 이러한 부분이 바로 결국 느린 학습으로 연결이 되는 것입니다.\n\n논문에서 2차미분 방법론들과 짧게 비교합니다. 하지만 개인적인 의견으로 2차미분을 이용한 다른 방법들과의 비교가 생각보다 없는 점이 부족해 보였습니다.(Hessian을 이용한다거나 conjugate gradient method를 이용한다거나). 실험을 통해 Fisher Information Matrix(FIM)가 hessian에 수렴안하는 거라던지 Hessian 방법론이 local maxima 부근에서 상당히 느리다던지의 결과를 보여줬었으면 좋았을 것 같습니다.\n\n또한 natural gradient의 단점으로 natural gradient 만으로 업데이트하면 policy의 improvement 보장이 안될 수 있습니다. policy의 improvement를 보장하기 위해 line search도 써야하는데 line search를 어떻게 쓰는지에 대한 자세한 언급이 없습니다. 다시 말해 자세한 algorithm 설명이 없다는 것입니다.\n\nnatural policy gradient 논문은 natural gradient + policy gradient를 처음 적용했다는데 의의가 있습니다. 하지만 이 논문이 문제 삼은 gradient는 non-covariant하다라는 문제를 natural gradient를 통해 해결하지는 못했습니다(Experiment를 통해 covariant gradient가 되지 못했다는 것이 보입니다). NPG의 뒤를 잇는 논문이 “covariant policy search”와 “natural actor-critic”에서 covariant(2차 근사)하지 못하다는 것을 해결하기 위해 Fisher Information Matrix를 sample 하나 하나에 대해서 구하는 것이 아니라 trajectory 전체에 대해서 구합니다.\n\n또한 논문은 pg의 두 가지 세팅 중에 average-reward setting(infinite horizon)에서만 NPG를 다룹니다. “covariant policy search” 논문에서는 average-reward setting과 start-state setting 모두에 대해서 npg를 적용합니다.\n\nnatural gradient + policy gradient를 처음 제시했다는 것은 좋지만 npg 학습의 과정을 자세하게 설명하지 않았고 다른 2차 미분 방법들과 비교를 많이 하지 않은 점이 아쉬운 논문이었습니다.\n\n<br><br>\n\n# 2. Introduction\n\n소개는 앞에서 다 했기 때문에 간략하게 다시 한 번 정리하겠습니다. direct policy gradient method는 future reward의 gradient를 따라 policy를 update합니다. 하지만 gradient descent는 non-covariant입니다. 따라서 이 논문에서는 covarient gradient를 제시합니다. 바로 \"Natural Gradient\" 입니다. \n\n또한 natural gradient와 policy iteration의 연관성을 설명합니다. natural policy gradient is moving toward choosing a greedy optimal action (이런 연결점은 아마도 step-size를 덜 신경쓰고 싶어서 그런게 아닌가 싶습니다)\n\n논문의 Introduction 부분에 다음 멘트가 있습니다. 이 글만 봐서는 이해가 안갔는데 Mackay 논문에 좀 더 자세히 나와있었습니다.\n<center> <img src=\"https://www.dropbox.com/s/41xhhr7lgfk24a1/Screenshot%202018-06-10%2011.45.18.png?dl=1\"> </center>\n\n[Mackay](http://www.inference.org.uk/mackay/ica.pdf)논문에서는 다음과 같이 언급하고 있습니다. Back-propagation을 사용할 경우에 learning rate를 dimension에 1/n로 사용하면 수렴한다는 것이 증명됐습니다. 하지만 너무 느리다고 합니다.\n<center> <img src=\"https://www.dropbox.com/s/us9ezc7vxgrkez6/Screenshot%202018-06-10%2011.47.21.png?dl=1\"> </center>\n\n<br><br>\n\n# 3. A Natural Gradient\n\n<br>\n## 3.1 Notation\n\n이 논문에서 제시하는 Notation은 다음과 같습니다.\n\n- MDP : tuple $(S, s_0, A, R, P)$\n- $S$ : a finite set of states\n- $s_0$ : a start state\n- $A$ : a finite set of actions\n- $R$ : reward function $R: S \\times A -> [0, R_{max}]$\n- $\\pi(a;s, \\theta)$ : stochastic policy parameterized by $\\theta$\n- 모든 정책 $\\pi$는 ergodic : stationary distribution $\\rho^{\\pi}$이 잘 정의되어 있다고 봅니다.\n- 이 논문에서는 sutton의 pg 논문의 두 세팅(start-state formulation, average-reward formulation) 중에 두 번째인 average-reward formulation을 가정합니다.\n- performance or average reward : $\\eta(\\pi)=\\sum_{s,a}\\rho^{\\pi}(s)\\pi(a;s)R(s,a)$\n- state-action value : $Q^{\\pi}(s,a)=E_{\\pi}[\\sum_{t=0}^{\\infty}R(s_t, a_t)-\\eta(\\pi)\\vert s_0=s, a_0=a]$ ([Sutton PG](https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/) 를 참고해주시기 바랍니다.)\n- 정책이 $\\theta$로 parameterize되어있으므로 performance는 $\\eta(\\pi_{\\theta})$인데 $\\eta(\\theta)$로 씁니다.\n\n<br>\n## 3.2 Natural Gradient\n\nSutton PG 논문의 policy gradient theorem에 따라 exact gradient of the average reward는 다음과 같습니다. 다음 수식이 어떻게 유도되었는지, 어떤 의미인지 모른다면 [Sutton PG](https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/)을 통해 제대로 이해하는 것이 좋습니다. Euclidean space에서는 objective function의 일반적인 policy gradient는 Sutton PG에서 논의한 바와 같이 다음과 같이 구할 수 있습니다.\n\n$$\\nabla\\eta(\\pi_\\theta) = \\Sigma_{s,a}\\rho^\\pi(s)\\nabla\\pi(a;s,\\theta)Q^\\pi(s,a)$$\n\n여기서 steepest descent direction of $\\eta(\\theta)$는 $\\eta(\\theta + d\\theta)$를 최소화하는 $d\\theta$로 정의됩니다. 이 때, $\\vert d\\theta \\vert^2$가 일정 크기 이하인 것으로 제약조건을 주는데(held to small constant) Euclidian space에서는 $\\eta(\\theta)$가 steepest direction이지만 Riemannian space에서는 natural gradient가 steepest direction입니다.\n\n추가적으로 결국 우리가 원하는 건 ‘policy 최적화를 좀 더 스마트하게 해 보자!’입니다. Policy 최적화를 잘한다는 것은 Policy를 어느 방향으로 얼만큼 업데이트 하느냐에 따라 달라집니다.\n\n### 3.2.1 Natural gradient 증명\n\nRiemannian space에서 거리는 다음과 같이 정의됩니다. 여기서 $G(\\theta)$는 특정한 양수로 이루어진 matrix입니다. 자세한 내용은 [양의 정부호 행렬(positive definite matrix)이란?](http://bskyvision.com/205)을 참고해주시기 바랍니다.\n\n$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$\n\n이 수식은 [Natural gradient works in efficiently in learning](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&rep=rep1&type=pdf) 논문에서 증명되어 있습니다. 다음은 natural gradient 증명입니다.\n\nsteepest direction을 구할 때 $\\theta$의 크기를 제약조건으로 줍니다. 제약조건은 다음과 같습니다.\n\n$$\\vert d\\theta \\vert^2 = \\epsilon^2$$\n\n그리고 steepest vector인 $d\\theta$는 다음과 같이 정의할 수 있습니다.\n\n$$d\\theta = \\epsilon a$$\n\n$$\\vert a \\vert^2=a^TG(\\theta)a = 1$$\n\n이 때, $a$가 steepest direction unit vector이 되려면 다음 수식을 최소로 만들어야 합니다.\n\n$$\\eta(\\theta + d\\theta) = \\eta(\\theta) + \\epsilon\\nabla\\eta(\\theta)^Ta$$\n\n위 수식이 제약조건 아래 최소가 되는 $a$를 구하기 위해 Lagrangian method를 사용합니다. Lagrangian method를 모른다면 [위키피디아](https://en.wikipedia.org/wiki/Lagrange_multiplier)를 참고하는 것을 추천합니다. 위 수식이 최소라는 것은 $\\nabla\\eta(\\theta)^Ta$가 최소라는 것입니다.\n\n$$\\frac{\\partial}{\\partial a_i}(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$$\n\n따라서 $(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$는 상수입니다. 상수를 미분하면 0이므로 이 식을 $a$로 미분합니다. 그러면 다음과 같이 steepest direction을 구한 것입니다.\n\n$$\\nabla\\eta(\\theta) = 2 \\lambda G(\\theta)a$$\n\n$$a=\\frac{1}{2\\lambda}G^{-1}\\nabla\\eta(\\theta)$$\n\n이 때, 다음 식을 natural gradient라고 정의합니다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = G^{-1}\\nabla\\eta(\\theta)$$\n\nnatural gradient를 이용한 업데이트는 다음과 같습니다.\n\n$$\\theta_{t+1}=\\theta_t - \\alpha_tG^{-1}\\nabla\\eta(\\theta)$$\n\n여기까지는 natural gradient의 증명이었습니다. 이 natural gradient를 policy gradient에 적용한 것이 natural policy gradient입니다. natural policy gradient는 다음과 같이 정의됩니다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$\n\n여기서 $G$대신 $F$를 사용했는데 $F$는 Fisher information matix입니다. 수식은 다음과 같습니다.\n\n$$F(\\theta) = E_{\\rho^\\pi(s)}[F_s(\\theta)]$$\n\n$$F_s(\\theta)=E_{\\pi(a;s,\\theta)}[\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial \\theta_i}\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial\\theta_j}]$$\n\n### 3.2.2 Fisher Information Matrix에 정의된 Metric\n\n추가적으로 Fisher Information Matrix(FIM)에 대해서 설명하겠습니다. 위의 문제를 우리가 생각하기 쉬운 Neural Network(NN)으로 구성하고 해결할 수 있다고 해봅시다. NN은 여러가지 parameter set들로 구성될 수 있습니다. 게다가 다른 parameter set을 가지지만 같은 policy를 가질 수도 있습니다. 이 경우 steepest direction은 같은 policy이기 때문에 같은 방향을 가리키고 있어야 하는데 non-covariant한 경우 그렇지 못합니다. 떄문에 학습이 느려지고, 이러한 문제를 해결하기 위해 단순히 Positive-Definite Matrix $G(\\theta)$를 사용하지 않고 FIM $F_s(\\theta)$를 사용합니다. 어떠한 확률 변수 $X$가 임의의 매개변수 $\\theta$에 의해 정의되는 분포를 따른다고 하면 $X=x$일때 FIM은 다음과 같이 정의됩니다.\n\n$$F_x(\\theta)=E\\left[\\left(\\dfrac{\\partial}{\\partial\\theta}\\log\\Pr(x|\\theta)\\right)^2\\right]$$\n\n강화학습 관점에서 생각해보면 정보 $x$는 에피소드에 의해 관측된 상태값 $s$이며 매개변수 $\\theta$에 의해 선택될 수 있는 행동에 대한 분포가 나오게 됩니다. 이에 의해 위의 FIM는 다음과 같이 표현할 수 있습니다.\n\n$$F_s(\\theta) \\equiv E_{\\pi(a;s,\\theta)}\\left[\\left(\\dfrac{\\partial}{\\partial\\theta}\\log\\pi(a;s,\\theta)\\right)^2\\right] =E_{\\pi(a;s,\\theta)}\\left[\\dfrac{\\partial \\log\\pi(a;s,\\theta)}{\\partial \\theta_i}\\dfrac{\\partial \\log\\pi(a;s,\\theta)}{\\partial\\theta_j}\\right]$$\n\n그리고 위의 식들을 이용하여 objective function을 정리하면 아래의 식과 같이 표현됩니다.\n\n$$F(\\theta) = E_{\\rho^{\\pi}(s)}[F_s(\\theta)]$$\n\n또한 이 Fisher Information Matrix에 정의된 이 metric은 다음과 같은 성질을 가지고 있습니다.\n\n1. 업데이트되는 파라미터에 의해 구성되는 매니폴드에 기반한 metric입니다.\n2. 확률분포($\\pi(a;s,\\theta)$)를 구성하는 파라미터($\\theta$)의 변화에 독립적입니다.\n3. 마지막으로 positive-definite한 값을 가집니다. 이렇기 때문에 steepest gradient에서 objective function의 방향을 알기 위해 사용한 방법과 같은 방법으로 natural gradient direction을 다음과 같이 구할 수 있는 것입니다.\n\n$$\\bar{\\nabla}\\eta(\\theta) \\equiv F(\\theta)^{-1}\\nabla\\eta(\\theta)$$\n\n<br><br>\n\n# 4. The Natural Gradient and Policy Iteration\n\n4장에서는 Natural gradient를 통한 policy iteration을 수행하여 실제로 정책의 향상이 있는지를 증명합니다. 여기서 $Q^\\pi(s,a)$는 compatible function approximator $f^\\pi(s,a;w)$로 근사됩니다. ([Sutton PG](https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/)를 참고해주시기 바랍니다.)\n\n<br>\n## 4.1 Theorem 1: Compatible Function Approximation\n\napproximate하는 함수 $f^{\\pi}(s,a;w)$는 다음과 같습니다.(compatible value function)\n\n$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$\n\n$$\\psi^{\\pi}(s,a) = \\nabla log\\pi(a;s,\\theta)$$\n\n여기서 $[\\nabla \\log\\pi(a;s,\\theta)]_i=\\partial \\log\\pi(a;s,\\theta)/\\partial\\theta_i$입니다. $w$는 원래 approximate하는 함수 $Q$와 $f$의 차이를 줄이도록 학습합니다(mean square error). 수렴한 local minima의 $w$를 $\\bar{w}$라고 가정 하겠습니다. 에러는 다음과 같은 수식으로 나타낼 수 있습니다.\n\n$$\\epsilon(w,\\pi)\\equiv\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)(f^{\\pi}(s,a;w)-Q^{\\pi}(s,a))^2$$\n\n그 때, $\\bar{\\omega} = \\bar{\\nabla}\\eta(\\theta)$이면 function approximator의 gradient 방향과 정책의 gradient 방향이 같다는 것을 의미합니다.\n\n아래의 내용은 위의 Theorem에 대한 증명입니다.\n\n위의 수식이 local minima이면 미분값이 0이 됩니다. $w$에 대해서 미분하면 다음과 같습니다.\n\n$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)(\\psi^{\\pi}(s,a)^T\\bar{w}-Q^{\\pi}(s,a))=0$$\n\n$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)^T\\bar{w}=\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)Q^{\\pi}(s,a)$$\n\n이 때, 위 식의 우변은 $\\psi$의 정의에 의해 policy gradient가 됩니다. 또한 왼쪽 항에서는 Fisher information matrix가 나옵니다.\n\n$$F(\\theta)=\\sum_{s,a}\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)=E_{\\rho^\\pi(s)}[F_s(\\theta)]$$\n\n따라서 다음과 같이 쓸 수 있습니다.\n\n$$F(\\theta)\\bar{w}=\\nabla\\eta(\\theta)$$\n\n$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$\n\n위의 수식은 natural gradient식과 동일합니다. 위의 수식은 policy가 update 될 때, value function approximator의 parameter 방향으로 이동한다는 것을 의미합니다. function approximation이 정확하다면 그 parameter의 natural policy gradient와 inner product가 커야합니다.\n\n<br>\n## 4.2 Theorem 2: Greedy Policy Improvement\n\n이번 장에서는 natrual gradient는 다른 policy iteration 방법처럼 단순히 더 좋은 행동을 고르도록 학습하는 것이 아니라 가장 좋은(greedy) 행동을 고르도록 학습한다는 것을 증명하는 부분입니다. 이것을 일반적인 형태의 policy에 대해서 증명하기 전에 exponential 형태의 policy에 대해서 증명하는 것이 Theorem 2입니다. 특수한 정책을 가지는 상황안에서 학습속도($\\alpha$)를 무한대로 가져감으로서 어떤 action을 선택하는지를 알아봅니다.\n\npolicy를 다음과 같이 정의합니다.\n\n$$\\pi(a;s,\\theta) \\propto \\exp(\\theta^T\\phi_{sa})$$\n\n여기서 $\\bar{\\nabla}\\eta(\\theta)$가 0이 아니고 $\\bar{w}$는 approximation error를 최소화된 $w$라고 가정합니다. 이 상태에서 natural gradient update를 생각해봅시다. 그리고 policy gradient는 gradient ascent임을 기억합시다.\n\n$$\\theta_{t+1}=\\theta_t + \\alpha_t\\bar{\\nabla}\\eta(\\theta)$$\n\n이 때 $\\alpha$가 learning rate로 parameter를 얼마나 업데이트하는지를 결정합니다. 이 값을 무한대로 늘렸을 때 policy가 어떻게 업데이트되는지 생각해봅시다.\n\n$$\\pi_{\\infty}(a;s)=lim_{\\alpha\\rightarrow\\infty}\\pi(a;s,\\theta+\\alpha\\bar{\\nabla}\\eta(\\theta))-(1)$$\n\n이 때,\n\n$$\\pi_{\\infty}=0 \\, \\, \\, if \\, and \\, only \\, if(필요충분조건) \\, \\, \\, a \\notin argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n이라고 말할 수 있습니다.\n\n아래의 내용은 위의 Theorem에 대한 증명입니다.\n\n먼저 function approximator는 앞서 다뤘듯이 아래와 같습니다.\n\n$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$\n\n여기서 function approximator는 Theorem 1에 의해 아래와 같이 쓸 수 있습니다.\n\n$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T\\psi^{\\pi}(s,a)$$\n\n$\\theta$의 정의에 의해 $\\psi$는 다음과 같습니다.\n\n$$\\psi^{\\pi}(s,a)=\\phi_{sa}-E_{\\pi(a';s,\\theta)}[\\phi_{sa'}]$$\n\n따라서 function approximator는 다음과 같이 다시 쓸 수 있습니다.\n\n$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T(\\phi_{sa}-E_{\\pi(a';s,\\theta)}[\\phi_{sa'}])$$\n\ngreedy policy improvement가 Q function 값 중 가장 큰 값을 가지는 action을 선택하듯이 여기서도 function approximator의 값이 가장 큰 action을 선택하는 상황을 가정해봅시다. 이 때 function approximator의 argmax는 다음과 같이 쓸 수 있습니다.\n\n$$argmax_{a'}f^{\\pi}(s,a)=argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n(1) 식을 다시 살펴봅시다. 그러면 policy의 정의에 따라 다음과 같이 쓸 수 있습니다.\n\n$$\\pi(a;s,\\theta + \\alpha\\bar{\\nabla}\\eta(\\theta)) \\propto exp(\\theta^T\\phi_{sa} + \\alpha\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa})$$\n\n$\\bar{\\nabla}\\eta(\\theta) \\neq 0$이고 $\\alpha\\rightarrow\\infty$이면 exp안의 항 중에서 뒤의 항이 dominate하게 됩니다. 여러 행동 중에 $\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa}$가 가장 큰 행동이 있다면 이 행동의 policy probability가 1이 되고 나머지는 0이 됩니다. 따라서 다음이 성립합니다.\n\n$$\\pi_{\\infty}=0 \\, \\, \\, if \\, and \\, only \\, if \\, \\, \\, a \\notin argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n이 결과로부터 natural policy gradient는 단지 더 좋은 action이 아니라 best action을 고르도록 학습이 됩니다. 반면에 non-covariant gradient(1차미분)에서는 그저 더 좋은 action을 고르도록 학습이 됩니다. 이 natural policy gradient에 대한 결과는 infinite learning rate 세팅에서만 성립합니다. 좀 더 일반적인 경우에 대해서 살펴봅시다.\n\n<br>\n## 4.3 Theorem 3: General Parameterized Policy\n\nTheorem 2에서와는 달리 일반적인 policy를 가정해봅시다(general parameterized policy). Theorem 3는 이 상황에서 natural gradient를 통한 업데이트가 best action를 고르는 방향으로 학습이 된다는 것을 보여줍니다.\n\nnatural gradien에 따른 policy parameter의 업데이트는 다음과 같습니다. $\\bar{w}$는 approximation error를 minimize하는 $w$입니다.\n\n$$\\delta\\theta = \\theta' - \\theta = \\alpha\\bar{\\nabla}\\eta(\\theta)=\\alpha\\bar{w}$$\n\npolicy에 대해서 1차근사를 하면 다음과 같습니다.\n\n$$\\pi(a;s,\\theta')=\\pi(a;s,\\theta)+\\frac{\\partial\\pi(a;s,\\theta)^T}{\\partial\\theta}\\delta\\theta + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\psi(s,a)^T\\delta\\theta) + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\alpha\\psi(s,a)^T\\bar{w}) + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\alpha f^{\\pi}(s,a;\\bar{w})) + O(\\delta\\theta^2)$$\n\npolicy 자체가 function approximator의 크기대로 업데이트가 되므로 local하게 best action의 probability는 커지고 다른 probability의 크기는 작아질 것입니다. 하지만 만약 greedy improvement가 된다하더라도 그게 performance의 improvement를 보장하는 것은 아닙니다. 하지만 line search와 함께 사용할 경우 improvement를 보장할 수 있습니다. 왜 그런지는 처음에 말씀드린 블로그 [다크 프로그래머님의 블로그](http://darkpgmr.tistory.com/149)를 참고해주시기 바랍니다.\n\n<br><br>\n\n# 5. Metrics and Curvatures\n\n$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$\n\n이 파트에서는 FIM과 다른 metric 사이의 관계를 다룹니다.\n\n위의 식에 해당하는 G는 Fisher Information Matrix만 사용할 수 있는 것이 아닙니다. Positive-Definite Matrix인 FIM이외의 다른 Matrix도 사용할 수 있습니다. 또한 다양한 파라미터 추정에서 FIM은 Hessian Matrix에 수렴하지 않을 수 있다고 합니다. 이 말은 2nd order(2차 근사) 수렴이 보장되지 않는다는 말입니다.\n\n논문에 있는 내용들을 조금 더 추가적으로 보면 아래와 같이 나옵니다.\n\nIn the different setting of parameter estimation, the Fisher information converges to the ```Hessian```, so it is [asymptotically efficient](https://en.wikipedia.org/wiki/Efficiency_(statistics)) 이지만,\n\n이 논문의 경우, 아마리 논문의 'blind separation case'와 유사한데 이 때는 꼭 asymtotically efficient하지 않다고 말합니다. 이 말은 즉 2nd order 수렴이 보장되지 않는다는 것이다.\n\n[Mackay](http://www.inference.org.uk/mackay/ica.pdf) 논문에서는 Hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했습니다. 그래서 performance를 2번 미분해보면 아래의 수식과 같습니다. 하지만 다음 식에서는 모든 항이 data dependent합니다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있습니다.\n\n$$\\nabla^2\\eta(\\theta)=\\sum_{sa}\\rho^{\\pi}(s)(\\nabla^2\\pi(a;s)Q^{\\pi}(s,a)+\\nabla\\pi(a;s)\\nabla Q^{\\pi}(s,a)^T+\\nabla Q^{\\pi}(s,a)\\nabla\\pi(a;s)^T)$$\n\nhessian은 보통 positive definite가 아닐수도 있습니다. 따라서 local maxima가 될 때까지 Hessian이 사용하기 별로 안좋습니다. 그리고 local maxima에서는 Hessian보다는 Conjugate methods가 더 효율적이라고 합니다.\n\n사실 이 파트에서는 무엇을 말하고 있는지 알기가 어렵습니다. FIM과 Hessian이 관련이 있다는 것을 알겠는데 asymtotically efficient와 같은 내용을 모르므로 내용의 이해가 어려웠습니다.\n\nMackay 논문에서 해당 부분은 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/x4n6z6pdyi7xtb9/Screenshot%202018-06-10%2012.04.13.png?dl=1\"> </center>\n\n<br>\n## 5.1 Fisher Information Matrix(FIM) vs. Hessian\n\nFIM과 Hessian에 대해서 추가적인 설명을 하고자 합니다.\n\n- FIM\n\n일단 정의되려면 space 자체가 stochastic한 성질을 가지고 있어야 합니다. 모든 parameter를 표현하는 함수가 deterministic한 함수가 아니라 확률로 나타낸 분포입니다. (probability distribution)\n\n결국은 FIM에서도 hessian처럼 비슷한 과정을 취하고 싶은 것입니다. 따라서 확률 변수이기 때문에 어떠한 특정 sample을 취하면 그게 항상 다른값이 됩니다. 그래서 FIM에다가 expectation을 취한 것입니다. expectation을 취함으로써 hessian같은 성질을 가집니다. 왜냐하면 expectation을 취함으로써 constant한 값이 되기 때문입니다.\n\n- Hessian\n\n그냥 deterministic한 함수에서 정의됩니다. 그냥 함수를 두 번 미분 한 것이라고 보면 됩니다.\n\n<br>\n## 5.2 Conjugate Gradient Method\n\n추가적으로 Conjugate Gradient Method(CGM)에 대해서 다루고자 합니다.\n\n- 참고자료\n    - https://www.quora.com/What-is-an-intuitive-explanation-of-what-the-conjugate-gradient-method-is\n    - https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf\n    - https://en.wikipedia.org/wiki/Conjugate_gradient_method\n\n### $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해 구하기\n\n$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해를 구하는 문제를 생각해봅시다. $\\mathbf{A}$의 역행렬을 구해서 양변에 곱해주면 $\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$가 되어 쉽게 해를 구할 수 있습니다. 하지만 $\\mathbf{A}^{-1}$은 계산이 많이 필요 자원소모가 큰 연산입니다. 역행렬을 취하지 않고 해를 구할 수 있는 방법이 있을까요?\n\n위의 방정식에서 $\\mathbf{b}$를 이항시키면 $\\mathbf{A}\\mathbf{x} - \\mathbf{b} = 0$이 됩니다. 최적화문제는 많은 경우 1차 미분이 0이 되는 지점이 해일 확률이 높습니다. $\\mathbf{A}\\mathbf{x} - \\mathbf{b} = 0$을 1차 미분으로 가지는 함수는 무엇일까요?\n\n$$f( \\mathbf{x} ) = \\frac{1}{2}\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x} - \\mathbf{x}^\\mathrm{T}\\mathbf{b}$$\n\n위의 함수는 $\\mathbf{A}\\mathbf{x} - \\mathbf{b}$를 1차 미분값으로 가집니다. 이 때 $\\mathbf{A}$는 symmetric positive definite해야 합니다.\n\n- symmetric: $\\mathbf{A}=\\mathbf{A}^\\mathrm{T}$\n- positive definite: $\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}>0,\\quad\\forall\\mathbf{x}$ or all eigenvalues of $\\mathbf{A}$ are positive\n\nsymmetric positive definite한 성질은 $\\mathbf{x}$를 strict convex function이 되게 만들어서 unique solution이 존재하게 합니다. $\\mathbf{A}$는 $\\mathbf{x}$의 Hessian이기도 합니다.\n\n자, 우리는 위의 방정식의 해 $\\mathbf{x}^*$를 iterative한 방식으로 찾고자 합니다. $\\mathbf{x}_0$을 초기값이라고 합시다.\n\n우리는 <img src=\"https://www.dropbox.com/s/g91yqajf72jzjoc/Screen%20Shot%202018-08-14%20at%208.43.28%20AM.png?dl=1\" width=\"80\">가 되는 <img src=\"https://www.dropbox.com/s/aj3j5fjtiyewlo1/Screen%20Shot%202018-08-14%20at%208.43.37%20AM.png?dl=1\" width=\"30\">를 찾고 싶은데 이를 위한 현재의 추정값은 $\\mathbf{A} \\mathbf{x}_0$입니다. 최적의 값을 찾았을 때와 비교하면 현재의 오차는 $\\mathbf{b} - \\mathbf{A}\\mathbf{x}_0 = \\mathbf{r}_0$입니다. 이것을 residual이라고 합니다. 어떻게 효율적으로 값을 변화시켜서 오차를 0으로 만들 수 있을까요? 일단 매 iteration마다 residual이 작아지는 방향으로 나아가야겠죠?\n\n가장 널리 알려진 방법은 gradient descent 방향입니다. gradient가 증가하면 반대로 가고 gradient가 감소하면 그 방향으로 가는 것입니다. 그리고 어떤 방향으로든 gradient가 0이면 그 지점에 멈추는 것이죠. 이 방법은 수렴은 하지만 zigzag하게 움직여서 느립니다. 이것보다 더 좋은 방법이 없을까요?\n\n### Conjugate Gradient Method\n\n#### Gram-Schmidt orthgonalization\n\n다음과 같은 vector들의 집합 <img src=\"https://www.dropbox.com/s/csgdsfofe19q5r6/Screen%20Shot%202018-08-14%20at%208.56.04%20AM.png?dl=1\" width=\"135\">이 있다고 해봅시다. 이 vector들이 서로 linearly independent하다면 이 vector들은 $\\mathbb{R}^n$ 공간 상의 basis입니다. 이 vector들을 이용해서 orthogonal한 vector들을 만들어보겠습니다. 새로운 vector들을 <img src=\"https://www.dropbox.com/s/7xg7pciar1ndu44/Screen%20Shot%202018-08-14%20at%208.58.25%20AM.png?dl=1\" width=\"130\">라고 한다면 아래와 같은 과정을 통해 만들 수 있습니다. 이 방법을 Gram-Schmidt orthogonalization이라고 합니다.\n\n- $\\mathbf{d}_1=\\mathbf{v}_1$\n- $\\mathbf{d}_2=\\mathbf{v}_2 - \\left\\langle\\mathbf{v}_2,\\frac{\\mathbf{d}_1}{\\parallel\\mathbf{d}_1\\parallel}\\right\\rangle\\frac{\\mathbf{d}_1}{\\parallel\\mathbf{d}_1\\parallel}$\n...\n- <img src=\"https://www.dropbox.com/s/arzc5cez8az0j7h/Screen%20Shot%202018-08-14%20at%209.00.04%20AM.png?dl=1\" width=\"290\">\n\n간단히 설명을 하면, 첫 vector는 basis와 같은 방향으로 출발합니다. 그 다음 vector는 그 다음 basis를 이전 vector로 projection한 다음 해당 basis로부터 빼줍니다. 그 다음 vector를 만들 때는 이전에 만든 모든 vector들에 대해서 projection을 취한 다음 모두 더한 것을 해당 basis에서 빼줍니다. \n\n#### $A$-conjugate\n\n이 방법을 이용하여 matrix $A$에 관하여 orthogonal한 새로운 vector들의 집합을 만들어보겠습니다. 일반적인 inner product와 약간 다른 다음과 같은 inner product를 정의할 수 있습니다.\n\n$$<\\mathbf{x},\\mathbf{y}>_\\mathbf{A} = \\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{y}$$\n\n이 때 $<\\mathbf{x},\\mathbf{y}>_\\mathbf{A} = 0$이면 $\\mathbf{x}와 \\mathbf{y}$는 $\\mathbf{A}$-orthogonal 또는 $\\mathbf{A}$-conjugate하다라고 합니다. vector norm $\\parallel\\cdot\\parallel$도 일반적인 vector norm이 아닌 A-norm을 다음과 같이 정의합니다.\n\n$$\\parallel\\mathbf{x}\\parallel_\\mathbf{A}^2=\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}$$\n\nGram-Schmidt orthogonalization을 또 이용해볼까요? 아래와 같습니다.\n\n* $\\mathbf{d}_1=\\mathbf{v}_1$\n* <img src=\"https://www.dropbox.com/s/i8wnvoccefgg7t8/Screen%20Shot%202018-08-14%20at%209.08.42%20AM.png?dl=1\" width=\"400\">\n...\n* <img src=\"https://www.dropbox.com/s/045x2v5sx7s0wot/Screen%20Shot%202018-08-14%20at%209.09.03%20AM.png?dl=1\" width=\"500\">\n\n그런데 이 Gram-Schmidt 방법은 단점이 하나 있습니다. 새로운 vector를 만들어내기 위해서 이전에 만든 vector들을 모두 가지고 있어야 하고 projection도 다시 계산해야 한다는 점입니다. 그런데 마법같은 이유($\\approx$ 여기서 설명하지 않는 이유)로 인해서 오직 마지막으로 만들어낸 vector만 가지고 있어도 기존의 모든 vector들이 성질을 표현해낼 수 있습니다. 그러면 우리가 만들어낸 vector는 다음과 같이 간단해집니다.\n\n<center> <img src=\"https://www.dropbox.com/s/wlyu7ax6qt0aw9w/Screen%20Shot%202018-08-14%20at%209.09.43%20AM.png?dl=1\" width=\"550\"> </center>\n\n이 방향이 우리가 업데이트시키고 싶은 방향입니다. 즉 우리는 다음 수식처럼 움직이고자 합니다.\n\n<center> <img src=\"https://www.dropbox.com/s/053o4ocvxsgxfl9/Screen%20Shot%202018-08-14%20at%209.09.56%20AM.png?dl=1\" width=\"180\"> </center>\n\n$d_k$는 방향이고 $\\alpha_{k}$는 step size입니다. 이러한 방법을 일반적으로 line search method라고 부릅니다. 방향은 위에서 구한 conjugate 방향을 이용합니다. 그래서 이 line search를 conjugate gradient method라고 부릅니다. 이 방법의 한가지 중요한 장점은 업데이트가 무조건 n번만 일어난다는 것입니다! n-dimensional space에서는 basis가 n개 밖에 없거든요. 그렇다면 얼마만큼 많이 이 방향으로 움직여야 할까요? 더 이상 $f(\\mathbf{x})$가 감소하지 않을 때까지 움직이는게 좋지 않을까요? 이것은 다시 말하면 gradient가 0이 될 때까지 움직인다는 뜻입니다. 위에서 설명한 residual과도 관계있을 것 같지 않나요?\n \n$\\nabla f(\\mathbf{x})=\\mathbf{A}\\mathbf{x} - \\mathbf{b}$에 $x_k + \\alpha_{k} d_k$를 대입해 봅시다. 아래와 같이 step size를 구할 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/xikpfgcix64ngmp/Screen%20Shot%202018-08-14%20at%209.13.20%20AM.png?dl=1\" width=\"300\"> </center>\n\n네, 이제 우리는 방향과 step size를 모두 알았습니다. 이제 update만 하면 해를 찾을 수 있겠군요!\n\n<center> <img src=\"https://www.dropbox.com/s/avjw4kbzd4ormv5/conjugate_gradient_wikipedia.png?dl=1\" width=\"200\"> </center>\n\n위키피디아에서 가져온 위의 그림을 봅시다. 그림의 녹색선이 gradient descent이고 빨간선이 conjugate gradient입니다. conjugate gradient는 gradient descent보다 빨리 수렴합니다. 녹색선은 항상 이전 이동 방향과 직각으로 이동합니다. 빨간선은 보다 빠르게 더 낮은 값이 있는 곳으로 이동합니다.\n\n위에서 설명을 생략했지만 왜 conjugate gradient는 빠르게 수렴(n-step)하며 Gram-Schmidt orthogonalization을 할 때도 메모리가 적게 필요할까요? 수학적으로 엄밀하게 설명하기 보다는 개념적으로 설명을 하겠습니다. 그 비밀은 바로 $\\mathbf{A}$-orthogonal 또는 $\\mathbf{A}$-conjugate vector들을 이용한데 있습니다. 이 vector들은 basis라고 했습니다. 즉, 처음에 우리가 구하고자 했던 해 $\\mathbf{x}^*$를 이 vector들을 이용해서 표현할 수 있습니다. 또한 iterative하게 찾아가기 위한 초기 vector $\\mathbf{x}_0$도 이 vector들로 표현할 수 있습니다. 그렇다면 이 둘 간의 오차도 이 vector들도 표현할 수 있게 됩니다. 다음 수식처럼 말입니다.\n\n$$\\mathbf{x}^\\* - \\mathbf{x}_0 = \\epsilon_1\\mathbf{d}_1 + \\epsilon_2\\mathbf{d}_2 + \\cdots + \\epsilon_n\\mathbf{d}_n$$\n\n우리의 목표는 이 오차를 줄이는 것입니다. 각각의 basis마다 오차가 있으며 이들은 서로 independent합니다. 즉, 특정 iteration에서 특정 basis에 대한 오차를 0으로 만들면 다음 번 iteration에서는 이 오차는 영향을 받지 않습니다! 이러한 이유로 n번의 iteration만으로 해를 찾을 수 있고 다른 vector들을 저장할 필요도 없는 것입니다. 선형대수의 아름다움이 느껴지지 않으시나요? 오래되었지만 참 잘 디자인된 기법이라는 생각이 듭니다.\n\n<br><br>\n\n# 6. Experiment\n\n이 논문에서는 natural gradient를 simple MDP와 tetris MDP에 대해서 실험을 진행했습니다. FIM은 다음과 같은 식으로 업데이트합니다.\n\n$$f\\leftarrow f+\\nabla \\log \\pi(a_t; s_t, \\theta)\\nabla \\log \\pi(a_t; s_t, \\theta)^T$$\n\n$T$ length trajectory에 대해서 $f/T$를 이용해 $F$의 기대값($E$)를 구합니다.\n\n<br>\n## 6.1 LQR(Linear Quadratic Regulator)\n\nAgent를 실험할 환경은 다음과 같은 dynamics를 가지고 있습니다.\n\n$x(t+1) = 0.7x(t)+u(t)+\\epsilon(t)$\n\n$u(t)$는 control 신호로서 에이전트의 행동입니다. $\\epsilon$은 noise distribution으로 환경에 가해지는 노이즈입니다. 에이전트의 목표는 적절한 $u(t)$를 통해 $x(t)$를 0으로 유지하는 것입니다. $x(t)$를 0으로 유지하기 위해서 필요한 소모값(cost)는 $x(t)^2$로 정의하며 cost를 최소화하도록 학습합니다. 이 논문에서는 실험할 때 복잡성을 더 해주기 위해 noise distribution인 $\\epsilon$을 dynamics에 추가하였습니다.\n\n이 실험에서 policy는 다음과 같이 설정하였습니다. 파라미터가 2개 밖에 없는 간단한 policy입니다.\n\n$\\pi(u;x,\\theta)\\propto \\exp(\\theta_1s_1x^2+\\theta_2s_2x)$\n\n이 policy를 간단히 그래프로 그려보면 다음과 같습니다. 가로축은 $x$, 세로축은 cost입니다. $\\theta_1$과 $\\theta_2$를 (0.5, 0.5), (1, 0), (0, 1)로 설정하고 $s_1$, $s_2$는, 1로 두었습니다. $x$는 -1~1사이의 범위로 그렸습니다. \n\n<center> <img src='https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1' width='500px'> </center>\n\n아래의 그림은 LQR을 학습한 그래프입니다. cost가 $x^2$이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있습니다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과입니다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 곡선입니다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습합니다(time 축이 log scale인 것을 감안하면 차이가 많이 납니다.). \n\n하지만 문제가 있습니다. NPG를 학습한 세 개의 곡선은 $\\theta$를 rescale 한 것입니다. $\\theta$앞에 곱해지는 숫자에 따라 학습의 과정이 다릅니다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것입니다. 즉, covariant gradient가 아니라는 뜻입니다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1\" width=\"300px\"> </center>\n\nnatural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문입니다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 $\\rho_s$가 곱해지기 때문입니다. 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것입니다!\n\n<br>\n## 6.2 Simple 2-state MDP\n\n이제 다른 예제에서 NPG를 테스트합니다. 2개의 state만 가지는 MDP를 고려해봅시다. [그림출처](http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&context=robotics). 그림으로보면 다음과 같습니다. $x=0$ 상태와 $x=1$ 상태 두 개가 존재합니다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있습니다. 상태 $x=0$에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 $x=1$에서 자기 자신으로 돌아오면 2의 보상을 받습니다. 결국 optimal policy는 상태 $x=1$에서 계속 자기 자신으로 돌아오는 행동을 취하는 것입니다. \n\n<img src=\"https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1\">\n\n문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정합니다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것입니다.\n\n$$\\rho(x=0)=0.8,  \\rho(x=1)=0.2$$\n\n일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 됩니다. 이 때, $\\rho(s)$가 gradient에 곱해지므로 상대적으로 상태 0에서의 gradient 값이 커지게 됩니다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update합니다. 따라서 아래 그림의 첫번째 그림에서처럼 reward가 1에서 오랜 시간동안 머무릅니다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것입니다.\n\n$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$\n\n<center><img src=\"https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1\" width=\"300px\"></center>\n\n하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달합니다. gradient 방법이 $1.7\\times 10^7$정도의 시간만에 2에 도달한 반면 NPG는 2의 시간만에 도달합니다. 또한 $\\rho(x=0)$가 $10^{-5}$이하로 떨어지지 않습니다.\n\n한 가지 그래프를 더 살펴봅시다. 다음 그래프는 parameter $\\theta$가 업데이트 되는 과정을 보여줍니다. 이 그래프에서는 parameter가 2개 있습니다. 일반적인 gradient가 아래 그래프에서 실선에 해당합니다. 이 실선의 그래프는 보면 처음부터 중반까지 $\\theta_i$만 거의 업데이트하는 것을 볼 수 있습니다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있습니다. \n\n<center><img src=\"https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1\" width=\"300px\"></center>\n\npolicy가 $\\pi(a;s,\\theta)\\propto \\exp(\\theta_{sa})$일 때, 다음과 같이 $F_{-1}$이 gradient 앞에 weight로 곱해지는데 이게 $\\rho$와는 달리 각 parameter에 대해 균등합니다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것입니다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$\n\n<br>\n## 6.3 Tetris\n\nNPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어 있습니다. 다음 그림은 tetris 예제를 보여줍니다. 보통 그림에서와 같이 state의 feature를 정해줍니다. [그림 출처](http://slideplayer.com/slide/5215520/)\n\n<img src=\"https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1\">\n\n이 예제에서도 exponential family로 policy를 표현합니다. $\\pi(a;s,\\theta) \\propto \\exp(\\theta^T\\phi_{sa})$ 로 표현합니다.\n\ntetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있습니다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우입니다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법입니다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷합니다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지합니다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있습니다.\n\n<img src=\"https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1\">\n\n<br><br>\n\n# 7. Discussion\n\nNatural Gradient Method는  Policy Iteration에서와 같이 Greedy Action을 선택하도록 학습합니다. Line search 기법과 함께 사용하면 더 Policy Iteration과 비슷해집니다. Greedy Policy Iteration에서와 비교하면 Performance Improvement가 보장됩니다. 하지만 FIM이 asymtotically Hessian으로 수렴하지 않습니다. 그렇기 때문에 Conjugate Gradient Method로 구하는 방법이 더 좋을수 있습니다.\n\n살펴봤듯이 본 논문의 NPG는 완벽하지 않습니다. 위의 몇가지 문제점을 극복하기 위한 추가적인 연구가 필요하다고 할 수 있습니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [DDPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/)\n\n<br>\n\n# 다음으로\n\n## [NPG Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py)\n\n## [TRPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/)","source":"_posts/4_maxent.md","raw":"---\ntitle: Maximum Entropy Inverse Reinforcement Learning\ndate: 2019-02-10\ntags: [\"프로젝트\", \"GAIL하자!\"]\ncategories: 프로젝트\nauthor: 이승현\nsubtitle: Inverse RL 4번째 논문\n---\n\n<center> <img src=\"../../../../img/irl/maxent_1.png\" width=\"850\"> </center>\n\nAuthor : Brian D. Ziebart, Andrew Maas, J.Andrew Bagnell, Anind K. Dey\nPaper Link : http://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf\nProceeding : Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence 2008\n\n---\n\n# 0. Abstract\n\n이 논문이 발표된 2002년 당시에도 많은 연구자들이 objective function의 gradient 값을 따라서 좋은 policy $\\pi$를 찾고자 하였습니다. 하지만 기존의 우리가 알던 gradient descent method는 steepest descent direction이 아닐 수 있기 때문에(쉽게 말해 가장 가파른 방향을 따라서 내려가야 하는데 그러지 못할 수도 있다는 것입니다.) 이 논문에서 steepest descent direction를 나타내는 natural gradient method를 policy gradient에 적용하여 좋은 policy $\\pi$를 찾습니다. \n\n<br>\n## 1.1 NPG 흐름 잡기\n\n### 1.1.1 매니폴드(manifold)\n\n<center> <img src=\"https://www.dropbox.com/s/cstjgemqpby4ysr/Screen%20Shot%202018-08-12%20at%208.28.29%20PM.png?dl=1\" width=\"300\"> </center>\n\n매니폴드는 간단하게 말해 위의 그림에서의 점들을 아우르는 subspace입니다. 그래서 NPG 공부하기 전에 매니폴드를 왜 배울까라는 의문이 들으실텐데 그 이유는 위에서도 언급했듯이 natural gradient method는 어떠한 파라미터 공간에서의 steepest descent direction을 강조하기 때문입니다. 여기서 파라미터 공간이 바로 리만 매니폴드입니다. 리만 매니폴드는 매니폴드가 각지지 않고 미분가능하게 부드럽게 곡률을 가진 면이라고 생각하면 편합니다.\n\nNeural Network(NN)을 사용할 경우 NN의 parameter space가 우리가 보통 생각하는 직선으로 쭉쭉 뻗어있는 유클리디안 공간(Euclidean space)가 아닙니다. 좀 더 일반적으로는 구의 표면과 같이 휘어져있는 공간 즉, 리만 공간(Riemannian space)로 표현할 수 있습니다. 다음 그림들을 보겠습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/jnlm6he18ar64yc/Screen%20Shot%202018-08-12%20at%208.14.13%20PM.png?dl=1\" width=\"400\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/tufjqmaeaz29kaz/Screen%20Shot%202018-08-12%20at%208.15.53%20PM.png?dl=1\" width=\"400\"> </center>\n\n아래의 그림처럼 어떠한 확률 분포가 있다고 해봅시다.\n\n<center> <img src=\"https://www.dropbox.com/s/mpoop19eyu1vp0a/Screen%20Shot%202018-08-13%20at%2012.52.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n어떠한 공간의 확률 분포에 있는 한 점은 다른 공간의 확률 분포에서의 한 점이 될 것입니다. 이렇게 위의 그림들과 같이 유클리디안 공간의 확률 분포에서의 한 점이 리만 공간의 확률 분포의 한 점이 되는 것이고, 곡률의 일차 근사가 유클리디안 공간에서는 일차 근사가 아닌 이차 근사이고, 리만 공간에서는 휘어진 공간이기 때문에 곡률을 일차 근사라고 보는 것입니다. 추가적으로 일차 근사와 이차 근사의 차이는 [다크 프로그래머님의 블로그](http://darkpgmr.tistory.com/149)를 참고해주시기 바랍니다. (꼭 보세요!) \n\n일차 근사와 이차 근사의 차이점과 각각의 장단점, 그리고 추가적으로 Line Search까지 알고 난 후에 이 논문을 보시는 것을 권장해드립니다.\n\n### 1.1.2 Natural Gradient + Policy Gradient\n\n먼저 아래의 그림들을 보여드리겠습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/65hra43zadsubff/Screen%20Shot%202018-08-13%20at%2012.51.45%20PM.png?dl=1\" width=\"500\"> <img src=\"https://www.dropbox.com/s/8g332zceeordqwv/Screen%20Shot%202018-08-13%20at%2012.51.51%20PM.png?dl=1\" width=\"400\"> </center>\n\n위에서도 언급했듯이 natural gradient가 steepest direction이 된다는 연구가 이뤄지고 있었습니다. 강화학습의 policy gradient은 objective function의 gradient를 따라 policy를 업데이트를 합니다. 이 때 policy는 parameterization되는데 이 경우에도 gradient 대신에 natural gradient가 좋다는 것을 실험해보는 논문이 지금 다루고 있는 논문입니다.\n\ngradient가 non-covariant(1차 근사)해서 생기는 문제는 간단히 말하자면 다음과 같습니다. policy가 parameterized된 상황에서는 같은 policy라도 다른 parameter를 가질 수 있습니다. 이 때, steepest direction은 두 경우에 같은 방향을 가리켜야 하는데 non-covariant한 경우 그렇지 못합니다. 이러한 부분이 바로 결국 느린 학습으로 연결이 되는 것입니다.\n\n논문에서 2차미분 방법론들과 짧게 비교합니다. 하지만 개인적인 의견으로 2차미분을 이용한 다른 방법들과의 비교가 생각보다 없는 점이 부족해 보였습니다.(Hessian을 이용한다거나 conjugate gradient method를 이용한다거나). 실험을 통해 Fisher Information Matrix(FIM)가 hessian에 수렴안하는 거라던지 Hessian 방법론이 local maxima 부근에서 상당히 느리다던지의 결과를 보여줬었으면 좋았을 것 같습니다.\n\n또한 natural gradient의 단점으로 natural gradient 만으로 업데이트하면 policy의 improvement 보장이 안될 수 있습니다. policy의 improvement를 보장하기 위해 line search도 써야하는데 line search를 어떻게 쓰는지에 대한 자세한 언급이 없습니다. 다시 말해 자세한 algorithm 설명이 없다는 것입니다.\n\nnatural policy gradient 논문은 natural gradient + policy gradient를 처음 적용했다는데 의의가 있습니다. 하지만 이 논문이 문제 삼은 gradient는 non-covariant하다라는 문제를 natural gradient를 통해 해결하지는 못했습니다(Experiment를 통해 covariant gradient가 되지 못했다는 것이 보입니다). NPG의 뒤를 잇는 논문이 “covariant policy search”와 “natural actor-critic”에서 covariant(2차 근사)하지 못하다는 것을 해결하기 위해 Fisher Information Matrix를 sample 하나 하나에 대해서 구하는 것이 아니라 trajectory 전체에 대해서 구합니다.\n\n또한 논문은 pg의 두 가지 세팅 중에 average-reward setting(infinite horizon)에서만 NPG를 다룹니다. “covariant policy search” 논문에서는 average-reward setting과 start-state setting 모두에 대해서 npg를 적용합니다.\n\nnatural gradient + policy gradient를 처음 제시했다는 것은 좋지만 npg 학습의 과정을 자세하게 설명하지 않았고 다른 2차 미분 방법들과 비교를 많이 하지 않은 점이 아쉬운 논문이었습니다.\n\n<br><br>\n\n# 2. Introduction\n\n소개는 앞에서 다 했기 때문에 간략하게 다시 한 번 정리하겠습니다. direct policy gradient method는 future reward의 gradient를 따라 policy를 update합니다. 하지만 gradient descent는 non-covariant입니다. 따라서 이 논문에서는 covarient gradient를 제시합니다. 바로 \"Natural Gradient\" 입니다. \n\n또한 natural gradient와 policy iteration의 연관성을 설명합니다. natural policy gradient is moving toward choosing a greedy optimal action (이런 연결점은 아마도 step-size를 덜 신경쓰고 싶어서 그런게 아닌가 싶습니다)\n\n논문의 Introduction 부분에 다음 멘트가 있습니다. 이 글만 봐서는 이해가 안갔는데 Mackay 논문에 좀 더 자세히 나와있었습니다.\n<center> <img src=\"https://www.dropbox.com/s/41xhhr7lgfk24a1/Screenshot%202018-06-10%2011.45.18.png?dl=1\"> </center>\n\n[Mackay](http://www.inference.org.uk/mackay/ica.pdf)논문에서는 다음과 같이 언급하고 있습니다. Back-propagation을 사용할 경우에 learning rate를 dimension에 1/n로 사용하면 수렴한다는 것이 증명됐습니다. 하지만 너무 느리다고 합니다.\n<center> <img src=\"https://www.dropbox.com/s/us9ezc7vxgrkez6/Screenshot%202018-06-10%2011.47.21.png?dl=1\"> </center>\n\n<br><br>\n\n# 3. A Natural Gradient\n\n<br>\n## 3.1 Notation\n\n이 논문에서 제시하는 Notation은 다음과 같습니다.\n\n- MDP : tuple $(S, s_0, A, R, P)$\n- $S$ : a finite set of states\n- $s_0$ : a start state\n- $A$ : a finite set of actions\n- $R$ : reward function $R: S \\times A -> [0, R_{max}]$\n- $\\pi(a;s, \\theta)$ : stochastic policy parameterized by $\\theta$\n- 모든 정책 $\\pi$는 ergodic : stationary distribution $\\rho^{\\pi}$이 잘 정의되어 있다고 봅니다.\n- 이 논문에서는 sutton의 pg 논문의 두 세팅(start-state formulation, average-reward formulation) 중에 두 번째인 average-reward formulation을 가정합니다.\n- performance or average reward : $\\eta(\\pi)=\\sum_{s,a}\\rho^{\\pi}(s)\\pi(a;s)R(s,a)$\n- state-action value : $Q^{\\pi}(s,a)=E_{\\pi}[\\sum_{t=0}^{\\infty}R(s_t, a_t)-\\eta(\\pi)\\vert s_0=s, a_0=a]$ ([Sutton PG](https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/) 를 참고해주시기 바랍니다.)\n- 정책이 $\\theta$로 parameterize되어있으므로 performance는 $\\eta(\\pi_{\\theta})$인데 $\\eta(\\theta)$로 씁니다.\n\n<br>\n## 3.2 Natural Gradient\n\nSutton PG 논문의 policy gradient theorem에 따라 exact gradient of the average reward는 다음과 같습니다. 다음 수식이 어떻게 유도되었는지, 어떤 의미인지 모른다면 [Sutton PG](https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/)을 통해 제대로 이해하는 것이 좋습니다. Euclidean space에서는 objective function의 일반적인 policy gradient는 Sutton PG에서 논의한 바와 같이 다음과 같이 구할 수 있습니다.\n\n$$\\nabla\\eta(\\pi_\\theta) = \\Sigma_{s,a}\\rho^\\pi(s)\\nabla\\pi(a;s,\\theta)Q^\\pi(s,a)$$\n\n여기서 steepest descent direction of $\\eta(\\theta)$는 $\\eta(\\theta + d\\theta)$를 최소화하는 $d\\theta$로 정의됩니다. 이 때, $\\vert d\\theta \\vert^2$가 일정 크기 이하인 것으로 제약조건을 주는데(held to small constant) Euclidian space에서는 $\\eta(\\theta)$가 steepest direction이지만 Riemannian space에서는 natural gradient가 steepest direction입니다.\n\n추가적으로 결국 우리가 원하는 건 ‘policy 최적화를 좀 더 스마트하게 해 보자!’입니다. Policy 최적화를 잘한다는 것은 Policy를 어느 방향으로 얼만큼 업데이트 하느냐에 따라 달라집니다.\n\n### 3.2.1 Natural gradient 증명\n\nRiemannian space에서 거리는 다음과 같이 정의됩니다. 여기서 $G(\\theta)$는 특정한 양수로 이루어진 matrix입니다. 자세한 내용은 [양의 정부호 행렬(positive definite matrix)이란?](http://bskyvision.com/205)을 참고해주시기 바랍니다.\n\n$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$\n\n이 수식은 [Natural gradient works in efficiently in learning](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&rep=rep1&type=pdf) 논문에서 증명되어 있습니다. 다음은 natural gradient 증명입니다.\n\nsteepest direction을 구할 때 $\\theta$의 크기를 제약조건으로 줍니다. 제약조건은 다음과 같습니다.\n\n$$\\vert d\\theta \\vert^2 = \\epsilon^2$$\n\n그리고 steepest vector인 $d\\theta$는 다음과 같이 정의할 수 있습니다.\n\n$$d\\theta = \\epsilon a$$\n\n$$\\vert a \\vert^2=a^TG(\\theta)a = 1$$\n\n이 때, $a$가 steepest direction unit vector이 되려면 다음 수식을 최소로 만들어야 합니다.\n\n$$\\eta(\\theta + d\\theta) = \\eta(\\theta) + \\epsilon\\nabla\\eta(\\theta)^Ta$$\n\n위 수식이 제약조건 아래 최소가 되는 $a$를 구하기 위해 Lagrangian method를 사용합니다. Lagrangian method를 모른다면 [위키피디아](https://en.wikipedia.org/wiki/Lagrange_multiplier)를 참고하는 것을 추천합니다. 위 수식이 최소라는 것은 $\\nabla\\eta(\\theta)^Ta$가 최소라는 것입니다.\n\n$$\\frac{\\partial}{\\partial a_i}(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$$\n\n따라서 $(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$는 상수입니다. 상수를 미분하면 0이므로 이 식을 $a$로 미분합니다. 그러면 다음과 같이 steepest direction을 구한 것입니다.\n\n$$\\nabla\\eta(\\theta) = 2 \\lambda G(\\theta)a$$\n\n$$a=\\frac{1}{2\\lambda}G^{-1}\\nabla\\eta(\\theta)$$\n\n이 때, 다음 식을 natural gradient라고 정의합니다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = G^{-1}\\nabla\\eta(\\theta)$$\n\nnatural gradient를 이용한 업데이트는 다음과 같습니다.\n\n$$\\theta_{t+1}=\\theta_t - \\alpha_tG^{-1}\\nabla\\eta(\\theta)$$\n\n여기까지는 natural gradient의 증명이었습니다. 이 natural gradient를 policy gradient에 적용한 것이 natural policy gradient입니다. natural policy gradient는 다음과 같이 정의됩니다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$\n\n여기서 $G$대신 $F$를 사용했는데 $F$는 Fisher information matix입니다. 수식은 다음과 같습니다.\n\n$$F(\\theta) = E_{\\rho^\\pi(s)}[F_s(\\theta)]$$\n\n$$F_s(\\theta)=E_{\\pi(a;s,\\theta)}[\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial \\theta_i}\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial\\theta_j}]$$\n\n### 3.2.2 Fisher Information Matrix에 정의된 Metric\n\n추가적으로 Fisher Information Matrix(FIM)에 대해서 설명하겠습니다. 위의 문제를 우리가 생각하기 쉬운 Neural Network(NN)으로 구성하고 해결할 수 있다고 해봅시다. NN은 여러가지 parameter set들로 구성될 수 있습니다. 게다가 다른 parameter set을 가지지만 같은 policy를 가질 수도 있습니다. 이 경우 steepest direction은 같은 policy이기 때문에 같은 방향을 가리키고 있어야 하는데 non-covariant한 경우 그렇지 못합니다. 떄문에 학습이 느려지고, 이러한 문제를 해결하기 위해 단순히 Positive-Definite Matrix $G(\\theta)$를 사용하지 않고 FIM $F_s(\\theta)$를 사용합니다. 어떠한 확률 변수 $X$가 임의의 매개변수 $\\theta$에 의해 정의되는 분포를 따른다고 하면 $X=x$일때 FIM은 다음과 같이 정의됩니다.\n\n$$F_x(\\theta)=E\\left[\\left(\\dfrac{\\partial}{\\partial\\theta}\\log\\Pr(x|\\theta)\\right)^2\\right]$$\n\n강화학습 관점에서 생각해보면 정보 $x$는 에피소드에 의해 관측된 상태값 $s$이며 매개변수 $\\theta$에 의해 선택될 수 있는 행동에 대한 분포가 나오게 됩니다. 이에 의해 위의 FIM는 다음과 같이 표현할 수 있습니다.\n\n$$F_s(\\theta) \\equiv E_{\\pi(a;s,\\theta)}\\left[\\left(\\dfrac{\\partial}{\\partial\\theta}\\log\\pi(a;s,\\theta)\\right)^2\\right] =E_{\\pi(a;s,\\theta)}\\left[\\dfrac{\\partial \\log\\pi(a;s,\\theta)}{\\partial \\theta_i}\\dfrac{\\partial \\log\\pi(a;s,\\theta)}{\\partial\\theta_j}\\right]$$\n\n그리고 위의 식들을 이용하여 objective function을 정리하면 아래의 식과 같이 표현됩니다.\n\n$$F(\\theta) = E_{\\rho^{\\pi}(s)}[F_s(\\theta)]$$\n\n또한 이 Fisher Information Matrix에 정의된 이 metric은 다음과 같은 성질을 가지고 있습니다.\n\n1. 업데이트되는 파라미터에 의해 구성되는 매니폴드에 기반한 metric입니다.\n2. 확률분포($\\pi(a;s,\\theta)$)를 구성하는 파라미터($\\theta$)의 변화에 독립적입니다.\n3. 마지막으로 positive-definite한 값을 가집니다. 이렇기 때문에 steepest gradient에서 objective function의 방향을 알기 위해 사용한 방법과 같은 방법으로 natural gradient direction을 다음과 같이 구할 수 있는 것입니다.\n\n$$\\bar{\\nabla}\\eta(\\theta) \\equiv F(\\theta)^{-1}\\nabla\\eta(\\theta)$$\n\n<br><br>\n\n# 4. The Natural Gradient and Policy Iteration\n\n4장에서는 Natural gradient를 통한 policy iteration을 수행하여 실제로 정책의 향상이 있는지를 증명합니다. 여기서 $Q^\\pi(s,a)$는 compatible function approximator $f^\\pi(s,a;w)$로 근사됩니다. ([Sutton PG](https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/)를 참고해주시기 바랍니다.)\n\n<br>\n## 4.1 Theorem 1: Compatible Function Approximation\n\napproximate하는 함수 $f^{\\pi}(s,a;w)$는 다음과 같습니다.(compatible value function)\n\n$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$\n\n$$\\psi^{\\pi}(s,a) = \\nabla log\\pi(a;s,\\theta)$$\n\n여기서 $[\\nabla \\log\\pi(a;s,\\theta)]_i=\\partial \\log\\pi(a;s,\\theta)/\\partial\\theta_i$입니다. $w$는 원래 approximate하는 함수 $Q$와 $f$의 차이를 줄이도록 학습합니다(mean square error). 수렴한 local minima의 $w$를 $\\bar{w}$라고 가정 하겠습니다. 에러는 다음과 같은 수식으로 나타낼 수 있습니다.\n\n$$\\epsilon(w,\\pi)\\equiv\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)(f^{\\pi}(s,a;w)-Q^{\\pi}(s,a))^2$$\n\n그 때, $\\bar{\\omega} = \\bar{\\nabla}\\eta(\\theta)$이면 function approximator의 gradient 방향과 정책의 gradient 방향이 같다는 것을 의미합니다.\n\n아래의 내용은 위의 Theorem에 대한 증명입니다.\n\n위의 수식이 local minima이면 미분값이 0이 됩니다. $w$에 대해서 미분하면 다음과 같습니다.\n\n$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)(\\psi^{\\pi}(s,a)^T\\bar{w}-Q^{\\pi}(s,a))=0$$\n\n$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)^T\\bar{w}=\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)Q^{\\pi}(s,a)$$\n\n이 때, 위 식의 우변은 $\\psi$의 정의에 의해 policy gradient가 됩니다. 또한 왼쪽 항에서는 Fisher information matrix가 나옵니다.\n\n$$F(\\theta)=\\sum_{s,a}\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)=E_{\\rho^\\pi(s)}[F_s(\\theta)]$$\n\n따라서 다음과 같이 쓸 수 있습니다.\n\n$$F(\\theta)\\bar{w}=\\nabla\\eta(\\theta)$$\n\n$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$\n\n위의 수식은 natural gradient식과 동일합니다. 위의 수식은 policy가 update 될 때, value function approximator의 parameter 방향으로 이동한다는 것을 의미합니다. function approximation이 정확하다면 그 parameter의 natural policy gradient와 inner product가 커야합니다.\n\n<br>\n## 4.2 Theorem 2: Greedy Policy Improvement\n\n이번 장에서는 natrual gradient는 다른 policy iteration 방법처럼 단순히 더 좋은 행동을 고르도록 학습하는 것이 아니라 가장 좋은(greedy) 행동을 고르도록 학습한다는 것을 증명하는 부분입니다. 이것을 일반적인 형태의 policy에 대해서 증명하기 전에 exponential 형태의 policy에 대해서 증명하는 것이 Theorem 2입니다. 특수한 정책을 가지는 상황안에서 학습속도($\\alpha$)를 무한대로 가져감으로서 어떤 action을 선택하는지를 알아봅니다.\n\npolicy를 다음과 같이 정의합니다.\n\n$$\\pi(a;s,\\theta) \\propto \\exp(\\theta^T\\phi_{sa})$$\n\n여기서 $\\bar{\\nabla}\\eta(\\theta)$가 0이 아니고 $\\bar{w}$는 approximation error를 최소화된 $w$라고 가정합니다. 이 상태에서 natural gradient update를 생각해봅시다. 그리고 policy gradient는 gradient ascent임을 기억합시다.\n\n$$\\theta_{t+1}=\\theta_t + \\alpha_t\\bar{\\nabla}\\eta(\\theta)$$\n\n이 때 $\\alpha$가 learning rate로 parameter를 얼마나 업데이트하는지를 결정합니다. 이 값을 무한대로 늘렸을 때 policy가 어떻게 업데이트되는지 생각해봅시다.\n\n$$\\pi_{\\infty}(a;s)=lim_{\\alpha\\rightarrow\\infty}\\pi(a;s,\\theta+\\alpha\\bar{\\nabla}\\eta(\\theta))-(1)$$\n\n이 때,\n\n$$\\pi_{\\infty}=0 \\, \\, \\, if \\, and \\, only \\, if(필요충분조건) \\, \\, \\, a \\notin argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n이라고 말할 수 있습니다.\n\n아래의 내용은 위의 Theorem에 대한 증명입니다.\n\n먼저 function approximator는 앞서 다뤘듯이 아래와 같습니다.\n\n$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$\n\n여기서 function approximator는 Theorem 1에 의해 아래와 같이 쓸 수 있습니다.\n\n$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T\\psi^{\\pi}(s,a)$$\n\n$\\theta$의 정의에 의해 $\\psi$는 다음과 같습니다.\n\n$$\\psi^{\\pi}(s,a)=\\phi_{sa}-E_{\\pi(a';s,\\theta)}[\\phi_{sa'}]$$\n\n따라서 function approximator는 다음과 같이 다시 쓸 수 있습니다.\n\n$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T(\\phi_{sa}-E_{\\pi(a';s,\\theta)}[\\phi_{sa'}])$$\n\ngreedy policy improvement가 Q function 값 중 가장 큰 값을 가지는 action을 선택하듯이 여기서도 function approximator의 값이 가장 큰 action을 선택하는 상황을 가정해봅시다. 이 때 function approximator의 argmax는 다음과 같이 쓸 수 있습니다.\n\n$$argmax_{a'}f^{\\pi}(s,a)=argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n(1) 식을 다시 살펴봅시다. 그러면 policy의 정의에 따라 다음과 같이 쓸 수 있습니다.\n\n$$\\pi(a;s,\\theta + \\alpha\\bar{\\nabla}\\eta(\\theta)) \\propto exp(\\theta^T\\phi_{sa} + \\alpha\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa})$$\n\n$\\bar{\\nabla}\\eta(\\theta) \\neq 0$이고 $\\alpha\\rightarrow\\infty$이면 exp안의 항 중에서 뒤의 항이 dominate하게 됩니다. 여러 행동 중에 $\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa}$가 가장 큰 행동이 있다면 이 행동의 policy probability가 1이 되고 나머지는 0이 됩니다. 따라서 다음이 성립합니다.\n\n$$\\pi_{\\infty}=0 \\, \\, \\, if \\, and \\, only \\, if \\, \\, \\, a \\notin argmax_{a'}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa'}$$\n\n이 결과로부터 natural policy gradient는 단지 더 좋은 action이 아니라 best action을 고르도록 학습이 됩니다. 반면에 non-covariant gradient(1차미분)에서는 그저 더 좋은 action을 고르도록 학습이 됩니다. 이 natural policy gradient에 대한 결과는 infinite learning rate 세팅에서만 성립합니다. 좀 더 일반적인 경우에 대해서 살펴봅시다.\n\n<br>\n## 4.3 Theorem 3: General Parameterized Policy\n\nTheorem 2에서와는 달리 일반적인 policy를 가정해봅시다(general parameterized policy). Theorem 3는 이 상황에서 natural gradient를 통한 업데이트가 best action를 고르는 방향으로 학습이 된다는 것을 보여줍니다.\n\nnatural gradien에 따른 policy parameter의 업데이트는 다음과 같습니다. $\\bar{w}$는 approximation error를 minimize하는 $w$입니다.\n\n$$\\delta\\theta = \\theta' - \\theta = \\alpha\\bar{\\nabla}\\eta(\\theta)=\\alpha\\bar{w}$$\n\npolicy에 대해서 1차근사를 하면 다음과 같습니다.\n\n$$\\pi(a;s,\\theta')=\\pi(a;s,\\theta)+\\frac{\\partial\\pi(a;s,\\theta)^T}{\\partial\\theta}\\delta\\theta + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\psi(s,a)^T\\delta\\theta) + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\alpha\\psi(s,a)^T\\bar{w}) + O(\\delta\\theta^2)$$\n\n$$=\\pi(a;s,\\theta)(1+\\alpha f^{\\pi}(s,a;\\bar{w})) + O(\\delta\\theta^2)$$\n\npolicy 자체가 function approximator의 크기대로 업데이트가 되므로 local하게 best action의 probability는 커지고 다른 probability의 크기는 작아질 것입니다. 하지만 만약 greedy improvement가 된다하더라도 그게 performance의 improvement를 보장하는 것은 아닙니다. 하지만 line search와 함께 사용할 경우 improvement를 보장할 수 있습니다. 왜 그런지는 처음에 말씀드린 블로그 [다크 프로그래머님의 블로그](http://darkpgmr.tistory.com/149)를 참고해주시기 바랍니다.\n\n<br><br>\n\n# 5. Metrics and Curvatures\n\n$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$\n\n이 파트에서는 FIM과 다른 metric 사이의 관계를 다룹니다.\n\n위의 식에 해당하는 G는 Fisher Information Matrix만 사용할 수 있는 것이 아닙니다. Positive-Definite Matrix인 FIM이외의 다른 Matrix도 사용할 수 있습니다. 또한 다양한 파라미터 추정에서 FIM은 Hessian Matrix에 수렴하지 않을 수 있다고 합니다. 이 말은 2nd order(2차 근사) 수렴이 보장되지 않는다는 말입니다.\n\n논문에 있는 내용들을 조금 더 추가적으로 보면 아래와 같이 나옵니다.\n\nIn the different setting of parameter estimation, the Fisher information converges to the ```Hessian```, so it is [asymptotically efficient](https://en.wikipedia.org/wiki/Efficiency_(statistics)) 이지만,\n\n이 논문의 경우, 아마리 논문의 'blind separation case'와 유사한데 이 때는 꼭 asymtotically efficient하지 않다고 말합니다. 이 말은 즉 2nd order 수렴이 보장되지 않는다는 것이다.\n\n[Mackay](http://www.inference.org.uk/mackay/ica.pdf) 논문에서는 Hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했습니다. 그래서 performance를 2번 미분해보면 아래의 수식과 같습니다. 하지만 다음 식에서는 모든 항이 data dependent합니다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있습니다.\n\n$$\\nabla^2\\eta(\\theta)=\\sum_{sa}\\rho^{\\pi}(s)(\\nabla^2\\pi(a;s)Q^{\\pi}(s,a)+\\nabla\\pi(a;s)\\nabla Q^{\\pi}(s,a)^T+\\nabla Q^{\\pi}(s,a)\\nabla\\pi(a;s)^T)$$\n\nhessian은 보통 positive definite가 아닐수도 있습니다. 따라서 local maxima가 될 때까지 Hessian이 사용하기 별로 안좋습니다. 그리고 local maxima에서는 Hessian보다는 Conjugate methods가 더 효율적이라고 합니다.\n\n사실 이 파트에서는 무엇을 말하고 있는지 알기가 어렵습니다. FIM과 Hessian이 관련이 있다는 것을 알겠는데 asymtotically efficient와 같은 내용을 모르므로 내용의 이해가 어려웠습니다.\n\nMackay 논문에서 해당 부분은 다음과 같습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/x4n6z6pdyi7xtb9/Screenshot%202018-06-10%2012.04.13.png?dl=1\"> </center>\n\n<br>\n## 5.1 Fisher Information Matrix(FIM) vs. Hessian\n\nFIM과 Hessian에 대해서 추가적인 설명을 하고자 합니다.\n\n- FIM\n\n일단 정의되려면 space 자체가 stochastic한 성질을 가지고 있어야 합니다. 모든 parameter를 표현하는 함수가 deterministic한 함수가 아니라 확률로 나타낸 분포입니다. (probability distribution)\n\n결국은 FIM에서도 hessian처럼 비슷한 과정을 취하고 싶은 것입니다. 따라서 확률 변수이기 때문에 어떠한 특정 sample을 취하면 그게 항상 다른값이 됩니다. 그래서 FIM에다가 expectation을 취한 것입니다. expectation을 취함으로써 hessian같은 성질을 가집니다. 왜냐하면 expectation을 취함으로써 constant한 값이 되기 때문입니다.\n\n- Hessian\n\n그냥 deterministic한 함수에서 정의됩니다. 그냥 함수를 두 번 미분 한 것이라고 보면 됩니다.\n\n<br>\n## 5.2 Conjugate Gradient Method\n\n추가적으로 Conjugate Gradient Method(CGM)에 대해서 다루고자 합니다.\n\n- 참고자료\n    - https://www.quora.com/What-is-an-intuitive-explanation-of-what-the-conjugate-gradient-method-is\n    - https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf\n    - https://en.wikipedia.org/wiki/Conjugate_gradient_method\n\n### $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해 구하기\n\n$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해를 구하는 문제를 생각해봅시다. $\\mathbf{A}$의 역행렬을 구해서 양변에 곱해주면 $\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$가 되어 쉽게 해를 구할 수 있습니다. 하지만 $\\mathbf{A}^{-1}$은 계산이 많이 필요 자원소모가 큰 연산입니다. 역행렬을 취하지 않고 해를 구할 수 있는 방법이 있을까요?\n\n위의 방정식에서 $\\mathbf{b}$를 이항시키면 $\\mathbf{A}\\mathbf{x} - \\mathbf{b} = 0$이 됩니다. 최적화문제는 많은 경우 1차 미분이 0이 되는 지점이 해일 확률이 높습니다. $\\mathbf{A}\\mathbf{x} - \\mathbf{b} = 0$을 1차 미분으로 가지는 함수는 무엇일까요?\n\n$$f( \\mathbf{x} ) = \\frac{1}{2}\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x} - \\mathbf{x}^\\mathrm{T}\\mathbf{b}$$\n\n위의 함수는 $\\mathbf{A}\\mathbf{x} - \\mathbf{b}$를 1차 미분값으로 가집니다. 이 때 $\\mathbf{A}$는 symmetric positive definite해야 합니다.\n\n- symmetric: $\\mathbf{A}=\\mathbf{A}^\\mathrm{T}$\n- positive definite: $\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}>0,\\quad\\forall\\mathbf{x}$ or all eigenvalues of $\\mathbf{A}$ are positive\n\nsymmetric positive definite한 성질은 $\\mathbf{x}$를 strict convex function이 되게 만들어서 unique solution이 존재하게 합니다. $\\mathbf{A}$는 $\\mathbf{x}$의 Hessian이기도 합니다.\n\n자, 우리는 위의 방정식의 해 $\\mathbf{x}^*$를 iterative한 방식으로 찾고자 합니다. $\\mathbf{x}_0$을 초기값이라고 합시다.\n\n우리는 <img src=\"https://www.dropbox.com/s/g91yqajf72jzjoc/Screen%20Shot%202018-08-14%20at%208.43.28%20AM.png?dl=1\" width=\"80\">가 되는 <img src=\"https://www.dropbox.com/s/aj3j5fjtiyewlo1/Screen%20Shot%202018-08-14%20at%208.43.37%20AM.png?dl=1\" width=\"30\">를 찾고 싶은데 이를 위한 현재의 추정값은 $\\mathbf{A} \\mathbf{x}_0$입니다. 최적의 값을 찾았을 때와 비교하면 현재의 오차는 $\\mathbf{b} - \\mathbf{A}\\mathbf{x}_0 = \\mathbf{r}_0$입니다. 이것을 residual이라고 합니다. 어떻게 효율적으로 값을 변화시켜서 오차를 0으로 만들 수 있을까요? 일단 매 iteration마다 residual이 작아지는 방향으로 나아가야겠죠?\n\n가장 널리 알려진 방법은 gradient descent 방향입니다. gradient가 증가하면 반대로 가고 gradient가 감소하면 그 방향으로 가는 것입니다. 그리고 어떤 방향으로든 gradient가 0이면 그 지점에 멈추는 것이죠. 이 방법은 수렴은 하지만 zigzag하게 움직여서 느립니다. 이것보다 더 좋은 방법이 없을까요?\n\n### Conjugate Gradient Method\n\n#### Gram-Schmidt orthgonalization\n\n다음과 같은 vector들의 집합 <img src=\"https://www.dropbox.com/s/csgdsfofe19q5r6/Screen%20Shot%202018-08-14%20at%208.56.04%20AM.png?dl=1\" width=\"135\">이 있다고 해봅시다. 이 vector들이 서로 linearly independent하다면 이 vector들은 $\\mathbb{R}^n$ 공간 상의 basis입니다. 이 vector들을 이용해서 orthogonal한 vector들을 만들어보겠습니다. 새로운 vector들을 <img src=\"https://www.dropbox.com/s/7xg7pciar1ndu44/Screen%20Shot%202018-08-14%20at%208.58.25%20AM.png?dl=1\" width=\"130\">라고 한다면 아래와 같은 과정을 통해 만들 수 있습니다. 이 방법을 Gram-Schmidt orthogonalization이라고 합니다.\n\n- $\\mathbf{d}_1=\\mathbf{v}_1$\n- $\\mathbf{d}_2=\\mathbf{v}_2 - \\left\\langle\\mathbf{v}_2,\\frac{\\mathbf{d}_1}{\\parallel\\mathbf{d}_1\\parallel}\\right\\rangle\\frac{\\mathbf{d}_1}{\\parallel\\mathbf{d}_1\\parallel}$\n...\n- <img src=\"https://www.dropbox.com/s/arzc5cez8az0j7h/Screen%20Shot%202018-08-14%20at%209.00.04%20AM.png?dl=1\" width=\"290\">\n\n간단히 설명을 하면, 첫 vector는 basis와 같은 방향으로 출발합니다. 그 다음 vector는 그 다음 basis를 이전 vector로 projection한 다음 해당 basis로부터 빼줍니다. 그 다음 vector를 만들 때는 이전에 만든 모든 vector들에 대해서 projection을 취한 다음 모두 더한 것을 해당 basis에서 빼줍니다. \n\n#### $A$-conjugate\n\n이 방법을 이용하여 matrix $A$에 관하여 orthogonal한 새로운 vector들의 집합을 만들어보겠습니다. 일반적인 inner product와 약간 다른 다음과 같은 inner product를 정의할 수 있습니다.\n\n$$<\\mathbf{x},\\mathbf{y}>_\\mathbf{A} = \\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{y}$$\n\n이 때 $<\\mathbf{x},\\mathbf{y}>_\\mathbf{A} = 0$이면 $\\mathbf{x}와 \\mathbf{y}$는 $\\mathbf{A}$-orthogonal 또는 $\\mathbf{A}$-conjugate하다라고 합니다. vector norm $\\parallel\\cdot\\parallel$도 일반적인 vector norm이 아닌 A-norm을 다음과 같이 정의합니다.\n\n$$\\parallel\\mathbf{x}\\parallel_\\mathbf{A}^2=\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}$$\n\nGram-Schmidt orthogonalization을 또 이용해볼까요? 아래와 같습니다.\n\n* $\\mathbf{d}_1=\\mathbf{v}_1$\n* <img src=\"https://www.dropbox.com/s/i8wnvoccefgg7t8/Screen%20Shot%202018-08-14%20at%209.08.42%20AM.png?dl=1\" width=\"400\">\n...\n* <img src=\"https://www.dropbox.com/s/045x2v5sx7s0wot/Screen%20Shot%202018-08-14%20at%209.09.03%20AM.png?dl=1\" width=\"500\">\n\n그런데 이 Gram-Schmidt 방법은 단점이 하나 있습니다. 새로운 vector를 만들어내기 위해서 이전에 만든 vector들을 모두 가지고 있어야 하고 projection도 다시 계산해야 한다는 점입니다. 그런데 마법같은 이유($\\approx$ 여기서 설명하지 않는 이유)로 인해서 오직 마지막으로 만들어낸 vector만 가지고 있어도 기존의 모든 vector들이 성질을 표현해낼 수 있습니다. 그러면 우리가 만들어낸 vector는 다음과 같이 간단해집니다.\n\n<center> <img src=\"https://www.dropbox.com/s/wlyu7ax6qt0aw9w/Screen%20Shot%202018-08-14%20at%209.09.43%20AM.png?dl=1\" width=\"550\"> </center>\n\n이 방향이 우리가 업데이트시키고 싶은 방향입니다. 즉 우리는 다음 수식처럼 움직이고자 합니다.\n\n<center> <img src=\"https://www.dropbox.com/s/053o4ocvxsgxfl9/Screen%20Shot%202018-08-14%20at%209.09.56%20AM.png?dl=1\" width=\"180\"> </center>\n\n$d_k$는 방향이고 $\\alpha_{k}$는 step size입니다. 이러한 방법을 일반적으로 line search method라고 부릅니다. 방향은 위에서 구한 conjugate 방향을 이용합니다. 그래서 이 line search를 conjugate gradient method라고 부릅니다. 이 방법의 한가지 중요한 장점은 업데이트가 무조건 n번만 일어난다는 것입니다! n-dimensional space에서는 basis가 n개 밖에 없거든요. 그렇다면 얼마만큼 많이 이 방향으로 움직여야 할까요? 더 이상 $f(\\mathbf{x})$가 감소하지 않을 때까지 움직이는게 좋지 않을까요? 이것은 다시 말하면 gradient가 0이 될 때까지 움직인다는 뜻입니다. 위에서 설명한 residual과도 관계있을 것 같지 않나요?\n \n$\\nabla f(\\mathbf{x})=\\mathbf{A}\\mathbf{x} - \\mathbf{b}$에 $x_k + \\alpha_{k} d_k$를 대입해 봅시다. 아래와 같이 step size를 구할 수 있습니다.\n\n<center> <img src=\"https://www.dropbox.com/s/xikpfgcix64ngmp/Screen%20Shot%202018-08-14%20at%209.13.20%20AM.png?dl=1\" width=\"300\"> </center>\n\n네, 이제 우리는 방향과 step size를 모두 알았습니다. 이제 update만 하면 해를 찾을 수 있겠군요!\n\n<center> <img src=\"https://www.dropbox.com/s/avjw4kbzd4ormv5/conjugate_gradient_wikipedia.png?dl=1\" width=\"200\"> </center>\n\n위키피디아에서 가져온 위의 그림을 봅시다. 그림의 녹색선이 gradient descent이고 빨간선이 conjugate gradient입니다. conjugate gradient는 gradient descent보다 빨리 수렴합니다. 녹색선은 항상 이전 이동 방향과 직각으로 이동합니다. 빨간선은 보다 빠르게 더 낮은 값이 있는 곳으로 이동합니다.\n\n위에서 설명을 생략했지만 왜 conjugate gradient는 빠르게 수렴(n-step)하며 Gram-Schmidt orthogonalization을 할 때도 메모리가 적게 필요할까요? 수학적으로 엄밀하게 설명하기 보다는 개념적으로 설명을 하겠습니다. 그 비밀은 바로 $\\mathbf{A}$-orthogonal 또는 $\\mathbf{A}$-conjugate vector들을 이용한데 있습니다. 이 vector들은 basis라고 했습니다. 즉, 처음에 우리가 구하고자 했던 해 $\\mathbf{x}^*$를 이 vector들을 이용해서 표현할 수 있습니다. 또한 iterative하게 찾아가기 위한 초기 vector $\\mathbf{x}_0$도 이 vector들로 표현할 수 있습니다. 그렇다면 이 둘 간의 오차도 이 vector들도 표현할 수 있게 됩니다. 다음 수식처럼 말입니다.\n\n$$\\mathbf{x}^\\* - \\mathbf{x}_0 = \\epsilon_1\\mathbf{d}_1 + \\epsilon_2\\mathbf{d}_2 + \\cdots + \\epsilon_n\\mathbf{d}_n$$\n\n우리의 목표는 이 오차를 줄이는 것입니다. 각각의 basis마다 오차가 있으며 이들은 서로 independent합니다. 즉, 특정 iteration에서 특정 basis에 대한 오차를 0으로 만들면 다음 번 iteration에서는 이 오차는 영향을 받지 않습니다! 이러한 이유로 n번의 iteration만으로 해를 찾을 수 있고 다른 vector들을 저장할 필요도 없는 것입니다. 선형대수의 아름다움이 느껴지지 않으시나요? 오래되었지만 참 잘 디자인된 기법이라는 생각이 듭니다.\n\n<br><br>\n\n# 6. Experiment\n\n이 논문에서는 natural gradient를 simple MDP와 tetris MDP에 대해서 실험을 진행했습니다. FIM은 다음과 같은 식으로 업데이트합니다.\n\n$$f\\leftarrow f+\\nabla \\log \\pi(a_t; s_t, \\theta)\\nabla \\log \\pi(a_t; s_t, \\theta)^T$$\n\n$T$ length trajectory에 대해서 $f/T$를 이용해 $F$의 기대값($E$)를 구합니다.\n\n<br>\n## 6.1 LQR(Linear Quadratic Regulator)\n\nAgent를 실험할 환경은 다음과 같은 dynamics를 가지고 있습니다.\n\n$x(t+1) = 0.7x(t)+u(t)+\\epsilon(t)$\n\n$u(t)$는 control 신호로서 에이전트의 행동입니다. $\\epsilon$은 noise distribution으로 환경에 가해지는 노이즈입니다. 에이전트의 목표는 적절한 $u(t)$를 통해 $x(t)$를 0으로 유지하는 것입니다. $x(t)$를 0으로 유지하기 위해서 필요한 소모값(cost)는 $x(t)^2$로 정의하며 cost를 최소화하도록 학습합니다. 이 논문에서는 실험할 때 복잡성을 더 해주기 위해 noise distribution인 $\\epsilon$을 dynamics에 추가하였습니다.\n\n이 실험에서 policy는 다음과 같이 설정하였습니다. 파라미터가 2개 밖에 없는 간단한 policy입니다.\n\n$\\pi(u;x,\\theta)\\propto \\exp(\\theta_1s_1x^2+\\theta_2s_2x)$\n\n이 policy를 간단히 그래프로 그려보면 다음과 같습니다. 가로축은 $x$, 세로축은 cost입니다. $\\theta_1$과 $\\theta_2$를 (0.5, 0.5), (1, 0), (0, 1)로 설정하고 $s_1$, $s_2$는, 1로 두었습니다. $x$는 -1~1사이의 범위로 그렸습니다. \n\n<center> <img src='https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1' width='500px'> </center>\n\n아래의 그림은 LQR을 학습한 그래프입니다. cost가 $x^2$이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있습니다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과입니다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 곡선입니다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습합니다(time 축이 log scale인 것을 감안하면 차이가 많이 납니다.). \n\n하지만 문제가 있습니다. NPG를 학습한 세 개의 곡선은 $\\theta$를 rescale 한 것입니다. $\\theta$앞에 곱해지는 숫자에 따라 학습의 과정이 다릅니다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것입니다. 즉, covariant gradient가 아니라는 뜻입니다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것입니다.\n\n<center> <img src=\"https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1\" width=\"300px\"> </center>\n\nnatural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문입니다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 $\\rho_s$가 곱해지기 때문입니다. 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것입니다!\n\n<br>\n## 6.2 Simple 2-state MDP\n\n이제 다른 예제에서 NPG를 테스트합니다. 2개의 state만 가지는 MDP를 고려해봅시다. [그림출처](http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&context=robotics). 그림으로보면 다음과 같습니다. $x=0$ 상태와 $x=1$ 상태 두 개가 존재합니다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있습니다. 상태 $x=0$에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 $x=1$에서 자기 자신으로 돌아오면 2의 보상을 받습니다. 결국 optimal policy는 상태 $x=1$에서 계속 자기 자신으로 돌아오는 행동을 취하는 것입니다. \n\n<img src=\"https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1\">\n\n문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정합니다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것입니다.\n\n$$\\rho(x=0)=0.8,  \\rho(x=1)=0.2$$\n\n일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 됩니다. 이 때, $\\rho(s)$가 gradient에 곱해지므로 상대적으로 상태 0에서의 gradient 값이 커지게 됩니다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update합니다. 따라서 아래 그림의 첫번째 그림에서처럼 reward가 1에서 오랜 시간동안 머무릅니다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것입니다.\n\n$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$\n\n<center><img src=\"https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1\" width=\"300px\"></center>\n\n하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달합니다. gradient 방법이 $1.7\\times 10^7$정도의 시간만에 2에 도달한 반면 NPG는 2의 시간만에 도달합니다. 또한 $\\rho(x=0)$가 $10^{-5}$이하로 떨어지지 않습니다.\n\n한 가지 그래프를 더 살펴봅시다. 다음 그래프는 parameter $\\theta$가 업데이트 되는 과정을 보여줍니다. 이 그래프에서는 parameter가 2개 있습니다. 일반적인 gradient가 아래 그래프에서 실선에 해당합니다. 이 실선의 그래프는 보면 처음부터 중반까지 $\\theta_i$만 거의 업데이트하는 것을 볼 수 있습니다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있습니다. \n\n<center><img src=\"https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1\" width=\"300px\"></center>\n\npolicy가 $\\pi(a;s,\\theta)\\propto \\exp(\\theta_{sa})$일 때, 다음과 같이 $F_{-1}$이 gradient 앞에 weight로 곱해지는데 이게 $\\rho$와는 달리 각 parameter에 대해 균등합니다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것입니다.\n\n$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$\n\n<br>\n## 6.3 Tetris\n\nNPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어 있습니다. 다음 그림은 tetris 예제를 보여줍니다. 보통 그림에서와 같이 state의 feature를 정해줍니다. [그림 출처](http://slideplayer.com/slide/5215520/)\n\n<img src=\"https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1\">\n\n이 예제에서도 exponential family로 policy를 표현합니다. $\\pi(a;s,\\theta) \\propto \\exp(\\theta^T\\phi_{sa})$ 로 표현합니다.\n\ntetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있습니다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우입니다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법입니다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷합니다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지합니다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있습니다.\n\n<img src=\"https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1\">\n\n<br><br>\n\n# 7. Discussion\n\nNatural Gradient Method는  Policy Iteration에서와 같이 Greedy Action을 선택하도록 학습합니다. Line search 기법과 함께 사용하면 더 Policy Iteration과 비슷해집니다. Greedy Policy Iteration에서와 비교하면 Performance Improvement가 보장됩니다. 하지만 FIM이 asymtotically Hessian으로 수렴하지 않습니다. 그렇기 때문에 Conjugate Gradient Method로 구하는 방법이 더 좋을수 있습니다.\n\n살펴봤듯이 본 논문의 NPG는 완벽하지 않습니다. 위의 몇가지 문제점을 극복하기 위한 추가적인 연구가 필요하다고 할 수 있습니다.\n\n<br><br>\n\n# 처음으로\n\n## [PG Travel Guide](https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/)\n\n<br>\n\n# 이전으로\n\n## [DDPG 여행하기](https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/)\n\n<br>\n\n# 다음으로\n\n## [NPG Code](https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py)\n\n## [TRPO 여행하기](https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/)","slug":"4_maxent","published":1,"updated":"2019-02-07T11:21:31.943Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjrujlu20002k5wfeutanpeid","content":"<center> <img src=\"../../../../img/irl/maxent_1.png\" width=\"850\"> </center>\n\n<p>Author : Brian D. Ziebart, Andrew Maas, J.Andrew Bagnell, Anind K. Dey<br>Paper Link : <a href=\"http://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf\" target=\"_blank\" rel=\"noopener\">http://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf</a><br>Proceeding : Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence 2008</p>\n<hr>\n<h1 id=\"0-Abstract\"><a href=\"#0-Abstract\" class=\"headerlink\" title=\"0. Abstract\"></a>0. Abstract</h1><p>이 논문이 발표된 2002년 당시에도 많은 연구자들이 objective function의 gradient 값을 따라서 좋은 policy $\\pi$를 찾고자 하였습니다. 하지만 기존의 우리가 알던 gradient descent method는 steepest descent direction이 아닐 수 있기 때문에(쉽게 말해 가장 가파른 방향을 따라서 내려가야 하는데 그러지 못할 수도 있다는 것입니다.) 이 논문에서 steepest descent direction를 나타내는 natural gradient method를 policy gradient에 적용하여 좋은 policy $\\pi$를 찾습니다. </p>\n<p><br></p>\n<h2 id=\"1-1-NPG-흐름-잡기\"><a href=\"#1-1-NPG-흐름-잡기\" class=\"headerlink\" title=\"1.1 NPG 흐름 잡기\"></a>1.1 NPG 흐름 잡기</h2><h3 id=\"1-1-1-매니폴드-manifold\"><a href=\"#1-1-1-매니폴드-manifold\" class=\"headerlink\" title=\"1.1.1 매니폴드(manifold)\"></a>1.1.1 매니폴드(manifold)</h3><center> <img src=\"https://www.dropbox.com/s/cstjgemqpby4ysr/Screen%20Shot%202018-08-12%20at%208.28.29%20PM.png?dl=1\" width=\"300\"> </center>\n\n<p>매니폴드는 간단하게 말해 위의 그림에서의 점들을 아우르는 subspace입니다. 그래서 NPG 공부하기 전에 매니폴드를 왜 배울까라는 의문이 들으실텐데 그 이유는 위에서도 언급했듯이 natural gradient method는 어떠한 파라미터 공간에서의 steepest descent direction을 강조하기 때문입니다. 여기서 파라미터 공간이 바로 리만 매니폴드입니다. 리만 매니폴드는 매니폴드가 각지지 않고 미분가능하게 부드럽게 곡률을 가진 면이라고 생각하면 편합니다.</p>\n<p>Neural Network(NN)을 사용할 경우 NN의 parameter space가 우리가 보통 생각하는 직선으로 쭉쭉 뻗어있는 유클리디안 공간(Euclidean space)가 아닙니다. 좀 더 일반적으로는 구의 표면과 같이 휘어져있는 공간 즉, 리만 공간(Riemannian space)로 표현할 수 있습니다. 다음 그림들을 보겠습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/jnlm6he18ar64yc/Screen%20Shot%202018-08-12%20at%208.14.13%20PM.png?dl=1\" width=\"400\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/tufjqmaeaz29kaz/Screen%20Shot%202018-08-12%20at%208.15.53%20PM.png?dl=1\" width=\"400\"> </center>\n\n<p>아래의 그림처럼 어떠한 확률 분포가 있다고 해봅시다.</p>\n<center> <img src=\"https://www.dropbox.com/s/mpoop19eyu1vp0a/Screen%20Shot%202018-08-13%20at%2012.52.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n<p>어떠한 공간의 확률 분포에 있는 한 점은 다른 공간의 확률 분포에서의 한 점이 될 것입니다. 이렇게 위의 그림들과 같이 유클리디안 공간의 확률 분포에서의 한 점이 리만 공간의 확률 분포의 한 점이 되는 것이고, 곡률의 일차 근사가 유클리디안 공간에서는 일차 근사가 아닌 이차 근사이고, 리만 공간에서는 휘어진 공간이기 때문에 곡률을 일차 근사라고 보는 것입니다. 추가적으로 일차 근사와 이차 근사의 차이는 <a href=\"http://darkpgmr.tistory.com/149\" target=\"_blank\" rel=\"noopener\">다크 프로그래머님의 블로그</a>를 참고해주시기 바랍니다. (꼭 보세요!) </p>\n<p>일차 근사와 이차 근사의 차이점과 각각의 장단점, 그리고 추가적으로 Line Search까지 알고 난 후에 이 논문을 보시는 것을 권장해드립니다.</p>\n<h3 id=\"1-1-2-Natural-Gradient-Policy-Gradient\"><a href=\"#1-1-2-Natural-Gradient-Policy-Gradient\" class=\"headerlink\" title=\"1.1.2 Natural Gradient + Policy Gradient\"></a>1.1.2 Natural Gradient + Policy Gradient</h3><p>먼저 아래의 그림들을 보여드리겠습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/65hra43zadsubff/Screen%20Shot%202018-08-13%20at%2012.51.45%20PM.png?dl=1\" width=\"500\"> <img src=\"https://www.dropbox.com/s/8g332zceeordqwv/Screen%20Shot%202018-08-13%20at%2012.51.51%20PM.png?dl=1\" width=\"400\"> </center>\n\n<p>위에서도 언급했듯이 natural gradient가 steepest direction이 된다는 연구가 이뤄지고 있었습니다. 강화학습의 policy gradient은 objective function의 gradient를 따라 policy를 업데이트를 합니다. 이 때 policy는 parameterization되는데 이 경우에도 gradient 대신에 natural gradient가 좋다는 것을 실험해보는 논문이 지금 다루고 있는 논문입니다.</p>\n<p>gradient가 non-covariant(1차 근사)해서 생기는 문제는 간단히 말하자면 다음과 같습니다. policy가 parameterized된 상황에서는 같은 policy라도 다른 parameter를 가질 수 있습니다. 이 때, steepest direction은 두 경우에 같은 방향을 가리켜야 하는데 non-covariant한 경우 그렇지 못합니다. 이러한 부분이 바로 결국 느린 학습으로 연결이 되는 것입니다.</p>\n<p>논문에서 2차미분 방법론들과 짧게 비교합니다. 하지만 개인적인 의견으로 2차미분을 이용한 다른 방법들과의 비교가 생각보다 없는 점이 부족해 보였습니다.(Hessian을 이용한다거나 conjugate gradient method를 이용한다거나). 실험을 통해 Fisher Information Matrix(FIM)가 hessian에 수렴안하는 거라던지 Hessian 방법론이 local maxima 부근에서 상당히 느리다던지의 결과를 보여줬었으면 좋았을 것 같습니다.</p>\n<p>또한 natural gradient의 단점으로 natural gradient 만으로 업데이트하면 policy의 improvement 보장이 안될 수 있습니다. policy의 improvement를 보장하기 위해 line search도 써야하는데 line search를 어떻게 쓰는지에 대한 자세한 언급이 없습니다. 다시 말해 자세한 algorithm 설명이 없다는 것입니다.</p>\n<p>natural policy gradient 논문은 natural gradient + policy gradient를 처음 적용했다는데 의의가 있습니다. 하지만 이 논문이 문제 삼은 gradient는 non-covariant하다라는 문제를 natural gradient를 통해 해결하지는 못했습니다(Experiment를 통해 covariant gradient가 되지 못했다는 것이 보입니다). NPG의 뒤를 잇는 논문이 “covariant policy search”와 “natural actor-critic”에서 covariant(2차 근사)하지 못하다는 것을 해결하기 위해 Fisher Information Matrix를 sample 하나 하나에 대해서 구하는 것이 아니라 trajectory 전체에 대해서 구합니다.</p>\n<p>또한 논문은 pg의 두 가지 세팅 중에 average-reward setting(infinite horizon)에서만 NPG를 다룹니다. “covariant policy search” 논문에서는 average-reward setting과 start-state setting 모두에 대해서 npg를 적용합니다.</p>\n<p>natural gradient + policy gradient를 처음 제시했다는 것은 좋지만 npg 학습의 과정을 자세하게 설명하지 않았고 다른 2차 미분 방법들과 비교를 많이 하지 않은 점이 아쉬운 논문이었습니다.</p>\n<p><br><br></p>\n<h1 id=\"2-Introduction\"><a href=\"#2-Introduction\" class=\"headerlink\" title=\"2. Introduction\"></a>2. Introduction</h1><p>소개는 앞에서 다 했기 때문에 간략하게 다시 한 번 정리하겠습니다. direct policy gradient method는 future reward의 gradient를 따라 policy를 update합니다. 하지만 gradient descent는 non-covariant입니다. 따라서 이 논문에서는 covarient gradient를 제시합니다. 바로 “Natural Gradient” 입니다. </p>\n<p>또한 natural gradient와 policy iteration의 연관성을 설명합니다. natural policy gradient is moving toward choosing a greedy optimal action (이런 연결점은 아마도 step-size를 덜 신경쓰고 싶어서 그런게 아닌가 싶습니다)</p>\n<p>논문의 Introduction 부분에 다음 멘트가 있습니다. 이 글만 봐서는 이해가 안갔는데 Mackay 논문에 좀 더 자세히 나와있었습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/41xhhr7lgfk24a1/Screenshot%202018-06-10%2011.45.18.png?dl=1\"> </center>\n\n<p><a href=\"http://www.inference.org.uk/mackay/ica.pdf\" target=\"_blank\" rel=\"noopener\">Mackay</a>논문에서는 다음과 같이 언급하고 있습니다. Back-propagation을 사용할 경우에 learning rate를 dimension에 1/n로 사용하면 수렴한다는 것이 증명됐습니다. 하지만 너무 느리다고 합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/us9ezc7vxgrkez6/Screenshot%202018-06-10%2011.47.21.png?dl=1\"> </center>\n\n<p><br><br></p>\n<h1 id=\"3-A-Natural-Gradient\"><a href=\"#3-A-Natural-Gradient\" class=\"headerlink\" title=\"3. A Natural Gradient\"></a>3. A Natural Gradient</h1><p><br></p>\n<h2 id=\"3-1-Notation\"><a href=\"#3-1-Notation\" class=\"headerlink\" title=\"3.1 Notation\"></a>3.1 Notation</h2><p>이 논문에서 제시하는 Notation은 다음과 같습니다.</p>\n<ul>\n<li>MDP : tuple $(S, s_0, A, R, P)$</li>\n<li>$S$ : a finite set of states</li>\n<li>$s_0$ : a start state</li>\n<li>$A$ : a finite set of actions</li>\n<li>$R$ : reward function $R: S \\times A -&gt; [0, R_{max}]$</li>\n<li>$\\pi(a;s, \\theta)$ : stochastic policy parameterized by $\\theta$</li>\n<li>모든 정책 $\\pi$는 ergodic : stationary distribution $\\rho^{\\pi}$이 잘 정의되어 있다고 봅니다.</li>\n<li>이 논문에서는 sutton의 pg 논문의 두 세팅(start-state formulation, average-reward formulation) 중에 두 번째인 average-reward formulation을 가정합니다.</li>\n<li>performance or average reward : $\\eta(\\pi)=\\sum_{s,a}\\rho^{\\pi}(s)\\pi(a;s)R(s,a)$</li>\n<li>state-action value : $Q^{\\pi}(s,a)=E_{\\pi}[\\sum_{t=0}^{\\infty}R(s_t, a_t)-\\eta(\\pi)\\vert s_0=s, a_0=a]$ (<a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/\">Sutton PG</a> 를 참고해주시기 바랍니다.)</li>\n<li>정책이 $\\theta$로 parameterize되어있으므로 performance는 $\\eta(\\pi_{\\theta})$인데 $\\eta(\\theta)$로 씁니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-2-Natural-Gradient\"><a href=\"#3-2-Natural-Gradient\" class=\"headerlink\" title=\"3.2 Natural Gradient\"></a>3.2 Natural Gradient</h2><p>Sutton PG 논문의 policy gradient theorem에 따라 exact gradient of the average reward는 다음과 같습니다. 다음 수식이 어떻게 유도되었는지, 어떤 의미인지 모른다면 <a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/\">Sutton PG</a>을 통해 제대로 이해하는 것이 좋습니다. Euclidean space에서는 objective function의 일반적인 policy gradient는 Sutton PG에서 논의한 바와 같이 다음과 같이 구할 수 있습니다.</p>\n<p>$$\\nabla\\eta(\\pi_\\theta) = \\Sigma_{s,a}\\rho^\\pi(s)\\nabla\\pi(a;s,\\theta)Q^\\pi(s,a)$$</p>\n<p>여기서 steepest descent direction of $\\eta(\\theta)$는 $\\eta(\\theta + d\\theta)$를 최소화하는 $d\\theta$로 정의됩니다. 이 때, $\\vert d\\theta \\vert^2$가 일정 크기 이하인 것으로 제약조건을 주는데(held to small constant) Euclidian space에서는 $\\eta(\\theta)$가 steepest direction이지만 Riemannian space에서는 natural gradient가 steepest direction입니다.</p>\n<p>추가적으로 결국 우리가 원하는 건 ‘policy 최적화를 좀 더 스마트하게 해 보자!’입니다. Policy 최적화를 잘한다는 것은 Policy를 어느 방향으로 얼만큼 업데이트 하느냐에 따라 달라집니다.</p>\n<h3 id=\"3-2-1-Natural-gradient-증명\"><a href=\"#3-2-1-Natural-gradient-증명\" class=\"headerlink\" title=\"3.2.1 Natural gradient 증명\"></a>3.2.1 Natural gradient 증명</h3><p>Riemannian space에서 거리는 다음과 같이 정의됩니다. 여기서 $G(\\theta)$는 특정한 양수로 이루어진 matrix입니다. 자세한 내용은 <a href=\"http://bskyvision.com/205\" target=\"_blank\" rel=\"noopener\">양의 정부호 행렬(positive definite matrix)이란?</a>을 참고해주시기 바랍니다.</p>\n<p>$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$</p>\n<p>이 수식은 <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&amp;rep=rep1&amp;type=pdf\" target=\"_blank\" rel=\"noopener\">Natural gradient works in efficiently in learning</a> 논문에서 증명되어 있습니다. 다음은 natural gradient 증명입니다.</p>\n<p>steepest direction을 구할 때 $\\theta$의 크기를 제약조건으로 줍니다. 제약조건은 다음과 같습니다.</p>\n<p>$$\\vert d\\theta \\vert^2 = \\epsilon^2$$</p>\n<p>그리고 steepest vector인 $d\\theta$는 다음과 같이 정의할 수 있습니다.</p>\n<p>$$d\\theta = \\epsilon a$$</p>\n<p>$$\\vert a \\vert^2=a^TG(\\theta)a = 1$$</p>\n<p>이 때, $a$가 steepest direction unit vector이 되려면 다음 수식을 최소로 만들어야 합니다.</p>\n<p>$$\\eta(\\theta + d\\theta) = \\eta(\\theta) + \\epsilon\\nabla\\eta(\\theta)^Ta$$</p>\n<p>위 수식이 제약조건 아래 최소가 되는 $a$를 구하기 위해 Lagrangian method를 사용합니다. Lagrangian method를 모른다면 <a href=\"https://en.wikipedia.org/wiki/Lagrange_multiplier\" target=\"_blank\" rel=\"noopener\">위키피디아</a>를 참고하는 것을 추천합니다. 위 수식이 최소라는 것은 $\\nabla\\eta(\\theta)^Ta$가 최소라는 것입니다.</p>\n<p>$$\\frac{\\partial}{\\partial a_i}(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$$</p>\n<p>따라서 $(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$는 상수입니다. 상수를 미분하면 0이므로 이 식을 $a$로 미분합니다. 그러면 다음과 같이 steepest direction을 구한 것입니다.</p>\n<p>$$\\nabla\\eta(\\theta) = 2 \\lambda G(\\theta)a$$</p>\n<p>$$a=\\frac{1}{2\\lambda}G^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>이 때, 다음 식을 natural gradient라고 정의합니다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = G^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>natural gradient를 이용한 업데이트는 다음과 같습니다.</p>\n<p>$$\\theta_{t+1}=\\theta_t - \\alpha_tG^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>여기까지는 natural gradient의 증명이었습니다. 이 natural gradient를 policy gradient에 적용한 것이 natural policy gradient입니다. natural policy gradient는 다음과 같이 정의됩니다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>여기서 $G$대신 $F$를 사용했는데 $F$는 Fisher information matix입니다. 수식은 다음과 같습니다.</p>\n<p>$$F(\\theta) = E_{\\rho^\\pi(s)}[F_s(\\theta)]$$</p>\n<p>$$F_s(\\theta)=E_{\\pi(a;s,\\theta)}[\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial \\theta_i}\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial\\theta_j}]$$</p>\n<h3 id=\"3-2-2-Fisher-Information-Matrix에-정의된-Metric\"><a href=\"#3-2-2-Fisher-Information-Matrix에-정의된-Metric\" class=\"headerlink\" title=\"3.2.2 Fisher Information Matrix에 정의된 Metric\"></a>3.2.2 Fisher Information Matrix에 정의된 Metric</h3><p>추가적으로 Fisher Information Matrix(FIM)에 대해서 설명하겠습니다. 위의 문제를 우리가 생각하기 쉬운 Neural Network(NN)으로 구성하고 해결할 수 있다고 해봅시다. NN은 여러가지 parameter set들로 구성될 수 있습니다. 게다가 다른 parameter set을 가지지만 같은 policy를 가질 수도 있습니다. 이 경우 steepest direction은 같은 policy이기 때문에 같은 방향을 가리키고 있어야 하는데 non-covariant한 경우 그렇지 못합니다. 떄문에 학습이 느려지고, 이러한 문제를 해결하기 위해 단순히 Positive-Definite Matrix $G(\\theta)$를 사용하지 않고 FIM $F_s(\\theta)$를 사용합니다. 어떠한 확률 변수 $X$가 임의의 매개변수 $\\theta$에 의해 정의되는 분포를 따른다고 하면 $X=x$일때 FIM은 다음과 같이 정의됩니다.</p>\n<p>$$F_x(\\theta)=E\\left[\\left(\\dfrac{\\partial}{\\partial\\theta}\\log\\Pr(x|\\theta)\\right)^2\\right]$$</p>\n<p>강화학습 관점에서 생각해보면 정보 $x$는 에피소드에 의해 관측된 상태값 $s$이며 매개변수 $\\theta$에 의해 선택될 수 있는 행동에 대한 분포가 나오게 됩니다. 이에 의해 위의 FIM는 다음과 같이 표현할 수 있습니다.</p>\n<p>$$F_s(\\theta) \\equiv E_{\\pi(a;s,\\theta)}\\left[\\left(\\dfrac{\\partial}{\\partial\\theta}\\log\\pi(a;s,\\theta)\\right)^2\\right] =E_{\\pi(a;s,\\theta)}\\left[\\dfrac{\\partial \\log\\pi(a;s,\\theta)}{\\partial \\theta_i}\\dfrac{\\partial \\log\\pi(a;s,\\theta)}{\\partial\\theta_j}\\right]$$</p>\n<p>그리고 위의 식들을 이용하여 objective function을 정리하면 아래의 식과 같이 표현됩니다.</p>\n<p>$$F(\\theta) = E_{\\rho^{\\pi}(s)}[F_s(\\theta)]$$</p>\n<p>또한 이 Fisher Information Matrix에 정의된 이 metric은 다음과 같은 성질을 가지고 있습니다.</p>\n<ol>\n<li>업데이트되는 파라미터에 의해 구성되는 매니폴드에 기반한 metric입니다.</li>\n<li>확률분포($\\pi(a;s,\\theta)$)를 구성하는 파라미터($\\theta$)의 변화에 독립적입니다.</li>\n<li>마지막으로 positive-definite한 값을 가집니다. 이렇기 때문에 steepest gradient에서 objective function의 방향을 알기 위해 사용한 방법과 같은 방법으로 natural gradient direction을 다음과 같이 구할 수 있는 것입니다.</li>\n</ol>\n<p>$$\\bar{\\nabla}\\eta(\\theta) \\equiv F(\\theta)^{-1}\\nabla\\eta(\\theta)$$</p>\n<p><br><br></p>\n<h1 id=\"4-The-Natural-Gradient-and-Policy-Iteration\"><a href=\"#4-The-Natural-Gradient-and-Policy-Iteration\" class=\"headerlink\" title=\"4. The Natural Gradient and Policy Iteration\"></a>4. The Natural Gradient and Policy Iteration</h1><p>4장에서는 Natural gradient를 통한 policy iteration을 수행하여 실제로 정책의 향상이 있는지를 증명합니다. 여기서 $Q^\\pi(s,a)$는 compatible function approximator $f^\\pi(s,a;w)$로 근사됩니다. (<a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/\">Sutton PG</a>를 참고해주시기 바랍니다.)</p>\n<p><br></p>\n<h2 id=\"4-1-Theorem-1-Compatible-Function-Approximation\"><a href=\"#4-1-Theorem-1-Compatible-Function-Approximation\" class=\"headerlink\" title=\"4.1 Theorem 1: Compatible Function Approximation\"></a>4.1 Theorem 1: Compatible Function Approximation</h2><p>approximate하는 함수 $f^{\\pi}(s,a;w)$는 다음과 같습니다.(compatible value function)</p>\n<p>$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$</p>\n<p>$$\\psi^{\\pi}(s,a) = \\nabla log\\pi(a;s,\\theta)$$</p>\n<p>여기서 $[\\nabla \\log\\pi(a;s,\\theta)]_i=\\partial \\log\\pi(a;s,\\theta)/\\partial\\theta_i$입니다. $w$는 원래 approximate하는 함수 $Q$와 $f$의 차이를 줄이도록 학습합니다(mean square error). 수렴한 local minima의 $w$를 $\\bar{w}$라고 가정 하겠습니다. 에러는 다음과 같은 수식으로 나타낼 수 있습니다.</p>\n<p>$$\\epsilon(w,\\pi)\\equiv\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)(f^{\\pi}(s,a;w)-Q^{\\pi}(s,a))^2$$</p>\n<p>그 때, $\\bar{\\omega} = \\bar{\\nabla}\\eta(\\theta)$이면 function approximator의 gradient 방향과 정책의 gradient 방향이 같다는 것을 의미합니다.</p>\n<p>아래의 내용은 위의 Theorem에 대한 증명입니다.</p>\n<p>위의 수식이 local minima이면 미분값이 0이 됩니다. $w$에 대해서 미분하면 다음과 같습니다.</p>\n<p>$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)(\\psi^{\\pi}(s,a)^T\\bar{w}-Q^{\\pi}(s,a))=0$$</p>\n<p>$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)^T\\bar{w}=\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)Q^{\\pi}(s,a)$$</p>\n<p>이 때, 위 식의 우변은 $\\psi$의 정의에 의해 policy gradient가 됩니다. 또한 왼쪽 항에서는 Fisher information matrix가 나옵니다.</p>\n<p>$$F(\\theta)=\\sum_{s,a}\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)=E_{\\rho^\\pi(s)}[F_s(\\theta)]$$</p>\n<p>따라서 다음과 같이 쓸 수 있습니다.</p>\n<p>$$F(\\theta)\\bar{w}=\\nabla\\eta(\\theta)$$</p>\n<p>$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>위의 수식은 natural gradient식과 동일합니다. 위의 수식은 policy가 update 될 때, value function approximator의 parameter 방향으로 이동한다는 것을 의미합니다. function approximation이 정확하다면 그 parameter의 natural policy gradient와 inner product가 커야합니다.</p>\n<p><br></p>\n<h2 id=\"4-2-Theorem-2-Greedy-Policy-Improvement\"><a href=\"#4-2-Theorem-2-Greedy-Policy-Improvement\" class=\"headerlink\" title=\"4.2 Theorem 2: Greedy Policy Improvement\"></a>4.2 Theorem 2: Greedy Policy Improvement</h2><p>이번 장에서는 natrual gradient는 다른 policy iteration 방법처럼 단순히 더 좋은 행동을 고르도록 학습하는 것이 아니라 가장 좋은(greedy) 행동을 고르도록 학습한다는 것을 증명하는 부분입니다. 이것을 일반적인 형태의 policy에 대해서 증명하기 전에 exponential 형태의 policy에 대해서 증명하는 것이 Theorem 2입니다. 특수한 정책을 가지는 상황안에서 학습속도($\\alpha$)를 무한대로 가져감으로서 어떤 action을 선택하는지를 알아봅니다.</p>\n<p>policy를 다음과 같이 정의합니다.</p>\n<p>$$\\pi(a;s,\\theta) \\propto \\exp(\\theta^T\\phi_{sa})$$</p>\n<p>여기서 $\\bar{\\nabla}\\eta(\\theta)$가 0이 아니고 $\\bar{w}$는 approximation error를 최소화된 $w$라고 가정합니다. 이 상태에서 natural gradient update를 생각해봅시다. 그리고 policy gradient는 gradient ascent임을 기억합시다.</p>\n<p>$$\\theta_{t+1}=\\theta_t + \\alpha_t\\bar{\\nabla}\\eta(\\theta)$$</p>\n<p>이 때 $\\alpha$가 learning rate로 parameter를 얼마나 업데이트하는지를 결정합니다. 이 값을 무한대로 늘렸을 때 policy가 어떻게 업데이트되는지 생각해봅시다.</p>\n<p>$$\\pi_{\\infty}(a;s)=lim_{\\alpha\\rightarrow\\infty}\\pi(a;s,\\theta+\\alpha\\bar{\\nabla}\\eta(\\theta))-(1)$$</p>\n<p>이 때,</p>\n<p>$$\\pi_{\\infty}=0 \\, \\, \\, if \\, and \\, only \\, if(필요충분조건) \\, \\, \\, a \\notin argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>이라고 말할 수 있습니다.</p>\n<p>아래의 내용은 위의 Theorem에 대한 증명입니다.</p>\n<p>먼저 function approximator는 앞서 다뤘듯이 아래와 같습니다.</p>\n<p>$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$</p>\n<p>여기서 function approximator는 Theorem 1에 의해 아래와 같이 쓸 수 있습니다.</p>\n<p>$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T\\psi^{\\pi}(s,a)$$</p>\n<p>$\\theta$의 정의에 의해 $\\psi$는 다음과 같습니다.</p>\n<p>$$\\psi^{\\pi}(s,a)=\\phi_{sa}-E_{\\pi(a’;s,\\theta)}[\\phi_{sa’}]$$</p>\n<p>따라서 function approximator는 다음과 같이 다시 쓸 수 있습니다.</p>\n<p>$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T(\\phi_{sa}-E_{\\pi(a’;s,\\theta)}[\\phi_{sa’}])$$</p>\n<p>greedy policy improvement가 Q function 값 중 가장 큰 값을 가지는 action을 선택하듯이 여기서도 function approximator의 값이 가장 큰 action을 선택하는 상황을 가정해봅시다. 이 때 function approximator의 argmax는 다음과 같이 쓸 수 있습니다.</p>\n<p>$$argmax_{a’}f^{\\pi}(s,a)=argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>(1) 식을 다시 살펴봅시다. 그러면 policy의 정의에 따라 다음과 같이 쓸 수 있습니다.</p>\n<p>$$\\pi(a;s,\\theta + \\alpha\\bar{\\nabla}\\eta(\\theta)) \\propto exp(\\theta^T\\phi_{sa} + \\alpha\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa})$$</p>\n<p>$\\bar{\\nabla}\\eta(\\theta) \\neq 0$이고 $\\alpha\\rightarrow\\infty$이면 exp안의 항 중에서 뒤의 항이 dominate하게 됩니다. 여러 행동 중에 $\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa}$가 가장 큰 행동이 있다면 이 행동의 policy probability가 1이 되고 나머지는 0이 됩니다. 따라서 다음이 성립합니다.</p>\n<p>$$\\pi_{\\infty}=0 \\, \\, \\, if \\, and \\, only \\, if \\, \\, \\, a \\notin argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>이 결과로부터 natural policy gradient는 단지 더 좋은 action이 아니라 best action을 고르도록 학습이 됩니다. 반면에 non-covariant gradient(1차미분)에서는 그저 더 좋은 action을 고르도록 학습이 됩니다. 이 natural policy gradient에 대한 결과는 infinite learning rate 세팅에서만 성립합니다. 좀 더 일반적인 경우에 대해서 살펴봅시다.</p>\n<p><br></p>\n<h2 id=\"4-3-Theorem-3-General-Parameterized-Policy\"><a href=\"#4-3-Theorem-3-General-Parameterized-Policy\" class=\"headerlink\" title=\"4.3 Theorem 3: General Parameterized Policy\"></a>4.3 Theorem 3: General Parameterized Policy</h2><p>Theorem 2에서와는 달리 일반적인 policy를 가정해봅시다(general parameterized policy). Theorem 3는 이 상황에서 natural gradient를 통한 업데이트가 best action를 고르는 방향으로 학습이 된다는 것을 보여줍니다.</p>\n<p>natural gradien에 따른 policy parameter의 업데이트는 다음과 같습니다. $\\bar{w}$는 approximation error를 minimize하는 $w$입니다.</p>\n<p>$$\\delta\\theta = \\theta’ - \\theta = \\alpha\\bar{\\nabla}\\eta(\\theta)=\\alpha\\bar{w}$$</p>\n<p>policy에 대해서 1차근사를 하면 다음과 같습니다.</p>\n<p>$$\\pi(a;s,\\theta’)=\\pi(a;s,\\theta)+\\frac{\\partial\\pi(a;s,\\theta)^T}{\\partial\\theta}\\delta\\theta + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\psi(s,a)^T\\delta\\theta) + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\alpha\\psi(s,a)^T\\bar{w}) + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\alpha f^{\\pi}(s,a;\\bar{w})) + O(\\delta\\theta^2)$$</p>\n<p>policy 자체가 function approximator의 크기대로 업데이트가 되므로 local하게 best action의 probability는 커지고 다른 probability의 크기는 작아질 것입니다. 하지만 만약 greedy improvement가 된다하더라도 그게 performance의 improvement를 보장하는 것은 아닙니다. 하지만 line search와 함께 사용할 경우 improvement를 보장할 수 있습니다. 왜 그런지는 처음에 말씀드린 블로그 <a href=\"http://darkpgmr.tistory.com/149\" target=\"_blank\" rel=\"noopener\">다크 프로그래머님의 블로그</a>를 참고해주시기 바랍니다.</p>\n<p><br><br></p>\n<h1 id=\"5-Metrics-and-Curvatures\"><a href=\"#5-Metrics-and-Curvatures\" class=\"headerlink\" title=\"5. Metrics and Curvatures\"></a>5. Metrics and Curvatures</h1><p>$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$</p>\n<p>이 파트에서는 FIM과 다른 metric 사이의 관계를 다룹니다.</p>\n<p>위의 식에 해당하는 G는 Fisher Information Matrix만 사용할 수 있는 것이 아닙니다. Positive-Definite Matrix인 FIM이외의 다른 Matrix도 사용할 수 있습니다. 또한 다양한 파라미터 추정에서 FIM은 Hessian Matrix에 수렴하지 않을 수 있다고 합니다. 이 말은 2nd order(2차 근사) 수렴이 보장되지 않는다는 말입니다.</p>\n<p>논문에 있는 내용들을 조금 더 추가적으로 보면 아래와 같이 나옵니다.</p>\n<p>In the different setting of parameter estimation, the Fisher information converges to the <code>Hessian</code>, so it is <a href=\"https://en.wikipedia.org/wiki/Efficiency_(statistics\" target=\"_blank\" rel=\"noopener\">asymptotically efficient</a>) 이지만,</p>\n<p>이 논문의 경우, 아마리 논문의 ‘blind separation case’와 유사한데 이 때는 꼭 asymtotically efficient하지 않다고 말합니다. 이 말은 즉 2nd order 수렴이 보장되지 않는다는 것이다.</p>\n<p><a href=\"http://www.inference.org.uk/mackay/ica.pdf\" target=\"_blank\" rel=\"noopener\">Mackay</a> 논문에서는 Hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했습니다. 그래서 performance를 2번 미분해보면 아래의 수식과 같습니다. 하지만 다음 식에서는 모든 항이 data dependent합니다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있습니다.</p>\n<p>$$\\nabla^2\\eta(\\theta)=\\sum_{sa}\\rho^{\\pi}(s)(\\nabla^2\\pi(a;s)Q^{\\pi}(s,a)+\\nabla\\pi(a;s)\\nabla Q^{\\pi}(s,a)^T+\\nabla Q^{\\pi}(s,a)\\nabla\\pi(a;s)^T)$$</p>\n<p>hessian은 보통 positive definite가 아닐수도 있습니다. 따라서 local maxima가 될 때까지 Hessian이 사용하기 별로 안좋습니다. 그리고 local maxima에서는 Hessian보다는 Conjugate methods가 더 효율적이라고 합니다.</p>\n<p>사실 이 파트에서는 무엇을 말하고 있는지 알기가 어렵습니다. FIM과 Hessian이 관련이 있다는 것을 알겠는데 asymtotically efficient와 같은 내용을 모르므로 내용의 이해가 어려웠습니다.</p>\n<p>Mackay 논문에서 해당 부분은 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/x4n6z6pdyi7xtb9/Screenshot%202018-06-10%2012.04.13.png?dl=1\"> </center>\n\n<p><br></p>\n<h2 id=\"5-1-Fisher-Information-Matrix-FIM-vs-Hessian\"><a href=\"#5-1-Fisher-Information-Matrix-FIM-vs-Hessian\" class=\"headerlink\" title=\"5.1 Fisher Information Matrix(FIM) vs. Hessian\"></a>5.1 Fisher Information Matrix(FIM) vs. Hessian</h2><p>FIM과 Hessian에 대해서 추가적인 설명을 하고자 합니다.</p>\n<ul>\n<li>FIM</li>\n</ul>\n<p>일단 정의되려면 space 자체가 stochastic한 성질을 가지고 있어야 합니다. 모든 parameter를 표현하는 함수가 deterministic한 함수가 아니라 확률로 나타낸 분포입니다. (probability distribution)</p>\n<p>결국은 FIM에서도 hessian처럼 비슷한 과정을 취하고 싶은 것입니다. 따라서 확률 변수이기 때문에 어떠한 특정 sample을 취하면 그게 항상 다른값이 됩니다. 그래서 FIM에다가 expectation을 취한 것입니다. expectation을 취함으로써 hessian같은 성질을 가집니다. 왜냐하면 expectation을 취함으로써 constant한 값이 되기 때문입니다.</p>\n<ul>\n<li>Hessian</li>\n</ul>\n<p>그냥 deterministic한 함수에서 정의됩니다. 그냥 함수를 두 번 미분 한 것이라고 보면 됩니다.</p>\n<p><br></p>\n<h2 id=\"5-2-Conjugate-Gradient-Method\"><a href=\"#5-2-Conjugate-Gradient-Method\" class=\"headerlink\" title=\"5.2 Conjugate Gradient Method\"></a>5.2 Conjugate Gradient Method</h2><p>추가적으로 Conjugate Gradient Method(CGM)에 대해서 다루고자 합니다.</p>\n<ul>\n<li>참고자료<ul>\n<li><a href=\"https://www.quora.com/What-is-an-intuitive-explanation-of-what-the-conjugate-gradient-method-is\" target=\"_blank\" rel=\"noopener\">https://www.quora.com/What-is-an-intuitive-explanation-of-what-the-conjugate-gradient-method-is</a></li>\n<li><a href=\"https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf\" target=\"_blank\" rel=\"noopener\">https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Conjugate_gradient_method\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Conjugate_gradient_method</a></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"mathbf-A-mathbf-x-mathbf-b-의-해-구하기\"><a href=\"#mathbf-A-mathbf-x-mathbf-b-의-해-구하기\" class=\"headerlink\" title=\"$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해 구하기\"></a>$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해 구하기</h3><p>$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해를 구하는 문제를 생각해봅시다. $\\mathbf{A}$의 역행렬을 구해서 양변에 곱해주면 $\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$가 되어 쉽게 해를 구할 수 있습니다. 하지만 $\\mathbf{A}^{-1}$은 계산이 많이 필요 자원소모가 큰 연산입니다. 역행렬을 취하지 않고 해를 구할 수 있는 방법이 있을까요?</p>\n<p>위의 방정식에서 $\\mathbf{b}$를 이항시키면 $\\mathbf{A}\\mathbf{x} - \\mathbf{b} = 0$이 됩니다. 최적화문제는 많은 경우 1차 미분이 0이 되는 지점이 해일 확률이 높습니다. $\\mathbf{A}\\mathbf{x} - \\mathbf{b} = 0$을 1차 미분으로 가지는 함수는 무엇일까요?</p>\n<p>$$f( \\mathbf{x} ) = \\frac{1}{2}\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x} - \\mathbf{x}^\\mathrm{T}\\mathbf{b}$$</p>\n<p>위의 함수는 $\\mathbf{A}\\mathbf{x} - \\mathbf{b}$를 1차 미분값으로 가집니다. 이 때 $\\mathbf{A}$는 symmetric positive definite해야 합니다.</p>\n<ul>\n<li>symmetric: $\\mathbf{A}=\\mathbf{A}^\\mathrm{T}$</li>\n<li>positive definite: $\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}&gt;0,\\quad\\forall\\mathbf{x}$ or all eigenvalues of $\\mathbf{A}$ are positive</li>\n</ul>\n<p>symmetric positive definite한 성질은 $\\mathbf{x}$를 strict convex function이 되게 만들어서 unique solution이 존재하게 합니다. $\\mathbf{A}$는 $\\mathbf{x}$의 Hessian이기도 합니다.</p>\n<p>자, 우리는 위의 방정식의 해 $\\mathbf{x}^*$를 iterative한 방식으로 찾고자 합니다. $\\mathbf{x}_0$을 초기값이라고 합시다.</p>\n<p>우리는 <img src=\"https://www.dropbox.com/s/g91yqajf72jzjoc/Screen%20Shot%202018-08-14%20at%208.43.28%20AM.png?dl=1\" width=\"80\">가 되는 <img src=\"https://www.dropbox.com/s/aj3j5fjtiyewlo1/Screen%20Shot%202018-08-14%20at%208.43.37%20AM.png?dl=1\" width=\"30\">를 찾고 싶은데 이를 위한 현재의 추정값은 $\\mathbf{A} \\mathbf{x}_0$입니다. 최적의 값을 찾았을 때와 비교하면 현재의 오차는 $\\mathbf{b} - \\mathbf{A}\\mathbf{x}_0 = \\mathbf{r}_0$입니다. 이것을 residual이라고 합니다. 어떻게 효율적으로 값을 변화시켜서 오차를 0으로 만들 수 있을까요? 일단 매 iteration마다 residual이 작아지는 방향으로 나아가야겠죠?</p>\n<p>가장 널리 알려진 방법은 gradient descent 방향입니다. gradient가 증가하면 반대로 가고 gradient가 감소하면 그 방향으로 가는 것입니다. 그리고 어떤 방향으로든 gradient가 0이면 그 지점에 멈추는 것이죠. 이 방법은 수렴은 하지만 zigzag하게 움직여서 느립니다. 이것보다 더 좋은 방법이 없을까요?</p>\n<h3 id=\"Conjugate-Gradient-Method\"><a href=\"#Conjugate-Gradient-Method\" class=\"headerlink\" title=\"Conjugate Gradient Method\"></a>Conjugate Gradient Method</h3><h4 id=\"Gram-Schmidt-orthgonalization\"><a href=\"#Gram-Schmidt-orthgonalization\" class=\"headerlink\" title=\"Gram-Schmidt orthgonalization\"></a>Gram-Schmidt orthgonalization</h4><p>다음과 같은 vector들의 집합 <img src=\"https://www.dropbox.com/s/csgdsfofe19q5r6/Screen%20Shot%202018-08-14%20at%208.56.04%20AM.png?dl=1\" width=\"135\">이 있다고 해봅시다. 이 vector들이 서로 linearly independent하다면 이 vector들은 $\\mathbb{R}^n$ 공간 상의 basis입니다. 이 vector들을 이용해서 orthogonal한 vector들을 만들어보겠습니다. 새로운 vector들을 <img src=\"https://www.dropbox.com/s/7xg7pciar1ndu44/Screen%20Shot%202018-08-14%20at%208.58.25%20AM.png?dl=1\" width=\"130\">라고 한다면 아래와 같은 과정을 통해 만들 수 있습니다. 이 방법을 Gram-Schmidt orthogonalization이라고 합니다.</p>\n<ul>\n<li>$\\mathbf{d}_1=\\mathbf{v}_1$</li>\n<li>$\\mathbf{d}_2=\\mathbf{v}_2 - \\left\\langle\\mathbf{v}_2,\\frac{\\mathbf{d}_1}{\\parallel\\mathbf{d}_1\\parallel}\\right\\rangle\\frac{\\mathbf{d}_1}{\\parallel\\mathbf{d}_1\\parallel}$<br>…</li>\n<li><img src=\"https://www.dropbox.com/s/arzc5cez8az0j7h/Screen%20Shot%202018-08-14%20at%209.00.04%20AM.png?dl=1\" width=\"290\"></li>\n</ul>\n<p>간단히 설명을 하면, 첫 vector는 basis와 같은 방향으로 출발합니다. 그 다음 vector는 그 다음 basis를 이전 vector로 projection한 다음 해당 basis로부터 빼줍니다. 그 다음 vector를 만들 때는 이전에 만든 모든 vector들에 대해서 projection을 취한 다음 모두 더한 것을 해당 basis에서 빼줍니다. </p>\n<h4 id=\"A-conjugate\"><a href=\"#A-conjugate\" class=\"headerlink\" title=\"$A$-conjugate\"></a>$A$-conjugate</h4><p>이 방법을 이용하여 matrix $A$에 관하여 orthogonal한 새로운 vector들의 집합을 만들어보겠습니다. 일반적인 inner product와 약간 다른 다음과 같은 inner product를 정의할 수 있습니다.</p>\n<p>$$&lt;\\mathbf{x},\\mathbf{y}&gt;_\\mathbf{A} = \\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{y}$$</p>\n<p>이 때 $&lt;\\mathbf{x},\\mathbf{y}&gt;_\\mathbf{A} = 0$이면 $\\mathbf{x}와 \\mathbf{y}$는 $\\mathbf{A}$-orthogonal 또는 $\\mathbf{A}$-conjugate하다라고 합니다. vector norm $\\parallel\\cdot\\parallel$도 일반적인 vector norm이 아닌 A-norm을 다음과 같이 정의합니다.</p>\n<p>$$\\parallel\\mathbf{x}\\parallel_\\mathbf{A}^2=\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}$$</p>\n<p>Gram-Schmidt orthogonalization을 또 이용해볼까요? 아래와 같습니다.</p>\n<ul>\n<li>$\\mathbf{d}_1=\\mathbf{v}_1$</li>\n<li><img src=\"https://www.dropbox.com/s/i8wnvoccefgg7t8/Screen%20Shot%202018-08-14%20at%209.08.42%20AM.png?dl=1\" width=\"400\"><br>…</li>\n<li><img src=\"https://www.dropbox.com/s/045x2v5sx7s0wot/Screen%20Shot%202018-08-14%20at%209.09.03%20AM.png?dl=1\" width=\"500\"></li>\n</ul>\n<p>그런데 이 Gram-Schmidt 방법은 단점이 하나 있습니다. 새로운 vector를 만들어내기 위해서 이전에 만든 vector들을 모두 가지고 있어야 하고 projection도 다시 계산해야 한다는 점입니다. 그런데 마법같은 이유($\\approx$ 여기서 설명하지 않는 이유)로 인해서 오직 마지막으로 만들어낸 vector만 가지고 있어도 기존의 모든 vector들이 성질을 표현해낼 수 있습니다. 그러면 우리가 만들어낸 vector는 다음과 같이 간단해집니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/wlyu7ax6qt0aw9w/Screen%20Shot%202018-08-14%20at%209.09.43%20AM.png?dl=1\" width=\"550\"> </center>\n\n<p>이 방향이 우리가 업데이트시키고 싶은 방향입니다. 즉 우리는 다음 수식처럼 움직이고자 합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/053o4ocvxsgxfl9/Screen%20Shot%202018-08-14%20at%209.09.56%20AM.png?dl=1\" width=\"180\"> </center>\n\n<p>$d_k$는 방향이고 $\\alpha_{k}$는 step size입니다. 이러한 방법을 일반적으로 line search method라고 부릅니다. 방향은 위에서 구한 conjugate 방향을 이용합니다. 그래서 이 line search를 conjugate gradient method라고 부릅니다. 이 방법의 한가지 중요한 장점은 업데이트가 무조건 n번만 일어난다는 것입니다! n-dimensional space에서는 basis가 n개 밖에 없거든요. 그렇다면 얼마만큼 많이 이 방향으로 움직여야 할까요? 더 이상 $f(\\mathbf{x})$가 감소하지 않을 때까지 움직이는게 좋지 않을까요? 이것은 다시 말하면 gradient가 0이 될 때까지 움직인다는 뜻입니다. 위에서 설명한 residual과도 관계있을 것 같지 않나요?</p>\n<p>$\\nabla f(\\mathbf{x})=\\mathbf{A}\\mathbf{x} - \\mathbf{b}$에 $x_k + \\alpha_{k} d_k$를 대입해 봅시다. 아래와 같이 step size를 구할 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/xikpfgcix64ngmp/Screen%20Shot%202018-08-14%20at%209.13.20%20AM.png?dl=1\" width=\"300\"> </center>\n\n<p>네, 이제 우리는 방향과 step size를 모두 알았습니다. 이제 update만 하면 해를 찾을 수 있겠군요!</p>\n<center> <img src=\"https://www.dropbox.com/s/avjw4kbzd4ormv5/conjugate_gradient_wikipedia.png?dl=1\" width=\"200\"> </center>\n\n<p>위키피디아에서 가져온 위의 그림을 봅시다. 그림의 녹색선이 gradient descent이고 빨간선이 conjugate gradient입니다. conjugate gradient는 gradient descent보다 빨리 수렴합니다. 녹색선은 항상 이전 이동 방향과 직각으로 이동합니다. 빨간선은 보다 빠르게 더 낮은 값이 있는 곳으로 이동합니다.</p>\n<p>위에서 설명을 생략했지만 왜 conjugate gradient는 빠르게 수렴(n-step)하며 Gram-Schmidt orthogonalization을 할 때도 메모리가 적게 필요할까요? 수학적으로 엄밀하게 설명하기 보다는 개념적으로 설명을 하겠습니다. 그 비밀은 바로 $\\mathbf{A}$-orthogonal 또는 $\\mathbf{A}$-conjugate vector들을 이용한데 있습니다. 이 vector들은 basis라고 했습니다. 즉, 처음에 우리가 구하고자 했던 해 $\\mathbf{x}^*$를 이 vector들을 이용해서 표현할 수 있습니다. 또한 iterative하게 찾아가기 위한 초기 vector $\\mathbf{x}_0$도 이 vector들로 표현할 수 있습니다. 그렇다면 이 둘 간의 오차도 이 vector들도 표현할 수 있게 됩니다. 다음 수식처럼 말입니다.</p>\n<p>$$\\mathbf{x}^* - \\mathbf{x}_0 = \\epsilon_1\\mathbf{d}_1 + \\epsilon_2\\mathbf{d}_2 + \\cdots + \\epsilon_n\\mathbf{d}_n$$</p>\n<p>우리의 목표는 이 오차를 줄이는 것입니다. 각각의 basis마다 오차가 있으며 이들은 서로 independent합니다. 즉, 특정 iteration에서 특정 basis에 대한 오차를 0으로 만들면 다음 번 iteration에서는 이 오차는 영향을 받지 않습니다! 이러한 이유로 n번의 iteration만으로 해를 찾을 수 있고 다른 vector들을 저장할 필요도 없는 것입니다. 선형대수의 아름다움이 느껴지지 않으시나요? 오래되었지만 참 잘 디자인된 기법이라는 생각이 듭니다.</p>\n<p><br><br></p>\n<h1 id=\"6-Experiment\"><a href=\"#6-Experiment\" class=\"headerlink\" title=\"6. Experiment\"></a>6. Experiment</h1><p>이 논문에서는 natural gradient를 simple MDP와 tetris MDP에 대해서 실험을 진행했습니다. FIM은 다음과 같은 식으로 업데이트합니다.</p>\n<p>$$f\\leftarrow f+\\nabla \\log \\pi(a_t; s_t, \\theta)\\nabla \\log \\pi(a_t; s_t, \\theta)^T$$</p>\n<p>$T$ length trajectory에 대해서 $f/T$를 이용해 $F$의 기대값($E$)를 구합니다.</p>\n<p><br></p>\n<h2 id=\"6-1-LQR-Linear-Quadratic-Regulator\"><a href=\"#6-1-LQR-Linear-Quadratic-Regulator\" class=\"headerlink\" title=\"6.1 LQR(Linear Quadratic Regulator)\"></a>6.1 LQR(Linear Quadratic Regulator)</h2><p>Agent를 실험할 환경은 다음과 같은 dynamics를 가지고 있습니다.</p>\n<p>$x(t+1) = 0.7x(t)+u(t)+\\epsilon(t)$</p>\n<p>$u(t)$는 control 신호로서 에이전트의 행동입니다. $\\epsilon$은 noise distribution으로 환경에 가해지는 노이즈입니다. 에이전트의 목표는 적절한 $u(t)$를 통해 $x(t)$를 0으로 유지하는 것입니다. $x(t)$를 0으로 유지하기 위해서 필요한 소모값(cost)는 $x(t)^2$로 정의하며 cost를 최소화하도록 학습합니다. 이 논문에서는 실험할 때 복잡성을 더 해주기 위해 noise distribution인 $\\epsilon$을 dynamics에 추가하였습니다.</p>\n<p>이 실험에서 policy는 다음과 같이 설정하였습니다. 파라미터가 2개 밖에 없는 간단한 policy입니다.</p>\n<p>$\\pi(u;x,\\theta)\\propto \\exp(\\theta_1s_1x^2+\\theta_2s_2x)$</p>\n<p>이 policy를 간단히 그래프로 그려보면 다음과 같습니다. 가로축은 $x$, 세로축은 cost입니다. $\\theta_1$과 $\\theta_2$를 (0.5, 0.5), (1, 0), (0, 1)로 설정하고 $s_1$, $s_2$는, 1로 두었습니다. $x$는 -1~1사이의 범위로 그렸습니다. </p>\n<center> <img src=\"https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1\" width=\"500px\"> </center>\n\n<p>아래의 그림은 LQR을 학습한 그래프입니다. cost가 $x^2$이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있습니다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과입니다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 곡선입니다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습합니다(time 축이 log scale인 것을 감안하면 차이가 많이 납니다.). </p>\n<p>하지만 문제가 있습니다. NPG를 학습한 세 개의 곡선은 $\\theta$를 rescale 한 것입니다. $\\theta$앞에 곱해지는 숫자에 따라 학습의 과정이 다릅니다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것입니다. 즉, covariant gradient가 아니라는 뜻입니다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것입니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1\" width=\"300px\"> </center>\n\n<p>natural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문입니다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 $\\rho_s$가 곱해지기 때문입니다. 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것입니다!</p>\n<p><br></p>\n<h2 id=\"6-2-Simple-2-state-MDP\"><a href=\"#6-2-Simple-2-state-MDP\" class=\"headerlink\" title=\"6.2 Simple 2-state MDP\"></a>6.2 Simple 2-state MDP</h2><p>이제 다른 예제에서 NPG를 테스트합니다. 2개의 state만 가지는 MDP를 고려해봅시다. <a href=\"http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&amp;context=robotics\" target=\"_blank\" rel=\"noopener\">그림출처</a>. 그림으로보면 다음과 같습니다. $x=0$ 상태와 $x=1$ 상태 두 개가 존재합니다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있습니다. 상태 $x=0$에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 $x=1$에서 자기 자신으로 돌아오면 2의 보상을 받습니다. 결국 optimal policy는 상태 $x=1$에서 계속 자기 자신으로 돌아오는 행동을 취하는 것입니다. </p>\n<p><img src=\"https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1\"></p>\n<p>문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정합니다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것입니다.</p>\n<p>$$\\rho(x=0)=0.8,  \\rho(x=1)=0.2$$</p>\n<p>일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 됩니다. 이 때, $\\rho(s)$가 gradient에 곱해지므로 상대적으로 상태 0에서의 gradient 값이 커지게 됩니다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update합니다. 따라서 아래 그림의 첫번째 그림에서처럼 reward가 1에서 오랜 시간동안 머무릅니다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것입니다.</p>\n<p>$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$</p>\n<center><img src=\"https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1\" width=\"300px\"></center>\n\n<p>하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달합니다. gradient 방법이 $1.7\\times 10^7$정도의 시간만에 2에 도달한 반면 NPG는 2의 시간만에 도달합니다. 또한 $\\rho(x=0)$가 $10^{-5}$이하로 떨어지지 않습니다.</p>\n<p>한 가지 그래프를 더 살펴봅시다. 다음 그래프는 parameter $\\theta$가 업데이트 되는 과정을 보여줍니다. 이 그래프에서는 parameter가 2개 있습니다. 일반적인 gradient가 아래 그래프에서 실선에 해당합니다. 이 실선의 그래프는 보면 처음부터 중반까지 $\\theta_i$만 거의 업데이트하는 것을 볼 수 있습니다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있습니다. </p>\n<center><img src=\"https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1\" width=\"300px\"></center>\n\n<p>policy가 $\\pi(a;s,\\theta)\\propto \\exp(\\theta_{sa})$일 때, 다음과 같이 $F_{-1}$이 gradient 앞에 weight로 곱해지는데 이게 $\\rho$와는 달리 각 parameter에 대해 균등합니다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것입니다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$</p>\n<p><br></p>\n<h2 id=\"6-3-Tetris\"><a href=\"#6-3-Tetris\" class=\"headerlink\" title=\"6.3 Tetris\"></a>6.3 Tetris</h2><p>NPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어 있습니다. 다음 그림은 tetris 예제를 보여줍니다. 보통 그림에서와 같이 state의 feature를 정해줍니다. <a href=\"http://slideplayer.com/slide/5215520/\" target=\"_blank\" rel=\"noopener\">그림 출처</a></p>\n<p><img src=\"https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1\"></p>\n<p>이 예제에서도 exponential family로 policy를 표현합니다. $\\pi(a;s,\\theta) \\propto \\exp(\\theta^T\\phi_{sa})$ 로 표현합니다.</p>\n<p>tetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있습니다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우입니다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법입니다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷합니다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지합니다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있습니다.</p>\n<p><img src=\"https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1\"></p>\n<p><br><br></p>\n<h1 id=\"7-Discussion\"><a href=\"#7-Discussion\" class=\"headerlink\" title=\"7. Discussion\"></a>7. Discussion</h1><p>Natural Gradient Method는  Policy Iteration에서와 같이 Greedy Action을 선택하도록 학습합니다. Line search 기법과 함께 사용하면 더 Policy Iteration과 비슷해집니다. Greedy Policy Iteration에서와 비교하면 Performance Improvement가 보장됩니다. 하지만 FIM이 asymtotically Hessian으로 수렴하지 않습니다. 그렇기 때문에 Conjugate Gradient Method로 구하는 방법이 더 좋을수 있습니다.</p>\n<p>살펴봤듯이 본 논문의 NPG는 완벽하지 않습니다. 위의 몇가지 문제점을 극복하기 위한 추가적인 연구가 필요하다고 할 수 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"DDPG-여행하기\"><a href=\"#DDPG-여행하기\" class=\"headerlink\" title=\"DDPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/\">DDPG 여행하기</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"NPG-Code\"><a href=\"#NPG-Code\" class=\"headerlink\" title=\"NPG Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py\" target=\"_blank\" rel=\"noopener\">NPG Code</a></h2><h2 id=\"TRPO-여행하기\"><a href=\"#TRPO-여행하기\" class=\"headerlink\" title=\"TRPO 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/\">TRPO 여행하기</a></h2>","site":{"data":{}},"excerpt":"","more":"<center> <img src=\"../../../../img/irl/maxent_1.png\" width=\"850\"> </center>\n\n<p>Author : Brian D. Ziebart, Andrew Maas, J.Andrew Bagnell, Anind K. Dey<br>Paper Link : <a href=\"http://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf\" target=\"_blank\" rel=\"noopener\">http://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf</a><br>Proceeding : Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence 2008</p>\n<hr>\n<h1 id=\"0-Abstract\"><a href=\"#0-Abstract\" class=\"headerlink\" title=\"0. Abstract\"></a>0. Abstract</h1><p>이 논문이 발표된 2002년 당시에도 많은 연구자들이 objective function의 gradient 값을 따라서 좋은 policy $\\pi$를 찾고자 하였습니다. 하지만 기존의 우리가 알던 gradient descent method는 steepest descent direction이 아닐 수 있기 때문에(쉽게 말해 가장 가파른 방향을 따라서 내려가야 하는데 그러지 못할 수도 있다는 것입니다.) 이 논문에서 steepest descent direction를 나타내는 natural gradient method를 policy gradient에 적용하여 좋은 policy $\\pi$를 찾습니다. </p>\n<p><br></p>\n<h2 id=\"1-1-NPG-흐름-잡기\"><a href=\"#1-1-NPG-흐름-잡기\" class=\"headerlink\" title=\"1.1 NPG 흐름 잡기\"></a>1.1 NPG 흐름 잡기</h2><h3 id=\"1-1-1-매니폴드-manifold\"><a href=\"#1-1-1-매니폴드-manifold\" class=\"headerlink\" title=\"1.1.1 매니폴드(manifold)\"></a>1.1.1 매니폴드(manifold)</h3><center> <img src=\"https://www.dropbox.com/s/cstjgemqpby4ysr/Screen%20Shot%202018-08-12%20at%208.28.29%20PM.png?dl=1\" width=\"300\"> </center>\n\n<p>매니폴드는 간단하게 말해 위의 그림에서의 점들을 아우르는 subspace입니다. 그래서 NPG 공부하기 전에 매니폴드를 왜 배울까라는 의문이 들으실텐데 그 이유는 위에서도 언급했듯이 natural gradient method는 어떠한 파라미터 공간에서의 steepest descent direction을 강조하기 때문입니다. 여기서 파라미터 공간이 바로 리만 매니폴드입니다. 리만 매니폴드는 매니폴드가 각지지 않고 미분가능하게 부드럽게 곡률을 가진 면이라고 생각하면 편합니다.</p>\n<p>Neural Network(NN)을 사용할 경우 NN의 parameter space가 우리가 보통 생각하는 직선으로 쭉쭉 뻗어있는 유클리디안 공간(Euclidean space)가 아닙니다. 좀 더 일반적으로는 구의 표면과 같이 휘어져있는 공간 즉, 리만 공간(Riemannian space)로 표현할 수 있습니다. 다음 그림들을 보겠습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/jnlm6he18ar64yc/Screen%20Shot%202018-08-12%20at%208.14.13%20PM.png?dl=1\" width=\"400\"> </center>\n\n<center> <img src=\"https://www.dropbox.com/s/tufjqmaeaz29kaz/Screen%20Shot%202018-08-12%20at%208.15.53%20PM.png?dl=1\" width=\"400\"> </center>\n\n<p>아래의 그림처럼 어떠한 확률 분포가 있다고 해봅시다.</p>\n<center> <img src=\"https://www.dropbox.com/s/mpoop19eyu1vp0a/Screen%20Shot%202018-08-13%20at%2012.52.09%20PM.png?dl=1\" width=\"600\"> </center>\n\n<p>어떠한 공간의 확률 분포에 있는 한 점은 다른 공간의 확률 분포에서의 한 점이 될 것입니다. 이렇게 위의 그림들과 같이 유클리디안 공간의 확률 분포에서의 한 점이 리만 공간의 확률 분포의 한 점이 되는 것이고, 곡률의 일차 근사가 유클리디안 공간에서는 일차 근사가 아닌 이차 근사이고, 리만 공간에서는 휘어진 공간이기 때문에 곡률을 일차 근사라고 보는 것입니다. 추가적으로 일차 근사와 이차 근사의 차이는 <a href=\"http://darkpgmr.tistory.com/149\" target=\"_blank\" rel=\"noopener\">다크 프로그래머님의 블로그</a>를 참고해주시기 바랍니다. (꼭 보세요!) </p>\n<p>일차 근사와 이차 근사의 차이점과 각각의 장단점, 그리고 추가적으로 Line Search까지 알고 난 후에 이 논문을 보시는 것을 권장해드립니다.</p>\n<h3 id=\"1-1-2-Natural-Gradient-Policy-Gradient\"><a href=\"#1-1-2-Natural-Gradient-Policy-Gradient\" class=\"headerlink\" title=\"1.1.2 Natural Gradient + Policy Gradient\"></a>1.1.2 Natural Gradient + Policy Gradient</h3><p>먼저 아래의 그림들을 보여드리겠습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/65hra43zadsubff/Screen%20Shot%202018-08-13%20at%2012.51.45%20PM.png?dl=1\" width=\"500\"> <img src=\"https://www.dropbox.com/s/8g332zceeordqwv/Screen%20Shot%202018-08-13%20at%2012.51.51%20PM.png?dl=1\" width=\"400\"> </center>\n\n<p>위에서도 언급했듯이 natural gradient가 steepest direction이 된다는 연구가 이뤄지고 있었습니다. 강화학습의 policy gradient은 objective function의 gradient를 따라 policy를 업데이트를 합니다. 이 때 policy는 parameterization되는데 이 경우에도 gradient 대신에 natural gradient가 좋다는 것을 실험해보는 논문이 지금 다루고 있는 논문입니다.</p>\n<p>gradient가 non-covariant(1차 근사)해서 생기는 문제는 간단히 말하자면 다음과 같습니다. policy가 parameterized된 상황에서는 같은 policy라도 다른 parameter를 가질 수 있습니다. 이 때, steepest direction은 두 경우에 같은 방향을 가리켜야 하는데 non-covariant한 경우 그렇지 못합니다. 이러한 부분이 바로 결국 느린 학습으로 연결이 되는 것입니다.</p>\n<p>논문에서 2차미분 방법론들과 짧게 비교합니다. 하지만 개인적인 의견으로 2차미분을 이용한 다른 방법들과의 비교가 생각보다 없는 점이 부족해 보였습니다.(Hessian을 이용한다거나 conjugate gradient method를 이용한다거나). 실험을 통해 Fisher Information Matrix(FIM)가 hessian에 수렴안하는 거라던지 Hessian 방법론이 local maxima 부근에서 상당히 느리다던지의 결과를 보여줬었으면 좋았을 것 같습니다.</p>\n<p>또한 natural gradient의 단점으로 natural gradient 만으로 업데이트하면 policy의 improvement 보장이 안될 수 있습니다. policy의 improvement를 보장하기 위해 line search도 써야하는데 line search를 어떻게 쓰는지에 대한 자세한 언급이 없습니다. 다시 말해 자세한 algorithm 설명이 없다는 것입니다.</p>\n<p>natural policy gradient 논문은 natural gradient + policy gradient를 처음 적용했다는데 의의가 있습니다. 하지만 이 논문이 문제 삼은 gradient는 non-covariant하다라는 문제를 natural gradient를 통해 해결하지는 못했습니다(Experiment를 통해 covariant gradient가 되지 못했다는 것이 보입니다). NPG의 뒤를 잇는 논문이 “covariant policy search”와 “natural actor-critic”에서 covariant(2차 근사)하지 못하다는 것을 해결하기 위해 Fisher Information Matrix를 sample 하나 하나에 대해서 구하는 것이 아니라 trajectory 전체에 대해서 구합니다.</p>\n<p>또한 논문은 pg의 두 가지 세팅 중에 average-reward setting(infinite horizon)에서만 NPG를 다룹니다. “covariant policy search” 논문에서는 average-reward setting과 start-state setting 모두에 대해서 npg를 적용합니다.</p>\n<p>natural gradient + policy gradient를 처음 제시했다는 것은 좋지만 npg 학습의 과정을 자세하게 설명하지 않았고 다른 2차 미분 방법들과 비교를 많이 하지 않은 점이 아쉬운 논문이었습니다.</p>\n<p><br><br></p>\n<h1 id=\"2-Introduction\"><a href=\"#2-Introduction\" class=\"headerlink\" title=\"2. Introduction\"></a>2. Introduction</h1><p>소개는 앞에서 다 했기 때문에 간략하게 다시 한 번 정리하겠습니다. direct policy gradient method는 future reward의 gradient를 따라 policy를 update합니다. 하지만 gradient descent는 non-covariant입니다. 따라서 이 논문에서는 covarient gradient를 제시합니다. 바로 “Natural Gradient” 입니다. </p>\n<p>또한 natural gradient와 policy iteration의 연관성을 설명합니다. natural policy gradient is moving toward choosing a greedy optimal action (이런 연결점은 아마도 step-size를 덜 신경쓰고 싶어서 그런게 아닌가 싶습니다)</p>\n<p>논문의 Introduction 부분에 다음 멘트가 있습니다. 이 글만 봐서는 이해가 안갔는데 Mackay 논문에 좀 더 자세히 나와있었습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/41xhhr7lgfk24a1/Screenshot%202018-06-10%2011.45.18.png?dl=1\"> </center>\n\n<p><a href=\"http://www.inference.org.uk/mackay/ica.pdf\" target=\"_blank\" rel=\"noopener\">Mackay</a>논문에서는 다음과 같이 언급하고 있습니다. Back-propagation을 사용할 경우에 learning rate를 dimension에 1/n로 사용하면 수렴한다는 것이 증명됐습니다. 하지만 너무 느리다고 합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/us9ezc7vxgrkez6/Screenshot%202018-06-10%2011.47.21.png?dl=1\"> </center>\n\n<p><br><br></p>\n<h1 id=\"3-A-Natural-Gradient\"><a href=\"#3-A-Natural-Gradient\" class=\"headerlink\" title=\"3. A Natural Gradient\"></a>3. A Natural Gradient</h1><p><br></p>\n<h2 id=\"3-1-Notation\"><a href=\"#3-1-Notation\" class=\"headerlink\" title=\"3.1 Notation\"></a>3.1 Notation</h2><p>이 논문에서 제시하는 Notation은 다음과 같습니다.</p>\n<ul>\n<li>MDP : tuple $(S, s_0, A, R, P)$</li>\n<li>$S$ : a finite set of states</li>\n<li>$s_0$ : a start state</li>\n<li>$A$ : a finite set of actions</li>\n<li>$R$ : reward function $R: S \\times A -&gt; [0, R_{max}]$</li>\n<li>$\\pi(a;s, \\theta)$ : stochastic policy parameterized by $\\theta$</li>\n<li>모든 정책 $\\pi$는 ergodic : stationary distribution $\\rho^{\\pi}$이 잘 정의되어 있다고 봅니다.</li>\n<li>이 논문에서는 sutton의 pg 논문의 두 세팅(start-state formulation, average-reward formulation) 중에 두 번째인 average-reward formulation을 가정합니다.</li>\n<li>performance or average reward : $\\eta(\\pi)=\\sum_{s,a}\\rho^{\\pi}(s)\\pi(a;s)R(s,a)$</li>\n<li>state-action value : $Q^{\\pi}(s,a)=E_{\\pi}[\\sum_{t=0}^{\\infty}R(s_t, a_t)-\\eta(\\pi)\\vert s_0=s, a_0=a]$ (<a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/\">Sutton PG</a> 를 참고해주시기 바랍니다.)</li>\n<li>정책이 $\\theta$로 parameterize되어있으므로 performance는 $\\eta(\\pi_{\\theta})$인데 $\\eta(\\theta)$로 씁니다.</li>\n</ul>\n<p><br></p>\n<h2 id=\"3-2-Natural-Gradient\"><a href=\"#3-2-Natural-Gradient\" class=\"headerlink\" title=\"3.2 Natural Gradient\"></a>3.2 Natural Gradient</h2><p>Sutton PG 논문의 policy gradient theorem에 따라 exact gradient of the average reward는 다음과 같습니다. 다음 수식이 어떻게 유도되었는지, 어떤 의미인지 모른다면 <a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/\">Sutton PG</a>을 통해 제대로 이해하는 것이 좋습니다. Euclidean space에서는 objective function의 일반적인 policy gradient는 Sutton PG에서 논의한 바와 같이 다음과 같이 구할 수 있습니다.</p>\n<p>$$\\nabla\\eta(\\pi_\\theta) = \\Sigma_{s,a}\\rho^\\pi(s)\\nabla\\pi(a;s,\\theta)Q^\\pi(s,a)$$</p>\n<p>여기서 steepest descent direction of $\\eta(\\theta)$는 $\\eta(\\theta + d\\theta)$를 최소화하는 $d\\theta$로 정의됩니다. 이 때, $\\vert d\\theta \\vert^2$가 일정 크기 이하인 것으로 제약조건을 주는데(held to small constant) Euclidian space에서는 $\\eta(\\theta)$가 steepest direction이지만 Riemannian space에서는 natural gradient가 steepest direction입니다.</p>\n<p>추가적으로 결국 우리가 원하는 건 ‘policy 최적화를 좀 더 스마트하게 해 보자!’입니다. Policy 최적화를 잘한다는 것은 Policy를 어느 방향으로 얼만큼 업데이트 하느냐에 따라 달라집니다.</p>\n<h3 id=\"3-2-1-Natural-gradient-증명\"><a href=\"#3-2-1-Natural-gradient-증명\" class=\"headerlink\" title=\"3.2.1 Natural gradient 증명\"></a>3.2.1 Natural gradient 증명</h3><p>Riemannian space에서 거리는 다음과 같이 정의됩니다. 여기서 $G(\\theta)$는 특정한 양수로 이루어진 matrix입니다. 자세한 내용은 <a href=\"http://bskyvision.com/205\" target=\"_blank\" rel=\"noopener\">양의 정부호 행렬(positive definite matrix)이란?</a>을 참고해주시기 바랍니다.</p>\n<p>$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$</p>\n<p>이 수식은 <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&amp;rep=rep1&amp;type=pdf\" target=\"_blank\" rel=\"noopener\">Natural gradient works in efficiently in learning</a> 논문에서 증명되어 있습니다. 다음은 natural gradient 증명입니다.</p>\n<p>steepest direction을 구할 때 $\\theta$의 크기를 제약조건으로 줍니다. 제약조건은 다음과 같습니다.</p>\n<p>$$\\vert d\\theta \\vert^2 = \\epsilon^2$$</p>\n<p>그리고 steepest vector인 $d\\theta$는 다음과 같이 정의할 수 있습니다.</p>\n<p>$$d\\theta = \\epsilon a$$</p>\n<p>$$\\vert a \\vert^2=a^TG(\\theta)a = 1$$</p>\n<p>이 때, $a$가 steepest direction unit vector이 되려면 다음 수식을 최소로 만들어야 합니다.</p>\n<p>$$\\eta(\\theta + d\\theta) = \\eta(\\theta) + \\epsilon\\nabla\\eta(\\theta)^Ta$$</p>\n<p>위 수식이 제약조건 아래 최소가 되는 $a$를 구하기 위해 Lagrangian method를 사용합니다. Lagrangian method를 모른다면 <a href=\"https://en.wikipedia.org/wiki/Lagrange_multiplier\" target=\"_blank\" rel=\"noopener\">위키피디아</a>를 참고하는 것을 추천합니다. 위 수식이 최소라는 것은 $\\nabla\\eta(\\theta)^Ta$가 최소라는 것입니다.</p>\n<p>$$\\frac{\\partial}{\\partial a_i}(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$$</p>\n<p>따라서 $(\\nabla\\eta(\\theta)^Ta - \\lambda a^TG(\\theta)a)=0$는 상수입니다. 상수를 미분하면 0이므로 이 식을 $a$로 미분합니다. 그러면 다음과 같이 steepest direction을 구한 것입니다.</p>\n<p>$$\\nabla\\eta(\\theta) = 2 \\lambda G(\\theta)a$$</p>\n<p>$$a=\\frac{1}{2\\lambda}G^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>이 때, 다음 식을 natural gradient라고 정의합니다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = G^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>natural gradient를 이용한 업데이트는 다음과 같습니다.</p>\n<p>$$\\theta_{t+1}=\\theta_t - \\alpha_tG^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>여기까지는 natural gradient의 증명이었습니다. 이 natural gradient를 policy gradient에 적용한 것이 natural policy gradient입니다. natural policy gradient는 다음과 같이 정의됩니다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>여기서 $G$대신 $F$를 사용했는데 $F$는 Fisher information matix입니다. 수식은 다음과 같습니다.</p>\n<p>$$F(\\theta) = E_{\\rho^\\pi(s)}[F_s(\\theta)]$$</p>\n<p>$$F_s(\\theta)=E_{\\pi(a;s,\\theta)}[\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial \\theta_i}\\frac{\\partial log \\pi(a;s, \\theta)}{\\partial\\theta_j}]$$</p>\n<h3 id=\"3-2-2-Fisher-Information-Matrix에-정의된-Metric\"><a href=\"#3-2-2-Fisher-Information-Matrix에-정의된-Metric\" class=\"headerlink\" title=\"3.2.2 Fisher Information Matrix에 정의된 Metric\"></a>3.2.2 Fisher Information Matrix에 정의된 Metric</h3><p>추가적으로 Fisher Information Matrix(FIM)에 대해서 설명하겠습니다. 위의 문제를 우리가 생각하기 쉬운 Neural Network(NN)으로 구성하고 해결할 수 있다고 해봅시다. NN은 여러가지 parameter set들로 구성될 수 있습니다. 게다가 다른 parameter set을 가지지만 같은 policy를 가질 수도 있습니다. 이 경우 steepest direction은 같은 policy이기 때문에 같은 방향을 가리키고 있어야 하는데 non-covariant한 경우 그렇지 못합니다. 떄문에 학습이 느려지고, 이러한 문제를 해결하기 위해 단순히 Positive-Definite Matrix $G(\\theta)$를 사용하지 않고 FIM $F_s(\\theta)$를 사용합니다. 어떠한 확률 변수 $X$가 임의의 매개변수 $\\theta$에 의해 정의되는 분포를 따른다고 하면 $X=x$일때 FIM은 다음과 같이 정의됩니다.</p>\n<p>$$F_x(\\theta)=E\\left[\\left(\\dfrac{\\partial}{\\partial\\theta}\\log\\Pr(x|\\theta)\\right)^2\\right]$$</p>\n<p>강화학습 관점에서 생각해보면 정보 $x$는 에피소드에 의해 관측된 상태값 $s$이며 매개변수 $\\theta$에 의해 선택될 수 있는 행동에 대한 분포가 나오게 됩니다. 이에 의해 위의 FIM는 다음과 같이 표현할 수 있습니다.</p>\n<p>$$F_s(\\theta) \\equiv E_{\\pi(a;s,\\theta)}\\left[\\left(\\dfrac{\\partial}{\\partial\\theta}\\log\\pi(a;s,\\theta)\\right)^2\\right] =E_{\\pi(a;s,\\theta)}\\left[\\dfrac{\\partial \\log\\pi(a;s,\\theta)}{\\partial \\theta_i}\\dfrac{\\partial \\log\\pi(a;s,\\theta)}{\\partial\\theta_j}\\right]$$</p>\n<p>그리고 위의 식들을 이용하여 objective function을 정리하면 아래의 식과 같이 표현됩니다.</p>\n<p>$$F(\\theta) = E_{\\rho^{\\pi}(s)}[F_s(\\theta)]$$</p>\n<p>또한 이 Fisher Information Matrix에 정의된 이 metric은 다음과 같은 성질을 가지고 있습니다.</p>\n<ol>\n<li>업데이트되는 파라미터에 의해 구성되는 매니폴드에 기반한 metric입니다.</li>\n<li>확률분포($\\pi(a;s,\\theta)$)를 구성하는 파라미터($\\theta$)의 변화에 독립적입니다.</li>\n<li>마지막으로 positive-definite한 값을 가집니다. 이렇기 때문에 steepest gradient에서 objective function의 방향을 알기 위해 사용한 방법과 같은 방법으로 natural gradient direction을 다음과 같이 구할 수 있는 것입니다.</li>\n</ol>\n<p>$$\\bar{\\nabla}\\eta(\\theta) \\equiv F(\\theta)^{-1}\\nabla\\eta(\\theta)$$</p>\n<p><br><br></p>\n<h1 id=\"4-The-Natural-Gradient-and-Policy-Iteration\"><a href=\"#4-The-Natural-Gradient-and-Policy-Iteration\" class=\"headerlink\" title=\"4. The Natural Gradient and Policy Iteration\"></a>4. The Natural Gradient and Policy Iteration</h1><p>4장에서는 Natural gradient를 통한 policy iteration을 수행하여 실제로 정책의 향상이 있는지를 증명합니다. 여기서 $Q^\\pi(s,a)$는 compatible function approximator $f^\\pi(s,a;w)$로 근사됩니다. (<a href=\"https://reinforcement-learning-kr.github.io/2018/06/28/sutton-pg/\">Sutton PG</a>를 참고해주시기 바랍니다.)</p>\n<p><br></p>\n<h2 id=\"4-1-Theorem-1-Compatible-Function-Approximation\"><a href=\"#4-1-Theorem-1-Compatible-Function-Approximation\" class=\"headerlink\" title=\"4.1 Theorem 1: Compatible Function Approximation\"></a>4.1 Theorem 1: Compatible Function Approximation</h2><p>approximate하는 함수 $f^{\\pi}(s,a;w)$는 다음과 같습니다.(compatible value function)</p>\n<p>$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$</p>\n<p>$$\\psi^{\\pi}(s,a) = \\nabla log\\pi(a;s,\\theta)$$</p>\n<p>여기서 $[\\nabla \\log\\pi(a;s,\\theta)]_i=\\partial \\log\\pi(a;s,\\theta)/\\partial\\theta_i$입니다. $w$는 원래 approximate하는 함수 $Q$와 $f$의 차이를 줄이도록 학습합니다(mean square error). 수렴한 local minima의 $w$를 $\\bar{w}$라고 가정 하겠습니다. 에러는 다음과 같은 수식으로 나타낼 수 있습니다.</p>\n<p>$$\\epsilon(w,\\pi)\\equiv\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)(f^{\\pi}(s,a;w)-Q^{\\pi}(s,a))^2$$</p>\n<p>그 때, $\\bar{\\omega} = \\bar{\\nabla}\\eta(\\theta)$이면 function approximator의 gradient 방향과 정책의 gradient 방향이 같다는 것을 의미합니다.</p>\n<p>아래의 내용은 위의 Theorem에 대한 증명입니다.</p>\n<p>위의 수식이 local minima이면 미분값이 0이 됩니다. $w$에 대해서 미분하면 다음과 같습니다.</p>\n<p>$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)(\\psi^{\\pi}(s,a)^T\\bar{w}-Q^{\\pi}(s,a))=0$$</p>\n<p>$$\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)^T\\bar{w}=\\sum_{s, a}\\rho^{\\pi}(s)\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)Q^{\\pi}(s,a)$$</p>\n<p>이 때, 위 식의 우변은 $\\psi$의 정의에 의해 policy gradient가 됩니다. 또한 왼쪽 항에서는 Fisher information matrix가 나옵니다.</p>\n<p>$$F(\\theta)=\\sum_{s,a}\\pi(a;s,\\theta)\\psi^{\\pi}(s,a)\\psi^{\\pi}(s,a)=E_{\\rho^\\pi(s)}[F_s(\\theta)]$$</p>\n<p>따라서 다음과 같이 쓸 수 있습니다.</p>\n<p>$$F(\\theta)\\bar{w}=\\nabla\\eta(\\theta)$$</p>\n<p>$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>위의 수식은 natural gradient식과 동일합니다. 위의 수식은 policy가 update 될 때, value function approximator의 parameter 방향으로 이동한다는 것을 의미합니다. function approximation이 정확하다면 그 parameter의 natural policy gradient와 inner product가 커야합니다.</p>\n<p><br></p>\n<h2 id=\"4-2-Theorem-2-Greedy-Policy-Improvement\"><a href=\"#4-2-Theorem-2-Greedy-Policy-Improvement\" class=\"headerlink\" title=\"4.2 Theorem 2: Greedy Policy Improvement\"></a>4.2 Theorem 2: Greedy Policy Improvement</h2><p>이번 장에서는 natrual gradient는 다른 policy iteration 방법처럼 단순히 더 좋은 행동을 고르도록 학습하는 것이 아니라 가장 좋은(greedy) 행동을 고르도록 학습한다는 것을 증명하는 부분입니다. 이것을 일반적인 형태의 policy에 대해서 증명하기 전에 exponential 형태의 policy에 대해서 증명하는 것이 Theorem 2입니다. 특수한 정책을 가지는 상황안에서 학습속도($\\alpha$)를 무한대로 가져감으로서 어떤 action을 선택하는지를 알아봅니다.</p>\n<p>policy를 다음과 같이 정의합니다.</p>\n<p>$$\\pi(a;s,\\theta) \\propto \\exp(\\theta^T\\phi_{sa})$$</p>\n<p>여기서 $\\bar{\\nabla}\\eta(\\theta)$가 0이 아니고 $\\bar{w}$는 approximation error를 최소화된 $w$라고 가정합니다. 이 상태에서 natural gradient update를 생각해봅시다. 그리고 policy gradient는 gradient ascent임을 기억합시다.</p>\n<p>$$\\theta_{t+1}=\\theta_t + \\alpha_t\\bar{\\nabla}\\eta(\\theta)$$</p>\n<p>이 때 $\\alpha$가 learning rate로 parameter를 얼마나 업데이트하는지를 결정합니다. 이 값을 무한대로 늘렸을 때 policy가 어떻게 업데이트되는지 생각해봅시다.</p>\n<p>$$\\pi_{\\infty}(a;s)=lim_{\\alpha\\rightarrow\\infty}\\pi(a;s,\\theta+\\alpha\\bar{\\nabla}\\eta(\\theta))-(1)$$</p>\n<p>이 때,</p>\n<p>$$\\pi_{\\infty}=0 \\, \\, \\, if \\, and \\, only \\, if(필요충분조건) \\, \\, \\, a \\notin argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>이라고 말할 수 있습니다.</p>\n<p>아래의 내용은 위의 Theorem에 대한 증명입니다.</p>\n<p>먼저 function approximator는 앞서 다뤘듯이 아래와 같습니다.</p>\n<p>$$f^{\\pi}(s,a;w)=w^T\\psi^{\\pi}(s,a)$$</p>\n<p>여기서 function approximator는 Theorem 1에 의해 아래와 같이 쓸 수 있습니다.</p>\n<p>$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T\\psi^{\\pi}(s,a)$$</p>\n<p>$\\theta$의 정의에 의해 $\\psi$는 다음과 같습니다.</p>\n<p>$$\\psi^{\\pi}(s,a)=\\phi_{sa}-E_{\\pi(a’;s,\\theta)}[\\phi_{sa’}]$$</p>\n<p>따라서 function approximator는 다음과 같이 다시 쓸 수 있습니다.</p>\n<p>$$f^{\\pi}(s,a;w)=\\bar{\\nabla}\\eta(\\theta)^T(\\phi_{sa}-E_{\\pi(a’;s,\\theta)}[\\phi_{sa’}])$$</p>\n<p>greedy policy improvement가 Q function 값 중 가장 큰 값을 가지는 action을 선택하듯이 여기서도 function approximator의 값이 가장 큰 action을 선택하는 상황을 가정해봅시다. 이 때 function approximator의 argmax는 다음과 같이 쓸 수 있습니다.</p>\n<p>$$argmax_{a’}f^{\\pi}(s,a)=argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>(1) 식을 다시 살펴봅시다. 그러면 policy의 정의에 따라 다음과 같이 쓸 수 있습니다.</p>\n<p>$$\\pi(a;s,\\theta + \\alpha\\bar{\\nabla}\\eta(\\theta)) \\propto exp(\\theta^T\\phi_{sa} + \\alpha\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa})$$</p>\n<p>$\\bar{\\nabla}\\eta(\\theta) \\neq 0$이고 $\\alpha\\rightarrow\\infty$이면 exp안의 항 중에서 뒤의 항이 dominate하게 됩니다. 여러 행동 중에 $\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa}$가 가장 큰 행동이 있다면 이 행동의 policy probability가 1이 되고 나머지는 0이 됩니다. 따라서 다음이 성립합니다.</p>\n<p>$$\\pi_{\\infty}=0 \\, \\, \\, if \\, and \\, only \\, if \\, \\, \\, a \\notin argmax_{a’}\\bar{\\nabla}\\eta(\\theta)^T\\phi_{sa’}$$</p>\n<p>이 결과로부터 natural policy gradient는 단지 더 좋은 action이 아니라 best action을 고르도록 학습이 됩니다. 반면에 non-covariant gradient(1차미분)에서는 그저 더 좋은 action을 고르도록 학습이 됩니다. 이 natural policy gradient에 대한 결과는 infinite learning rate 세팅에서만 성립합니다. 좀 더 일반적인 경우에 대해서 살펴봅시다.</p>\n<p><br></p>\n<h2 id=\"4-3-Theorem-3-General-Parameterized-Policy\"><a href=\"#4-3-Theorem-3-General-Parameterized-Policy\" class=\"headerlink\" title=\"4.3 Theorem 3: General Parameterized Policy\"></a>4.3 Theorem 3: General Parameterized Policy</h2><p>Theorem 2에서와는 달리 일반적인 policy를 가정해봅시다(general parameterized policy). Theorem 3는 이 상황에서 natural gradient를 통한 업데이트가 best action를 고르는 방향으로 학습이 된다는 것을 보여줍니다.</p>\n<p>natural gradien에 따른 policy parameter의 업데이트는 다음과 같습니다. $\\bar{w}$는 approximation error를 minimize하는 $w$입니다.</p>\n<p>$$\\delta\\theta = \\theta’ - \\theta = \\alpha\\bar{\\nabla}\\eta(\\theta)=\\alpha\\bar{w}$$</p>\n<p>policy에 대해서 1차근사를 하면 다음과 같습니다.</p>\n<p>$$\\pi(a;s,\\theta’)=\\pi(a;s,\\theta)+\\frac{\\partial\\pi(a;s,\\theta)^T}{\\partial\\theta}\\delta\\theta + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\psi(s,a)^T\\delta\\theta) + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\alpha\\psi(s,a)^T\\bar{w}) + O(\\delta\\theta^2)$$</p>\n<p>$$=\\pi(a;s,\\theta)(1+\\alpha f^{\\pi}(s,a;\\bar{w})) + O(\\delta\\theta^2)$$</p>\n<p>policy 자체가 function approximator의 크기대로 업데이트가 되므로 local하게 best action의 probability는 커지고 다른 probability의 크기는 작아질 것입니다. 하지만 만약 greedy improvement가 된다하더라도 그게 performance의 improvement를 보장하는 것은 아닙니다. 하지만 line search와 함께 사용할 경우 improvement를 보장할 수 있습니다. 왜 그런지는 처음에 말씀드린 블로그 <a href=\"http://darkpgmr.tistory.com/149\" target=\"_blank\" rel=\"noopener\">다크 프로그래머님의 블로그</a>를 참고해주시기 바랍니다.</p>\n<p><br><br></p>\n<h1 id=\"5-Metrics-and-Curvatures\"><a href=\"#5-Metrics-and-Curvatures\" class=\"headerlink\" title=\"5. Metrics and Curvatures\"></a>5. Metrics and Curvatures</h1><p>$$\\vert d\\theta \\vert^2=\\sum_{ij}(\\theta)d\\theta_id\\theta_i=d\\theta^TG(\\theta)d\\theta$$</p>\n<p>이 파트에서는 FIM과 다른 metric 사이의 관계를 다룹니다.</p>\n<p>위의 식에 해당하는 G는 Fisher Information Matrix만 사용할 수 있는 것이 아닙니다. Positive-Definite Matrix인 FIM이외의 다른 Matrix도 사용할 수 있습니다. 또한 다양한 파라미터 추정에서 FIM은 Hessian Matrix에 수렴하지 않을 수 있다고 합니다. 이 말은 2nd order(2차 근사) 수렴이 보장되지 않는다는 말입니다.</p>\n<p>논문에 있는 내용들을 조금 더 추가적으로 보면 아래와 같이 나옵니다.</p>\n<p>In the different setting of parameter estimation, the Fisher information converges to the <code>Hessian</code>, so it is <a href=\"https://en.wikipedia.org/wiki/Efficiency_(statistics\" target=\"_blank\" rel=\"noopener\">asymptotically efficient</a>) 이지만,</p>\n<p>이 논문의 경우, 아마리 논문의 ‘blind separation case’와 유사한데 이 때는 꼭 asymtotically efficient하지 않다고 말합니다. 이 말은 즉 2nd order 수렴이 보장되지 않는다는 것이다.</p>\n<p><a href=\"http://www.inference.org.uk/mackay/ica.pdf\" target=\"_blank\" rel=\"noopener\">Mackay</a> 논문에서는 Hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했습니다. 그래서 performance를 2번 미분해보면 아래의 수식과 같습니다. 하지만 다음 식에서는 모든 항이 data dependent합니다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있습니다.</p>\n<p>$$\\nabla^2\\eta(\\theta)=\\sum_{sa}\\rho^{\\pi}(s)(\\nabla^2\\pi(a;s)Q^{\\pi}(s,a)+\\nabla\\pi(a;s)\\nabla Q^{\\pi}(s,a)^T+\\nabla Q^{\\pi}(s,a)\\nabla\\pi(a;s)^T)$$</p>\n<p>hessian은 보통 positive definite가 아닐수도 있습니다. 따라서 local maxima가 될 때까지 Hessian이 사용하기 별로 안좋습니다. 그리고 local maxima에서는 Hessian보다는 Conjugate methods가 더 효율적이라고 합니다.</p>\n<p>사실 이 파트에서는 무엇을 말하고 있는지 알기가 어렵습니다. FIM과 Hessian이 관련이 있다는 것을 알겠는데 asymtotically efficient와 같은 내용을 모르므로 내용의 이해가 어려웠습니다.</p>\n<p>Mackay 논문에서 해당 부분은 다음과 같습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/x4n6z6pdyi7xtb9/Screenshot%202018-06-10%2012.04.13.png?dl=1\"> </center>\n\n<p><br></p>\n<h2 id=\"5-1-Fisher-Information-Matrix-FIM-vs-Hessian\"><a href=\"#5-1-Fisher-Information-Matrix-FIM-vs-Hessian\" class=\"headerlink\" title=\"5.1 Fisher Information Matrix(FIM) vs. Hessian\"></a>5.1 Fisher Information Matrix(FIM) vs. Hessian</h2><p>FIM과 Hessian에 대해서 추가적인 설명을 하고자 합니다.</p>\n<ul>\n<li>FIM</li>\n</ul>\n<p>일단 정의되려면 space 자체가 stochastic한 성질을 가지고 있어야 합니다. 모든 parameter를 표현하는 함수가 deterministic한 함수가 아니라 확률로 나타낸 분포입니다. (probability distribution)</p>\n<p>결국은 FIM에서도 hessian처럼 비슷한 과정을 취하고 싶은 것입니다. 따라서 확률 변수이기 때문에 어떠한 특정 sample을 취하면 그게 항상 다른값이 됩니다. 그래서 FIM에다가 expectation을 취한 것입니다. expectation을 취함으로써 hessian같은 성질을 가집니다. 왜냐하면 expectation을 취함으로써 constant한 값이 되기 때문입니다.</p>\n<ul>\n<li>Hessian</li>\n</ul>\n<p>그냥 deterministic한 함수에서 정의됩니다. 그냥 함수를 두 번 미분 한 것이라고 보면 됩니다.</p>\n<p><br></p>\n<h2 id=\"5-2-Conjugate-Gradient-Method\"><a href=\"#5-2-Conjugate-Gradient-Method\" class=\"headerlink\" title=\"5.2 Conjugate Gradient Method\"></a>5.2 Conjugate Gradient Method</h2><p>추가적으로 Conjugate Gradient Method(CGM)에 대해서 다루고자 합니다.</p>\n<ul>\n<li>참고자료<ul>\n<li><a href=\"https://www.quora.com/What-is-an-intuitive-explanation-of-what-the-conjugate-gradient-method-is\" target=\"_blank\" rel=\"noopener\">https://www.quora.com/What-is-an-intuitive-explanation-of-what-the-conjugate-gradient-method-is</a></li>\n<li><a href=\"https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf\" target=\"_blank\" rel=\"noopener\">https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Conjugate_gradient_method\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Conjugate_gradient_method</a></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"mathbf-A-mathbf-x-mathbf-b-의-해-구하기\"><a href=\"#mathbf-A-mathbf-x-mathbf-b-의-해-구하기\" class=\"headerlink\" title=\"$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해 구하기\"></a>$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해 구하기</h3><p>$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$의 해를 구하는 문제를 생각해봅시다. $\\mathbf{A}$의 역행렬을 구해서 양변에 곱해주면 $\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$가 되어 쉽게 해를 구할 수 있습니다. 하지만 $\\mathbf{A}^{-1}$은 계산이 많이 필요 자원소모가 큰 연산입니다. 역행렬을 취하지 않고 해를 구할 수 있는 방법이 있을까요?</p>\n<p>위의 방정식에서 $\\mathbf{b}$를 이항시키면 $\\mathbf{A}\\mathbf{x} - \\mathbf{b} = 0$이 됩니다. 최적화문제는 많은 경우 1차 미분이 0이 되는 지점이 해일 확률이 높습니다. $\\mathbf{A}\\mathbf{x} - \\mathbf{b} = 0$을 1차 미분으로 가지는 함수는 무엇일까요?</p>\n<p>$$f( \\mathbf{x} ) = \\frac{1}{2}\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x} - \\mathbf{x}^\\mathrm{T}\\mathbf{b}$$</p>\n<p>위의 함수는 $\\mathbf{A}\\mathbf{x} - \\mathbf{b}$를 1차 미분값으로 가집니다. 이 때 $\\mathbf{A}$는 symmetric positive definite해야 합니다.</p>\n<ul>\n<li>symmetric: $\\mathbf{A}=\\mathbf{A}^\\mathrm{T}$</li>\n<li>positive definite: $\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}&gt;0,\\quad\\forall\\mathbf{x}$ or all eigenvalues of $\\mathbf{A}$ are positive</li>\n</ul>\n<p>symmetric positive definite한 성질은 $\\mathbf{x}$를 strict convex function이 되게 만들어서 unique solution이 존재하게 합니다. $\\mathbf{A}$는 $\\mathbf{x}$의 Hessian이기도 합니다.</p>\n<p>자, 우리는 위의 방정식의 해 $\\mathbf{x}^*$를 iterative한 방식으로 찾고자 합니다. $\\mathbf{x}_0$을 초기값이라고 합시다.</p>\n<p>우리는 <img src=\"https://www.dropbox.com/s/g91yqajf72jzjoc/Screen%20Shot%202018-08-14%20at%208.43.28%20AM.png?dl=1\" width=\"80\">가 되는 <img src=\"https://www.dropbox.com/s/aj3j5fjtiyewlo1/Screen%20Shot%202018-08-14%20at%208.43.37%20AM.png?dl=1\" width=\"30\">를 찾고 싶은데 이를 위한 현재의 추정값은 $\\mathbf{A} \\mathbf{x}_0$입니다. 최적의 값을 찾았을 때와 비교하면 현재의 오차는 $\\mathbf{b} - \\mathbf{A}\\mathbf{x}_0 = \\mathbf{r}_0$입니다. 이것을 residual이라고 합니다. 어떻게 효율적으로 값을 변화시켜서 오차를 0으로 만들 수 있을까요? 일단 매 iteration마다 residual이 작아지는 방향으로 나아가야겠죠?</p>\n<p>가장 널리 알려진 방법은 gradient descent 방향입니다. gradient가 증가하면 반대로 가고 gradient가 감소하면 그 방향으로 가는 것입니다. 그리고 어떤 방향으로든 gradient가 0이면 그 지점에 멈추는 것이죠. 이 방법은 수렴은 하지만 zigzag하게 움직여서 느립니다. 이것보다 더 좋은 방법이 없을까요?</p>\n<h3 id=\"Conjugate-Gradient-Method\"><a href=\"#Conjugate-Gradient-Method\" class=\"headerlink\" title=\"Conjugate Gradient Method\"></a>Conjugate Gradient Method</h3><h4 id=\"Gram-Schmidt-orthgonalization\"><a href=\"#Gram-Schmidt-orthgonalization\" class=\"headerlink\" title=\"Gram-Schmidt orthgonalization\"></a>Gram-Schmidt orthgonalization</h4><p>다음과 같은 vector들의 집합 <img src=\"https://www.dropbox.com/s/csgdsfofe19q5r6/Screen%20Shot%202018-08-14%20at%208.56.04%20AM.png?dl=1\" width=\"135\">이 있다고 해봅시다. 이 vector들이 서로 linearly independent하다면 이 vector들은 $\\mathbb{R}^n$ 공간 상의 basis입니다. 이 vector들을 이용해서 orthogonal한 vector들을 만들어보겠습니다. 새로운 vector들을 <img src=\"https://www.dropbox.com/s/7xg7pciar1ndu44/Screen%20Shot%202018-08-14%20at%208.58.25%20AM.png?dl=1\" width=\"130\">라고 한다면 아래와 같은 과정을 통해 만들 수 있습니다. 이 방법을 Gram-Schmidt orthogonalization이라고 합니다.</p>\n<ul>\n<li>$\\mathbf{d}_1=\\mathbf{v}_1$</li>\n<li>$\\mathbf{d}_2=\\mathbf{v}_2 - \\left\\langle\\mathbf{v}_2,\\frac{\\mathbf{d}_1}{\\parallel\\mathbf{d}_1\\parallel}\\right\\rangle\\frac{\\mathbf{d}_1}{\\parallel\\mathbf{d}_1\\parallel}$<br>…</li>\n<li><img src=\"https://www.dropbox.com/s/arzc5cez8az0j7h/Screen%20Shot%202018-08-14%20at%209.00.04%20AM.png?dl=1\" width=\"290\"></li>\n</ul>\n<p>간단히 설명을 하면, 첫 vector는 basis와 같은 방향으로 출발합니다. 그 다음 vector는 그 다음 basis를 이전 vector로 projection한 다음 해당 basis로부터 빼줍니다. 그 다음 vector를 만들 때는 이전에 만든 모든 vector들에 대해서 projection을 취한 다음 모두 더한 것을 해당 basis에서 빼줍니다. </p>\n<h4 id=\"A-conjugate\"><a href=\"#A-conjugate\" class=\"headerlink\" title=\"$A$-conjugate\"></a>$A$-conjugate</h4><p>이 방법을 이용하여 matrix $A$에 관하여 orthogonal한 새로운 vector들의 집합을 만들어보겠습니다. 일반적인 inner product와 약간 다른 다음과 같은 inner product를 정의할 수 있습니다.</p>\n<p>$$&lt;\\mathbf{x},\\mathbf{y}&gt;_\\mathbf{A} = \\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{y}$$</p>\n<p>이 때 $&lt;\\mathbf{x},\\mathbf{y}&gt;_\\mathbf{A} = 0$이면 $\\mathbf{x}와 \\mathbf{y}$는 $\\mathbf{A}$-orthogonal 또는 $\\mathbf{A}$-conjugate하다라고 합니다. vector norm $\\parallel\\cdot\\parallel$도 일반적인 vector norm이 아닌 A-norm을 다음과 같이 정의합니다.</p>\n<p>$$\\parallel\\mathbf{x}\\parallel_\\mathbf{A}^2=\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}$$</p>\n<p>Gram-Schmidt orthogonalization을 또 이용해볼까요? 아래와 같습니다.</p>\n<ul>\n<li>$\\mathbf{d}_1=\\mathbf{v}_1$</li>\n<li><img src=\"https://www.dropbox.com/s/i8wnvoccefgg7t8/Screen%20Shot%202018-08-14%20at%209.08.42%20AM.png?dl=1\" width=\"400\"><br>…</li>\n<li><img src=\"https://www.dropbox.com/s/045x2v5sx7s0wot/Screen%20Shot%202018-08-14%20at%209.09.03%20AM.png?dl=1\" width=\"500\"></li>\n</ul>\n<p>그런데 이 Gram-Schmidt 방법은 단점이 하나 있습니다. 새로운 vector를 만들어내기 위해서 이전에 만든 vector들을 모두 가지고 있어야 하고 projection도 다시 계산해야 한다는 점입니다. 그런데 마법같은 이유($\\approx$ 여기서 설명하지 않는 이유)로 인해서 오직 마지막으로 만들어낸 vector만 가지고 있어도 기존의 모든 vector들이 성질을 표현해낼 수 있습니다. 그러면 우리가 만들어낸 vector는 다음과 같이 간단해집니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/wlyu7ax6qt0aw9w/Screen%20Shot%202018-08-14%20at%209.09.43%20AM.png?dl=1\" width=\"550\"> </center>\n\n<p>이 방향이 우리가 업데이트시키고 싶은 방향입니다. 즉 우리는 다음 수식처럼 움직이고자 합니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/053o4ocvxsgxfl9/Screen%20Shot%202018-08-14%20at%209.09.56%20AM.png?dl=1\" width=\"180\"> </center>\n\n<p>$d_k$는 방향이고 $\\alpha_{k}$는 step size입니다. 이러한 방법을 일반적으로 line search method라고 부릅니다. 방향은 위에서 구한 conjugate 방향을 이용합니다. 그래서 이 line search를 conjugate gradient method라고 부릅니다. 이 방법의 한가지 중요한 장점은 업데이트가 무조건 n번만 일어난다는 것입니다! n-dimensional space에서는 basis가 n개 밖에 없거든요. 그렇다면 얼마만큼 많이 이 방향으로 움직여야 할까요? 더 이상 $f(\\mathbf{x})$가 감소하지 않을 때까지 움직이는게 좋지 않을까요? 이것은 다시 말하면 gradient가 0이 될 때까지 움직인다는 뜻입니다. 위에서 설명한 residual과도 관계있을 것 같지 않나요?</p>\n<p>$\\nabla f(\\mathbf{x})=\\mathbf{A}\\mathbf{x} - \\mathbf{b}$에 $x_k + \\alpha_{k} d_k$를 대입해 봅시다. 아래와 같이 step size를 구할 수 있습니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/xikpfgcix64ngmp/Screen%20Shot%202018-08-14%20at%209.13.20%20AM.png?dl=1\" width=\"300\"> </center>\n\n<p>네, 이제 우리는 방향과 step size를 모두 알았습니다. 이제 update만 하면 해를 찾을 수 있겠군요!</p>\n<center> <img src=\"https://www.dropbox.com/s/avjw4kbzd4ormv5/conjugate_gradient_wikipedia.png?dl=1\" width=\"200\"> </center>\n\n<p>위키피디아에서 가져온 위의 그림을 봅시다. 그림의 녹색선이 gradient descent이고 빨간선이 conjugate gradient입니다. conjugate gradient는 gradient descent보다 빨리 수렴합니다. 녹색선은 항상 이전 이동 방향과 직각으로 이동합니다. 빨간선은 보다 빠르게 더 낮은 값이 있는 곳으로 이동합니다.</p>\n<p>위에서 설명을 생략했지만 왜 conjugate gradient는 빠르게 수렴(n-step)하며 Gram-Schmidt orthogonalization을 할 때도 메모리가 적게 필요할까요? 수학적으로 엄밀하게 설명하기 보다는 개념적으로 설명을 하겠습니다. 그 비밀은 바로 $\\mathbf{A}$-orthogonal 또는 $\\mathbf{A}$-conjugate vector들을 이용한데 있습니다. 이 vector들은 basis라고 했습니다. 즉, 처음에 우리가 구하고자 했던 해 $\\mathbf{x}^*$를 이 vector들을 이용해서 표현할 수 있습니다. 또한 iterative하게 찾아가기 위한 초기 vector $\\mathbf{x}_0$도 이 vector들로 표현할 수 있습니다. 그렇다면 이 둘 간의 오차도 이 vector들도 표현할 수 있게 됩니다. 다음 수식처럼 말입니다.</p>\n<p>$$\\mathbf{x}^* - \\mathbf{x}_0 = \\epsilon_1\\mathbf{d}_1 + \\epsilon_2\\mathbf{d}_2 + \\cdots + \\epsilon_n\\mathbf{d}_n$$</p>\n<p>우리의 목표는 이 오차를 줄이는 것입니다. 각각의 basis마다 오차가 있으며 이들은 서로 independent합니다. 즉, 특정 iteration에서 특정 basis에 대한 오차를 0으로 만들면 다음 번 iteration에서는 이 오차는 영향을 받지 않습니다! 이러한 이유로 n번의 iteration만으로 해를 찾을 수 있고 다른 vector들을 저장할 필요도 없는 것입니다. 선형대수의 아름다움이 느껴지지 않으시나요? 오래되었지만 참 잘 디자인된 기법이라는 생각이 듭니다.</p>\n<p><br><br></p>\n<h1 id=\"6-Experiment\"><a href=\"#6-Experiment\" class=\"headerlink\" title=\"6. Experiment\"></a>6. Experiment</h1><p>이 논문에서는 natural gradient를 simple MDP와 tetris MDP에 대해서 실험을 진행했습니다. FIM은 다음과 같은 식으로 업데이트합니다.</p>\n<p>$$f\\leftarrow f+\\nabla \\log \\pi(a_t; s_t, \\theta)\\nabla \\log \\pi(a_t; s_t, \\theta)^T$$</p>\n<p>$T$ length trajectory에 대해서 $f/T$를 이용해 $F$의 기대값($E$)를 구합니다.</p>\n<p><br></p>\n<h2 id=\"6-1-LQR-Linear-Quadratic-Regulator\"><a href=\"#6-1-LQR-Linear-Quadratic-Regulator\" class=\"headerlink\" title=\"6.1 LQR(Linear Quadratic Regulator)\"></a>6.1 LQR(Linear Quadratic Regulator)</h2><p>Agent를 실험할 환경은 다음과 같은 dynamics를 가지고 있습니다.</p>\n<p>$x(t+1) = 0.7x(t)+u(t)+\\epsilon(t)$</p>\n<p>$u(t)$는 control 신호로서 에이전트의 행동입니다. $\\epsilon$은 noise distribution으로 환경에 가해지는 노이즈입니다. 에이전트의 목표는 적절한 $u(t)$를 통해 $x(t)$를 0으로 유지하는 것입니다. $x(t)$를 0으로 유지하기 위해서 필요한 소모값(cost)는 $x(t)^2$로 정의하며 cost를 최소화하도록 학습합니다. 이 논문에서는 실험할 때 복잡성을 더 해주기 위해 noise distribution인 $\\epsilon$을 dynamics에 추가하였습니다.</p>\n<p>이 실험에서 policy는 다음과 같이 설정하였습니다. 파라미터가 2개 밖에 없는 간단한 policy입니다.</p>\n<p>$\\pi(u;x,\\theta)\\propto \\exp(\\theta_1s_1x^2+\\theta_2s_2x)$</p>\n<p>이 policy를 간단히 그래프로 그려보면 다음과 같습니다. 가로축은 $x$, 세로축은 cost입니다. $\\theta_1$과 $\\theta_2$를 (0.5, 0.5), (1, 0), (0, 1)로 설정하고 $s_1$, $s_2$는, 1로 두었습니다. $x$는 -1~1사이의 범위로 그렸습니다. </p>\n<center> <img src=\"https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1\" width=\"500px\"> </center>\n\n<p>아래의 그림은 LQR을 학습한 그래프입니다. cost가 $x^2$이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있습니다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과입니다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 곡선입니다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습합니다(time 축이 log scale인 것을 감안하면 차이가 많이 납니다.). </p>\n<p>하지만 문제가 있습니다. NPG를 학습한 세 개의 곡선은 $\\theta$를 rescale 한 것입니다. $\\theta$앞에 곱해지는 숫자에 따라 학습의 과정이 다릅니다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것입니다. 즉, covariant gradient가 아니라는 뜻입니다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것입니다.</p>\n<center> <img src=\"https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1\" width=\"300px\"> </center>\n\n<p>natural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문입니다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 $\\rho_s$가 곱해지기 때문입니다. 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것입니다!</p>\n<p><br></p>\n<h2 id=\"6-2-Simple-2-state-MDP\"><a href=\"#6-2-Simple-2-state-MDP\" class=\"headerlink\" title=\"6.2 Simple 2-state MDP\"></a>6.2 Simple 2-state MDP</h2><p>이제 다른 예제에서 NPG를 테스트합니다. 2개의 state만 가지는 MDP를 고려해봅시다. <a href=\"http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&amp;context=robotics\" target=\"_blank\" rel=\"noopener\">그림출처</a>. 그림으로보면 다음과 같습니다. $x=0$ 상태와 $x=1$ 상태 두 개가 존재합니다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있습니다. 상태 $x=0$에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 $x=1$에서 자기 자신으로 돌아오면 2의 보상을 받습니다. 결국 optimal policy는 상태 $x=1$에서 계속 자기 자신으로 돌아오는 행동을 취하는 것입니다. </p>\n<p><img src=\"https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1\"></p>\n<p>문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정합니다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것입니다.</p>\n<p>$$\\rho(x=0)=0.8,  \\rho(x=1)=0.2$$</p>\n<p>일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 됩니다. 이 때, $\\rho(s)$가 gradient에 곱해지므로 상대적으로 상태 0에서의 gradient 값이 커지게 됩니다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update합니다. 따라서 아래 그림의 첫번째 그림에서처럼 reward가 1에서 오랜 시간동안 머무릅니다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것입니다.</p>\n<p>$$\\nabla\\eta(\\theta)=\\sum_{s,a}\\rho^{\\pi}(s)\\nabla\\pi(a;s,\\theta)Q^{\\pi}(s,a)$$</p>\n<center><img src=\"https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1\" width=\"300px\"></center>\n\n<p>하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달합니다. gradient 방법이 $1.7\\times 10^7$정도의 시간만에 2에 도달한 반면 NPG는 2의 시간만에 도달합니다. 또한 $\\rho(x=0)$가 $10^{-5}$이하로 떨어지지 않습니다.</p>\n<p>한 가지 그래프를 더 살펴봅시다. 다음 그래프는 parameter $\\theta$가 업데이트 되는 과정을 보여줍니다. 이 그래프에서는 parameter가 2개 있습니다. 일반적인 gradient가 아래 그래프에서 실선에 해당합니다. 이 실선의 그래프는 보면 처음부터 중반까지 $\\theta_i$만 거의 업데이트하는 것을 볼 수 있습니다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있습니다. </p>\n<center><img src=\"https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1\" width=\"300px\"></center>\n\n<p>policy가 $\\pi(a;s,\\theta)\\propto \\exp(\\theta_{sa})$일 때, 다음과 같이 $F_{-1}$이 gradient 앞에 weight로 곱해지는데 이게 $\\rho$와는 달리 각 parameter에 대해 균등합니다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것입니다.</p>\n<p>$$\\bar{\\nabla}\\eta(\\theta) = F^{-1}\\nabla\\eta(\\theta)$$</p>\n<p><br></p>\n<h2 id=\"6-3-Tetris\"><a href=\"#6-3-Tetris\" class=\"headerlink\" title=\"6.3 Tetris\"></a>6.3 Tetris</h2><p>NPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어 있습니다. 다음 그림은 tetris 예제를 보여줍니다. 보통 그림에서와 같이 state의 feature를 정해줍니다. <a href=\"http://slideplayer.com/slide/5215520/\" target=\"_blank\" rel=\"noopener\">그림 출처</a></p>\n<p><img src=\"https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1\"></p>\n<p>이 예제에서도 exponential family로 policy를 표현합니다. $\\pi(a;s,\\theta) \\propto \\exp(\\theta^T\\phi_{sa})$ 로 표현합니다.</p>\n<p>tetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있습니다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우입니다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법입니다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷합니다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지합니다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있습니다.</p>\n<p><img src=\"https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1\"></p>\n<p><br><br></p>\n<h1 id=\"7-Discussion\"><a href=\"#7-Discussion\" class=\"headerlink\" title=\"7. Discussion\"></a>7. Discussion</h1><p>Natural Gradient Method는  Policy Iteration에서와 같이 Greedy Action을 선택하도록 학습합니다. Line search 기법과 함께 사용하면 더 Policy Iteration과 비슷해집니다. Greedy Policy Iteration에서와 비교하면 Performance Improvement가 보장됩니다. 하지만 FIM이 asymtotically Hessian으로 수렴하지 않습니다. 그렇기 때문에 Conjugate Gradient Method로 구하는 방법이 더 좋을수 있습니다.</p>\n<p>살펴봤듯이 본 논문의 NPG는 완벽하지 않습니다. 위의 몇가지 문제점을 극복하기 위한 추가적인 연구가 필요하다고 할 수 있습니다.</p>\n<p><br><br></p>\n<h1 id=\"처음으로\"><a href=\"#처음으로\" class=\"headerlink\" title=\"처음으로\"></a>처음으로</h1><h2 id=\"PG-Travel-Guide\"><a href=\"#PG-Travel-Guide\" class=\"headerlink\" title=\"PG Travel Guide\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/29/0_pg-travel-guide/\">PG Travel Guide</a></h2><p><br></p>\n<h1 id=\"이전으로\"><a href=\"#이전으로\" class=\"headerlink\" title=\"이전으로\"></a>이전으로</h1><h2 id=\"DDPG-여행하기\"><a href=\"#DDPG-여행하기\" class=\"headerlink\" title=\"DDPG 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/26/3_ddpg/\">DDPG 여행하기</a></h2><p><br></p>\n<h1 id=\"다음으로\"><a href=\"#다음으로\" class=\"headerlink\" title=\"다음으로\"></a>다음으로</h1><h2 id=\"NPG-Code\"><a href=\"#NPG-Code\" class=\"headerlink\" title=\"NPG Code\"></a><a href=\"https://github.com/reinforcement-learning-kr/pg_travel/blob/master/mujoco/agent/tnpg.py\" target=\"_blank\" rel=\"noopener\">NPG Code</a></h2><h2 id=\"TRPO-여행하기\"><a href=\"#TRPO-여행하기\" class=\"headerlink\" title=\"TRPO 여행하기\"></a><a href=\"https://reinforcement-learning-kr.github.io/2018/06/24/5_trpo/\">TRPO 여행하기</a></h2>"},{"title":"PG Travel implementation story","date":"2018-08-23T05:18:32.000Z","author":"이웅원, 장수영, 공민서, 양혁렬","subtitle":"피지여행 구현 이야기","comments":1,"_content":"\n\n# PG Travel implementation story\n\n- 구현 코드 링크 : [https://github.com/reinforcement-learning-kr/pg_travel](https://github.com/reinforcement-learning-kr/pg_travel)\n\n\n피지여행 프로젝트에서는 다음 7개 논문을 살펴보았습니다. 각 논문에 대한 리뷰는 이전 글들에서 다루고 있습니다. \n\n<a name=\"1\"></a>\n\n* [1] R. Sutton, et al., \"Policy Gradient Methods for Reinforcement Learning with Function Approximation\", NIPS 2000.\n<a name=\"2\"></a>\n* [2] D. Silver, et al., \"Deterministic Policy Gradient Algorithms\", ICML 2014.\n<a name=\"3\"></a>\n* [3] T. Lillicrap, et al., \"Continuous Control with Deep Reinforcement Learning\", ICLR 2016.\n<a name=\"4\"></a>\n* [4] S. Kakade, \"A Natural Policy Gradient\", NIPS 2002.\n<a name=\"5\"></a>\n* [5] J. Schulman, et al., \"Trust Region Policy Optimization\", ICML 2015.\n<a name=\"6\"></a>\n* [6] J. Schulman, et al., \"High-Dimensional Continuous Control using Generalized Advantage Estimation\", ICLR 2016.\n<a name=\"7\"></a>\n* [7] J. Schulman, et al., \"Proximal Policy Optimization Algorithms\", arXiv, https://arxiv.org/pdf/1707.06347.pdf.\n\n강화학습 알고리즘을 이해하는데 있어서 논문을 보고 이론적인 부분을 알아가는 것이 좋습니다. 하지만 실제 코드로 돌아가는 것은 논문만 보고는 알 수 없는 경우가 많습니다. 따라서 피지여행 프로젝트에서는 위 7개 논문 중에 DPG와 DDPG를 제외한 알고리즘을 구현해보았습니다. 구현한 알고리즘은 다음 4개입니다. 이 때, TRPO와 PPO 구현에는 GAE(General Advantage Estimator)가 함께 들어갑니다. \n \n* Vanilla Policy Gradient [[1](#1)]\n* TNPG(Truncated Natural Policy Gradient) [[4](#4)]\n* TRPO(Trust Region Policy Optimization) [[5](#5)]\n* PPO(Proximal Policy Optimization) [[7](#7)].\n\n바닥부터 저희가 구현한 것은 아니며 다음 코드들을 참고해서 구현하였습니다. Vanilla PG의 경우 RLCode의 깃헙을 참고하였습니다.\n\n* [OpenAI Baseline](https://github.com/openai/baselines/tree/master/baselines/trpo_mpi)\n* [Pytorch implemetation of TRPO](https://github.com/ikostrikov/pytorch-trpo)\n* [RLCode Actor-Critic](https://github.com/rlcode/reinforcement-learning-kr/tree/master/2-cartpole/2-actor-critic)\n\nGAE와 TRPO, PPO 논문에서는 Mujoco라는 물리 시뮬레이션을 학습 환경으로 사용합니다. 따라서 저희도 Mujoco로 처음 시작을 하였습니다. 하지만 Mujoco는 1달만 무료이고 그 이후부터 유료이며 확장성이 떨어집니다. Unity ml-agent는 기존 Unity를 그대로 사용하면서 쉽게 강화학습 에이전트를 붙일 수 있도록 설계되어 있습니다. Unity ml-agent에서는 저희가 살펴본 알고리즘 중에 가장 최신 알고리즘은 PPO를 적용해봤습니다. 기본적으로 제공하는 환경 이외에 저희가 customize 한 환경에서도 학습해봤습니다.  \n\n* mujoco-py: [https://github.com/openai/mujoco-py](https://github.com/openai/mujoco-py)\n* Unity ml-agent: [https://github.com/Unity-Technologies/ml-agents](https://github.com/Unity-Technologies/ml-agents)\n\n코드를 구현하고 환경에서 학습을 시키면서 여러가지 이슈들이 있었고 해결해내가는 과정이 있었습니다. 그 과정을 간단히 정리해서 공유하면 PG를 공부하는 분들께 도움일 될 것 같습니다. 저희가 구현한 순서대로 1. Mujoco 학습 2. Unity ml-agent 학습 3. Unity Curved Surface 로 이 포스트가 진행됩니다.\n\n<br/>\n## 1. Mujoco 학습\n일명 \"Continuous control\" 문제는 action이 discrete하지 않고 continuous한 경우를 말합니다. Mujoco는 continuous control에 강화학습을 적용한 논문들이 애용하는 시뮬레이터입니다. 저희가 리뷰한 논문 중에서도 TRPO, PPO, GAE에서 Mujoco를 사용합니다. 따라서 저희가 처음 피지여행 알고리즘을 적용한 환경으로 Mujoco를 선택했습니다. \n\nMujoco에는 Ant, HalfCheetah, Hopper, Humanoid, HumanoidStandup, InvertedPendulum, Reacher, Swimmer, Walker2d 과 같은 환경이 있습니다. 그 중에서 Hopper에 맞춰서 학습이 되도록 코드를 구현하였습니다. Mujoco 설치와 관련된 내용은 Wiki에 있습니다.\n\n</br>\n### 1.1 Hopper\nHopper는 외다리로 뛰어가는 것을 학습하는 것이 목표입니다. Hopper는 다음과 같이 생겼습니다. \n<img src=\"https://www.dropbox.com/s/wjxrelxyp014j3g/Screenshot%202018-08-23%2000.55.54.png?dl=1\">\n\n환경을 이해하려면 환경의 상태와 행동, 보상 그리고 학습하고 싶은 목표를 알아야합니다. \n\n- 상태 : 관절의 위치, 각도, 각속도\n- 행동 : 관절의 가해지는 토크\n- 보상 : 앞으로 나아가는 속도\n- 목표 : 최대한 앞으로 많이 나아가기\n\n즉 에이전트는 time step마다 관절의 위치와 각도를 받아와서 그 상태에서 어떻게 움직여야 앞으로 나아갈 수 있는지를 학습해야 합니다. 행동은 discrete action이 아닌 continuous action으로 -1과 1사이의 값을 가집니다. 만약 행동이 -1이라면 해당 관절에 시계반대방향으로 토크를 주는 것이고 행동이 +1이라면 해당 관절에 시계방향으로 토크를 주는 것입니다. \n\ncontinuous action을 주는 방법은 네트워크(Actor)의 output layer에서 activation function으로 tanh와 같은 것을 사용해서 continuous한 값을 출력하는 것이 있습니다. 하지만 피지여행 코드 구현에서는 action을 gaussian distribution에서 sampling 하였습니다. 이렇게 하면 분산을 일정하게 유지하면서 지속적인 exploration을 할 수 있습니다. 간단하게 그림으로 보자면 다음과 같습니다. \n<img src=\"https://www.dropbox.com/s/94g01zdxyf5oxu1/Screenshot%202018-08-23%2001.20.21.png?dl=1\">\n\n네트워크 구조와 행동을 선택하는 부분은 다음과 같습니다. Hidden Layer의 activation function으로 tanh를 사용했으며(ReLU를 테스트해보지는 않았습니다. 기존 TRPO, PPO 구현들과 논문에서 tanh를 사용하기 때문에 저희도 사용했습니다. 뒤에 유니티 환경에서는 Swish라는 것을 사용합니다.) log std를 0으로 고정함으로서 일정한 폭을 가지는 분포를 만들어낼 수 있습니다. 이 분포로부터 action을 sampling 합니다.\n\n<img src=\"https://www.dropbox.com/s/xfl9zxies0lmpm1/Screenshot%202018-08-23%2001.20.44.png?dl=1\">\n\n</br>\n### 1.2 Vanilla PG\nVanilla PG는 Actor-Critic의 가장 간단한 형태입니다. Vanilla PG는 이후의 구현에 대한 baseline이 됩니다. 구현이 가장 간단하면서 학습이 안되는 것은 아닙니다. 따라서 코드 전체 구조를 잡는데 Vanilla PG를 짜는 것이 도움이 됩니다. 전반적인 코드 구조는 다음과 같습니다.\n\n- iteration 마다 일정한 step 수만큼 환경에서 진행해서 샘플을 모은다\n- 모은 샘플로 Actor와 Critic을 학습한다\n- 반복한다\n \n\n```python\nepisodes = 0\nfor iter in range(15000):\n    actor.eval(), critic.eval()\n    memory = deque()\n    \n    while steps < 2048:\n        episodes += 1\n        state = env.reset()\n        state = running_state(state)\n        score = 0\n        for _ in range(10000):\n            mu, std, _ = actor(torch.Tensor(state).unsqueeze(0))\n            action = get_action(mu, std)[0]\n            next_state, reward, done, _ = env.step(action)\n            next_state = running_state(next_state)\n\n            if done:\n                mask = 0\n            else:\n                mask = 1\n\n            memory.append([state, action, reward, mask])\n            state = next_state\n\n            if done:\n                break\n                \n    actor.train(), critic.train()\n    train_model(actor, critic, memory, actor_optim, critic_optim)\n```\n\nmemory에 sample을 저장할 때 sample은 state와 action, reward, mask(마지막 state일 경우 0 나머지 1)입니다. mask의 경우 뒤에서 return이나 advantage를 계산할 때 사용됩니다. 또 하나 염두에 두어야할 것은 running_state 입니다. running_state는 input으로 들어오는 state의 scale이 일정하지 않기 때문에 사용합니다. 즉 state의 각 dimension을 평균 0 분산 1로 standardization 하는 것입니다. 따라서 모델을 저장할 때 각 dimension 마다의 평균과 분산도 같이 저장해서 테스트할 때 불러와서 사용해야 합니다.\n\nVanilla PG의 경우 학습 부분이 상당히 간단합니다. 다음 코드를 보시면 메모리에서 state, action, reward, mask를 꺼냅니다. reward와 mask를 통해 return을 구할 수 있고 이 return을 통해 actor를 업데이트 할 수 있습니다 (REINFORCE 알고리즘을 떠올리시면 됩니다). 여기서 critic이 하는 일은 없지만 뒤의 알고리즘들과 코드의 통일성을 위해 fake로 넣어놨습니다. Return은 평균을 빼고 분산으로 나눠서 standardize 합니다. \n\n```python\ndef train_model(actor, critic, memory, actor_optim, critic_optim):\n    memory = np.array(memory)\n    states = np.vstack(memory[:, 0])\n    actions = list(memory[:, 1])\n    rewards = list(memory[:, 2])\n    masks = list(memory[:, 3])\n\n    returns = get_returns(rewards, masks)\n    train_critic(critic, states, returns, critic_optim)\n    train_actor(actor, returns, states, actions, actor_optim)\n    return returns\n```\n\n이 코드로 Hopper 환경에서 학습한 그래프는 다음과 같습니다. \n<img src=\"https://www.dropbox.com/s/asoysfuk76zs1dk/Screenshot%202018-08-23%2001.30.58.png?dl=1\">\n\n</br>\n### 1.3 TNPG\nNPG를 이용한 parameter update 식은 다음과 같습니다. \n\n$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$\n\nNPG를 구현하려면 KL-divergence의 Hessian의 inverse를 구해야하는 문제가 생깁니다. 현재와 같이 Deep Neural Network를 쓰는 경우에 Hessian의 inverse를 직접적으로 구하는 것은 computationally inefficient 합니다. 따라서 직접 구하지 않고 Conjugate gradient 방법을 사용해서 Fisher Vector Product ($$F^{-1}g$$)를 구합니다. 이러한 알고리즘을 Truncated Natural Policy Gradient(TNPG)라고 부릅니다. \n\nTNPG에서 parameter update를 구하는 과정은 다음과 같습니다. \n1. Return 구하기\n2. Critic 학습하기\n3. logp * return --> loss 구하기\n4. loss의 미분과 kl-divergence의 2차 미분을 통해 step direction 구하기\n5. 구한 step direction으로 parameter update\n\n```python\ndef train_model(actor, critic, memory, actor_optim, critic_optim):\n    memory = np.array(memory)\n    states = np.vstack(memory[:, 0])\n    actions = list(memory[:, 1])\n    rewards = list(memory[:, 2])\n    masks = list(memory[:, 3])\n\n    # ----------------------------\n    # step 1: get returns\n    returns = get_returns(rewards, masks)\n\n    # ----------------------------\n    # step 2: train critic several steps with respect to returns\n    train_critic(critic, states, returns, critic_optim)\n\n    # ----------------------------\n    # step 3: get gradient of loss and hessian of kl\n    loss = get_loss(actor, returns, states, actions)\n    loss_grad = torch.autograd.grad(loss, actor.parameters())\n    loss_grad = flat_grad(loss_grad)\n    step_dir = conjugate_gradient(actor, states, loss_grad.data, nsteps=10)\n\n    # ----------------------------\n    # step 4: get step direction and step size and update actor\n    params = flat_params(actor)\n    new_params = params + 0.5 * step_dir\n    update_model(actor, new_params)\n    \n```\n\nconjugate gradient 코드는 OpenAI baseline에서 가져왔습니다. 이 코드는 원래 John schulmann 개인 repository에 있는 그대로 사용하는 것입니다. nsteps 만큼 iterataion을 반복하며 결국 x를 구하는 것인데 이 x가 step direction 입니다. \n\n```python\n# from openai baseline code\n# https://github.com/openai/baselines/blob/master/baselines/common/cg.py\ndef conjugate_gradient(actor, states, b, nsteps, residual_tol=1e-10):\n    x = torch.zeros(b.size())\n    r = b.clone()\n    p = b.clone()\n    rdotr = torch.dot(r, r)\n    for i in range(nsteps):\n        _Avp = fisher_vector_product(actor, states, p)\n        alpha = rdotr / torch.dot(p, _Avp)\n        x += alpha * p\n        r -= alpha * _Avp\n        new_rdotr = torch.dot(r, r)\n        betta = new_rdotr / rdotr\n        p = r + betta * p\n        rdotr = new_rdotr\n        if rdotr < residual_tol:\n            break\n    return x\n```\n\nfisher_vector_product는 kl-divergence의 2차미분과 어떠한 vector의 곱인데 p는 처음에 gradient 값이었다가 점차 업데이트가 됩니다. kl-divergence의 2차 미분을 구하는 과정은 다음과 같습니다. 일단 kl-divergence를 현재 policy에 대해서 구한 다음에 actor parameter에 대해서 미분합니다. 이렇게 미분한 gradient를 일단 flat하게 핀 다음에 p라는 벡터와 곱해서 하나의 값으로 만듭니다. 그 값을 다시 actor의 parameter로 만듦으로서 따로 KL-divergence의 2차미분을 구하지않고 Fisher vector product를 구할 수 있습니다.\n\n```python\ndef fisher_vector_product(actor, states, p):\n    p.detach()\n    kl = kl_divergence(new_actor=actor, old_actor=actor, states=states)\n    kl = kl.mean()\n    kl_grad = torch.autograd.grad(kl, actor.parameters(), create_graph=True)\n    kl_grad = flat_grad(kl_grad)  # check kl_grad == 0\n\n    kl_grad_p = (kl_grad * p).sum()\n    kl_hessian_p = torch.autograd.grad(kl_grad_p, actor.parameters())\n    kl_hessian_p = flat_hessian(kl_hessian_p)\n\n    return kl_hessian_p + 0.1 * p\n```\n\nTNPG 학습 결과는 다음과 같습니다. \n<img src=\"https://www.dropbox.com/s/uc4c0s00qbs33nr/Screenshot%202018-08-23%2001.53.17.png?dl=1\">\n\n</br>\n### 1.4 TRPO\nTRPO와 NPG가 다른 점은 surrogate loss 사용과 trust region 입니다. 하지만 실제로 구현해서 학습을 시켜본 결과 trust region을 넘어가서 back tracking line search를 하는 경우는 거의 없습니다. 따라서 주된 변화는 surrogate loss에 있다고 보셔도 됩니다. Surrogate loss에서 advantage function을 사용하는데 본 코드 구현에서는 GAE를 사용하였습니다. TRPO 업데이트 식은 다음과 같습니다. Q function 위치에 GAE가 들어갑니다.\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}\n$$\n\n\nGAE를 구하는 코드는 다음과 같습니다. GAE는 td-error의 discounted summation이라고 볼 수 있습니다. 마지막에 advants를 standardization 하는 것은 return에서 하는 것과 같은 효과를 봅니다. 하지만 standardization을 안하고 실험을 해보지는 않았습니다.\n\n```python\ndef get_gae(rewards, masks, values):\n    rewards = torch.Tensor(rewards)\n    masks = torch.Tensor(masks)\n    returns = torch.zeros_like(rewards)\n    advants = torch.zeros_like(rewards)\n\n    running_returns = 0\n    previous_value = 0\n    running_advants = 0\n\n    for t in reversed(range(0, len(rewards))):\n        running_returns = rewards[t] + hp.gamma * running_returns * masks[t]\n        running_tderror = rewards[t] + hp.gamma * previous_value * masks[t] - \\\n                    values.data[t]\n        running_advants = running_tderror + hp.gamma * hp.lamda * \\\n                          running_advants * masks[t]\n\n        returns[t] = running_returns\n        previous_value = values.data[t]\n        advants[t] = running_advants\n\n    advants = (advants - advants.mean()) / advants.std()\n    return returns, advants\n```\n\nSurrogate loss를 구하는 코드는 다음과 같습니다. Advantage function(GAE)를 구하고 나면 이전 policy와 현재 policy 사이의 ratio를 구해서 advantage function에 곱하면 됩니다. 이 때 사실 old policy와 new policy는 값은 같지만 old policy는 clone()이나 detach()를 사용해서 update가 안되게 만들어줍니다.\n\n```python\ndef surrogate_loss(actor, advants, states, old_policy, actions):\n    mu, std, logstd = actor(torch.Tensor(states))\n    new_policy = log_density(torch.Tensor(actions), mu, std, logstd)\n    advants = advants.unsqueeze(1)\n\n    surrogate = advants * torch.exp(new_policy - old_policy)\n    surrogate = surrogate.mean()\n    return surrogate\n\n```\n\nActor의 step direction을 구하는 것은 TNPG와 동일합니다. TNPG에서는 step direction으로 바로 업데이트 했지만 TRPO는 다음과 같은 작업을 해줍니다. Full step을 구하는 과정이라고 볼 수 있습니다. \n\n```python\n# ----------------------------\n# step 4: get step direction and step size and full step\nparams = flat_params(actor)\nshs = 0.5 * (step_dir * fisher_vector_product(actor, states, step_dir)\n             ).sum(0, keepdim=True)\nstep_size = 1 / torch.sqrt(shs / hp.max_kl)[0]\nfull_step = step_size * step_dir\n\n```\n\n이렇게 full step을 구하고 나면 Trust region optimization 단계에 들어갑니다. expected improvement는 구한 step 만큼 parameter space에서 움직였을 때 예상되는 performance 변화입니다. 이 값은 kl-divergence와 함께 trust region 안에 있는지 밖에 있는지 판단하는 근거가 됩니다. expected improve는 출발점에서의 gradient * full step으로 구합니다. 그리고 10번을 돌아가며 Back-tracking line search를 실시합니다. 처음에는 full step 만큼 가본 다음에 kl-divergence와 emprovement를 통해 trust region 안이면 루프 탈출, 밖이면 full step을 반만큼 쪼개서 다시 이 과정을 반복합니다.  \n\n```python\n# ----------------------------\n# step 5: do backtracking line search for n times\nold_actor = Actor(actor.num_inputs, actor.num_outputs)\nupdate_model(old_actor, params)\nexpected_improve = (loss_grad * full_step).sum(0, keepdim=True)\nexpected_improve = expected_improve.data.numpy()\n\nflag = False\nfraction = 1.0\nfor i in range(10):\n    new_params = params + fraction * full_step\n    update_model(actor, new_params)\n    new_loss = surrogate_loss(actor, advants, states, old_policy.detach(),\n                              actions)\n    new_loss = new_loss.data.numpy()\n    loss_improve = new_loss - loss\n    expected_improve *= fraction\n    kl = kl_divergence(new_actor=actor, old_actor=old_actor, states=states)\n    kl = kl.mean()\n\n    print('kl: {:.4f}  loss improve: {:.4f}  expected improve: {:.4f}  '\n          'number of line search: {}'\n          .format(kl.data.numpy(), loss_improve, expected_improve[0], i))\n\n    # see https: // en.wikipedia.org / wiki / Backtracking_line_search\n    if kl < hp.max_kl and (loss_improve / expected_improve) > 0.5:\n        flag = True\n        break\n\n    fraction *= 0.5\n```\n\nCritic의 학습은 단순히 value function과 return의 MSE error를 계산해서 loss로 잡고 loss를 최소화하도록 학습합니다. TRPO 학습 결과는 다음과 같습니다.\n<img src=\"https://www.dropbox.com/s/rc9hxsx1kvokcrv/Screenshot%202018-08-23%2013.36.51.png?dl=1\">\n\n</br>\n### 1.5 PPO\nPPO의 장점을 꼽으라면 GPU 사용하기 좋고 sample efficiency가 늘어난다는 것입니다. TNPG와 TRPO의 경우 한 번 모은 sample은 모델을 단 한 번 업데이트하는데 사용하지만 PPO의 경우 몇 mini-batch로 epoch를 돌리기 때문입니다. GAE를 사용한다는 것은 같고 Conjugate gradient나 Fisher vector product나 back tracking line search가 다 빠집니다. 대신 loss function clip으로 monotonic improvement를 보장하게 학습합니다. 따라서 코드가 상당히 간단해집니다. \n\n다음 코드 부분이 PPO의 전체라고 봐도 무방합니다. PPO는 다음과 같은 순서로 학습합니다. \n\n- batch를 random suffling하고 mini batch를 추출\n- value function 구하기\n- critic loss 구하기 (clip을 사용해도 되고 TRPO와 같이 그냥 학습시켜도 됌)\n- surrogate loss 구하기\n- surrogate loss clip해서 actor loss 만들기\n- actor와 critic 업데이트\n\nActor의 loss를 구하는 것은 다음 식의 값을 구하는 것입니다. 이 식을 구하려면 ratio에 한 번 클립하고 loss 값을 한 번 min을 취하면 됩니다.\n\n$$L^{CLIP}(\\theta) = \\hat{E}_t [min(r_t(\\theta) \\, \\hat{A}_t,  clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\, \\hat{A}_t)]$$\n\n이 코드 구현에서는 actor와 critic을 따로 모델로 만들어서 따로 따로 업데이트를 하지만 하나로 만든다면 loss로 한 번만 업데이트하면 됩니다. 또한 entropy loss를 최종 loss에 더해서 regularization 효과를 볼 수도 있습니다. Critic loss에 clip 해주는 것은 OpenAI baseline의 ppo2 코드를 참조하였습니다.\n\n```python\n# step 2: get value loss and actor loss and update actor & critic\nfor epoch in range(10):\n    np.random.shuffle(arr)\n\n    for i in range(n // hp.batch_size):\n        batch_index = arr[hp.batch_size * i: hp.batch_size * (i + 1)]\n        batch_index = torch.LongTensor(batch_index)\n        inputs = torch.Tensor(states)[batch_index]\n        returns_samples = returns.unsqueeze(1)[batch_index]\n        advants_samples = advants.unsqueeze(1)[batch_index]\n        actions_samples = torch.Tensor(actions)[batch_index]\n        oldvalue_samples = old_values[batch_index].detach()\n\n        loss, ratio = surrogate_loss(actor, advants_samples, inputs,\n                                     old_policy.detach(), actions_samples,\n                                     batch_index)\n\n        values = critic(inputs)\n        clipped_values = oldvalue_samples + \\\n                         torch.clamp(values - oldvalue_samples,\n                                     -hp.clip_param,\n                                     hp.clip_param)\n        critic_loss1 = criterion(clipped_values, returns_samples)\n        critic_loss2 = criterion(values, returns_samples)\n        critic_loss = torch.max(critic_loss1, critic_loss2).mean()\n\n        clipped_ratio = torch.clamp(ratio,\n                                    1.0 - hp.clip_param,\n                                    1.0 + hp.clip_param)\n        clipped_loss = clipped_ratio * advants_samples\n        actor_loss = -torch.min(loss, clipped_loss).mean()\n\n        loss = actor_loss + 0.5 * critic_loss\n\n        critic_optim.zero_grad()\n        loss.backward(retain_graph=True)\n        critic_optim.step()\n\n        actor_optim.zero_grad()\n        loss.backward()\n        actor_optim.step()\n```\n\nPPO의 학습 결과는 다음과 같습니다. \n<img src=\"https://www.dropbox.com/s/rkxa836ap931kbd/Screenshot%202018-08-23%2013.50.57.png?dl=1\">\n\n\n</br>\n## 2. Unity ml-agent 학습\nMujoco Hopper(half-cheetah와 같은 것도)에 Vanilla PG, TNPG, TRPO, PPO를 구현해서 적용했습니다. Mujoco의 경우 이미 Hyper parameter와 같은 정보들이 논문이나 블로그에 있기 때문에 상대적으로 continuous control로 시작하기에는 좋습니다. 맨 처음에 말했듯이 Mujoco는 1달만 무료이고 그 이후부터 유료이며 확장성이 떨어집니다. 좀 더 general한 agent를 학습시키기에 좋은 환경이 필요합니다. 따라서 Unity ml-agent를 살펴봤습니다. Repository는 다음과 같습니다. \n- [Unity ml-agent repository](https://github.com/Unity-Technologies/ml-agents)\n- [Unity ml-agent homepage](https://unity3d.com/machine-learning/)\n\n<img src=\"https://www.dropbox.com/s/lapholj8r4nxmb1/Screenshot%202018-08-24%2013.41.31.png?dl=1\">\n\n현재 Unity ml-agent에서 기본으로 제공하는 환경은 다음과 같습니다. Unity ml-agent는 기존 Unity를 그대로 사용하면서 쉽게 강화학습 에이전트를 붙일 수 있도록 설계되어 있습니다. Unity ml-agent에서는 Walker 환경에서 저희가 살펴본 알고리즘 중에 가장 최신 알고리즘은 PPO를 적용해봤습니다. 이 포스트를 보시는 분들은 이 많은 다른 환경에 자유롭게 저희 코드를 적용할 수 있습니다.\n- [각 환경에 대한 설명](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md)\n\n<img src=\"https://www.dropbox.com/s/lrbodw5dypxowmw/Screenshot%202018-08-24%2014.06.14.png?dl=1\">\n\nUnity ml-agent를 이용해서 강화학습을 하기 위해서는 다음과 같이 진행됩니다. 단계별로 설명하겠습니다. \n\n- Unity에서 환경 만들기\n- Python에서 unity 환경 불러와서 테스트하기\n- 기존에 하던대로 학습하기\n\n\n</br>\n### 2.1 Walker 환경 만들기\n강화학습을 하는 많은 분들이 Unity를 한 번도 다뤄보지 않은 경우가 많습니다. 저도 그런 경우라서 어떻게 환경을 만들어야할지 처음에는 감이 잡히지 않았습니다. 하지만 Unity ml-agent에서는 상당히 자세한 guide를 제공합니다. 다음은 Unity ml-agent의 가장 기본적인 환경인 3DBall에 대한 tutorial입니다. 설치 guide도 제공하고 있으니 참고하시면 될 것 같습니다.\n- [3DBall 예제 tutorial](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Basic-Guide.md)\n- [Unity ml-agent 설치 guide](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md)\n\nUnity ml-agent에서 제공하는 3DBall tutorial을 참고해서 Walker 환경을 만들었습니다. Walker 환경을 만드는 과정을 간단히 말씀드리겠습니다. 다음 그림의 단계들을 동일하므로 따라하시면 됩니다. Unity를 열고 unity-environment로 들어가시면 됩니다.\n<img src=\"https://www.dropbox.com/s/fbdqg781w46a5mz/Screenshot%202018-08-24%2014.50.50.png?dl=1\">\n\n그러면 화면 하단에서 다음과 같은 것을 볼 수 있습니다. Assets/ML-Agents/Examples로 들어가보면 Walker가 있습니다. Scenes에서 Walker를 더블클릭하면 됩니다.\n<img src=\"https://www.dropbox.com/s/h349xml3faln0wy/Screenshot%202018-08-24%2014.52.14.png?dl=1\">\n\n더블클릭해서 나온 화면에서 오른쪽 상단의 파란색 화살표를 누르면 환경이 실행이 됩니다. 저희가 학습하고자 하는 agent는 바로 이녀석입니다. 왼쪽 리스트를 보면 WalkerPair가 11개가 있는 것을 볼 수 있습니다. Unity ml-agent 환경은 기본적으로 Multi-agent로 학습하도록 설정되어있습니다. 따라서 여러개의 Walker들이 화면에 보이는 것입니다. \n<img src=\"https://www.dropbox.com/s/cy8m5kqdmkhopjo/Screenshot%202018-08-24%2014.54.57.png?dl=1\">\n\n리스트 중에 Walker Academy를 클릭해서 그 하위에 있는 WalkerBrain을 더블클릭합니다. 그러면 화면 오른쪽에 다음과 같은 Brain 설정을 볼 수 있습니다. Brain은 쉽게 말해서 Agent라고 생각하면 됩니다. 이 Agent는 상태로 212차원의 vector가 주어지며 다 continuous한 값을 가집니다. 행동은 39개의 행동을 할 수 있으며 다 Continuous입니다. Mujoco에 비해서 상태나 행동의 차원이 상당히 높습니다. 여기서 중요한 것은 Brain Type입니다. Brain type은 internal, external, player, heuristic이 있습니다. player로 type을 설정하고 화면 상단의 play 버튼을 누르면 여러분이 agent를 움직일 수 있습니다. 하지만 Walker는 사람이 움직이는게 거의 불가능하므로 player 기능은 사용할 수 없습니다. 다른 환경에서는 사용해볼 수 있으니 재미로 한 번 플레이해보시면 좋습니다! \n<center><img src=\"https://www.dropbox.com/s/uxfm162f1scbzo5/Screenshot%202018-08-24%2015.09.04.png?dl=1\" width=\"400px\"></center>\n\n이번에는 WalkerPair에서 WalkerAgent를 더블클릭해보겠습니다. 이 설정을 보아 5000 step이 episode의 max step인 것을 볼 수 있습니다.\n<center><img src=\"https://www.dropbox.com/s/r6gwemlczwic2ma/Screenshot%202018-08-24%2015.16.19.png?dl=1\" width=\"400px\"></center>\n\n이제 상단 file menu에서 build setting에 들어갑니다. 환경을 build해서 python 코드에서 import하기 위해서입니다. 물론 unity 환경과 python 코드를 binding해주는 부분은 ml-agent 코드 안에 있습니다. Build 버튼을 누르면 환경이 build가 됩니다.\n<center><img src=\"https://www.dropbox.com/s/4dtgoz1k8896vxs/Screenshot%202018-08-24%2015.19.07.png?dl=1\" width=\"500px\"></center>\n\n\n</br>\n### 2.2 Python에서 unity 환경 불러와서 테스트하기\n환경을 build 했으면 build한 환경을 python에서 불러와서 random action으로 테스트해봅니다. 환경을 테스트하는 코드는 pg_travel repository에서 unity 폴더 밑에 있습니다. test_env.py라는 코드는 간단하게 다음과 같습니다. Build한 walker 환경은 env라는 폴더 밑에 넣어줍니다. unityagent를 import하는데 ml-agent를 git clone 해서 python 폴더 내에서 \"python setup.py install\"을 실행했다면 문제없이 import 됩니다. UnityEnvironment를 통해 env라는 환경을 선언할 수 있습니다. 이렇게 선언하고 나면 gym과 상당히 유사한 형태로 환경과 상호작용이 가능합니다. \n\n```python\nimport numpy as np\nfrom unityagents import UnityEnvironment\nfrom utils.utils import get_action\n\nif __name__==\"__main__\":\n    env_name = \"./env/walker_test\"\n    train_mode = False\n\n    env = UnityEnvironment(file_name=env_name)\n\n    default_brain = env.brain_names[0]\n    brain = env.brains[default_brain]\n    env_info = env.reset(train_mode=train_mode)[default_brain]\n\n    num_inputs = brain.vector_observation_space_size\n    num_actions = brain.vector_action_space_size\n    num_agent = env._n_agents[default_brain]\n\n    print('the size of input dimension is ', num_inputs)\n    print('the size of action dimension is ', num_actions)\n    print('the number of agents is ', num_agent)\n   \n    score = 0\n    episode = 0\n    actions = [0 for i in range(num_actions)] * num_agent\n    for iter in range(1000):\n        env_info = env.step(actions)[default_brain]\n        rewards = env_info.rewards\n        dones = env_info.local_done\n        score += rewards[0]\n\n        if dones[0]:\n            episode += 1\n            score = 0\n            print('{}th episode : mean score of 1st agent is {:.2f}'.format(\n                episode, score))\n```\n\n위 코드를 실행하면 다음과 같이 실행창에 뜹니다. External brain인 것을 알 수 있고 default_brain은 brain 중에 하나만 가져왔기 때문에 number of brain은 1이라고 출력합니다. input dimension은 212이고 action dimension은 39이고 agent 수는 11인 것으로봐서 제대로 환경이 불러와진 것을 확인할 수 있습니다. \n<img src=\"https://www.dropbox.com/s/cioa9h7qu25vonz/Screenshot%202018-08-24%2015.47.43.png?dl=1\">\n\n이 환경에서 행동하려면 agent 숫자만큼 행동을 줘야합니다. 모두 0로 행동을 주고 실행하면 다음과 같이 뒤로 넘어지는 행동을 반복합니다. env.step(actions)[default_brain]으로 env_info를 받아오면 거기서부터 reward와 done, next_state를 받아올 수 있습니다. 이제 학습하기만 하면 됩니다. \n<img src=\"https://www.dropbox.com/s/8qrmxoski6p4n07/Screenshot%202018-08-24%2016.00.21.png?dl=1\">\n\n</br>\n### 2.3 Walker 학습하기\n기존에 Mujoco에 적용했던 PPO 코드를 그대로 Walker에 적용하니 잘 학습이 안되었습니다. 다음 사진이 저희가 중간 해커톤으로 모여서 이 상황을 공유할 때의 사진입니다.\n<img src=\"https://i.imgur.com/1aR2Z77.png\" width=500px>\n\nUnity ml-agent에서는 PPO를 기본 agent로 제공합니다. 학습 코드도 제공하기 때문에 mujoco에 적용했던 코드와의 차이점을 분석할 수 있었습니다. mujoco 코드와 ml-agent baseline 코드의 차이점은 다음과 같습니다. \n\n- agent 여러개를 이용, 별개의 memory에 저장한 후에 gradient를 합침\n- GAE 및 time horizon 등 hyper parameter가 다름\n- Actor와 Critic의 layer가 1층 더 두꺼우며 hidden layer 자체의 사이즈도 더 큼\n- hidden layer의 activation function이 tanh가 아닌 swish\n\nml-agent baseline 코드리뷰할 때 작성했던 마인드맵은 다음과 같습니다.\n<img src=\"https://i.imgur.com/YeaEntG.png\">\n\n크게는 두 가지를 개선해서 성능이 많이 향상했습니다.\n1. Network 수정\n2. multi-agent를 활용해서 학습\n\nNetwork 코드는 다음과 같습니다. Hidden Layer를 하나 더 늘렸으며 swish activation function을 사용할 수 있도록 변경했습니다. 사실 swish라는 activation function은 처음 들어보는 생소한 함수였습니다. 하지만 ml-agent baseline에서 사용했다는 사실과 구현이 상당히 간단하다는 점에서 저희 코드에 적용했습니다. 단순히 x * sigmoid(x) 를 하면 됩니다. swish는 별거 아닌 것 같지만 상당한 성능 개선을 가져다줬습니다. 사실 ReLU나 ELU 등 여러 다른 activation function을 적용해서 비교해보는게 best긴 하지만 시간 관계상 그렇게까지 테스트해보지는 못했습니다. 기존에 TRPO나 PPO는 왜 tanh를 사용했었는지도 의문인 점입니다.\n\n```python\nclass Actor(nn.Module):\n    def __init__(self, num_inputs, num_outputs, args):\n        self.args = args\n        self.num_inputs = num_inputs\n        self.num_outputs = num_outputs\n        super(Actor, self).__init__()\n        self.fc1 = nn.Linear(num_inputs, args.hidden_size)\n        self.fc2 = nn.Linear(args.hidden_size, args.hidden_size)\n        self.fc3 = nn.Linear(args.hidden_size, args.hidden_size)\n        self.fc4 = nn.Linear(args.hidden_size, num_outputs)\n\n        self.fc4.weight.data.mul_(0.1)\n        self.fc4.bias.data.mul_(0.0)\n\n    def forward(self, x):\n        if self.args.activation == 'tanh':\n            x = F.tanh(self.fc1(x))\n            x = F.tanh(self.fc2(x))\n            x = F.tanh(self.fc3(x))\n            mu = self.fc4(x)\n        elif self.args.activation == 'swish':\n            x = self.fc1(x)\n            x = x * F.sigmoid(x)\n            x = self.fc2(x)\n            x = x * F.sigmoid(x)\n            x = self.fc3(x)\n            x = x * F.sigmoid(x)\n            mu = self.fc4(x)\n        else:\n            raise ValueError\n\n        logstd = torch.zeros_like(mu)\n        std = torch.exp(logstd)\n        return mu, std, logstd\n```\n\nswish와 tanh를 사용한 학습을 비교한 그래프입니다. 하늘색 그래프가 swish를 사용한 결과, 파란색이 tanh를 사용한 결과입니다. score는 episode 마다의 reward의 합입니다.\n<center><img src=\"https://www.dropbox.com/s/3d07c1kql4h5oqk/Screenshot%202018-08-24%2016.33.45.png?dl=1\" width=\"350px\"></center>\n\n이제 multi-agent로 학습하도록 변경하면 됩니다. PPO의 경우 memory에 time horizon 동안의 sample을 시간순서대로 저장하고 GAE를 구한 이후에 minibatch로 추출해서 학습합니다. 따라서 여러개의 agent로 학습하기 위해서는 memory를 따로 만들어서 각각의 GAE를 구해서 학습해야합니다. Unity에서는 Mujoco에서 했던 것처럼 deque로 memory를 만들지 않고 따로 named tuple로 구현한 memory class를 import 해서 사용했습니다. utils 폴더 밑에 memory.py 코드에 구현되어있으며 코드는 https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb\n에서 가져왔습니다. \n\nstate, action, reward, mask를 저장하는데 불러올 때 각각을 따로 불러올 수 있기 때문에 비효율적 시간을 많이 줄여줍니다. \n```python\nTransition = namedtuple('Transition', ('state', 'action', 'reward', 'mask'))\n\n\nclass Memory(object):\n    def __init__(self):\n        self.memory = []\n\n    def push(self, state, action, reward, mask):\n        \"\"\"Saves a transition.\"\"\"\n        self.memory.append(Transition(state, action, reward, mask))\n\n    def sample(self):\n        return Transition(*zip(*self.memory))\n\n    def __len__(self):\n        return len(self.memory)\n```\n\nmain.py 에서는 이 memory를 agent의 개수만큼 생성합니다. \n\n```python\nmemory = [Memory() for _ in range(num_agent)]\n```\nsample을 저장할 때도 agent마다 따로 따로 저장합니다. \n\n```python\nfor i in range(num_agent):\n    memory[i].push(states[i], actions[i], rewards[i], masks[i])\n\n```\n\ntime horizon이 끝나면 모은 sample 들을 가지고 학습하기 위한 값으로 만드는 과정을 진행합니다. 각각의 memory를 가지고 GAE와 old_policy, old_value 등을 계산해서 하나의 batch로 합칩니다. 그렇게 train_model 메소드에 전달하면 기존과 동일하게 agent를 업데이트합니다.\n\n```python\nsts, ats, returns, advants, old_policy, old_value = [], [], [], [], [], []\n\nfor i in range(num_agent):\n    batch = memory[i].sample()\n    st, at, rt, adv, old_p, old_v = process_memory(actor, critic, batch, args)\n    sts.append(st)\n    ats.append(at)\n    returns.append(rt)\n    advants.append(adv)\n    old_policy.append(old_p)\n    old_value.append(old_v)\n\nsts = torch.cat(sts)\nats = torch.cat(ats)\nreturns = torch.cat(returns)\nadvants = torch.cat(advants)\nold_policy = torch.cat(old_policy)\nold_value = torch.cat(old_value)\n\ntrain_model(actor, critic, actor_optim, critic_optim, sts, ats, returns, advants,\n            old_policy, old_value, args)\n```\n\n이렇게 학습한 에이전트는 다음과 같이 걷습니다. 이렇게 walker를 학습시키고 나니 어떻게 하면 사람처럼 자연스럽게 걷는 것을 agent 스스로 학습할 수 있을까라는 고민을 하게 되었습니다.\n<center><img src=\"https://www.dropbox.com/s/fyz1kn5v92l3rrk/plane-595.gif?dl=1\"></center>\n\nUnity ml-agent에서 제공하는 pretrained된 모델을 다음과 같이 걷습니다. 저희가 학습한 agent와 상당히 다르게 걷는데 왜 그런 차이가 나는지도 분석하고 싶습니다. \n<center><img src=\"https://www.dropbox.com/s/xwz766g7c4eiaia/plane-unity.gif?dl=1\"></center>\n\n\n</br>\n## 3. Unity Curved Surface 제작 및 학습기\nUnity ml-agent에서 제공하는 기본 Walker 환경에서 학습하고 나니 바닥을 조금 울퉁불퉁하게 혹은 경사가 진 곳에서 걷는 것을 학습해보고 싶다라는 생각이 들었습니다. 따라서 간단하게 걷는 배경을 다르게 하는 시도를 해봤습니다. \n\n</br>\n### 3.1 Curved Surface 만들기\nAgent가 걸어갈 배경을 처음부터 만드는 것보다 구할 수 있다면 만들어진 배경을 구하기로 했습니다. Unity를 무료라는 점에서 선택했듯이 배경을 무료로 구할 수 있는 방법을 선택했습니다. \n<center><img src=\"https://www.dropbox.com/s/e0tsp3e3c9uq2zh/Screenshot%202018-08-23%2000.19.14.png?dl=1\"></center>\n\n무료로 공개되어있는 Unity 배경 중에서 Curved Ground 라는 것을 가져와서 작업하기로 했습니다. 이 환경 같은 경우 spline을 그리듯이 중간의 점을 이동시키면서 사용자가 곡면을 수정할 수 있습니다.\n<center><img src=\"https://www.dropbox.com/s/3ppmxotrf6qhzaf/Screenshot%202018-08-23%2000.20.25.png?dl=1\"></center>\n\n간단하게 곡면을 만들어서 공을 굴려보면 다음과 같이 잘 굴러갑니다. \n<center><img src=\"https://www.dropbox.com/s/2e8yqvqj1a4th27/slope_walker_ball.gif?dl=1\"></center>\n\n여러 에이전트가 학습할 수 있도록 오목한 경사면을 제작했습니다. 초반의 모습은 다음과 같았습니다. \n<img src=\"https://www.dropbox.com/s/m492xsfp4bolmz5/Screenshot%202018-08-23%2000.36.06.png?dl=1\">\n\n하지만 최종으로는 다음과 같은 곡면으로 사용했습니다. 위 사진의 배경과 아래 사진의 배경이 다른 점은 slope 길이, 내리막 경사, 오르막 경사입니다. Slope 길이의 경우 길이를 기존 plane 과 동일하게 했더니, 오르막 올라가는 부분이 학습이 잘 안 되었습니다. 따라서 길이를 줄였습니다. 내리막 경사의 경우 너무 경사지면 학습이 잘 안 되고, 너무 완만하니 내리막 티가 잘 안 나기 때문에 적절한 경사를 설정했습니다. 오르막 경사의 경우 내리막보다는 오르막이 더 어려울 것이라고 판단해서 오르막 경사를 낮게 설정했습니다. \n<img src=\"https://www.dropbox.com/s/idbov4wtd6jeqb2/Screenshot%202018-08-23%2000.36.54.png?dl=1\">\n\n</br>\n\n### 3.2 Curved Surface에서 학습하기\n위 환경으로 학습을 할 때, agent가 너무 초반에 빨리 쓰러지는 현상이 발생했습니다. 혹시 발의 각도가 문제일까 싶어서 발 각도를 변경해보았습니다. \n\n<center><img src=\"https://www.dropbox.com/s/znvikbeoj7gku0u/Screenshot%202018-08-23%2000.38.22.png?dl=1\" width=\"400px\"></center>\n\n하지만 역시 평지에서 걷는 것처럼 걷도록 학습이 안되었습니다. 이 환경에서 더 잘 학습하려면 더 여러가지를 시도해봐야할 것 같습니다. (그래도 걷는 게 기특합니다..)\n<center><img src=\"https://www.dropbox.com/s/4fqpsdmnzvnvia0/curved-736.gif?dl=1\"></center>\n\n<img src=\"https://www.dropbox.com/s/t5ngr0io4xeex6y/curved-736-overview.gif?dl=1\">\n\n</br>\n## 4. 구현 후기\n피지여행 구현팀은 총 4명으로 진행했습니다. 각 팀원의 후기를 적어보겠습니다.\n\n- 팀원 장수영: 사랑합니다. 행복합니다.\n- 팀원 공민서: 제가 핵심적인 기능을 구현하지는 못했지만 무조코 설치와 모델 테스트를 맡으면서 딥마인드나 openai의 영상으로만 보던 에이전트의 성장과정을 눈으로 지켜볼 수 있었습니다. 제대로 서있지도 못하던 hopper가 어느정도 훈련이 되고서는 넘어지려하다가도 추진력을 얻기위해 웅크렸다 뛰는 것을 관찰하는 것도 재미있고 육아일기를 보는 아버지의 마음을 조금이나마 이해할 수 있었습니다. 텐서보드를 넣는 걸 깜빡해 일일히 에피소드 별 스코어를 시각화 하면서 텐서보드의 소중함을 알았습니다. 유니티 코드리뷰를 하면서도 시스템 아키텍쳐 설계에 대해서도 배울 점이 있었던 것 같고 swish라는 활성화함수의 존재도 알았었고 curiosity도 알게되었고 역시 다른 사람의 코드를 읽는 것도 많은 공부가 된다고 되새기던 시간이었습니다. 물론 너무 크기가 방대해서 가독성은 많이 떨어졌습니다만 무조코보다 유니티가 훨씬 흥할거라고 생각했습니다. 마지막으로 누구 하나 열정적이지 않은 사람이 없이 치열한 고민을 함께 한 PG여행팀 분들, 저의 부족함과 상생의 기쁨을 알게해주셔서 정말 감사드립니다.\n- 팀원 양혁렬: 여러 에이전트가 함께하면 더 잘하는 걸 보면서 새삼 좋은 분들과 함께 할 수 있어서 행복했습니다\n- 팀원 이웅원: 저희가 직접 바닥부터 다 구현했던 것은 아니지만 구현을 해보면서 논문의 내용을 더 잘 이해할 수 있었습니다. 논문에 나와있지 않은 여러 노하우가 필요한 점들도 많았습니다. 역시 코드로 보고 성능 재현이 되어야 제대로 알고리즘을 이해하게 된다는 것을 다시 느낀 시간이었습니다. 또한 강화학습은 역시 환경세팅이 어렵다는 생각을 했습니다. 하지만 unity ml-agent를 사용해보면서 앞으로 강화학습 환경으로서 가능성이 상당히 크다는 생각을 했습니다. 또한 구현팀과 슬랙, 깃헙으로 협업하면서 온라인 협업에 대해서 더 배워가는 것 같습니다. 아직은 익숙하지 않지만 앞으로는 마치 바로 옆에서 같이 코딩하는 것 같이 될 거라고 생각합니다.","source":"_posts/8_implement.md","raw":"---\ntitle: PG Travel implementation story\ndate: 2018-08-23 14:18:32\ntags: [\"프로젝트\", \"피지여행\"]\ncategories: 프로젝트\nauthor: 이웅원, 장수영, 공민서, 양혁렬\nsubtitle: 피지여행 구현 이야기\ncomments: true\n---\n\n\n# PG Travel implementation story\n\n- 구현 코드 링크 : [https://github.com/reinforcement-learning-kr/pg_travel](https://github.com/reinforcement-learning-kr/pg_travel)\n\n\n피지여행 프로젝트에서는 다음 7개 논문을 살펴보았습니다. 각 논문에 대한 리뷰는 이전 글들에서 다루고 있습니다. \n\n<a name=\"1\"></a>\n\n* [1] R. Sutton, et al., \"Policy Gradient Methods for Reinforcement Learning with Function Approximation\", NIPS 2000.\n<a name=\"2\"></a>\n* [2] D. Silver, et al., \"Deterministic Policy Gradient Algorithms\", ICML 2014.\n<a name=\"3\"></a>\n* [3] T. Lillicrap, et al., \"Continuous Control with Deep Reinforcement Learning\", ICLR 2016.\n<a name=\"4\"></a>\n* [4] S. Kakade, \"A Natural Policy Gradient\", NIPS 2002.\n<a name=\"5\"></a>\n* [5] J. Schulman, et al., \"Trust Region Policy Optimization\", ICML 2015.\n<a name=\"6\"></a>\n* [6] J. Schulman, et al., \"High-Dimensional Continuous Control using Generalized Advantage Estimation\", ICLR 2016.\n<a name=\"7\"></a>\n* [7] J. Schulman, et al., \"Proximal Policy Optimization Algorithms\", arXiv, https://arxiv.org/pdf/1707.06347.pdf.\n\n강화학습 알고리즘을 이해하는데 있어서 논문을 보고 이론적인 부분을 알아가는 것이 좋습니다. 하지만 실제 코드로 돌아가는 것은 논문만 보고는 알 수 없는 경우가 많습니다. 따라서 피지여행 프로젝트에서는 위 7개 논문 중에 DPG와 DDPG를 제외한 알고리즘을 구현해보았습니다. 구현한 알고리즘은 다음 4개입니다. 이 때, TRPO와 PPO 구현에는 GAE(General Advantage Estimator)가 함께 들어갑니다. \n \n* Vanilla Policy Gradient [[1](#1)]\n* TNPG(Truncated Natural Policy Gradient) [[4](#4)]\n* TRPO(Trust Region Policy Optimization) [[5](#5)]\n* PPO(Proximal Policy Optimization) [[7](#7)].\n\n바닥부터 저희가 구현한 것은 아니며 다음 코드들을 참고해서 구현하였습니다. Vanilla PG의 경우 RLCode의 깃헙을 참고하였습니다.\n\n* [OpenAI Baseline](https://github.com/openai/baselines/tree/master/baselines/trpo_mpi)\n* [Pytorch implemetation of TRPO](https://github.com/ikostrikov/pytorch-trpo)\n* [RLCode Actor-Critic](https://github.com/rlcode/reinforcement-learning-kr/tree/master/2-cartpole/2-actor-critic)\n\nGAE와 TRPO, PPO 논문에서는 Mujoco라는 물리 시뮬레이션을 학습 환경으로 사용합니다. 따라서 저희도 Mujoco로 처음 시작을 하였습니다. 하지만 Mujoco는 1달만 무료이고 그 이후부터 유료이며 확장성이 떨어집니다. Unity ml-agent는 기존 Unity를 그대로 사용하면서 쉽게 강화학습 에이전트를 붙일 수 있도록 설계되어 있습니다. Unity ml-agent에서는 저희가 살펴본 알고리즘 중에 가장 최신 알고리즘은 PPO를 적용해봤습니다. 기본적으로 제공하는 환경 이외에 저희가 customize 한 환경에서도 학습해봤습니다.  \n\n* mujoco-py: [https://github.com/openai/mujoco-py](https://github.com/openai/mujoco-py)\n* Unity ml-agent: [https://github.com/Unity-Technologies/ml-agents](https://github.com/Unity-Technologies/ml-agents)\n\n코드를 구현하고 환경에서 학습을 시키면서 여러가지 이슈들이 있었고 해결해내가는 과정이 있었습니다. 그 과정을 간단히 정리해서 공유하면 PG를 공부하는 분들께 도움일 될 것 같습니다. 저희가 구현한 순서대로 1. Mujoco 학습 2. Unity ml-agent 학습 3. Unity Curved Surface 로 이 포스트가 진행됩니다.\n\n<br/>\n## 1. Mujoco 학습\n일명 \"Continuous control\" 문제는 action이 discrete하지 않고 continuous한 경우를 말합니다. Mujoco는 continuous control에 강화학습을 적용한 논문들이 애용하는 시뮬레이터입니다. 저희가 리뷰한 논문 중에서도 TRPO, PPO, GAE에서 Mujoco를 사용합니다. 따라서 저희가 처음 피지여행 알고리즘을 적용한 환경으로 Mujoco를 선택했습니다. \n\nMujoco에는 Ant, HalfCheetah, Hopper, Humanoid, HumanoidStandup, InvertedPendulum, Reacher, Swimmer, Walker2d 과 같은 환경이 있습니다. 그 중에서 Hopper에 맞춰서 학습이 되도록 코드를 구현하였습니다. Mujoco 설치와 관련된 내용은 Wiki에 있습니다.\n\n</br>\n### 1.1 Hopper\nHopper는 외다리로 뛰어가는 것을 학습하는 것이 목표입니다. Hopper는 다음과 같이 생겼습니다. \n<img src=\"https://www.dropbox.com/s/wjxrelxyp014j3g/Screenshot%202018-08-23%2000.55.54.png?dl=1\">\n\n환경을 이해하려면 환경의 상태와 행동, 보상 그리고 학습하고 싶은 목표를 알아야합니다. \n\n- 상태 : 관절의 위치, 각도, 각속도\n- 행동 : 관절의 가해지는 토크\n- 보상 : 앞으로 나아가는 속도\n- 목표 : 최대한 앞으로 많이 나아가기\n\n즉 에이전트는 time step마다 관절의 위치와 각도를 받아와서 그 상태에서 어떻게 움직여야 앞으로 나아갈 수 있는지를 학습해야 합니다. 행동은 discrete action이 아닌 continuous action으로 -1과 1사이의 값을 가집니다. 만약 행동이 -1이라면 해당 관절에 시계반대방향으로 토크를 주는 것이고 행동이 +1이라면 해당 관절에 시계방향으로 토크를 주는 것입니다. \n\ncontinuous action을 주는 방법은 네트워크(Actor)의 output layer에서 activation function으로 tanh와 같은 것을 사용해서 continuous한 값을 출력하는 것이 있습니다. 하지만 피지여행 코드 구현에서는 action을 gaussian distribution에서 sampling 하였습니다. 이렇게 하면 분산을 일정하게 유지하면서 지속적인 exploration을 할 수 있습니다. 간단하게 그림으로 보자면 다음과 같습니다. \n<img src=\"https://www.dropbox.com/s/94g01zdxyf5oxu1/Screenshot%202018-08-23%2001.20.21.png?dl=1\">\n\n네트워크 구조와 행동을 선택하는 부분은 다음과 같습니다. Hidden Layer의 activation function으로 tanh를 사용했으며(ReLU를 테스트해보지는 않았습니다. 기존 TRPO, PPO 구현들과 논문에서 tanh를 사용하기 때문에 저희도 사용했습니다. 뒤에 유니티 환경에서는 Swish라는 것을 사용합니다.) log std를 0으로 고정함으로서 일정한 폭을 가지는 분포를 만들어낼 수 있습니다. 이 분포로부터 action을 sampling 합니다.\n\n<img src=\"https://www.dropbox.com/s/xfl9zxies0lmpm1/Screenshot%202018-08-23%2001.20.44.png?dl=1\">\n\n</br>\n### 1.2 Vanilla PG\nVanilla PG는 Actor-Critic의 가장 간단한 형태입니다. Vanilla PG는 이후의 구현에 대한 baseline이 됩니다. 구현이 가장 간단하면서 학습이 안되는 것은 아닙니다. 따라서 코드 전체 구조를 잡는데 Vanilla PG를 짜는 것이 도움이 됩니다. 전반적인 코드 구조는 다음과 같습니다.\n\n- iteration 마다 일정한 step 수만큼 환경에서 진행해서 샘플을 모은다\n- 모은 샘플로 Actor와 Critic을 학습한다\n- 반복한다\n \n\n```python\nepisodes = 0\nfor iter in range(15000):\n    actor.eval(), critic.eval()\n    memory = deque()\n    \n    while steps < 2048:\n        episodes += 1\n        state = env.reset()\n        state = running_state(state)\n        score = 0\n        for _ in range(10000):\n            mu, std, _ = actor(torch.Tensor(state).unsqueeze(0))\n            action = get_action(mu, std)[0]\n            next_state, reward, done, _ = env.step(action)\n            next_state = running_state(next_state)\n\n            if done:\n                mask = 0\n            else:\n                mask = 1\n\n            memory.append([state, action, reward, mask])\n            state = next_state\n\n            if done:\n                break\n                \n    actor.train(), critic.train()\n    train_model(actor, critic, memory, actor_optim, critic_optim)\n```\n\nmemory에 sample을 저장할 때 sample은 state와 action, reward, mask(마지막 state일 경우 0 나머지 1)입니다. mask의 경우 뒤에서 return이나 advantage를 계산할 때 사용됩니다. 또 하나 염두에 두어야할 것은 running_state 입니다. running_state는 input으로 들어오는 state의 scale이 일정하지 않기 때문에 사용합니다. 즉 state의 각 dimension을 평균 0 분산 1로 standardization 하는 것입니다. 따라서 모델을 저장할 때 각 dimension 마다의 평균과 분산도 같이 저장해서 테스트할 때 불러와서 사용해야 합니다.\n\nVanilla PG의 경우 학습 부분이 상당히 간단합니다. 다음 코드를 보시면 메모리에서 state, action, reward, mask를 꺼냅니다. reward와 mask를 통해 return을 구할 수 있고 이 return을 통해 actor를 업데이트 할 수 있습니다 (REINFORCE 알고리즘을 떠올리시면 됩니다). 여기서 critic이 하는 일은 없지만 뒤의 알고리즘들과 코드의 통일성을 위해 fake로 넣어놨습니다. Return은 평균을 빼고 분산으로 나눠서 standardize 합니다. \n\n```python\ndef train_model(actor, critic, memory, actor_optim, critic_optim):\n    memory = np.array(memory)\n    states = np.vstack(memory[:, 0])\n    actions = list(memory[:, 1])\n    rewards = list(memory[:, 2])\n    masks = list(memory[:, 3])\n\n    returns = get_returns(rewards, masks)\n    train_critic(critic, states, returns, critic_optim)\n    train_actor(actor, returns, states, actions, actor_optim)\n    return returns\n```\n\n이 코드로 Hopper 환경에서 학습한 그래프는 다음과 같습니다. \n<img src=\"https://www.dropbox.com/s/asoysfuk76zs1dk/Screenshot%202018-08-23%2001.30.58.png?dl=1\">\n\n</br>\n### 1.3 TNPG\nNPG를 이용한 parameter update 식은 다음과 같습니다. \n\n$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$\n\nNPG를 구현하려면 KL-divergence의 Hessian의 inverse를 구해야하는 문제가 생깁니다. 현재와 같이 Deep Neural Network를 쓰는 경우에 Hessian의 inverse를 직접적으로 구하는 것은 computationally inefficient 합니다. 따라서 직접 구하지 않고 Conjugate gradient 방법을 사용해서 Fisher Vector Product ($$F^{-1}g$$)를 구합니다. 이러한 알고리즘을 Truncated Natural Policy Gradient(TNPG)라고 부릅니다. \n\nTNPG에서 parameter update를 구하는 과정은 다음과 같습니다. \n1. Return 구하기\n2. Critic 학습하기\n3. logp * return --> loss 구하기\n4. loss의 미분과 kl-divergence의 2차 미분을 통해 step direction 구하기\n5. 구한 step direction으로 parameter update\n\n```python\ndef train_model(actor, critic, memory, actor_optim, critic_optim):\n    memory = np.array(memory)\n    states = np.vstack(memory[:, 0])\n    actions = list(memory[:, 1])\n    rewards = list(memory[:, 2])\n    masks = list(memory[:, 3])\n\n    # ----------------------------\n    # step 1: get returns\n    returns = get_returns(rewards, masks)\n\n    # ----------------------------\n    # step 2: train critic several steps with respect to returns\n    train_critic(critic, states, returns, critic_optim)\n\n    # ----------------------------\n    # step 3: get gradient of loss and hessian of kl\n    loss = get_loss(actor, returns, states, actions)\n    loss_grad = torch.autograd.grad(loss, actor.parameters())\n    loss_grad = flat_grad(loss_grad)\n    step_dir = conjugate_gradient(actor, states, loss_grad.data, nsteps=10)\n\n    # ----------------------------\n    # step 4: get step direction and step size and update actor\n    params = flat_params(actor)\n    new_params = params + 0.5 * step_dir\n    update_model(actor, new_params)\n    \n```\n\nconjugate gradient 코드는 OpenAI baseline에서 가져왔습니다. 이 코드는 원래 John schulmann 개인 repository에 있는 그대로 사용하는 것입니다. nsteps 만큼 iterataion을 반복하며 결국 x를 구하는 것인데 이 x가 step direction 입니다. \n\n```python\n# from openai baseline code\n# https://github.com/openai/baselines/blob/master/baselines/common/cg.py\ndef conjugate_gradient(actor, states, b, nsteps, residual_tol=1e-10):\n    x = torch.zeros(b.size())\n    r = b.clone()\n    p = b.clone()\n    rdotr = torch.dot(r, r)\n    for i in range(nsteps):\n        _Avp = fisher_vector_product(actor, states, p)\n        alpha = rdotr / torch.dot(p, _Avp)\n        x += alpha * p\n        r -= alpha * _Avp\n        new_rdotr = torch.dot(r, r)\n        betta = new_rdotr / rdotr\n        p = r + betta * p\n        rdotr = new_rdotr\n        if rdotr < residual_tol:\n            break\n    return x\n```\n\nfisher_vector_product는 kl-divergence의 2차미분과 어떠한 vector의 곱인데 p는 처음에 gradient 값이었다가 점차 업데이트가 됩니다. kl-divergence의 2차 미분을 구하는 과정은 다음과 같습니다. 일단 kl-divergence를 현재 policy에 대해서 구한 다음에 actor parameter에 대해서 미분합니다. 이렇게 미분한 gradient를 일단 flat하게 핀 다음에 p라는 벡터와 곱해서 하나의 값으로 만듭니다. 그 값을 다시 actor의 parameter로 만듦으로서 따로 KL-divergence의 2차미분을 구하지않고 Fisher vector product를 구할 수 있습니다.\n\n```python\ndef fisher_vector_product(actor, states, p):\n    p.detach()\n    kl = kl_divergence(new_actor=actor, old_actor=actor, states=states)\n    kl = kl.mean()\n    kl_grad = torch.autograd.grad(kl, actor.parameters(), create_graph=True)\n    kl_grad = flat_grad(kl_grad)  # check kl_grad == 0\n\n    kl_grad_p = (kl_grad * p).sum()\n    kl_hessian_p = torch.autograd.grad(kl_grad_p, actor.parameters())\n    kl_hessian_p = flat_hessian(kl_hessian_p)\n\n    return kl_hessian_p + 0.1 * p\n```\n\nTNPG 학습 결과는 다음과 같습니다. \n<img src=\"https://www.dropbox.com/s/uc4c0s00qbs33nr/Screenshot%202018-08-23%2001.53.17.png?dl=1\">\n\n</br>\n### 1.4 TRPO\nTRPO와 NPG가 다른 점은 surrogate loss 사용과 trust region 입니다. 하지만 실제로 구현해서 학습을 시켜본 결과 trust region을 넘어가서 back tracking line search를 하는 경우는 거의 없습니다. 따라서 주된 변화는 surrogate loss에 있다고 보셔도 됩니다. Surrogate loss에서 advantage function을 사용하는데 본 코드 구현에서는 GAE를 사용하였습니다. TRPO 업데이트 식은 다음과 같습니다. Q function 위치에 GAE가 들어갑니다.\n\n$$\n\\begin{align}\n\\max\\_\\theta\\quad &E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi\\_\\theta(a\\vert s)}{q(a\\vert s)}Q\\_{\\theta\\_\\mathrm{old} }(s,a)\\right] \\\\\\\\\n\\mathrm{s.t.\\ }&E\\_{s\\sim\\rho\\_{\\theta\\_\\mathrm{old} }}\\left[D\\_\\mathrm{KL}\\left(\\pi\\_{\\theta\\_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi\\_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta\n\\end{align}\n$$\n\n\nGAE를 구하는 코드는 다음과 같습니다. GAE는 td-error의 discounted summation이라고 볼 수 있습니다. 마지막에 advants를 standardization 하는 것은 return에서 하는 것과 같은 효과를 봅니다. 하지만 standardization을 안하고 실험을 해보지는 않았습니다.\n\n```python\ndef get_gae(rewards, masks, values):\n    rewards = torch.Tensor(rewards)\n    masks = torch.Tensor(masks)\n    returns = torch.zeros_like(rewards)\n    advants = torch.zeros_like(rewards)\n\n    running_returns = 0\n    previous_value = 0\n    running_advants = 0\n\n    for t in reversed(range(0, len(rewards))):\n        running_returns = rewards[t] + hp.gamma * running_returns * masks[t]\n        running_tderror = rewards[t] + hp.gamma * previous_value * masks[t] - \\\n                    values.data[t]\n        running_advants = running_tderror + hp.gamma * hp.lamda * \\\n                          running_advants * masks[t]\n\n        returns[t] = running_returns\n        previous_value = values.data[t]\n        advants[t] = running_advants\n\n    advants = (advants - advants.mean()) / advants.std()\n    return returns, advants\n```\n\nSurrogate loss를 구하는 코드는 다음과 같습니다. Advantage function(GAE)를 구하고 나면 이전 policy와 현재 policy 사이의 ratio를 구해서 advantage function에 곱하면 됩니다. 이 때 사실 old policy와 new policy는 값은 같지만 old policy는 clone()이나 detach()를 사용해서 update가 안되게 만들어줍니다.\n\n```python\ndef surrogate_loss(actor, advants, states, old_policy, actions):\n    mu, std, logstd = actor(torch.Tensor(states))\n    new_policy = log_density(torch.Tensor(actions), mu, std, logstd)\n    advants = advants.unsqueeze(1)\n\n    surrogate = advants * torch.exp(new_policy - old_policy)\n    surrogate = surrogate.mean()\n    return surrogate\n\n```\n\nActor의 step direction을 구하는 것은 TNPG와 동일합니다. TNPG에서는 step direction으로 바로 업데이트 했지만 TRPO는 다음과 같은 작업을 해줍니다. Full step을 구하는 과정이라고 볼 수 있습니다. \n\n```python\n# ----------------------------\n# step 4: get step direction and step size and full step\nparams = flat_params(actor)\nshs = 0.5 * (step_dir * fisher_vector_product(actor, states, step_dir)\n             ).sum(0, keepdim=True)\nstep_size = 1 / torch.sqrt(shs / hp.max_kl)[0]\nfull_step = step_size * step_dir\n\n```\n\n이렇게 full step을 구하고 나면 Trust region optimization 단계에 들어갑니다. expected improvement는 구한 step 만큼 parameter space에서 움직였을 때 예상되는 performance 변화입니다. 이 값은 kl-divergence와 함께 trust region 안에 있는지 밖에 있는지 판단하는 근거가 됩니다. expected improve는 출발점에서의 gradient * full step으로 구합니다. 그리고 10번을 돌아가며 Back-tracking line search를 실시합니다. 처음에는 full step 만큼 가본 다음에 kl-divergence와 emprovement를 통해 trust region 안이면 루프 탈출, 밖이면 full step을 반만큼 쪼개서 다시 이 과정을 반복합니다.  \n\n```python\n# ----------------------------\n# step 5: do backtracking line search for n times\nold_actor = Actor(actor.num_inputs, actor.num_outputs)\nupdate_model(old_actor, params)\nexpected_improve = (loss_grad * full_step).sum(0, keepdim=True)\nexpected_improve = expected_improve.data.numpy()\n\nflag = False\nfraction = 1.0\nfor i in range(10):\n    new_params = params + fraction * full_step\n    update_model(actor, new_params)\n    new_loss = surrogate_loss(actor, advants, states, old_policy.detach(),\n                              actions)\n    new_loss = new_loss.data.numpy()\n    loss_improve = new_loss - loss\n    expected_improve *= fraction\n    kl = kl_divergence(new_actor=actor, old_actor=old_actor, states=states)\n    kl = kl.mean()\n\n    print('kl: {:.4f}  loss improve: {:.4f}  expected improve: {:.4f}  '\n          'number of line search: {}'\n          .format(kl.data.numpy(), loss_improve, expected_improve[0], i))\n\n    # see https: // en.wikipedia.org / wiki / Backtracking_line_search\n    if kl < hp.max_kl and (loss_improve / expected_improve) > 0.5:\n        flag = True\n        break\n\n    fraction *= 0.5\n```\n\nCritic의 학습은 단순히 value function과 return의 MSE error를 계산해서 loss로 잡고 loss를 최소화하도록 학습합니다. TRPO 학습 결과는 다음과 같습니다.\n<img src=\"https://www.dropbox.com/s/rc9hxsx1kvokcrv/Screenshot%202018-08-23%2013.36.51.png?dl=1\">\n\n</br>\n### 1.5 PPO\nPPO의 장점을 꼽으라면 GPU 사용하기 좋고 sample efficiency가 늘어난다는 것입니다. TNPG와 TRPO의 경우 한 번 모은 sample은 모델을 단 한 번 업데이트하는데 사용하지만 PPO의 경우 몇 mini-batch로 epoch를 돌리기 때문입니다. GAE를 사용한다는 것은 같고 Conjugate gradient나 Fisher vector product나 back tracking line search가 다 빠집니다. 대신 loss function clip으로 monotonic improvement를 보장하게 학습합니다. 따라서 코드가 상당히 간단해집니다. \n\n다음 코드 부분이 PPO의 전체라고 봐도 무방합니다. PPO는 다음과 같은 순서로 학습합니다. \n\n- batch를 random suffling하고 mini batch를 추출\n- value function 구하기\n- critic loss 구하기 (clip을 사용해도 되고 TRPO와 같이 그냥 학습시켜도 됌)\n- surrogate loss 구하기\n- surrogate loss clip해서 actor loss 만들기\n- actor와 critic 업데이트\n\nActor의 loss를 구하는 것은 다음 식의 값을 구하는 것입니다. 이 식을 구하려면 ratio에 한 번 클립하고 loss 값을 한 번 min을 취하면 됩니다.\n\n$$L^{CLIP}(\\theta) = \\hat{E}_t [min(r_t(\\theta) \\, \\hat{A}_t,  clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\, \\hat{A}_t)]$$\n\n이 코드 구현에서는 actor와 critic을 따로 모델로 만들어서 따로 따로 업데이트를 하지만 하나로 만든다면 loss로 한 번만 업데이트하면 됩니다. 또한 entropy loss를 최종 loss에 더해서 regularization 효과를 볼 수도 있습니다. Critic loss에 clip 해주는 것은 OpenAI baseline의 ppo2 코드를 참조하였습니다.\n\n```python\n# step 2: get value loss and actor loss and update actor & critic\nfor epoch in range(10):\n    np.random.shuffle(arr)\n\n    for i in range(n // hp.batch_size):\n        batch_index = arr[hp.batch_size * i: hp.batch_size * (i + 1)]\n        batch_index = torch.LongTensor(batch_index)\n        inputs = torch.Tensor(states)[batch_index]\n        returns_samples = returns.unsqueeze(1)[batch_index]\n        advants_samples = advants.unsqueeze(1)[batch_index]\n        actions_samples = torch.Tensor(actions)[batch_index]\n        oldvalue_samples = old_values[batch_index].detach()\n\n        loss, ratio = surrogate_loss(actor, advants_samples, inputs,\n                                     old_policy.detach(), actions_samples,\n                                     batch_index)\n\n        values = critic(inputs)\n        clipped_values = oldvalue_samples + \\\n                         torch.clamp(values - oldvalue_samples,\n                                     -hp.clip_param,\n                                     hp.clip_param)\n        critic_loss1 = criterion(clipped_values, returns_samples)\n        critic_loss2 = criterion(values, returns_samples)\n        critic_loss = torch.max(critic_loss1, critic_loss2).mean()\n\n        clipped_ratio = torch.clamp(ratio,\n                                    1.0 - hp.clip_param,\n                                    1.0 + hp.clip_param)\n        clipped_loss = clipped_ratio * advants_samples\n        actor_loss = -torch.min(loss, clipped_loss).mean()\n\n        loss = actor_loss + 0.5 * critic_loss\n\n        critic_optim.zero_grad()\n        loss.backward(retain_graph=True)\n        critic_optim.step()\n\n        actor_optim.zero_grad()\n        loss.backward()\n        actor_optim.step()\n```\n\nPPO의 학습 결과는 다음과 같습니다. \n<img src=\"https://www.dropbox.com/s/rkxa836ap931kbd/Screenshot%202018-08-23%2013.50.57.png?dl=1\">\n\n\n</br>\n## 2. Unity ml-agent 학습\nMujoco Hopper(half-cheetah와 같은 것도)에 Vanilla PG, TNPG, TRPO, PPO를 구현해서 적용했습니다. Mujoco의 경우 이미 Hyper parameter와 같은 정보들이 논문이나 블로그에 있기 때문에 상대적으로 continuous control로 시작하기에는 좋습니다. 맨 처음에 말했듯이 Mujoco는 1달만 무료이고 그 이후부터 유료이며 확장성이 떨어집니다. 좀 더 general한 agent를 학습시키기에 좋은 환경이 필요합니다. 따라서 Unity ml-agent를 살펴봤습니다. Repository는 다음과 같습니다. \n- [Unity ml-agent repository](https://github.com/Unity-Technologies/ml-agents)\n- [Unity ml-agent homepage](https://unity3d.com/machine-learning/)\n\n<img src=\"https://www.dropbox.com/s/lapholj8r4nxmb1/Screenshot%202018-08-24%2013.41.31.png?dl=1\">\n\n현재 Unity ml-agent에서 기본으로 제공하는 환경은 다음과 같습니다. Unity ml-agent는 기존 Unity를 그대로 사용하면서 쉽게 강화학습 에이전트를 붙일 수 있도록 설계되어 있습니다. Unity ml-agent에서는 Walker 환경에서 저희가 살펴본 알고리즘 중에 가장 최신 알고리즘은 PPO를 적용해봤습니다. 이 포스트를 보시는 분들은 이 많은 다른 환경에 자유롭게 저희 코드를 적용할 수 있습니다.\n- [각 환경에 대한 설명](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md)\n\n<img src=\"https://www.dropbox.com/s/lrbodw5dypxowmw/Screenshot%202018-08-24%2014.06.14.png?dl=1\">\n\nUnity ml-agent를 이용해서 강화학습을 하기 위해서는 다음과 같이 진행됩니다. 단계별로 설명하겠습니다. \n\n- Unity에서 환경 만들기\n- Python에서 unity 환경 불러와서 테스트하기\n- 기존에 하던대로 학습하기\n\n\n</br>\n### 2.1 Walker 환경 만들기\n강화학습을 하는 많은 분들이 Unity를 한 번도 다뤄보지 않은 경우가 많습니다. 저도 그런 경우라서 어떻게 환경을 만들어야할지 처음에는 감이 잡히지 않았습니다. 하지만 Unity ml-agent에서는 상당히 자세한 guide를 제공합니다. 다음은 Unity ml-agent의 가장 기본적인 환경인 3DBall에 대한 tutorial입니다. 설치 guide도 제공하고 있으니 참고하시면 될 것 같습니다.\n- [3DBall 예제 tutorial](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Basic-Guide.md)\n- [Unity ml-agent 설치 guide](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md)\n\nUnity ml-agent에서 제공하는 3DBall tutorial을 참고해서 Walker 환경을 만들었습니다. Walker 환경을 만드는 과정을 간단히 말씀드리겠습니다. 다음 그림의 단계들을 동일하므로 따라하시면 됩니다. Unity를 열고 unity-environment로 들어가시면 됩니다.\n<img src=\"https://www.dropbox.com/s/fbdqg781w46a5mz/Screenshot%202018-08-24%2014.50.50.png?dl=1\">\n\n그러면 화면 하단에서 다음과 같은 것을 볼 수 있습니다. Assets/ML-Agents/Examples로 들어가보면 Walker가 있습니다. Scenes에서 Walker를 더블클릭하면 됩니다.\n<img src=\"https://www.dropbox.com/s/h349xml3faln0wy/Screenshot%202018-08-24%2014.52.14.png?dl=1\">\n\n더블클릭해서 나온 화면에서 오른쪽 상단의 파란색 화살표를 누르면 환경이 실행이 됩니다. 저희가 학습하고자 하는 agent는 바로 이녀석입니다. 왼쪽 리스트를 보면 WalkerPair가 11개가 있는 것을 볼 수 있습니다. Unity ml-agent 환경은 기본적으로 Multi-agent로 학습하도록 설정되어있습니다. 따라서 여러개의 Walker들이 화면에 보이는 것입니다. \n<img src=\"https://www.dropbox.com/s/cy8m5kqdmkhopjo/Screenshot%202018-08-24%2014.54.57.png?dl=1\">\n\n리스트 중에 Walker Academy를 클릭해서 그 하위에 있는 WalkerBrain을 더블클릭합니다. 그러면 화면 오른쪽에 다음과 같은 Brain 설정을 볼 수 있습니다. Brain은 쉽게 말해서 Agent라고 생각하면 됩니다. 이 Agent는 상태로 212차원의 vector가 주어지며 다 continuous한 값을 가집니다. 행동은 39개의 행동을 할 수 있으며 다 Continuous입니다. Mujoco에 비해서 상태나 행동의 차원이 상당히 높습니다. 여기서 중요한 것은 Brain Type입니다. Brain type은 internal, external, player, heuristic이 있습니다. player로 type을 설정하고 화면 상단의 play 버튼을 누르면 여러분이 agent를 움직일 수 있습니다. 하지만 Walker는 사람이 움직이는게 거의 불가능하므로 player 기능은 사용할 수 없습니다. 다른 환경에서는 사용해볼 수 있으니 재미로 한 번 플레이해보시면 좋습니다! \n<center><img src=\"https://www.dropbox.com/s/uxfm162f1scbzo5/Screenshot%202018-08-24%2015.09.04.png?dl=1\" width=\"400px\"></center>\n\n이번에는 WalkerPair에서 WalkerAgent를 더블클릭해보겠습니다. 이 설정을 보아 5000 step이 episode의 max step인 것을 볼 수 있습니다.\n<center><img src=\"https://www.dropbox.com/s/r6gwemlczwic2ma/Screenshot%202018-08-24%2015.16.19.png?dl=1\" width=\"400px\"></center>\n\n이제 상단 file menu에서 build setting에 들어갑니다. 환경을 build해서 python 코드에서 import하기 위해서입니다. 물론 unity 환경과 python 코드를 binding해주는 부분은 ml-agent 코드 안에 있습니다. Build 버튼을 누르면 환경이 build가 됩니다.\n<center><img src=\"https://www.dropbox.com/s/4dtgoz1k8896vxs/Screenshot%202018-08-24%2015.19.07.png?dl=1\" width=\"500px\"></center>\n\n\n</br>\n### 2.2 Python에서 unity 환경 불러와서 테스트하기\n환경을 build 했으면 build한 환경을 python에서 불러와서 random action으로 테스트해봅니다. 환경을 테스트하는 코드는 pg_travel repository에서 unity 폴더 밑에 있습니다. test_env.py라는 코드는 간단하게 다음과 같습니다. Build한 walker 환경은 env라는 폴더 밑에 넣어줍니다. unityagent를 import하는데 ml-agent를 git clone 해서 python 폴더 내에서 \"python setup.py install\"을 실행했다면 문제없이 import 됩니다. UnityEnvironment를 통해 env라는 환경을 선언할 수 있습니다. 이렇게 선언하고 나면 gym과 상당히 유사한 형태로 환경과 상호작용이 가능합니다. \n\n```python\nimport numpy as np\nfrom unityagents import UnityEnvironment\nfrom utils.utils import get_action\n\nif __name__==\"__main__\":\n    env_name = \"./env/walker_test\"\n    train_mode = False\n\n    env = UnityEnvironment(file_name=env_name)\n\n    default_brain = env.brain_names[0]\n    brain = env.brains[default_brain]\n    env_info = env.reset(train_mode=train_mode)[default_brain]\n\n    num_inputs = brain.vector_observation_space_size\n    num_actions = brain.vector_action_space_size\n    num_agent = env._n_agents[default_brain]\n\n    print('the size of input dimension is ', num_inputs)\n    print('the size of action dimension is ', num_actions)\n    print('the number of agents is ', num_agent)\n   \n    score = 0\n    episode = 0\n    actions = [0 for i in range(num_actions)] * num_agent\n    for iter in range(1000):\n        env_info = env.step(actions)[default_brain]\n        rewards = env_info.rewards\n        dones = env_info.local_done\n        score += rewards[0]\n\n        if dones[0]:\n            episode += 1\n            score = 0\n            print('{}th episode : mean score of 1st agent is {:.2f}'.format(\n                episode, score))\n```\n\n위 코드를 실행하면 다음과 같이 실행창에 뜹니다. External brain인 것을 알 수 있고 default_brain은 brain 중에 하나만 가져왔기 때문에 number of brain은 1이라고 출력합니다. input dimension은 212이고 action dimension은 39이고 agent 수는 11인 것으로봐서 제대로 환경이 불러와진 것을 확인할 수 있습니다. \n<img src=\"https://www.dropbox.com/s/cioa9h7qu25vonz/Screenshot%202018-08-24%2015.47.43.png?dl=1\">\n\n이 환경에서 행동하려면 agent 숫자만큼 행동을 줘야합니다. 모두 0로 행동을 주고 실행하면 다음과 같이 뒤로 넘어지는 행동을 반복합니다. env.step(actions)[default_brain]으로 env_info를 받아오면 거기서부터 reward와 done, next_state를 받아올 수 있습니다. 이제 학습하기만 하면 됩니다. \n<img src=\"https://www.dropbox.com/s/8qrmxoski6p4n07/Screenshot%202018-08-24%2016.00.21.png?dl=1\">\n\n</br>\n### 2.3 Walker 학습하기\n기존에 Mujoco에 적용했던 PPO 코드를 그대로 Walker에 적용하니 잘 학습이 안되었습니다. 다음 사진이 저희가 중간 해커톤으로 모여서 이 상황을 공유할 때의 사진입니다.\n<img src=\"https://i.imgur.com/1aR2Z77.png\" width=500px>\n\nUnity ml-agent에서는 PPO를 기본 agent로 제공합니다. 학습 코드도 제공하기 때문에 mujoco에 적용했던 코드와의 차이점을 분석할 수 있었습니다. mujoco 코드와 ml-agent baseline 코드의 차이점은 다음과 같습니다. \n\n- agent 여러개를 이용, 별개의 memory에 저장한 후에 gradient를 합침\n- GAE 및 time horizon 등 hyper parameter가 다름\n- Actor와 Critic의 layer가 1층 더 두꺼우며 hidden layer 자체의 사이즈도 더 큼\n- hidden layer의 activation function이 tanh가 아닌 swish\n\nml-agent baseline 코드리뷰할 때 작성했던 마인드맵은 다음과 같습니다.\n<img src=\"https://i.imgur.com/YeaEntG.png\">\n\n크게는 두 가지를 개선해서 성능이 많이 향상했습니다.\n1. Network 수정\n2. multi-agent를 활용해서 학습\n\nNetwork 코드는 다음과 같습니다. Hidden Layer를 하나 더 늘렸으며 swish activation function을 사용할 수 있도록 변경했습니다. 사실 swish라는 activation function은 처음 들어보는 생소한 함수였습니다. 하지만 ml-agent baseline에서 사용했다는 사실과 구현이 상당히 간단하다는 점에서 저희 코드에 적용했습니다. 단순히 x * sigmoid(x) 를 하면 됩니다. swish는 별거 아닌 것 같지만 상당한 성능 개선을 가져다줬습니다. 사실 ReLU나 ELU 등 여러 다른 activation function을 적용해서 비교해보는게 best긴 하지만 시간 관계상 그렇게까지 테스트해보지는 못했습니다. 기존에 TRPO나 PPO는 왜 tanh를 사용했었는지도 의문인 점입니다.\n\n```python\nclass Actor(nn.Module):\n    def __init__(self, num_inputs, num_outputs, args):\n        self.args = args\n        self.num_inputs = num_inputs\n        self.num_outputs = num_outputs\n        super(Actor, self).__init__()\n        self.fc1 = nn.Linear(num_inputs, args.hidden_size)\n        self.fc2 = nn.Linear(args.hidden_size, args.hidden_size)\n        self.fc3 = nn.Linear(args.hidden_size, args.hidden_size)\n        self.fc4 = nn.Linear(args.hidden_size, num_outputs)\n\n        self.fc4.weight.data.mul_(0.1)\n        self.fc4.bias.data.mul_(0.0)\n\n    def forward(self, x):\n        if self.args.activation == 'tanh':\n            x = F.tanh(self.fc1(x))\n            x = F.tanh(self.fc2(x))\n            x = F.tanh(self.fc3(x))\n            mu = self.fc4(x)\n        elif self.args.activation == 'swish':\n            x = self.fc1(x)\n            x = x * F.sigmoid(x)\n            x = self.fc2(x)\n            x = x * F.sigmoid(x)\n            x = self.fc3(x)\n            x = x * F.sigmoid(x)\n            mu = self.fc4(x)\n        else:\n            raise ValueError\n\n        logstd = torch.zeros_like(mu)\n        std = torch.exp(logstd)\n        return mu, std, logstd\n```\n\nswish와 tanh를 사용한 학습을 비교한 그래프입니다. 하늘색 그래프가 swish를 사용한 결과, 파란색이 tanh를 사용한 결과입니다. score는 episode 마다의 reward의 합입니다.\n<center><img src=\"https://www.dropbox.com/s/3d07c1kql4h5oqk/Screenshot%202018-08-24%2016.33.45.png?dl=1\" width=\"350px\"></center>\n\n이제 multi-agent로 학습하도록 변경하면 됩니다. PPO의 경우 memory에 time horizon 동안의 sample을 시간순서대로 저장하고 GAE를 구한 이후에 minibatch로 추출해서 학습합니다. 따라서 여러개의 agent로 학습하기 위해서는 memory를 따로 만들어서 각각의 GAE를 구해서 학습해야합니다. Unity에서는 Mujoco에서 했던 것처럼 deque로 memory를 만들지 않고 따로 named tuple로 구현한 memory class를 import 해서 사용했습니다. utils 폴더 밑에 memory.py 코드에 구현되어있으며 코드는 https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb\n에서 가져왔습니다. \n\nstate, action, reward, mask를 저장하는데 불러올 때 각각을 따로 불러올 수 있기 때문에 비효율적 시간을 많이 줄여줍니다. \n```python\nTransition = namedtuple('Transition', ('state', 'action', 'reward', 'mask'))\n\n\nclass Memory(object):\n    def __init__(self):\n        self.memory = []\n\n    def push(self, state, action, reward, mask):\n        \"\"\"Saves a transition.\"\"\"\n        self.memory.append(Transition(state, action, reward, mask))\n\n    def sample(self):\n        return Transition(*zip(*self.memory))\n\n    def __len__(self):\n        return len(self.memory)\n```\n\nmain.py 에서는 이 memory를 agent의 개수만큼 생성합니다. \n\n```python\nmemory = [Memory() for _ in range(num_agent)]\n```\nsample을 저장할 때도 agent마다 따로 따로 저장합니다. \n\n```python\nfor i in range(num_agent):\n    memory[i].push(states[i], actions[i], rewards[i], masks[i])\n\n```\n\ntime horizon이 끝나면 모은 sample 들을 가지고 학습하기 위한 값으로 만드는 과정을 진행합니다. 각각의 memory를 가지고 GAE와 old_policy, old_value 등을 계산해서 하나의 batch로 합칩니다. 그렇게 train_model 메소드에 전달하면 기존과 동일하게 agent를 업데이트합니다.\n\n```python\nsts, ats, returns, advants, old_policy, old_value = [], [], [], [], [], []\n\nfor i in range(num_agent):\n    batch = memory[i].sample()\n    st, at, rt, adv, old_p, old_v = process_memory(actor, critic, batch, args)\n    sts.append(st)\n    ats.append(at)\n    returns.append(rt)\n    advants.append(adv)\n    old_policy.append(old_p)\n    old_value.append(old_v)\n\nsts = torch.cat(sts)\nats = torch.cat(ats)\nreturns = torch.cat(returns)\nadvants = torch.cat(advants)\nold_policy = torch.cat(old_policy)\nold_value = torch.cat(old_value)\n\ntrain_model(actor, critic, actor_optim, critic_optim, sts, ats, returns, advants,\n            old_policy, old_value, args)\n```\n\n이렇게 학습한 에이전트는 다음과 같이 걷습니다. 이렇게 walker를 학습시키고 나니 어떻게 하면 사람처럼 자연스럽게 걷는 것을 agent 스스로 학습할 수 있을까라는 고민을 하게 되었습니다.\n<center><img src=\"https://www.dropbox.com/s/fyz1kn5v92l3rrk/plane-595.gif?dl=1\"></center>\n\nUnity ml-agent에서 제공하는 pretrained된 모델을 다음과 같이 걷습니다. 저희가 학습한 agent와 상당히 다르게 걷는데 왜 그런 차이가 나는지도 분석하고 싶습니다. \n<center><img src=\"https://www.dropbox.com/s/xwz766g7c4eiaia/plane-unity.gif?dl=1\"></center>\n\n\n</br>\n## 3. Unity Curved Surface 제작 및 학습기\nUnity ml-agent에서 제공하는 기본 Walker 환경에서 학습하고 나니 바닥을 조금 울퉁불퉁하게 혹은 경사가 진 곳에서 걷는 것을 학습해보고 싶다라는 생각이 들었습니다. 따라서 간단하게 걷는 배경을 다르게 하는 시도를 해봤습니다. \n\n</br>\n### 3.1 Curved Surface 만들기\nAgent가 걸어갈 배경을 처음부터 만드는 것보다 구할 수 있다면 만들어진 배경을 구하기로 했습니다. Unity를 무료라는 점에서 선택했듯이 배경을 무료로 구할 수 있는 방법을 선택했습니다. \n<center><img src=\"https://www.dropbox.com/s/e0tsp3e3c9uq2zh/Screenshot%202018-08-23%2000.19.14.png?dl=1\"></center>\n\n무료로 공개되어있는 Unity 배경 중에서 Curved Ground 라는 것을 가져와서 작업하기로 했습니다. 이 환경 같은 경우 spline을 그리듯이 중간의 점을 이동시키면서 사용자가 곡면을 수정할 수 있습니다.\n<center><img src=\"https://www.dropbox.com/s/3ppmxotrf6qhzaf/Screenshot%202018-08-23%2000.20.25.png?dl=1\"></center>\n\n간단하게 곡면을 만들어서 공을 굴려보면 다음과 같이 잘 굴러갑니다. \n<center><img src=\"https://www.dropbox.com/s/2e8yqvqj1a4th27/slope_walker_ball.gif?dl=1\"></center>\n\n여러 에이전트가 학습할 수 있도록 오목한 경사면을 제작했습니다. 초반의 모습은 다음과 같았습니다. \n<img src=\"https://www.dropbox.com/s/m492xsfp4bolmz5/Screenshot%202018-08-23%2000.36.06.png?dl=1\">\n\n하지만 최종으로는 다음과 같은 곡면으로 사용했습니다. 위 사진의 배경과 아래 사진의 배경이 다른 점은 slope 길이, 내리막 경사, 오르막 경사입니다. Slope 길이의 경우 길이를 기존 plane 과 동일하게 했더니, 오르막 올라가는 부분이 학습이 잘 안 되었습니다. 따라서 길이를 줄였습니다. 내리막 경사의 경우 너무 경사지면 학습이 잘 안 되고, 너무 완만하니 내리막 티가 잘 안 나기 때문에 적절한 경사를 설정했습니다. 오르막 경사의 경우 내리막보다는 오르막이 더 어려울 것이라고 판단해서 오르막 경사를 낮게 설정했습니다. \n<img src=\"https://www.dropbox.com/s/idbov4wtd6jeqb2/Screenshot%202018-08-23%2000.36.54.png?dl=1\">\n\n</br>\n\n### 3.2 Curved Surface에서 학습하기\n위 환경으로 학습을 할 때, agent가 너무 초반에 빨리 쓰러지는 현상이 발생했습니다. 혹시 발의 각도가 문제일까 싶어서 발 각도를 변경해보았습니다. \n\n<center><img src=\"https://www.dropbox.com/s/znvikbeoj7gku0u/Screenshot%202018-08-23%2000.38.22.png?dl=1\" width=\"400px\"></center>\n\n하지만 역시 평지에서 걷는 것처럼 걷도록 학습이 안되었습니다. 이 환경에서 더 잘 학습하려면 더 여러가지를 시도해봐야할 것 같습니다. (그래도 걷는 게 기특합니다..)\n<center><img src=\"https://www.dropbox.com/s/4fqpsdmnzvnvia0/curved-736.gif?dl=1\"></center>\n\n<img src=\"https://www.dropbox.com/s/t5ngr0io4xeex6y/curved-736-overview.gif?dl=1\">\n\n</br>\n## 4. 구현 후기\n피지여행 구현팀은 총 4명으로 진행했습니다. 각 팀원의 후기를 적어보겠습니다.\n\n- 팀원 장수영: 사랑합니다. 행복합니다.\n- 팀원 공민서: 제가 핵심적인 기능을 구현하지는 못했지만 무조코 설치와 모델 테스트를 맡으면서 딥마인드나 openai의 영상으로만 보던 에이전트의 성장과정을 눈으로 지켜볼 수 있었습니다. 제대로 서있지도 못하던 hopper가 어느정도 훈련이 되고서는 넘어지려하다가도 추진력을 얻기위해 웅크렸다 뛰는 것을 관찰하는 것도 재미있고 육아일기를 보는 아버지의 마음을 조금이나마 이해할 수 있었습니다. 텐서보드를 넣는 걸 깜빡해 일일히 에피소드 별 스코어를 시각화 하면서 텐서보드의 소중함을 알았습니다. 유니티 코드리뷰를 하면서도 시스템 아키텍쳐 설계에 대해서도 배울 점이 있었던 것 같고 swish라는 활성화함수의 존재도 알았었고 curiosity도 알게되었고 역시 다른 사람의 코드를 읽는 것도 많은 공부가 된다고 되새기던 시간이었습니다. 물론 너무 크기가 방대해서 가독성은 많이 떨어졌습니다만 무조코보다 유니티가 훨씬 흥할거라고 생각했습니다. 마지막으로 누구 하나 열정적이지 않은 사람이 없이 치열한 고민을 함께 한 PG여행팀 분들, 저의 부족함과 상생의 기쁨을 알게해주셔서 정말 감사드립니다.\n- 팀원 양혁렬: 여러 에이전트가 함께하면 더 잘하는 걸 보면서 새삼 좋은 분들과 함께 할 수 있어서 행복했습니다\n- 팀원 이웅원: 저희가 직접 바닥부터 다 구현했던 것은 아니지만 구현을 해보면서 논문의 내용을 더 잘 이해할 수 있었습니다. 논문에 나와있지 않은 여러 노하우가 필요한 점들도 많았습니다. 역시 코드로 보고 성능 재현이 되어야 제대로 알고리즘을 이해하게 된다는 것을 다시 느낀 시간이었습니다. 또한 강화학습은 역시 환경세팅이 어렵다는 생각을 했습니다. 하지만 unity ml-agent를 사용해보면서 앞으로 강화학습 환경으로서 가능성이 상당히 크다는 생각을 했습니다. 또한 구현팀과 슬랙, 깃헙으로 협업하면서 온라인 협업에 대해서 더 배워가는 것 같습니다. 아직은 익숙하지 않지만 앞으로는 마치 바로 옆에서 같이 코딩하는 것 같이 될 거라고 생각합니다.","slug":"8_implement","published":1,"updated":"2019-02-07T11:21:31.978Z","layout":"post","photos":[],"link":"","_id":"cjrujlu23002m5wfe8cg7s2g6","content":"<h1 id=\"PG-Travel-implementation-story\"><a href=\"#PG-Travel-implementation-story\" class=\"headerlink\" title=\"PG Travel implementation story\"></a>PG Travel implementation story</h1><ul>\n<li>구현 코드 링크 : <a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">https://github.com/reinforcement-learning-kr/pg_travel</a></li>\n</ul>\n<p>피지여행 프로젝트에서는 다음 7개 논문을 살펴보았습니다. 각 논문에 대한 리뷰는 이전 글들에서 다루고 있습니다. </p>\n<p><a name=\"1\"></a></p>\n<ul>\n<li>[1] R. Sutton, et al., “Policy Gradient Methods for Reinforcement Learning with Function Approximation”, NIPS 2000.<br><a name=\"2\"></a></li>\n<li>[2] D. Silver, et al., “Deterministic Policy Gradient Algorithms”, ICML 2014.<br><a name=\"3\"></a></li>\n<li>[3] T. Lillicrap, et al., “Continuous Control with Deep Reinforcement Learning”, ICLR 2016.<br><a name=\"4\"></a></li>\n<li>[4] S. Kakade, “A Natural Policy Gradient”, NIPS 2002.<br><a name=\"5\"></a></li>\n<li>[5] J. Schulman, et al., “Trust Region Policy Optimization”, ICML 2015.<br><a name=\"6\"></a></li>\n<li>[6] J. Schulman, et al., “High-Dimensional Continuous Control using Generalized Advantage Estimation”, ICLR 2016.<br><a name=\"7\"></a></li>\n<li>[7] J. Schulman, et al., “Proximal Policy Optimization Algorithms”, arXiv, <a href=\"https://arxiv.org/pdf/1707.06347.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1707.06347.pdf</a>.</li>\n</ul>\n<p>강화학습 알고리즘을 이해하는데 있어서 논문을 보고 이론적인 부분을 알아가는 것이 좋습니다. 하지만 실제 코드로 돌아가는 것은 논문만 보고는 알 수 없는 경우가 많습니다. 따라서 피지여행 프로젝트에서는 위 7개 논문 중에 DPG와 DDPG를 제외한 알고리즘을 구현해보았습니다. 구현한 알고리즘은 다음 4개입니다. 이 때, TRPO와 PPO 구현에는 GAE(General Advantage Estimator)가 함께 들어갑니다. </p>\n<ul>\n<li>Vanilla Policy Gradient [<a href=\"#1\">1</a>]</li>\n<li>TNPG(Truncated Natural Policy Gradient) [<a href=\"#4\">4</a>]</li>\n<li>TRPO(Trust Region Policy Optimization) [<a href=\"#5\">5</a>]</li>\n<li>PPO(Proximal Policy Optimization) [<a href=\"#7\">7</a>].</li>\n</ul>\n<p>바닥부터 저희가 구현한 것은 아니며 다음 코드들을 참고해서 구현하였습니다. Vanilla PG의 경우 RLCode의 깃헙을 참고하였습니다.</p>\n<ul>\n<li><a href=\"https://github.com/openai/baselines/tree/master/baselines/trpo_mpi\" target=\"_blank\" rel=\"noopener\">OpenAI Baseline</a></li>\n<li><a href=\"https://github.com/ikostrikov/pytorch-trpo\" target=\"_blank\" rel=\"noopener\">Pytorch implemetation of TRPO</a></li>\n<li><a href=\"https://github.com/rlcode/reinforcement-learning-kr/tree/master/2-cartpole/2-actor-critic\" target=\"_blank\" rel=\"noopener\">RLCode Actor-Critic</a></li>\n</ul>\n<p>GAE와 TRPO, PPO 논문에서는 Mujoco라는 물리 시뮬레이션을 학습 환경으로 사용합니다. 따라서 저희도 Mujoco로 처음 시작을 하였습니다. 하지만 Mujoco는 1달만 무료이고 그 이후부터 유료이며 확장성이 떨어집니다. Unity ml-agent는 기존 Unity를 그대로 사용하면서 쉽게 강화학습 에이전트를 붙일 수 있도록 설계되어 있습니다. Unity ml-agent에서는 저희가 살펴본 알고리즘 중에 가장 최신 알고리즘은 PPO를 적용해봤습니다. 기본적으로 제공하는 환경 이외에 저희가 customize 한 환경에서도 학습해봤습니다.  </p>\n<ul>\n<li>mujoco-py: <a href=\"https://github.com/openai/mujoco-py\" target=\"_blank\" rel=\"noopener\">https://github.com/openai/mujoco-py</a></li>\n<li>Unity ml-agent: <a href=\"https://github.com/Unity-Technologies/ml-agents\" target=\"_blank\" rel=\"noopener\">https://github.com/Unity-Technologies/ml-agents</a></li>\n</ul>\n<p>코드를 구현하고 환경에서 학습을 시키면서 여러가지 이슈들이 있었고 해결해내가는 과정이 있었습니다. 그 과정을 간단히 정리해서 공유하면 PG를 공부하는 분들께 도움일 될 것 같습니다. 저희가 구현한 순서대로 1. Mujoco 학습 2. Unity ml-agent 학습 3. Unity Curved Surface 로 이 포스트가 진행됩니다.</p>\n<p><br></p>\n<h2 id=\"1-Mujoco-학습\"><a href=\"#1-Mujoco-학습\" class=\"headerlink\" title=\"1. Mujoco 학습\"></a>1. Mujoco 학습</h2><p>일명 “Continuous control” 문제는 action이 discrete하지 않고 continuous한 경우를 말합니다. Mujoco는 continuous control에 강화학습을 적용한 논문들이 애용하는 시뮬레이터입니다. 저희가 리뷰한 논문 중에서도 TRPO, PPO, GAE에서 Mujoco를 사용합니다. 따라서 저희가 처음 피지여행 알고리즘을 적용한 환경으로 Mujoco를 선택했습니다. </p>\n<p>Mujoco에는 Ant, HalfCheetah, Hopper, Humanoid, HumanoidStandup, InvertedPendulum, Reacher, Swimmer, Walker2d 과 같은 환경이 있습니다. 그 중에서 Hopper에 맞춰서 학습이 되도록 코드를 구현하였습니다. Mujoco 설치와 관련된 내용은 Wiki에 있습니다.</p>\n<p><br></p>\n<h3 id=\"1-1-Hopper\"><a href=\"#1-1-Hopper\" class=\"headerlink\" title=\"1.1 Hopper\"></a>1.1 Hopper</h3><p>Hopper는 외다리로 뛰어가는 것을 학습하는 것이 목표입니다. Hopper는 다음과 같이 생겼습니다.<br><img src=\"https://www.dropbox.com/s/wjxrelxyp014j3g/Screenshot%202018-08-23%2000.55.54.png?dl=1\"></p>\n<p>환경을 이해하려면 환경의 상태와 행동, 보상 그리고 학습하고 싶은 목표를 알아야합니다. </p>\n<ul>\n<li>상태 : 관절의 위치, 각도, 각속도</li>\n<li>행동 : 관절의 가해지는 토크</li>\n<li>보상 : 앞으로 나아가는 속도</li>\n<li>목표 : 최대한 앞으로 많이 나아가기</li>\n</ul>\n<p>즉 에이전트는 time step마다 관절의 위치와 각도를 받아와서 그 상태에서 어떻게 움직여야 앞으로 나아갈 수 있는지를 학습해야 합니다. 행동은 discrete action이 아닌 continuous action으로 -1과 1사이의 값을 가집니다. 만약 행동이 -1이라면 해당 관절에 시계반대방향으로 토크를 주는 것이고 행동이 +1이라면 해당 관절에 시계방향으로 토크를 주는 것입니다. </p>\n<p>continuous action을 주는 방법은 네트워크(Actor)의 output layer에서 activation function으로 tanh와 같은 것을 사용해서 continuous한 값을 출력하는 것이 있습니다. 하지만 피지여행 코드 구현에서는 action을 gaussian distribution에서 sampling 하였습니다. 이렇게 하면 분산을 일정하게 유지하면서 지속적인 exploration을 할 수 있습니다. 간단하게 그림으로 보자면 다음과 같습니다.<br><img src=\"https://www.dropbox.com/s/94g01zdxyf5oxu1/Screenshot%202018-08-23%2001.20.21.png?dl=1\"></p>\n<p>네트워크 구조와 행동을 선택하는 부분은 다음과 같습니다. Hidden Layer의 activation function으로 tanh를 사용했으며(ReLU를 테스트해보지는 않았습니다. 기존 TRPO, PPO 구현들과 논문에서 tanh를 사용하기 때문에 저희도 사용했습니다. 뒤에 유니티 환경에서는 Swish라는 것을 사용합니다.) log std를 0으로 고정함으로서 일정한 폭을 가지는 분포를 만들어낼 수 있습니다. 이 분포로부터 action을 sampling 합니다.</p>\n<p><img src=\"https://www.dropbox.com/s/xfl9zxies0lmpm1/Screenshot%202018-08-23%2001.20.44.png?dl=1\"></p>\n<p><br></p>\n<h3 id=\"1-2-Vanilla-PG\"><a href=\"#1-2-Vanilla-PG\" class=\"headerlink\" title=\"1.2 Vanilla PG\"></a>1.2 Vanilla PG</h3><p>Vanilla PG는 Actor-Critic의 가장 간단한 형태입니다. Vanilla PG는 이후의 구현에 대한 baseline이 됩니다. 구현이 가장 간단하면서 학습이 안되는 것은 아닙니다. 따라서 코드 전체 구조를 잡는데 Vanilla PG를 짜는 것이 도움이 됩니다. 전반적인 코드 구조는 다음과 같습니다.</p>\n<ul>\n<li>iteration 마다 일정한 step 수만큼 환경에서 진행해서 샘플을 모은다</li>\n<li>모은 샘플로 Actor와 Critic을 학습한다</li>\n<li>반복한다</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">episodes = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> iter <span class=\"keyword\">in</span> range(<span class=\"number\">15000</span>):</span><br><span class=\"line\">    actor.eval(), critic.eval()</span><br><span class=\"line\">    memory = deque()</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">while</span> steps &lt; <span class=\"number\">2048</span>:</span><br><span class=\"line\">        episodes += <span class=\"number\">1</span></span><br><span class=\"line\">        state = env.reset()</span><br><span class=\"line\">        state = running_state(state)</span><br><span class=\"line\">        score = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> range(<span class=\"number\">10000</span>):</span><br><span class=\"line\">            mu, std, _ = actor(torch.Tensor(state).unsqueeze(<span class=\"number\">0</span>))</span><br><span class=\"line\">            action = get_action(mu, std)[<span class=\"number\">0</span>]</span><br><span class=\"line\">            next_state, reward, done, _ = env.step(action)</span><br><span class=\"line\">            next_state = running_state(next_state)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> done:</span><br><span class=\"line\">                mask = <span class=\"number\">0</span></span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                mask = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">            memory.append([state, action, reward, mask])</span><br><span class=\"line\">            state = next_state</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> done:</span><br><span class=\"line\">                <span class=\"keyword\">break</span></span><br><span class=\"line\">                </span><br><span class=\"line\">    actor.train(), critic.train()</span><br><span class=\"line\">    train_model(actor, critic, memory, actor_optim, critic_optim)</span><br></pre></td></tr></table></figure>\n<p>memory에 sample을 저장할 때 sample은 state와 action, reward, mask(마지막 state일 경우 0 나머지 1)입니다. mask의 경우 뒤에서 return이나 advantage를 계산할 때 사용됩니다. 또 하나 염두에 두어야할 것은 running_state 입니다. running_state는 input으로 들어오는 state의 scale이 일정하지 않기 때문에 사용합니다. 즉 state의 각 dimension을 평균 0 분산 1로 standardization 하는 것입니다. 따라서 모델을 저장할 때 각 dimension 마다의 평균과 분산도 같이 저장해서 테스트할 때 불러와서 사용해야 합니다.</p>\n<p>Vanilla PG의 경우 학습 부분이 상당히 간단합니다. 다음 코드를 보시면 메모리에서 state, action, reward, mask를 꺼냅니다. reward와 mask를 통해 return을 구할 수 있고 이 return을 통해 actor를 업데이트 할 수 있습니다 (REINFORCE 알고리즘을 떠올리시면 됩니다). 여기서 critic이 하는 일은 없지만 뒤의 알고리즘들과 코드의 통일성을 위해 fake로 넣어놨습니다. Return은 평균을 빼고 분산으로 나눠서 standardize 합니다. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train_model</span><span class=\"params\">(actor, critic, memory, actor_optim, critic_optim)</span>:</span></span><br><span class=\"line\">    memory = np.array(memory)</span><br><span class=\"line\">    states = np.vstack(memory[:, <span class=\"number\">0</span>])</span><br><span class=\"line\">    actions = list(memory[:, <span class=\"number\">1</span>])</span><br><span class=\"line\">    rewards = list(memory[:, <span class=\"number\">2</span>])</span><br><span class=\"line\">    masks = list(memory[:, <span class=\"number\">3</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    returns = get_returns(rewards, masks)</span><br><span class=\"line\">    train_critic(critic, states, returns, critic_optim)</span><br><span class=\"line\">    train_actor(actor, returns, states, actions, actor_optim)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> returns</span><br></pre></td></tr></table></figure>\n<p>이 코드로 Hopper 환경에서 학습한 그래프는 다음과 같습니다.<br><img src=\"https://www.dropbox.com/s/asoysfuk76zs1dk/Screenshot%202018-08-23%2001.30.58.png?dl=1\"></p>\n<p><br></p>\n<h3 id=\"1-3-TNPG\"><a href=\"#1-3-TNPG\" class=\"headerlink\" title=\"1.3 TNPG\"></a>1.3 TNPG</h3><p>NPG를 이용한 parameter update 식은 다음과 같습니다. </p>\n<p>$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>NPG를 구현하려면 KL-divergence의 Hessian의 inverse를 구해야하는 문제가 생깁니다. 현재와 같이 Deep Neural Network를 쓰는 경우에 Hessian의 inverse를 직접적으로 구하는 것은 computationally inefficient 합니다. 따라서 직접 구하지 않고 Conjugate gradient 방법을 사용해서 Fisher Vector Product ($$F^{-1}g$$)를 구합니다. 이러한 알고리즘을 Truncated Natural Policy Gradient(TNPG)라고 부릅니다. </p>\n<p>TNPG에서 parameter update를 구하는 과정은 다음과 같습니다. </p>\n<ol>\n<li>Return 구하기</li>\n<li>Critic 학습하기</li>\n<li>logp * return –&gt; loss 구하기</li>\n<li>loss의 미분과 kl-divergence의 2차 미분을 통해 step direction 구하기</li>\n<li>구한 step direction으로 parameter update</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train_model</span><span class=\"params\">(actor, critic, memory, actor_optim, critic_optim)</span>:</span></span><br><span class=\"line\">    memory = np.array(memory)</span><br><span class=\"line\">    states = np.vstack(memory[:, <span class=\"number\">0</span>])</span><br><span class=\"line\">    actions = list(memory[:, <span class=\"number\">1</span>])</span><br><span class=\"line\">    rewards = list(memory[:, <span class=\"number\">2</span>])</span><br><span class=\"line\">    masks = list(memory[:, <span class=\"number\">3</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># ----------------------------</span></span><br><span class=\"line\">    <span class=\"comment\"># step 1: get returns</span></span><br><span class=\"line\">    returns = get_returns(rewards, masks)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># ----------------------------</span></span><br><span class=\"line\">    <span class=\"comment\"># step 2: train critic several steps with respect to returns</span></span><br><span class=\"line\">    train_critic(critic, states, returns, critic_optim)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># ----------------------------</span></span><br><span class=\"line\">    <span class=\"comment\"># step 3: get gradient of loss and hessian of kl</span></span><br><span class=\"line\">    loss = get_loss(actor, returns, states, actions)</span><br><span class=\"line\">    loss_grad = torch.autograd.grad(loss, actor.parameters())</span><br><span class=\"line\">    loss_grad = flat_grad(loss_grad)</span><br><span class=\"line\">    step_dir = conjugate_gradient(actor, states, loss_grad.data, nsteps=<span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># ----------------------------</span></span><br><span class=\"line\">    <span class=\"comment\"># step 4: get step direction and step size and update actor</span></span><br><span class=\"line\">    params = flat_params(actor)</span><br><span class=\"line\">    new_params = params + <span class=\"number\">0.5</span> * step_dir</span><br><span class=\"line\">    update_model(actor, new_params)</span><br></pre></td></tr></table></figure>\n<p>conjugate gradient 코드는 OpenAI baseline에서 가져왔습니다. 이 코드는 원래 John schulmann 개인 repository에 있는 그대로 사용하는 것입니다. nsteps 만큼 iterataion을 반복하며 결국 x를 구하는 것인데 이 x가 step direction 입니다. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># from openai baseline code</span></span><br><span class=\"line\"><span class=\"comment\"># https://github.com/openai/baselines/blob/master/baselines/common/cg.py</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">conjugate_gradient</span><span class=\"params\">(actor, states, b, nsteps, residual_tol=<span class=\"number\">1e-10</span>)</span>:</span></span><br><span class=\"line\">    x = torch.zeros(b.size())</span><br><span class=\"line\">    r = b.clone()</span><br><span class=\"line\">    p = b.clone()</span><br><span class=\"line\">    rdotr = torch.dot(r, r)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(nsteps):</span><br><span class=\"line\">        _Avp = fisher_vector_product(actor, states, p)</span><br><span class=\"line\">        alpha = rdotr / torch.dot(p, _Avp)</span><br><span class=\"line\">        x += alpha * p</span><br><span class=\"line\">        r -= alpha * _Avp</span><br><span class=\"line\">        new_rdotr = torch.dot(r, r)</span><br><span class=\"line\">        betta = new_rdotr / rdotr</span><br><span class=\"line\">        p = r + betta * p</span><br><span class=\"line\">        rdotr = new_rdotr</span><br><span class=\"line\">        <span class=\"keyword\">if</span> rdotr &lt; residual_tol:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n<p>fisher_vector_product는 kl-divergence의 2차미분과 어떠한 vector의 곱인데 p는 처음에 gradient 값이었다가 점차 업데이트가 됩니다. kl-divergence의 2차 미분을 구하는 과정은 다음과 같습니다. 일단 kl-divergence를 현재 policy에 대해서 구한 다음에 actor parameter에 대해서 미분합니다. 이렇게 미분한 gradient를 일단 flat하게 핀 다음에 p라는 벡터와 곱해서 하나의 값으로 만듭니다. 그 값을 다시 actor의 parameter로 만듦으로서 따로 KL-divergence의 2차미분을 구하지않고 Fisher vector product를 구할 수 있습니다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fisher_vector_product</span><span class=\"params\">(actor, states, p)</span>:</span></span><br><span class=\"line\">    p.detach()</span><br><span class=\"line\">    kl = kl_divergence(new_actor=actor, old_actor=actor, states=states)</span><br><span class=\"line\">    kl = kl.mean()</span><br><span class=\"line\">    kl_grad = torch.autograd.grad(kl, actor.parameters(), create_graph=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">    kl_grad = flat_grad(kl_grad)  <span class=\"comment\"># check kl_grad == 0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    kl_grad_p = (kl_grad * p).sum()</span><br><span class=\"line\">    kl_hessian_p = torch.autograd.grad(kl_grad_p, actor.parameters())</span><br><span class=\"line\">    kl_hessian_p = flat_hessian(kl_hessian_p)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> kl_hessian_p + <span class=\"number\">0.1</span> * p</span><br></pre></td></tr></table></figure>\n<p>TNPG 학습 결과는 다음과 같습니다.<br><img src=\"https://www.dropbox.com/s/uc4c0s00qbs33nr/Screenshot%202018-08-23%2001.53.17.png?dl=1\"></p>\n<p><br></p>\n<h3 id=\"1-4-TRPO\"><a href=\"#1-4-TRPO\" class=\"headerlink\" title=\"1.4 TRPO\"></a>1.4 TRPO</h3><p>TRPO와 NPG가 다른 점은 surrogate loss 사용과 trust region 입니다. 하지만 실제로 구현해서 학습을 시켜본 결과 trust region을 넘어가서 back tracking line search를 하는 경우는 거의 없습니다. 따라서 주된 변화는 surrogate loss에 있다고 보셔도 됩니다. Surrogate loss에서 advantage function을 사용하는데 본 코드 구현에서는 GAE를 사용하였습니다. TRPO 업데이트 식은 다음과 같습니다. Q function 위치에 GAE가 들어갑니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}<br>$$</p>\n<p>GAE를 구하는 코드는 다음과 같습니다. GAE는 td-error의 discounted summation이라고 볼 수 있습니다. 마지막에 advants를 standardization 하는 것은 return에서 하는 것과 같은 효과를 봅니다. 하지만 standardization을 안하고 실험을 해보지는 않았습니다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_gae</span><span class=\"params\">(rewards, masks, values)</span>:</span></span><br><span class=\"line\">    rewards = torch.Tensor(rewards)</span><br><span class=\"line\">    masks = torch.Tensor(masks)</span><br><span class=\"line\">    returns = torch.zeros_like(rewards)</span><br><span class=\"line\">    advants = torch.zeros_like(rewards)</span><br><span class=\"line\"></span><br><span class=\"line\">    running_returns = <span class=\"number\">0</span></span><br><span class=\"line\">    previous_value = <span class=\"number\">0</span></span><br><span class=\"line\">    running_advants = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> reversed(range(<span class=\"number\">0</span>, len(rewards))):</span><br><span class=\"line\">        running_returns = rewards[t] + hp.gamma * running_returns * masks[t]</span><br><span class=\"line\">        running_tderror = rewards[t] + hp.gamma * previous_value * masks[t] - \\</span><br><span class=\"line\">                    values.data[t]</span><br><span class=\"line\">        running_advants = running_tderror + hp.gamma * hp.lamda * \\</span><br><span class=\"line\">                          running_advants * masks[t]</span><br><span class=\"line\"></span><br><span class=\"line\">        returns[t] = running_returns</span><br><span class=\"line\">        previous_value = values.data[t]</span><br><span class=\"line\">        advants[t] = running_advants</span><br><span class=\"line\"></span><br><span class=\"line\">    advants = (advants - advants.mean()) / advants.std()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> returns, advants</span><br></pre></td></tr></table></figure>\n<p>Surrogate loss를 구하는 코드는 다음과 같습니다. Advantage function(GAE)를 구하고 나면 이전 policy와 현재 policy 사이의 ratio를 구해서 advantage function에 곱하면 됩니다. 이 때 사실 old policy와 new policy는 값은 같지만 old policy는 clone()이나 detach()를 사용해서 update가 안되게 만들어줍니다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">surrogate_loss</span><span class=\"params\">(actor, advants, states, old_policy, actions)</span>:</span></span><br><span class=\"line\">    mu, std, logstd = actor(torch.Tensor(states))</span><br><span class=\"line\">    new_policy = log_density(torch.Tensor(actions), mu, std, logstd)</span><br><span class=\"line\">    advants = advants.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    surrogate = advants * torch.exp(new_policy - old_policy)</span><br><span class=\"line\">    surrogate = surrogate.mean()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> surrogate</span><br></pre></td></tr></table></figure>\n<p>Actor의 step direction을 구하는 것은 TNPG와 동일합니다. TNPG에서는 step direction으로 바로 업데이트 했지만 TRPO는 다음과 같은 작업을 해줍니다. Full step을 구하는 과정이라고 볼 수 있습니다. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ----------------------------</span></span><br><span class=\"line\"><span class=\"comment\"># step 4: get step direction and step size and full step</span></span><br><span class=\"line\">params = flat_params(actor)</span><br><span class=\"line\">shs = <span class=\"number\">0.5</span> * (step_dir * fisher_vector_product(actor, states, step_dir)</span><br><span class=\"line\">             ).sum(<span class=\"number\">0</span>, keepdim=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">step_size = <span class=\"number\">1</span> / torch.sqrt(shs / hp.max_kl)[<span class=\"number\">0</span>]</span><br><span class=\"line\">full_step = step_size * step_dir</span><br></pre></td></tr></table></figure>\n<p>이렇게 full step을 구하고 나면 Trust region optimization 단계에 들어갑니다. expected improvement는 구한 step 만큼 parameter space에서 움직였을 때 예상되는 performance 변화입니다. 이 값은 kl-divergence와 함께 trust region 안에 있는지 밖에 있는지 판단하는 근거가 됩니다. expected improve는 출발점에서의 gradient * full step으로 구합니다. 그리고 10번을 돌아가며 Back-tracking line search를 실시합니다. 처음에는 full step 만큼 가본 다음에 kl-divergence와 emprovement를 통해 trust region 안이면 루프 탈출, 밖이면 full step을 반만큼 쪼개서 다시 이 과정을 반복합니다.  </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ----------------------------</span></span><br><span class=\"line\"><span class=\"comment\"># step 5: do backtracking line search for n times</span></span><br><span class=\"line\">old_actor = Actor(actor.num_inputs, actor.num_outputs)</span><br><span class=\"line\">update_model(old_actor, params)</span><br><span class=\"line\">expected_improve = (loss_grad * full_step).sum(<span class=\"number\">0</span>, keepdim=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">expected_improve = expected_improve.data.numpy()</span><br><span class=\"line\"></span><br><span class=\"line\">flag = <span class=\"keyword\">False</span></span><br><span class=\"line\">fraction = <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">10</span>):</span><br><span class=\"line\">    new_params = params + fraction * full_step</span><br><span class=\"line\">    update_model(actor, new_params)</span><br><span class=\"line\">    new_loss = surrogate_loss(actor, advants, states, old_policy.detach(),</span><br><span class=\"line\">                              actions)</span><br><span class=\"line\">    new_loss = new_loss.data.numpy()</span><br><span class=\"line\">    loss_improve = new_loss - loss</span><br><span class=\"line\">    expected_improve *= fraction</span><br><span class=\"line\">    kl = kl_divergence(new_actor=actor, old_actor=old_actor, states=states)</span><br><span class=\"line\">    kl = kl.mean()</span><br><span class=\"line\"></span><br><span class=\"line\">    print(<span class=\"string\">'kl: &#123;:.4f&#125;  loss improve: &#123;:.4f&#125;  expected improve: &#123;:.4f&#125;  '</span></span><br><span class=\"line\">          <span class=\"string\">'number of line search: &#123;&#125;'</span></span><br><span class=\"line\">          .format(kl.data.numpy(), loss_improve, expected_improve[<span class=\"number\">0</span>], i))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># see https: // en.wikipedia.org / wiki / Backtracking_line_search</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> kl &lt; hp.max_kl <span class=\"keyword\">and</span> (loss_improve / expected_improve) &gt; <span class=\"number\">0.5</span>:</span><br><span class=\"line\">        flag = <span class=\"keyword\">True</span></span><br><span class=\"line\">        <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">    fraction *= <span class=\"number\">0.5</span></span><br></pre></td></tr></table></figure>\n<p>Critic의 학습은 단순히 value function과 return의 MSE error를 계산해서 loss로 잡고 loss를 최소화하도록 학습합니다. TRPO 학습 결과는 다음과 같습니다.<br><img src=\"https://www.dropbox.com/s/rc9hxsx1kvokcrv/Screenshot%202018-08-23%2013.36.51.png?dl=1\"></p>\n<p><br></p>\n<h3 id=\"1-5-PPO\"><a href=\"#1-5-PPO\" class=\"headerlink\" title=\"1.5 PPO\"></a>1.5 PPO</h3><p>PPO의 장점을 꼽으라면 GPU 사용하기 좋고 sample efficiency가 늘어난다는 것입니다. TNPG와 TRPO의 경우 한 번 모은 sample은 모델을 단 한 번 업데이트하는데 사용하지만 PPO의 경우 몇 mini-batch로 epoch를 돌리기 때문입니다. GAE를 사용한다는 것은 같고 Conjugate gradient나 Fisher vector product나 back tracking line search가 다 빠집니다. 대신 loss function clip으로 monotonic improvement를 보장하게 학습합니다. 따라서 코드가 상당히 간단해집니다. </p>\n<p>다음 코드 부분이 PPO의 전체라고 봐도 무방합니다. PPO는 다음과 같은 순서로 학습합니다. </p>\n<ul>\n<li>batch를 random suffling하고 mini batch를 추출</li>\n<li>value function 구하기</li>\n<li>critic loss 구하기 (clip을 사용해도 되고 TRPO와 같이 그냥 학습시켜도 됌)</li>\n<li>surrogate loss 구하기</li>\n<li>surrogate loss clip해서 actor loss 만들기</li>\n<li>actor와 critic 업데이트</li>\n</ul>\n<p>Actor의 loss를 구하는 것은 다음 식의 값을 구하는 것입니다. 이 식을 구하려면 ratio에 한 번 클립하고 loss 값을 한 번 min을 취하면 됩니다.</p>\n<p>$$L^{CLIP}(\\theta) = \\hat{E}_t [min(r_t(\\theta) \\, \\hat{A}_t,  clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\, \\hat{A}_t)]$$</p>\n<p>이 코드 구현에서는 actor와 critic을 따로 모델로 만들어서 따로 따로 업데이트를 하지만 하나로 만든다면 loss로 한 번만 업데이트하면 됩니다. 또한 entropy loss를 최종 loss에 더해서 regularization 효과를 볼 수도 있습니다. Critic loss에 clip 해주는 것은 OpenAI baseline의 ppo2 코드를 참조하였습니다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># step 2: get value loss and actor loss and update actor &amp; critic</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(<span class=\"number\">10</span>):</span><br><span class=\"line\">    np.random.shuffle(arr)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n // hp.batch_size):</span><br><span class=\"line\">        batch_index = arr[hp.batch_size * i: hp.batch_size * (i + <span class=\"number\">1</span>)]</span><br><span class=\"line\">        batch_index = torch.LongTensor(batch_index)</span><br><span class=\"line\">        inputs = torch.Tensor(states)[batch_index]</span><br><span class=\"line\">        returns_samples = returns.unsqueeze(<span class=\"number\">1</span>)[batch_index]</span><br><span class=\"line\">        advants_samples = advants.unsqueeze(<span class=\"number\">1</span>)[batch_index]</span><br><span class=\"line\">        actions_samples = torch.Tensor(actions)[batch_index]</span><br><span class=\"line\">        oldvalue_samples = old_values[batch_index].detach()</span><br><span class=\"line\"></span><br><span class=\"line\">        loss, ratio = surrogate_loss(actor, advants_samples, inputs,</span><br><span class=\"line\">                                     old_policy.detach(), actions_samples,</span><br><span class=\"line\">                                     batch_index)</span><br><span class=\"line\"></span><br><span class=\"line\">        values = critic(inputs)</span><br><span class=\"line\">        clipped_values = oldvalue_samples + \\</span><br><span class=\"line\">                         torch.clamp(values - oldvalue_samples,</span><br><span class=\"line\">                                     -hp.clip_param,</span><br><span class=\"line\">                                     hp.clip_param)</span><br><span class=\"line\">        critic_loss1 = criterion(clipped_values, returns_samples)</span><br><span class=\"line\">        critic_loss2 = criterion(values, returns_samples)</span><br><span class=\"line\">        critic_loss = torch.max(critic_loss1, critic_loss2).mean()</span><br><span class=\"line\"></span><br><span class=\"line\">        clipped_ratio = torch.clamp(ratio,</span><br><span class=\"line\">                                    <span class=\"number\">1.0</span> - hp.clip_param,</span><br><span class=\"line\">                                    <span class=\"number\">1.0</span> + hp.clip_param)</span><br><span class=\"line\">        clipped_loss = clipped_ratio * advants_samples</span><br><span class=\"line\">        actor_loss = -torch.min(loss, clipped_loss).mean()</span><br><span class=\"line\"></span><br><span class=\"line\">        loss = actor_loss + <span class=\"number\">0.5</span> * critic_loss</span><br><span class=\"line\"></span><br><span class=\"line\">        critic_optim.zero_grad()</span><br><span class=\"line\">        loss.backward(retain_graph=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">        critic_optim.step()</span><br><span class=\"line\"></span><br><span class=\"line\">        actor_optim.zero_grad()</span><br><span class=\"line\">        loss.backward()</span><br><span class=\"line\">        actor_optim.step()</span><br></pre></td></tr></table></figure>\n<p>PPO의 학습 결과는 다음과 같습니다.<br><img src=\"https://www.dropbox.com/s/rkxa836ap931kbd/Screenshot%202018-08-23%2013.50.57.png?dl=1\"></p>\n<p><br></p>\n<h2 id=\"2-Unity-ml-agent-학습\"><a href=\"#2-Unity-ml-agent-학습\" class=\"headerlink\" title=\"2. Unity ml-agent 학습\"></a>2. Unity ml-agent 학습</h2><p>Mujoco Hopper(half-cheetah와 같은 것도)에 Vanilla PG, TNPG, TRPO, PPO를 구현해서 적용했습니다. Mujoco의 경우 이미 Hyper parameter와 같은 정보들이 논문이나 블로그에 있기 때문에 상대적으로 continuous control로 시작하기에는 좋습니다. 맨 처음에 말했듯이 Mujoco는 1달만 무료이고 그 이후부터 유료이며 확장성이 떨어집니다. 좀 더 general한 agent를 학습시키기에 좋은 환경이 필요합니다. 따라서 Unity ml-agent를 살펴봤습니다. Repository는 다음과 같습니다. </p>\n<ul>\n<li><a href=\"https://github.com/Unity-Technologies/ml-agents\" target=\"_blank\" rel=\"noopener\">Unity ml-agent repository</a></li>\n<li><a href=\"https://unity3d.com/machine-learning/\" target=\"_blank\" rel=\"noopener\">Unity ml-agent homepage</a></li>\n</ul>\n<p><img src=\"https://www.dropbox.com/s/lapholj8r4nxmb1/Screenshot%202018-08-24%2013.41.31.png?dl=1\"></p>\n<p>현재 Unity ml-agent에서 기본으로 제공하는 환경은 다음과 같습니다. Unity ml-agent는 기존 Unity를 그대로 사용하면서 쉽게 강화학습 에이전트를 붙일 수 있도록 설계되어 있습니다. Unity ml-agent에서는 Walker 환경에서 저희가 살펴본 알고리즘 중에 가장 최신 알고리즘은 PPO를 적용해봤습니다. 이 포스트를 보시는 분들은 이 많은 다른 환경에 자유롭게 저희 코드를 적용할 수 있습니다.</p>\n<ul>\n<li><a href=\"https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md\" target=\"_blank\" rel=\"noopener\">각 환경에 대한 설명</a></li>\n</ul>\n<p><img src=\"https://www.dropbox.com/s/lrbodw5dypxowmw/Screenshot%202018-08-24%2014.06.14.png?dl=1\"></p>\n<p>Unity ml-agent를 이용해서 강화학습을 하기 위해서는 다음과 같이 진행됩니다. 단계별로 설명하겠습니다. </p>\n<ul>\n<li>Unity에서 환경 만들기</li>\n<li>Python에서 unity 환경 불러와서 테스트하기</li>\n<li>기존에 하던대로 학습하기</li>\n</ul>\n<p><br></p>\n<h3 id=\"2-1-Walker-환경-만들기\"><a href=\"#2-1-Walker-환경-만들기\" class=\"headerlink\" title=\"2.1 Walker 환경 만들기\"></a>2.1 Walker 환경 만들기</h3><p>강화학습을 하는 많은 분들이 Unity를 한 번도 다뤄보지 않은 경우가 많습니다. 저도 그런 경우라서 어떻게 환경을 만들어야할지 처음에는 감이 잡히지 않았습니다. 하지만 Unity ml-agent에서는 상당히 자세한 guide를 제공합니다. 다음은 Unity ml-agent의 가장 기본적인 환경인 3DBall에 대한 tutorial입니다. 설치 guide도 제공하고 있으니 참고하시면 될 것 같습니다.</p>\n<ul>\n<li><a href=\"https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Basic-Guide.md\" target=\"_blank\" rel=\"noopener\">3DBall 예제 tutorial</a></li>\n<li><a href=\"https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md\" target=\"_blank\" rel=\"noopener\">Unity ml-agent 설치 guide</a></li>\n</ul>\n<p>Unity ml-agent에서 제공하는 3DBall tutorial을 참고해서 Walker 환경을 만들었습니다. Walker 환경을 만드는 과정을 간단히 말씀드리겠습니다. 다음 그림의 단계들을 동일하므로 따라하시면 됩니다. Unity를 열고 unity-environment로 들어가시면 됩니다.<br><img src=\"https://www.dropbox.com/s/fbdqg781w46a5mz/Screenshot%202018-08-24%2014.50.50.png?dl=1\"></p>\n<p>그러면 화면 하단에서 다음과 같은 것을 볼 수 있습니다. Assets/ML-Agents/Examples로 들어가보면 Walker가 있습니다. Scenes에서 Walker를 더블클릭하면 됩니다.<br><img src=\"https://www.dropbox.com/s/h349xml3faln0wy/Screenshot%202018-08-24%2014.52.14.png?dl=1\"></p>\n<p>더블클릭해서 나온 화면에서 오른쪽 상단의 파란색 화살표를 누르면 환경이 실행이 됩니다. 저희가 학습하고자 하는 agent는 바로 이녀석입니다. 왼쪽 리스트를 보면 WalkerPair가 11개가 있는 것을 볼 수 있습니다. Unity ml-agent 환경은 기본적으로 Multi-agent로 학습하도록 설정되어있습니다. 따라서 여러개의 Walker들이 화면에 보이는 것입니다.<br><img src=\"https://www.dropbox.com/s/cy8m5kqdmkhopjo/Screenshot%202018-08-24%2014.54.57.png?dl=1\"></p>\n<p>리스트 중에 Walker Academy를 클릭해서 그 하위에 있는 WalkerBrain을 더블클릭합니다. 그러면 화면 오른쪽에 다음과 같은 Brain 설정을 볼 수 있습니다. Brain은 쉽게 말해서 Agent라고 생각하면 됩니다. 이 Agent는 상태로 212차원의 vector가 주어지며 다 continuous한 값을 가집니다. 행동은 39개의 행동을 할 수 있으며 다 Continuous입니다. Mujoco에 비해서 상태나 행동의 차원이 상당히 높습니다. 여기서 중요한 것은 Brain Type입니다. Brain type은 internal, external, player, heuristic이 있습니다. player로 type을 설정하고 화면 상단의 play 버튼을 누르면 여러분이 agent를 움직일 수 있습니다. 하지만 Walker는 사람이 움직이는게 거의 불가능하므로 player 기능은 사용할 수 없습니다. 다른 환경에서는 사용해볼 수 있으니 재미로 한 번 플레이해보시면 좋습니다! </p>\n<center><img src=\"https://www.dropbox.com/s/uxfm162f1scbzo5/Screenshot%202018-08-24%2015.09.04.png?dl=1\" width=\"400px\"></center>\n\n<p>이번에는 WalkerPair에서 WalkerAgent를 더블클릭해보겠습니다. 이 설정을 보아 5000 step이 episode의 max step인 것을 볼 수 있습니다.</p>\n<center><img src=\"https://www.dropbox.com/s/r6gwemlczwic2ma/Screenshot%202018-08-24%2015.16.19.png?dl=1\" width=\"400px\"></center>\n\n<p>이제 상단 file menu에서 build setting에 들어갑니다. 환경을 build해서 python 코드에서 import하기 위해서입니다. 물론 unity 환경과 python 코드를 binding해주는 부분은 ml-agent 코드 안에 있습니다. Build 버튼을 누르면 환경이 build가 됩니다.</p>\n<center><img src=\"https://www.dropbox.com/s/4dtgoz1k8896vxs/Screenshot%202018-08-24%2015.19.07.png?dl=1\" width=\"500px\"></center>\n\n\n<p><br></p>\n<h3 id=\"2-2-Python에서-unity-환경-불러와서-테스트하기\"><a href=\"#2-2-Python에서-unity-환경-불러와서-테스트하기\" class=\"headerlink\" title=\"2.2 Python에서 unity 환경 불러와서 테스트하기\"></a>2.2 Python에서 unity 환경 불러와서 테스트하기</h3><p>환경을 build 했으면 build한 환경을 python에서 불러와서 random action으로 테스트해봅니다. 환경을 테스트하는 코드는 pg_travel repository에서 unity 폴더 밑에 있습니다. test_env.py라는 코드는 간단하게 다음과 같습니다. Build한 walker 환경은 env라는 폴더 밑에 넣어줍니다. unityagent를 import하는데 ml-agent를 git clone 해서 python 폴더 내에서 “python setup.py install”을 실행했다면 문제없이 import 됩니다. UnityEnvironment를 통해 env라는 환경을 선언할 수 있습니다. 이렇게 선언하고 나면 gym과 상당히 유사한 형태로 환경과 상호작용이 가능합니다. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> unityagents <span class=\"keyword\">import</span> UnityEnvironment</span><br><span class=\"line\"><span class=\"keyword\">from</span> utils.utils <span class=\"keyword\">import</span> get_action</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    env_name = <span class=\"string\">\"./env/walker_test\"</span></span><br><span class=\"line\">    train_mode = <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\">    env = UnityEnvironment(file_name=env_name)</span><br><span class=\"line\"></span><br><span class=\"line\">    default_brain = env.brain_names[<span class=\"number\">0</span>]</span><br><span class=\"line\">    brain = env.brains[default_brain]</span><br><span class=\"line\">    env_info = env.reset(train_mode=train_mode)[default_brain]</span><br><span class=\"line\"></span><br><span class=\"line\">    num_inputs = brain.vector_observation_space_size</span><br><span class=\"line\">    num_actions = brain.vector_action_space_size</span><br><span class=\"line\">    num_agent = env._n_agents[default_brain]</span><br><span class=\"line\"></span><br><span class=\"line\">    print(<span class=\"string\">'the size of input dimension is '</span>, num_inputs)</span><br><span class=\"line\">    print(<span class=\"string\">'the size of action dimension is '</span>, num_actions)</span><br><span class=\"line\">    print(<span class=\"string\">'the number of agents is '</span>, num_agent)</span><br><span class=\"line\">   </span><br><span class=\"line\">    score = <span class=\"number\">0</span></span><br><span class=\"line\">    episode = <span class=\"number\">0</span></span><br><span class=\"line\">    actions = [<span class=\"number\">0</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(num_actions)] * num_agent</span><br><span class=\"line\">    <span class=\"keyword\">for</span> iter <span class=\"keyword\">in</span> range(<span class=\"number\">1000</span>):</span><br><span class=\"line\">        env_info = env.step(actions)[default_brain]</span><br><span class=\"line\">        rewards = env_info.rewards</span><br><span class=\"line\">        dones = env_info.local_done</span><br><span class=\"line\">        score += rewards[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> dones[<span class=\"number\">0</span>]:</span><br><span class=\"line\">            episode += <span class=\"number\">1</span></span><br><span class=\"line\">            score = <span class=\"number\">0</span></span><br><span class=\"line\">            print(<span class=\"string\">'&#123;&#125;th episode : mean score of 1st agent is &#123;:.2f&#125;'</span>.format(</span><br><span class=\"line\">                episode, score))</span><br></pre></td></tr></table></figure>\n<p>위 코드를 실행하면 다음과 같이 실행창에 뜹니다. External brain인 것을 알 수 있고 default_brain은 brain 중에 하나만 가져왔기 때문에 number of brain은 1이라고 출력합니다. input dimension은 212이고 action dimension은 39이고 agent 수는 11인 것으로봐서 제대로 환경이 불러와진 것을 확인할 수 있습니다.<br><img src=\"https://www.dropbox.com/s/cioa9h7qu25vonz/Screenshot%202018-08-24%2015.47.43.png?dl=1\"></p>\n<p>이 환경에서 행동하려면 agent 숫자만큼 행동을 줘야합니다. 모두 0로 행동을 주고 실행하면 다음과 같이 뒤로 넘어지는 행동을 반복합니다. env.step(actions)[default_brain]으로 env_info를 받아오면 거기서부터 reward와 done, next_state를 받아올 수 있습니다. 이제 학습하기만 하면 됩니다.<br><img src=\"https://www.dropbox.com/s/8qrmxoski6p4n07/Screenshot%202018-08-24%2016.00.21.png?dl=1\"></p>\n<p><br></p>\n<h3 id=\"2-3-Walker-학습하기\"><a href=\"#2-3-Walker-학습하기\" class=\"headerlink\" title=\"2.3 Walker 학습하기\"></a>2.3 Walker 학습하기</h3><p>기존에 Mujoco에 적용했던 PPO 코드를 그대로 Walker에 적용하니 잘 학습이 안되었습니다. 다음 사진이 저희가 중간 해커톤으로 모여서 이 상황을 공유할 때의 사진입니다.<br><img src=\"https://i.imgur.com/1aR2Z77.png\" width=\"500px\"></p>\n<p>Unity ml-agent에서는 PPO를 기본 agent로 제공합니다. 학습 코드도 제공하기 때문에 mujoco에 적용했던 코드와의 차이점을 분석할 수 있었습니다. mujoco 코드와 ml-agent baseline 코드의 차이점은 다음과 같습니다. </p>\n<ul>\n<li>agent 여러개를 이용, 별개의 memory에 저장한 후에 gradient를 합침</li>\n<li>GAE 및 time horizon 등 hyper parameter가 다름</li>\n<li>Actor와 Critic의 layer가 1층 더 두꺼우며 hidden layer 자체의 사이즈도 더 큼</li>\n<li>hidden layer의 activation function이 tanh가 아닌 swish</li>\n</ul>\n<p>ml-agent baseline 코드리뷰할 때 작성했던 마인드맵은 다음과 같습니다.<br><img src=\"https://i.imgur.com/YeaEntG.png\"></p>\n<p>크게는 두 가지를 개선해서 성능이 많이 향상했습니다.</p>\n<ol>\n<li>Network 수정</li>\n<li>multi-agent를 활용해서 학습</li>\n</ol>\n<p>Network 코드는 다음과 같습니다. Hidden Layer를 하나 더 늘렸으며 swish activation function을 사용할 수 있도록 변경했습니다. 사실 swish라는 activation function은 처음 들어보는 생소한 함수였습니다. 하지만 ml-agent baseline에서 사용했다는 사실과 구현이 상당히 간단하다는 점에서 저희 코드에 적용했습니다. 단순히 x * sigmoid(x) 를 하면 됩니다. swish는 별거 아닌 것 같지만 상당한 성능 개선을 가져다줬습니다. 사실 ReLU나 ELU 등 여러 다른 activation function을 적용해서 비교해보는게 best긴 하지만 시간 관계상 그렇게까지 테스트해보지는 못했습니다. 기존에 TRPO나 PPO는 왜 tanh를 사용했었는지도 의문인 점입니다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Actor</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, num_inputs, num_outputs, args)</span>:</span></span><br><span class=\"line\">        self.args = args</span><br><span class=\"line\">        self.num_inputs = num_inputs</span><br><span class=\"line\">        self.num_outputs = num_outputs</span><br><span class=\"line\">        super(Actor, self).__init__()</span><br><span class=\"line\">        self.fc1 = nn.Linear(num_inputs, args.hidden_size)</span><br><span class=\"line\">        self.fc2 = nn.Linear(args.hidden_size, args.hidden_size)</span><br><span class=\"line\">        self.fc3 = nn.Linear(args.hidden_size, args.hidden_size)</span><br><span class=\"line\">        self.fc4 = nn.Linear(args.hidden_size, num_outputs)</span><br><span class=\"line\"></span><br><span class=\"line\">        self.fc4.weight.data.mul_(<span class=\"number\">0.1</span>)</span><br><span class=\"line\">        self.fc4.bias.data.mul_(<span class=\"number\">0.0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.args.activation == <span class=\"string\">'tanh'</span>:</span><br><span class=\"line\">            x = F.tanh(self.fc1(x))</span><br><span class=\"line\">            x = F.tanh(self.fc2(x))</span><br><span class=\"line\">            x = F.tanh(self.fc3(x))</span><br><span class=\"line\">            mu = self.fc4(x)</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> self.args.activation == <span class=\"string\">'swish'</span>:</span><br><span class=\"line\">            x = self.fc1(x)</span><br><span class=\"line\">            x = x * F.sigmoid(x)</span><br><span class=\"line\">            x = self.fc2(x)</span><br><span class=\"line\">            x = x * F.sigmoid(x)</span><br><span class=\"line\">            x = self.fc3(x)</span><br><span class=\"line\">            x = x * F.sigmoid(x)</span><br><span class=\"line\">            mu = self.fc4(x)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">raise</span> ValueError</span><br><span class=\"line\"></span><br><span class=\"line\">        logstd = torch.zeros_like(mu)</span><br><span class=\"line\">        std = torch.exp(logstd)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> mu, std, logstd</span><br></pre></td></tr></table></figure>\n<p>swish와 tanh를 사용한 학습을 비교한 그래프입니다. 하늘색 그래프가 swish를 사용한 결과, 파란색이 tanh를 사용한 결과입니다. score는 episode 마다의 reward의 합입니다.</p>\n<center><img src=\"https://www.dropbox.com/s/3d07c1kql4h5oqk/Screenshot%202018-08-24%2016.33.45.png?dl=1\" width=\"350px\"></center>\n\n<p>이제 multi-agent로 학습하도록 변경하면 됩니다. PPO의 경우 memory에 time horizon 동안의 sample을 시간순서대로 저장하고 GAE를 구한 이후에 minibatch로 추출해서 학습합니다. 따라서 여러개의 agent로 학습하기 위해서는 memory를 따로 만들어서 각각의 GAE를 구해서 학습해야합니다. Unity에서는 Mujoco에서 했던 것처럼 deque로 memory를 만들지 않고 따로 named tuple로 구현한 memory class를 import 해서 사용했습니다. utils 폴더 밑에 memory.py 코드에 구현되어있으며 코드는 <a href=\"https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb\" target=\"_blank\" rel=\"noopener\">https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb</a><br>에서 가져왔습니다. </p>\n<p>state, action, reward, mask를 저장하는데 불러올 때 각각을 따로 불러올 수 있기 때문에 비효율적 시간을 많이 줄여줍니다.<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Transition = namedtuple(<span class=\"string\">'Transition'</span>, (<span class=\"string\">'state'</span>, <span class=\"string\">'action'</span>, <span class=\"string\">'reward'</span>, <span class=\"string\">'mask'</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Memory</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        self.memory = []</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">push</span><span class=\"params\">(self, state, action, reward, mask)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">\"\"\"Saves a transition.\"\"\"</span></span><br><span class=\"line\">        self.memory.append(Transition(state, action, reward, mask))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sample</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> Transition(*zip(*self.memory))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__len__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> len(self.memory)</span><br></pre></td></tr></table></figure></p>\n<p>main.py 에서는 이 memory를 agent의 개수만큼 생성합니다. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">memory = [Memory() <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> range(num_agent)]</span><br></pre></td></tr></table></figure>\n<p>sample을 저장할 때도 agent마다 따로 따로 저장합니다. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(num_agent):</span><br><span class=\"line\">    memory[i].push(states[i], actions[i], rewards[i], masks[i])</span><br></pre></td></tr></table></figure>\n<p>time horizon이 끝나면 모은 sample 들을 가지고 학습하기 위한 값으로 만드는 과정을 진행합니다. 각각의 memory를 가지고 GAE와 old_policy, old_value 등을 계산해서 하나의 batch로 합칩니다. 그렇게 train_model 메소드에 전달하면 기존과 동일하게 agent를 업데이트합니다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sts, ats, returns, advants, old_policy, old_value = [], [], [], [], [], []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(num_agent):</span><br><span class=\"line\">    batch = memory[i].sample()</span><br><span class=\"line\">    st, at, rt, adv, old_p, old_v = process_memory(actor, critic, batch, args)</span><br><span class=\"line\">    sts.append(st)</span><br><span class=\"line\">    ats.append(at)</span><br><span class=\"line\">    returns.append(rt)</span><br><span class=\"line\">    advants.append(adv)</span><br><span class=\"line\">    old_policy.append(old_p)</span><br><span class=\"line\">    old_value.append(old_v)</span><br><span class=\"line\"></span><br><span class=\"line\">sts = torch.cat(sts)</span><br><span class=\"line\">ats = torch.cat(ats)</span><br><span class=\"line\">returns = torch.cat(returns)</span><br><span class=\"line\">advants = torch.cat(advants)</span><br><span class=\"line\">old_policy = torch.cat(old_policy)</span><br><span class=\"line\">old_value = torch.cat(old_value)</span><br><span class=\"line\"></span><br><span class=\"line\">train_model(actor, critic, actor_optim, critic_optim, sts, ats, returns, advants,</span><br><span class=\"line\">            old_policy, old_value, args)</span><br></pre></td></tr></table></figure>\n<p>이렇게 학습한 에이전트는 다음과 같이 걷습니다. 이렇게 walker를 학습시키고 나니 어떻게 하면 사람처럼 자연스럽게 걷는 것을 agent 스스로 학습할 수 있을까라는 고민을 하게 되었습니다.</p>\n<center><img src=\"https://www.dropbox.com/s/fyz1kn5v92l3rrk/plane-595.gif?dl=1\"></center>\n\n<p>Unity ml-agent에서 제공하는 pretrained된 모델을 다음과 같이 걷습니다. 저희가 학습한 agent와 상당히 다르게 걷는데 왜 그런 차이가 나는지도 분석하고 싶습니다. </p>\n<center><img src=\"https://www.dropbox.com/s/xwz766g7c4eiaia/plane-unity.gif?dl=1\"></center>\n\n\n<p><br></p>\n<h2 id=\"3-Unity-Curved-Surface-제작-및-학습기\"><a href=\"#3-Unity-Curved-Surface-제작-및-학습기\" class=\"headerlink\" title=\"3. Unity Curved Surface 제작 및 학습기\"></a>3. Unity Curved Surface 제작 및 학습기</h2><p>Unity ml-agent에서 제공하는 기본 Walker 환경에서 학습하고 나니 바닥을 조금 울퉁불퉁하게 혹은 경사가 진 곳에서 걷는 것을 학습해보고 싶다라는 생각이 들었습니다. 따라서 간단하게 걷는 배경을 다르게 하는 시도를 해봤습니다. </p>\n<p><br></p>\n<h3 id=\"3-1-Curved-Surface-만들기\"><a href=\"#3-1-Curved-Surface-만들기\" class=\"headerlink\" title=\"3.1 Curved Surface 만들기\"></a>3.1 Curved Surface 만들기</h3><p>Agent가 걸어갈 배경을 처음부터 만드는 것보다 구할 수 있다면 만들어진 배경을 구하기로 했습니다. Unity를 무료라는 점에서 선택했듯이 배경을 무료로 구할 수 있는 방법을 선택했습니다. </p>\n<center><img src=\"https://www.dropbox.com/s/e0tsp3e3c9uq2zh/Screenshot%202018-08-23%2000.19.14.png?dl=1\"></center>\n\n<p>무료로 공개되어있는 Unity 배경 중에서 Curved Ground 라는 것을 가져와서 작업하기로 했습니다. 이 환경 같은 경우 spline을 그리듯이 중간의 점을 이동시키면서 사용자가 곡면을 수정할 수 있습니다.</p>\n<center><img src=\"https://www.dropbox.com/s/3ppmxotrf6qhzaf/Screenshot%202018-08-23%2000.20.25.png?dl=1\"></center>\n\n<p>간단하게 곡면을 만들어서 공을 굴려보면 다음과 같이 잘 굴러갑니다. </p>\n<center><img src=\"https://www.dropbox.com/s/2e8yqvqj1a4th27/slope_walker_ball.gif?dl=1\"></center>\n\n<p>여러 에이전트가 학습할 수 있도록 오목한 경사면을 제작했습니다. 초반의 모습은 다음과 같았습니다.<br><img src=\"https://www.dropbox.com/s/m492xsfp4bolmz5/Screenshot%202018-08-23%2000.36.06.png?dl=1\"></p>\n<p>하지만 최종으로는 다음과 같은 곡면으로 사용했습니다. 위 사진의 배경과 아래 사진의 배경이 다른 점은 slope 길이, 내리막 경사, 오르막 경사입니다. Slope 길이의 경우 길이를 기존 plane 과 동일하게 했더니, 오르막 올라가는 부분이 학습이 잘 안 되었습니다. 따라서 길이를 줄였습니다. 내리막 경사의 경우 너무 경사지면 학습이 잘 안 되고, 너무 완만하니 내리막 티가 잘 안 나기 때문에 적절한 경사를 설정했습니다. 오르막 경사의 경우 내리막보다는 오르막이 더 어려울 것이라고 판단해서 오르막 경사를 낮게 설정했습니다.<br><img src=\"https://www.dropbox.com/s/idbov4wtd6jeqb2/Screenshot%202018-08-23%2000.36.54.png?dl=1\"></p>\n<p><br></p>\n<h3 id=\"3-2-Curved-Surface에서-학습하기\"><a href=\"#3-2-Curved-Surface에서-학습하기\" class=\"headerlink\" title=\"3.2 Curved Surface에서 학습하기\"></a>3.2 Curved Surface에서 학습하기</h3><p>위 환경으로 학습을 할 때, agent가 너무 초반에 빨리 쓰러지는 현상이 발생했습니다. 혹시 발의 각도가 문제일까 싶어서 발 각도를 변경해보았습니다. </p>\n<center><img src=\"https://www.dropbox.com/s/znvikbeoj7gku0u/Screenshot%202018-08-23%2000.38.22.png?dl=1\" width=\"400px\"></center>\n\n<p>하지만 역시 평지에서 걷는 것처럼 걷도록 학습이 안되었습니다. 이 환경에서 더 잘 학습하려면 더 여러가지를 시도해봐야할 것 같습니다. (그래도 걷는 게 기특합니다..)</p>\n<center><img src=\"https://www.dropbox.com/s/4fqpsdmnzvnvia0/curved-736.gif?dl=1\"></center>\n\n<p><img src=\"https://www.dropbox.com/s/t5ngr0io4xeex6y/curved-736-overview.gif?dl=1\"></p>\n<p><br></p>\n<h2 id=\"4-구현-후기\"><a href=\"#4-구현-후기\" class=\"headerlink\" title=\"4. 구현 후기\"></a>4. 구현 후기</h2><p>피지여행 구현팀은 총 4명으로 진행했습니다. 각 팀원의 후기를 적어보겠습니다.</p>\n<ul>\n<li>팀원 장수영: 사랑합니다. 행복합니다.</li>\n<li>팀원 공민서: 제가 핵심적인 기능을 구현하지는 못했지만 무조코 설치와 모델 테스트를 맡으면서 딥마인드나 openai의 영상으로만 보던 에이전트의 성장과정을 눈으로 지켜볼 수 있었습니다. 제대로 서있지도 못하던 hopper가 어느정도 훈련이 되고서는 넘어지려하다가도 추진력을 얻기위해 웅크렸다 뛰는 것을 관찰하는 것도 재미있고 육아일기를 보는 아버지의 마음을 조금이나마 이해할 수 있었습니다. 텐서보드를 넣는 걸 깜빡해 일일히 에피소드 별 스코어를 시각화 하면서 텐서보드의 소중함을 알았습니다. 유니티 코드리뷰를 하면서도 시스템 아키텍쳐 설계에 대해서도 배울 점이 있었던 것 같고 swish라는 활성화함수의 존재도 알았었고 curiosity도 알게되었고 역시 다른 사람의 코드를 읽는 것도 많은 공부가 된다고 되새기던 시간이었습니다. 물론 너무 크기가 방대해서 가독성은 많이 떨어졌습니다만 무조코보다 유니티가 훨씬 흥할거라고 생각했습니다. 마지막으로 누구 하나 열정적이지 않은 사람이 없이 치열한 고민을 함께 한 PG여행팀 분들, 저의 부족함과 상생의 기쁨을 알게해주셔서 정말 감사드립니다.</li>\n<li>팀원 양혁렬: 여러 에이전트가 함께하면 더 잘하는 걸 보면서 새삼 좋은 분들과 함께 할 수 있어서 행복했습니다</li>\n<li>팀원 이웅원: 저희가 직접 바닥부터 다 구현했던 것은 아니지만 구현을 해보면서 논문의 내용을 더 잘 이해할 수 있었습니다. 논문에 나와있지 않은 여러 노하우가 필요한 점들도 많았습니다. 역시 코드로 보고 성능 재현이 되어야 제대로 알고리즘을 이해하게 된다는 것을 다시 느낀 시간이었습니다. 또한 강화학습은 역시 환경세팅이 어렵다는 생각을 했습니다. 하지만 unity ml-agent를 사용해보면서 앞으로 강화학습 환경으로서 가능성이 상당히 크다는 생각을 했습니다. 또한 구현팀과 슬랙, 깃헙으로 협업하면서 온라인 협업에 대해서 더 배워가는 것 같습니다. 아직은 익숙하지 않지만 앞으로는 마치 바로 옆에서 같이 코딩하는 것 같이 될 거라고 생각합니다.</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"PG-Travel-implementation-story\"><a href=\"#PG-Travel-implementation-story\" class=\"headerlink\" title=\"PG Travel implementation story\"></a>PG Travel implementation story</h1><ul>\n<li>구현 코드 링크 : <a href=\"https://github.com/reinforcement-learning-kr/pg_travel\" target=\"_blank\" rel=\"noopener\">https://github.com/reinforcement-learning-kr/pg_travel</a></li>\n</ul>\n<p>피지여행 프로젝트에서는 다음 7개 논문을 살펴보았습니다. 각 논문에 대한 리뷰는 이전 글들에서 다루고 있습니다. </p>\n<p><a name=\"1\"></a></p>\n<ul>\n<li>[1] R. Sutton, et al., “Policy Gradient Methods for Reinforcement Learning with Function Approximation”, NIPS 2000.<br><a name=\"2\"></a></li>\n<li>[2] D. Silver, et al., “Deterministic Policy Gradient Algorithms”, ICML 2014.<br><a name=\"3\"></a></li>\n<li>[3] T. Lillicrap, et al., “Continuous Control with Deep Reinforcement Learning”, ICLR 2016.<br><a name=\"4\"></a></li>\n<li>[4] S. Kakade, “A Natural Policy Gradient”, NIPS 2002.<br><a name=\"5\"></a></li>\n<li>[5] J. Schulman, et al., “Trust Region Policy Optimization”, ICML 2015.<br><a name=\"6\"></a></li>\n<li>[6] J. Schulman, et al., “High-Dimensional Continuous Control using Generalized Advantage Estimation”, ICLR 2016.<br><a name=\"7\"></a></li>\n<li>[7] J. Schulman, et al., “Proximal Policy Optimization Algorithms”, arXiv, <a href=\"https://arxiv.org/pdf/1707.06347.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1707.06347.pdf</a>.</li>\n</ul>\n<p>강화학습 알고리즘을 이해하는데 있어서 논문을 보고 이론적인 부분을 알아가는 것이 좋습니다. 하지만 실제 코드로 돌아가는 것은 논문만 보고는 알 수 없는 경우가 많습니다. 따라서 피지여행 프로젝트에서는 위 7개 논문 중에 DPG와 DDPG를 제외한 알고리즘을 구현해보았습니다. 구현한 알고리즘은 다음 4개입니다. 이 때, TRPO와 PPO 구현에는 GAE(General Advantage Estimator)가 함께 들어갑니다. </p>\n<ul>\n<li>Vanilla Policy Gradient [<a href=\"#1\">1</a>]</li>\n<li>TNPG(Truncated Natural Policy Gradient) [<a href=\"#4\">4</a>]</li>\n<li>TRPO(Trust Region Policy Optimization) [<a href=\"#5\">5</a>]</li>\n<li>PPO(Proximal Policy Optimization) [<a href=\"#7\">7</a>].</li>\n</ul>\n<p>바닥부터 저희가 구현한 것은 아니며 다음 코드들을 참고해서 구현하였습니다. Vanilla PG의 경우 RLCode의 깃헙을 참고하였습니다.</p>\n<ul>\n<li><a href=\"https://github.com/openai/baselines/tree/master/baselines/trpo_mpi\" target=\"_blank\" rel=\"noopener\">OpenAI Baseline</a></li>\n<li><a href=\"https://github.com/ikostrikov/pytorch-trpo\" target=\"_blank\" rel=\"noopener\">Pytorch implemetation of TRPO</a></li>\n<li><a href=\"https://github.com/rlcode/reinforcement-learning-kr/tree/master/2-cartpole/2-actor-critic\" target=\"_blank\" rel=\"noopener\">RLCode Actor-Critic</a></li>\n</ul>\n<p>GAE와 TRPO, PPO 논문에서는 Mujoco라는 물리 시뮬레이션을 학습 환경으로 사용합니다. 따라서 저희도 Mujoco로 처음 시작을 하였습니다. 하지만 Mujoco는 1달만 무료이고 그 이후부터 유료이며 확장성이 떨어집니다. Unity ml-agent는 기존 Unity를 그대로 사용하면서 쉽게 강화학습 에이전트를 붙일 수 있도록 설계되어 있습니다. Unity ml-agent에서는 저희가 살펴본 알고리즘 중에 가장 최신 알고리즘은 PPO를 적용해봤습니다. 기본적으로 제공하는 환경 이외에 저희가 customize 한 환경에서도 학습해봤습니다.  </p>\n<ul>\n<li>mujoco-py: <a href=\"https://github.com/openai/mujoco-py\" target=\"_blank\" rel=\"noopener\">https://github.com/openai/mujoco-py</a></li>\n<li>Unity ml-agent: <a href=\"https://github.com/Unity-Technologies/ml-agents\" target=\"_blank\" rel=\"noopener\">https://github.com/Unity-Technologies/ml-agents</a></li>\n</ul>\n<p>코드를 구현하고 환경에서 학습을 시키면서 여러가지 이슈들이 있었고 해결해내가는 과정이 있었습니다. 그 과정을 간단히 정리해서 공유하면 PG를 공부하는 분들께 도움일 될 것 같습니다. 저희가 구현한 순서대로 1. Mujoco 학습 2. Unity ml-agent 학습 3. Unity Curved Surface 로 이 포스트가 진행됩니다.</p>\n<p><br></p>\n<h2 id=\"1-Mujoco-학습\"><a href=\"#1-Mujoco-학습\" class=\"headerlink\" title=\"1. Mujoco 학습\"></a>1. Mujoco 학습</h2><p>일명 “Continuous control” 문제는 action이 discrete하지 않고 continuous한 경우를 말합니다. Mujoco는 continuous control에 강화학습을 적용한 논문들이 애용하는 시뮬레이터입니다. 저희가 리뷰한 논문 중에서도 TRPO, PPO, GAE에서 Mujoco를 사용합니다. 따라서 저희가 처음 피지여행 알고리즘을 적용한 환경으로 Mujoco를 선택했습니다. </p>\n<p>Mujoco에는 Ant, HalfCheetah, Hopper, Humanoid, HumanoidStandup, InvertedPendulum, Reacher, Swimmer, Walker2d 과 같은 환경이 있습니다. 그 중에서 Hopper에 맞춰서 학습이 되도록 코드를 구현하였습니다. Mujoco 설치와 관련된 내용은 Wiki에 있습니다.</p>\n<p><br></p>\n<h3 id=\"1-1-Hopper\"><a href=\"#1-1-Hopper\" class=\"headerlink\" title=\"1.1 Hopper\"></a>1.1 Hopper</h3><p>Hopper는 외다리로 뛰어가는 것을 학습하는 것이 목표입니다. Hopper는 다음과 같이 생겼습니다.<br><img src=\"https://www.dropbox.com/s/wjxrelxyp014j3g/Screenshot%202018-08-23%2000.55.54.png?dl=1\"></p>\n<p>환경을 이해하려면 환경의 상태와 행동, 보상 그리고 학습하고 싶은 목표를 알아야합니다. </p>\n<ul>\n<li>상태 : 관절의 위치, 각도, 각속도</li>\n<li>행동 : 관절의 가해지는 토크</li>\n<li>보상 : 앞으로 나아가는 속도</li>\n<li>목표 : 최대한 앞으로 많이 나아가기</li>\n</ul>\n<p>즉 에이전트는 time step마다 관절의 위치와 각도를 받아와서 그 상태에서 어떻게 움직여야 앞으로 나아갈 수 있는지를 학습해야 합니다. 행동은 discrete action이 아닌 continuous action으로 -1과 1사이의 값을 가집니다. 만약 행동이 -1이라면 해당 관절에 시계반대방향으로 토크를 주는 것이고 행동이 +1이라면 해당 관절에 시계방향으로 토크를 주는 것입니다. </p>\n<p>continuous action을 주는 방법은 네트워크(Actor)의 output layer에서 activation function으로 tanh와 같은 것을 사용해서 continuous한 값을 출력하는 것이 있습니다. 하지만 피지여행 코드 구현에서는 action을 gaussian distribution에서 sampling 하였습니다. 이렇게 하면 분산을 일정하게 유지하면서 지속적인 exploration을 할 수 있습니다. 간단하게 그림으로 보자면 다음과 같습니다.<br><img src=\"https://www.dropbox.com/s/94g01zdxyf5oxu1/Screenshot%202018-08-23%2001.20.21.png?dl=1\"></p>\n<p>네트워크 구조와 행동을 선택하는 부분은 다음과 같습니다. Hidden Layer의 activation function으로 tanh를 사용했으며(ReLU를 테스트해보지는 않았습니다. 기존 TRPO, PPO 구현들과 논문에서 tanh를 사용하기 때문에 저희도 사용했습니다. 뒤에 유니티 환경에서는 Swish라는 것을 사용합니다.) log std를 0으로 고정함으로서 일정한 폭을 가지는 분포를 만들어낼 수 있습니다. 이 분포로부터 action을 sampling 합니다.</p>\n<p><img src=\"https://www.dropbox.com/s/xfl9zxies0lmpm1/Screenshot%202018-08-23%2001.20.44.png?dl=1\"></p>\n<p><br></p>\n<h3 id=\"1-2-Vanilla-PG\"><a href=\"#1-2-Vanilla-PG\" class=\"headerlink\" title=\"1.2 Vanilla PG\"></a>1.2 Vanilla PG</h3><p>Vanilla PG는 Actor-Critic의 가장 간단한 형태입니다. Vanilla PG는 이후의 구현에 대한 baseline이 됩니다. 구현이 가장 간단하면서 학습이 안되는 것은 아닙니다. 따라서 코드 전체 구조를 잡는데 Vanilla PG를 짜는 것이 도움이 됩니다. 전반적인 코드 구조는 다음과 같습니다.</p>\n<ul>\n<li>iteration 마다 일정한 step 수만큼 환경에서 진행해서 샘플을 모은다</li>\n<li>모은 샘플로 Actor와 Critic을 학습한다</li>\n<li>반복한다</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">episodes = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> iter <span class=\"keyword\">in</span> range(<span class=\"number\">15000</span>):</span><br><span class=\"line\">    actor.eval(), critic.eval()</span><br><span class=\"line\">    memory = deque()</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">while</span> steps &lt; <span class=\"number\">2048</span>:</span><br><span class=\"line\">        episodes += <span class=\"number\">1</span></span><br><span class=\"line\">        state = env.reset()</span><br><span class=\"line\">        state = running_state(state)</span><br><span class=\"line\">        score = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> range(<span class=\"number\">10000</span>):</span><br><span class=\"line\">            mu, std, _ = actor(torch.Tensor(state).unsqueeze(<span class=\"number\">0</span>))</span><br><span class=\"line\">            action = get_action(mu, std)[<span class=\"number\">0</span>]</span><br><span class=\"line\">            next_state, reward, done, _ = env.step(action)</span><br><span class=\"line\">            next_state = running_state(next_state)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> done:</span><br><span class=\"line\">                mask = <span class=\"number\">0</span></span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                mask = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">            memory.append([state, action, reward, mask])</span><br><span class=\"line\">            state = next_state</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> done:</span><br><span class=\"line\">                <span class=\"keyword\">break</span></span><br><span class=\"line\">                </span><br><span class=\"line\">    actor.train(), critic.train()</span><br><span class=\"line\">    train_model(actor, critic, memory, actor_optim, critic_optim)</span><br></pre></td></tr></table></figure>\n<p>memory에 sample을 저장할 때 sample은 state와 action, reward, mask(마지막 state일 경우 0 나머지 1)입니다. mask의 경우 뒤에서 return이나 advantage를 계산할 때 사용됩니다. 또 하나 염두에 두어야할 것은 running_state 입니다. running_state는 input으로 들어오는 state의 scale이 일정하지 않기 때문에 사용합니다. 즉 state의 각 dimension을 평균 0 분산 1로 standardization 하는 것입니다. 따라서 모델을 저장할 때 각 dimension 마다의 평균과 분산도 같이 저장해서 테스트할 때 불러와서 사용해야 합니다.</p>\n<p>Vanilla PG의 경우 학습 부분이 상당히 간단합니다. 다음 코드를 보시면 메모리에서 state, action, reward, mask를 꺼냅니다. reward와 mask를 통해 return을 구할 수 있고 이 return을 통해 actor를 업데이트 할 수 있습니다 (REINFORCE 알고리즘을 떠올리시면 됩니다). 여기서 critic이 하는 일은 없지만 뒤의 알고리즘들과 코드의 통일성을 위해 fake로 넣어놨습니다. Return은 평균을 빼고 분산으로 나눠서 standardize 합니다. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train_model</span><span class=\"params\">(actor, critic, memory, actor_optim, critic_optim)</span>:</span></span><br><span class=\"line\">    memory = np.array(memory)</span><br><span class=\"line\">    states = np.vstack(memory[:, <span class=\"number\">0</span>])</span><br><span class=\"line\">    actions = list(memory[:, <span class=\"number\">1</span>])</span><br><span class=\"line\">    rewards = list(memory[:, <span class=\"number\">2</span>])</span><br><span class=\"line\">    masks = list(memory[:, <span class=\"number\">3</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    returns = get_returns(rewards, masks)</span><br><span class=\"line\">    train_critic(critic, states, returns, critic_optim)</span><br><span class=\"line\">    train_actor(actor, returns, states, actions, actor_optim)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> returns</span><br></pre></td></tr></table></figure>\n<p>이 코드로 Hopper 환경에서 학습한 그래프는 다음과 같습니다.<br><img src=\"https://www.dropbox.com/s/asoysfuk76zs1dk/Screenshot%202018-08-23%2001.30.58.png?dl=1\"></p>\n<p><br></p>\n<h3 id=\"1-3-TNPG\"><a href=\"#1-3-TNPG\" class=\"headerlink\" title=\"1.3 TNPG\"></a>1.3 TNPG</h3><p>NPG를 이용한 parameter update 식은 다음과 같습니다. </p>\n<p>$$\\bar{w}=F(\\theta)^{-1}\\nabla\\eta(\\theta)$$</p>\n<p>NPG를 구현하려면 KL-divergence의 Hessian의 inverse를 구해야하는 문제가 생깁니다. 현재와 같이 Deep Neural Network를 쓰는 경우에 Hessian의 inverse를 직접적으로 구하는 것은 computationally inefficient 합니다. 따라서 직접 구하지 않고 Conjugate gradient 방법을 사용해서 Fisher Vector Product ($$F^{-1}g$$)를 구합니다. 이러한 알고리즘을 Truncated Natural Policy Gradient(TNPG)라고 부릅니다. </p>\n<p>TNPG에서 parameter update를 구하는 과정은 다음과 같습니다. </p>\n<ol>\n<li>Return 구하기</li>\n<li>Critic 학습하기</li>\n<li>logp * return –&gt; loss 구하기</li>\n<li>loss의 미분과 kl-divergence의 2차 미분을 통해 step direction 구하기</li>\n<li>구한 step direction으로 parameter update</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train_model</span><span class=\"params\">(actor, critic, memory, actor_optim, critic_optim)</span>:</span></span><br><span class=\"line\">    memory = np.array(memory)</span><br><span class=\"line\">    states = np.vstack(memory[:, <span class=\"number\">0</span>])</span><br><span class=\"line\">    actions = list(memory[:, <span class=\"number\">1</span>])</span><br><span class=\"line\">    rewards = list(memory[:, <span class=\"number\">2</span>])</span><br><span class=\"line\">    masks = list(memory[:, <span class=\"number\">3</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># ----------------------------</span></span><br><span class=\"line\">    <span class=\"comment\"># step 1: get returns</span></span><br><span class=\"line\">    returns = get_returns(rewards, masks)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># ----------------------------</span></span><br><span class=\"line\">    <span class=\"comment\"># step 2: train critic several steps with respect to returns</span></span><br><span class=\"line\">    train_critic(critic, states, returns, critic_optim)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># ----------------------------</span></span><br><span class=\"line\">    <span class=\"comment\"># step 3: get gradient of loss and hessian of kl</span></span><br><span class=\"line\">    loss = get_loss(actor, returns, states, actions)</span><br><span class=\"line\">    loss_grad = torch.autograd.grad(loss, actor.parameters())</span><br><span class=\"line\">    loss_grad = flat_grad(loss_grad)</span><br><span class=\"line\">    step_dir = conjugate_gradient(actor, states, loss_grad.data, nsteps=<span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># ----------------------------</span></span><br><span class=\"line\">    <span class=\"comment\"># step 4: get step direction and step size and update actor</span></span><br><span class=\"line\">    params = flat_params(actor)</span><br><span class=\"line\">    new_params = params + <span class=\"number\">0.5</span> * step_dir</span><br><span class=\"line\">    update_model(actor, new_params)</span><br></pre></td></tr></table></figure>\n<p>conjugate gradient 코드는 OpenAI baseline에서 가져왔습니다. 이 코드는 원래 John schulmann 개인 repository에 있는 그대로 사용하는 것입니다. nsteps 만큼 iterataion을 반복하며 결국 x를 구하는 것인데 이 x가 step direction 입니다. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># from openai baseline code</span></span><br><span class=\"line\"><span class=\"comment\"># https://github.com/openai/baselines/blob/master/baselines/common/cg.py</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">conjugate_gradient</span><span class=\"params\">(actor, states, b, nsteps, residual_tol=<span class=\"number\">1e-10</span>)</span>:</span></span><br><span class=\"line\">    x = torch.zeros(b.size())</span><br><span class=\"line\">    r = b.clone()</span><br><span class=\"line\">    p = b.clone()</span><br><span class=\"line\">    rdotr = torch.dot(r, r)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(nsteps):</span><br><span class=\"line\">        _Avp = fisher_vector_product(actor, states, p)</span><br><span class=\"line\">        alpha = rdotr / torch.dot(p, _Avp)</span><br><span class=\"line\">        x += alpha * p</span><br><span class=\"line\">        r -= alpha * _Avp</span><br><span class=\"line\">        new_rdotr = torch.dot(r, r)</span><br><span class=\"line\">        betta = new_rdotr / rdotr</span><br><span class=\"line\">        p = r + betta * p</span><br><span class=\"line\">        rdotr = new_rdotr</span><br><span class=\"line\">        <span class=\"keyword\">if</span> rdotr &lt; residual_tol:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n<p>fisher_vector_product는 kl-divergence의 2차미분과 어떠한 vector의 곱인데 p는 처음에 gradient 값이었다가 점차 업데이트가 됩니다. kl-divergence의 2차 미분을 구하는 과정은 다음과 같습니다. 일단 kl-divergence를 현재 policy에 대해서 구한 다음에 actor parameter에 대해서 미분합니다. 이렇게 미분한 gradient를 일단 flat하게 핀 다음에 p라는 벡터와 곱해서 하나의 값으로 만듭니다. 그 값을 다시 actor의 parameter로 만듦으로서 따로 KL-divergence의 2차미분을 구하지않고 Fisher vector product를 구할 수 있습니다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fisher_vector_product</span><span class=\"params\">(actor, states, p)</span>:</span></span><br><span class=\"line\">    p.detach()</span><br><span class=\"line\">    kl = kl_divergence(new_actor=actor, old_actor=actor, states=states)</span><br><span class=\"line\">    kl = kl.mean()</span><br><span class=\"line\">    kl_grad = torch.autograd.grad(kl, actor.parameters(), create_graph=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">    kl_grad = flat_grad(kl_grad)  <span class=\"comment\"># check kl_grad == 0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    kl_grad_p = (kl_grad * p).sum()</span><br><span class=\"line\">    kl_hessian_p = torch.autograd.grad(kl_grad_p, actor.parameters())</span><br><span class=\"line\">    kl_hessian_p = flat_hessian(kl_hessian_p)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> kl_hessian_p + <span class=\"number\">0.1</span> * p</span><br></pre></td></tr></table></figure>\n<p>TNPG 학습 결과는 다음과 같습니다.<br><img src=\"https://www.dropbox.com/s/uc4c0s00qbs33nr/Screenshot%202018-08-23%2001.53.17.png?dl=1\"></p>\n<p><br></p>\n<h3 id=\"1-4-TRPO\"><a href=\"#1-4-TRPO\" class=\"headerlink\" title=\"1.4 TRPO\"></a>1.4 TRPO</h3><p>TRPO와 NPG가 다른 점은 surrogate loss 사용과 trust region 입니다. 하지만 실제로 구현해서 학습을 시켜본 결과 trust region을 넘어가서 back tracking line search를 하는 경우는 거의 없습니다. 따라서 주된 변화는 surrogate loss에 있다고 보셔도 됩니다. Surrogate loss에서 advantage function을 사용하는데 본 코드 구현에서는 GAE를 사용하였습니다. TRPO 업데이트 식은 다음과 같습니다. Q function 위치에 GAE가 들어갑니다.</p>\n<p>$$<br>\\begin{align}<br>\\max_\\theta\\quad &amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} },a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\mathrm{old} }(s,a)\\right] \\\\<br>\\mathrm{s.t.\\ }&amp;E_{s\\sim\\rho_{\\theta_\\mathrm{old} }}\\left[D_\\mathrm{KL}\\left(\\pi_{\\theta_\\mathrm{old} }(\\cdot\\vert s) \\parallel \\pi_\\theta(\\cdot\\vert s)\\right)\\right] \\leq \\delta<br>\\end{align}<br>$$</p>\n<p>GAE를 구하는 코드는 다음과 같습니다. GAE는 td-error의 discounted summation이라고 볼 수 있습니다. 마지막에 advants를 standardization 하는 것은 return에서 하는 것과 같은 효과를 봅니다. 하지만 standardization을 안하고 실험을 해보지는 않았습니다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_gae</span><span class=\"params\">(rewards, masks, values)</span>:</span></span><br><span class=\"line\">    rewards = torch.Tensor(rewards)</span><br><span class=\"line\">    masks = torch.Tensor(masks)</span><br><span class=\"line\">    returns = torch.zeros_like(rewards)</span><br><span class=\"line\">    advants = torch.zeros_like(rewards)</span><br><span class=\"line\"></span><br><span class=\"line\">    running_returns = <span class=\"number\">0</span></span><br><span class=\"line\">    previous_value = <span class=\"number\">0</span></span><br><span class=\"line\">    running_advants = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> reversed(range(<span class=\"number\">0</span>, len(rewards))):</span><br><span class=\"line\">        running_returns = rewards[t] + hp.gamma * running_returns * masks[t]</span><br><span class=\"line\">        running_tderror = rewards[t] + hp.gamma * previous_value * masks[t] - \\</span><br><span class=\"line\">                    values.data[t]</span><br><span class=\"line\">        running_advants = running_tderror + hp.gamma * hp.lamda * \\</span><br><span class=\"line\">                          running_advants * masks[t]</span><br><span class=\"line\"></span><br><span class=\"line\">        returns[t] = running_returns</span><br><span class=\"line\">        previous_value = values.data[t]</span><br><span class=\"line\">        advants[t] = running_advants</span><br><span class=\"line\"></span><br><span class=\"line\">    advants = (advants - advants.mean()) / advants.std()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> returns, advants</span><br></pre></td></tr></table></figure>\n<p>Surrogate loss를 구하는 코드는 다음과 같습니다. Advantage function(GAE)를 구하고 나면 이전 policy와 현재 policy 사이의 ratio를 구해서 advantage function에 곱하면 됩니다. 이 때 사실 old policy와 new policy는 값은 같지만 old policy는 clone()이나 detach()를 사용해서 update가 안되게 만들어줍니다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">surrogate_loss</span><span class=\"params\">(actor, advants, states, old_policy, actions)</span>:</span></span><br><span class=\"line\">    mu, std, logstd = actor(torch.Tensor(states))</span><br><span class=\"line\">    new_policy = log_density(torch.Tensor(actions), mu, std, logstd)</span><br><span class=\"line\">    advants = advants.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    surrogate = advants * torch.exp(new_policy - old_policy)</span><br><span class=\"line\">    surrogate = surrogate.mean()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> surrogate</span><br></pre></td></tr></table></figure>\n<p>Actor의 step direction을 구하는 것은 TNPG와 동일합니다. TNPG에서는 step direction으로 바로 업데이트 했지만 TRPO는 다음과 같은 작업을 해줍니다. Full step을 구하는 과정이라고 볼 수 있습니다. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ----------------------------</span></span><br><span class=\"line\"><span class=\"comment\"># step 4: get step direction and step size and full step</span></span><br><span class=\"line\">params = flat_params(actor)</span><br><span class=\"line\">shs = <span class=\"number\">0.5</span> * (step_dir * fisher_vector_product(actor, states, step_dir)</span><br><span class=\"line\">             ).sum(<span class=\"number\">0</span>, keepdim=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">step_size = <span class=\"number\">1</span> / torch.sqrt(shs / hp.max_kl)[<span class=\"number\">0</span>]</span><br><span class=\"line\">full_step = step_size * step_dir</span><br></pre></td></tr></table></figure>\n<p>이렇게 full step을 구하고 나면 Trust region optimization 단계에 들어갑니다. expected improvement는 구한 step 만큼 parameter space에서 움직였을 때 예상되는 performance 변화입니다. 이 값은 kl-divergence와 함께 trust region 안에 있는지 밖에 있는지 판단하는 근거가 됩니다. expected improve는 출발점에서의 gradient * full step으로 구합니다. 그리고 10번을 돌아가며 Back-tracking line search를 실시합니다. 처음에는 full step 만큼 가본 다음에 kl-divergence와 emprovement를 통해 trust region 안이면 루프 탈출, 밖이면 full step을 반만큼 쪼개서 다시 이 과정을 반복합니다.  </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ----------------------------</span></span><br><span class=\"line\"><span class=\"comment\"># step 5: do backtracking line search for n times</span></span><br><span class=\"line\">old_actor = Actor(actor.num_inputs, actor.num_outputs)</span><br><span class=\"line\">update_model(old_actor, params)</span><br><span class=\"line\">expected_improve = (loss_grad * full_step).sum(<span class=\"number\">0</span>, keepdim=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">expected_improve = expected_improve.data.numpy()</span><br><span class=\"line\"></span><br><span class=\"line\">flag = <span class=\"keyword\">False</span></span><br><span class=\"line\">fraction = <span class=\"number\">1.0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">10</span>):</span><br><span class=\"line\">    new_params = params + fraction * full_step</span><br><span class=\"line\">    update_model(actor, new_params)</span><br><span class=\"line\">    new_loss = surrogate_loss(actor, advants, states, old_policy.detach(),</span><br><span class=\"line\">                              actions)</span><br><span class=\"line\">    new_loss = new_loss.data.numpy()</span><br><span class=\"line\">    loss_improve = new_loss - loss</span><br><span class=\"line\">    expected_improve *= fraction</span><br><span class=\"line\">    kl = kl_divergence(new_actor=actor, old_actor=old_actor, states=states)</span><br><span class=\"line\">    kl = kl.mean()</span><br><span class=\"line\"></span><br><span class=\"line\">    print(<span class=\"string\">'kl: &#123;:.4f&#125;  loss improve: &#123;:.4f&#125;  expected improve: &#123;:.4f&#125;  '</span></span><br><span class=\"line\">          <span class=\"string\">'number of line search: &#123;&#125;'</span></span><br><span class=\"line\">          .format(kl.data.numpy(), loss_improve, expected_improve[<span class=\"number\">0</span>], i))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># see https: // en.wikipedia.org / wiki / Backtracking_line_search</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> kl &lt; hp.max_kl <span class=\"keyword\">and</span> (loss_improve / expected_improve) &gt; <span class=\"number\">0.5</span>:</span><br><span class=\"line\">        flag = <span class=\"keyword\">True</span></span><br><span class=\"line\">        <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">    fraction *= <span class=\"number\">0.5</span></span><br></pre></td></tr></table></figure>\n<p>Critic의 학습은 단순히 value function과 return의 MSE error를 계산해서 loss로 잡고 loss를 최소화하도록 학습합니다. TRPO 학습 결과는 다음과 같습니다.<br><img src=\"https://www.dropbox.com/s/rc9hxsx1kvokcrv/Screenshot%202018-08-23%2013.36.51.png?dl=1\"></p>\n<p><br></p>\n<h3 id=\"1-5-PPO\"><a href=\"#1-5-PPO\" class=\"headerlink\" title=\"1.5 PPO\"></a>1.5 PPO</h3><p>PPO의 장점을 꼽으라면 GPU 사용하기 좋고 sample efficiency가 늘어난다는 것입니다. TNPG와 TRPO의 경우 한 번 모은 sample은 모델을 단 한 번 업데이트하는데 사용하지만 PPO의 경우 몇 mini-batch로 epoch를 돌리기 때문입니다. GAE를 사용한다는 것은 같고 Conjugate gradient나 Fisher vector product나 back tracking line search가 다 빠집니다. 대신 loss function clip으로 monotonic improvement를 보장하게 학습합니다. 따라서 코드가 상당히 간단해집니다. </p>\n<p>다음 코드 부분이 PPO의 전체라고 봐도 무방합니다. PPO는 다음과 같은 순서로 학습합니다. </p>\n<ul>\n<li>batch를 random suffling하고 mini batch를 추출</li>\n<li>value function 구하기</li>\n<li>critic loss 구하기 (clip을 사용해도 되고 TRPO와 같이 그냥 학습시켜도 됌)</li>\n<li>surrogate loss 구하기</li>\n<li>surrogate loss clip해서 actor loss 만들기</li>\n<li>actor와 critic 업데이트</li>\n</ul>\n<p>Actor의 loss를 구하는 것은 다음 식의 값을 구하는 것입니다. 이 식을 구하려면 ratio에 한 번 클립하고 loss 값을 한 번 min을 취하면 됩니다.</p>\n<p>$$L^{CLIP}(\\theta) = \\hat{E}_t [min(r_t(\\theta) \\, \\hat{A}_t,  clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\, \\hat{A}_t)]$$</p>\n<p>이 코드 구현에서는 actor와 critic을 따로 모델로 만들어서 따로 따로 업데이트를 하지만 하나로 만든다면 loss로 한 번만 업데이트하면 됩니다. 또한 entropy loss를 최종 loss에 더해서 regularization 효과를 볼 수도 있습니다. Critic loss에 clip 해주는 것은 OpenAI baseline의 ppo2 코드를 참조하였습니다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># step 2: get value loss and actor loss and update actor &amp; critic</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(<span class=\"number\">10</span>):</span><br><span class=\"line\">    np.random.shuffle(arr)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n // hp.batch_size):</span><br><span class=\"line\">        batch_index = arr[hp.batch_size * i: hp.batch_size * (i + <span class=\"number\">1</span>)]</span><br><span class=\"line\">        batch_index = torch.LongTensor(batch_index)</span><br><span class=\"line\">        inputs = torch.Tensor(states)[batch_index]</span><br><span class=\"line\">        returns_samples = returns.unsqueeze(<span class=\"number\">1</span>)[batch_index]</span><br><span class=\"line\">        advants_samples = advants.unsqueeze(<span class=\"number\">1</span>)[batch_index]</span><br><span class=\"line\">        actions_samples = torch.Tensor(actions)[batch_index]</span><br><span class=\"line\">        oldvalue_samples = old_values[batch_index].detach()</span><br><span class=\"line\"></span><br><span class=\"line\">        loss, ratio = surrogate_loss(actor, advants_samples, inputs,</span><br><span class=\"line\">                                     old_policy.detach(), actions_samples,</span><br><span class=\"line\">                                     batch_index)</span><br><span class=\"line\"></span><br><span class=\"line\">        values = critic(inputs)</span><br><span class=\"line\">        clipped_values = oldvalue_samples + \\</span><br><span class=\"line\">                         torch.clamp(values - oldvalue_samples,</span><br><span class=\"line\">                                     -hp.clip_param,</span><br><span class=\"line\">                                     hp.clip_param)</span><br><span class=\"line\">        critic_loss1 = criterion(clipped_values, returns_samples)</span><br><span class=\"line\">        critic_loss2 = criterion(values, returns_samples)</span><br><span class=\"line\">        critic_loss = torch.max(critic_loss1, critic_loss2).mean()</span><br><span class=\"line\"></span><br><span class=\"line\">        clipped_ratio = torch.clamp(ratio,</span><br><span class=\"line\">                                    <span class=\"number\">1.0</span> - hp.clip_param,</span><br><span class=\"line\">                                    <span class=\"number\">1.0</span> + hp.clip_param)</span><br><span class=\"line\">        clipped_loss = clipped_ratio * advants_samples</span><br><span class=\"line\">        actor_loss = -torch.min(loss, clipped_loss).mean()</span><br><span class=\"line\"></span><br><span class=\"line\">        loss = actor_loss + <span class=\"number\">0.5</span> * critic_loss</span><br><span class=\"line\"></span><br><span class=\"line\">        critic_optim.zero_grad()</span><br><span class=\"line\">        loss.backward(retain_graph=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">        critic_optim.step()</span><br><span class=\"line\"></span><br><span class=\"line\">        actor_optim.zero_grad()</span><br><span class=\"line\">        loss.backward()</span><br><span class=\"line\">        actor_optim.step()</span><br></pre></td></tr></table></figure>\n<p>PPO의 학습 결과는 다음과 같습니다.<br><img src=\"https://www.dropbox.com/s/rkxa836ap931kbd/Screenshot%202018-08-23%2013.50.57.png?dl=1\"></p>\n<p><br></p>\n<h2 id=\"2-Unity-ml-agent-학습\"><a href=\"#2-Unity-ml-agent-학습\" class=\"headerlink\" title=\"2. Unity ml-agent 학습\"></a>2. Unity ml-agent 학습</h2><p>Mujoco Hopper(half-cheetah와 같은 것도)에 Vanilla PG, TNPG, TRPO, PPO를 구현해서 적용했습니다. Mujoco의 경우 이미 Hyper parameter와 같은 정보들이 논문이나 블로그에 있기 때문에 상대적으로 continuous control로 시작하기에는 좋습니다. 맨 처음에 말했듯이 Mujoco는 1달만 무료이고 그 이후부터 유료이며 확장성이 떨어집니다. 좀 더 general한 agent를 학습시키기에 좋은 환경이 필요합니다. 따라서 Unity ml-agent를 살펴봤습니다. Repository는 다음과 같습니다. </p>\n<ul>\n<li><a href=\"https://github.com/Unity-Technologies/ml-agents\" target=\"_blank\" rel=\"noopener\">Unity ml-agent repository</a></li>\n<li><a href=\"https://unity3d.com/machine-learning/\" target=\"_blank\" rel=\"noopener\">Unity ml-agent homepage</a></li>\n</ul>\n<p><img src=\"https://www.dropbox.com/s/lapholj8r4nxmb1/Screenshot%202018-08-24%2013.41.31.png?dl=1\"></p>\n<p>현재 Unity ml-agent에서 기본으로 제공하는 환경은 다음과 같습니다. Unity ml-agent는 기존 Unity를 그대로 사용하면서 쉽게 강화학습 에이전트를 붙일 수 있도록 설계되어 있습니다. Unity ml-agent에서는 Walker 환경에서 저희가 살펴본 알고리즘 중에 가장 최신 알고리즘은 PPO를 적용해봤습니다. 이 포스트를 보시는 분들은 이 많은 다른 환경에 자유롭게 저희 코드를 적용할 수 있습니다.</p>\n<ul>\n<li><a href=\"https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md\" target=\"_blank\" rel=\"noopener\">각 환경에 대한 설명</a></li>\n</ul>\n<p><img src=\"https://www.dropbox.com/s/lrbodw5dypxowmw/Screenshot%202018-08-24%2014.06.14.png?dl=1\"></p>\n<p>Unity ml-agent를 이용해서 강화학습을 하기 위해서는 다음과 같이 진행됩니다. 단계별로 설명하겠습니다. </p>\n<ul>\n<li>Unity에서 환경 만들기</li>\n<li>Python에서 unity 환경 불러와서 테스트하기</li>\n<li>기존에 하던대로 학습하기</li>\n</ul>\n<p><br></p>\n<h3 id=\"2-1-Walker-환경-만들기\"><a href=\"#2-1-Walker-환경-만들기\" class=\"headerlink\" title=\"2.1 Walker 환경 만들기\"></a>2.1 Walker 환경 만들기</h3><p>강화학습을 하는 많은 분들이 Unity를 한 번도 다뤄보지 않은 경우가 많습니다. 저도 그런 경우라서 어떻게 환경을 만들어야할지 처음에는 감이 잡히지 않았습니다. 하지만 Unity ml-agent에서는 상당히 자세한 guide를 제공합니다. 다음은 Unity ml-agent의 가장 기본적인 환경인 3DBall에 대한 tutorial입니다. 설치 guide도 제공하고 있으니 참고하시면 될 것 같습니다.</p>\n<ul>\n<li><a href=\"https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Basic-Guide.md\" target=\"_blank\" rel=\"noopener\">3DBall 예제 tutorial</a></li>\n<li><a href=\"https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md\" target=\"_blank\" rel=\"noopener\">Unity ml-agent 설치 guide</a></li>\n</ul>\n<p>Unity ml-agent에서 제공하는 3DBall tutorial을 참고해서 Walker 환경을 만들었습니다. Walker 환경을 만드는 과정을 간단히 말씀드리겠습니다. 다음 그림의 단계들을 동일하므로 따라하시면 됩니다. Unity를 열고 unity-environment로 들어가시면 됩니다.<br><img src=\"https://www.dropbox.com/s/fbdqg781w46a5mz/Screenshot%202018-08-24%2014.50.50.png?dl=1\"></p>\n<p>그러면 화면 하단에서 다음과 같은 것을 볼 수 있습니다. Assets/ML-Agents/Examples로 들어가보면 Walker가 있습니다. Scenes에서 Walker를 더블클릭하면 됩니다.<br><img src=\"https://www.dropbox.com/s/h349xml3faln0wy/Screenshot%202018-08-24%2014.52.14.png?dl=1\"></p>\n<p>더블클릭해서 나온 화면에서 오른쪽 상단의 파란색 화살표를 누르면 환경이 실행이 됩니다. 저희가 학습하고자 하는 agent는 바로 이녀석입니다. 왼쪽 리스트를 보면 WalkerPair가 11개가 있는 것을 볼 수 있습니다. Unity ml-agent 환경은 기본적으로 Multi-agent로 학습하도록 설정되어있습니다. 따라서 여러개의 Walker들이 화면에 보이는 것입니다.<br><img src=\"https://www.dropbox.com/s/cy8m5kqdmkhopjo/Screenshot%202018-08-24%2014.54.57.png?dl=1\"></p>\n<p>리스트 중에 Walker Academy를 클릭해서 그 하위에 있는 WalkerBrain을 더블클릭합니다. 그러면 화면 오른쪽에 다음과 같은 Brain 설정을 볼 수 있습니다. Brain은 쉽게 말해서 Agent라고 생각하면 됩니다. 이 Agent는 상태로 212차원의 vector가 주어지며 다 continuous한 값을 가집니다. 행동은 39개의 행동을 할 수 있으며 다 Continuous입니다. Mujoco에 비해서 상태나 행동의 차원이 상당히 높습니다. 여기서 중요한 것은 Brain Type입니다. Brain type은 internal, external, player, heuristic이 있습니다. player로 type을 설정하고 화면 상단의 play 버튼을 누르면 여러분이 agent를 움직일 수 있습니다. 하지만 Walker는 사람이 움직이는게 거의 불가능하므로 player 기능은 사용할 수 없습니다. 다른 환경에서는 사용해볼 수 있으니 재미로 한 번 플레이해보시면 좋습니다! </p>\n<center><img src=\"https://www.dropbox.com/s/uxfm162f1scbzo5/Screenshot%202018-08-24%2015.09.04.png?dl=1\" width=\"400px\"></center>\n\n<p>이번에는 WalkerPair에서 WalkerAgent를 더블클릭해보겠습니다. 이 설정을 보아 5000 step이 episode의 max step인 것을 볼 수 있습니다.</p>\n<center><img src=\"https://www.dropbox.com/s/r6gwemlczwic2ma/Screenshot%202018-08-24%2015.16.19.png?dl=1\" width=\"400px\"></center>\n\n<p>이제 상단 file menu에서 build setting에 들어갑니다. 환경을 build해서 python 코드에서 import하기 위해서입니다. 물론 unity 환경과 python 코드를 binding해주는 부분은 ml-agent 코드 안에 있습니다. Build 버튼을 누르면 환경이 build가 됩니다.</p>\n<center><img src=\"https://www.dropbox.com/s/4dtgoz1k8896vxs/Screenshot%202018-08-24%2015.19.07.png?dl=1\" width=\"500px\"></center>\n\n\n<p><br></p>\n<h3 id=\"2-2-Python에서-unity-환경-불러와서-테스트하기\"><a href=\"#2-2-Python에서-unity-환경-불러와서-테스트하기\" class=\"headerlink\" title=\"2.2 Python에서 unity 환경 불러와서 테스트하기\"></a>2.2 Python에서 unity 환경 불러와서 테스트하기</h3><p>환경을 build 했으면 build한 환경을 python에서 불러와서 random action으로 테스트해봅니다. 환경을 테스트하는 코드는 pg_travel repository에서 unity 폴더 밑에 있습니다. test_env.py라는 코드는 간단하게 다음과 같습니다. Build한 walker 환경은 env라는 폴더 밑에 넣어줍니다. unityagent를 import하는데 ml-agent를 git clone 해서 python 폴더 내에서 “python setup.py install”을 실행했다면 문제없이 import 됩니다. UnityEnvironment를 통해 env라는 환경을 선언할 수 있습니다. 이렇게 선언하고 나면 gym과 상당히 유사한 형태로 환경과 상호작용이 가능합니다. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> unityagents <span class=\"keyword\">import</span> UnityEnvironment</span><br><span class=\"line\"><span class=\"keyword\">from</span> utils.utils <span class=\"keyword\">import</span> get_action</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    env_name = <span class=\"string\">\"./env/walker_test\"</span></span><br><span class=\"line\">    train_mode = <span class=\"keyword\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\">    env = UnityEnvironment(file_name=env_name)</span><br><span class=\"line\"></span><br><span class=\"line\">    default_brain = env.brain_names[<span class=\"number\">0</span>]</span><br><span class=\"line\">    brain = env.brains[default_brain]</span><br><span class=\"line\">    env_info = env.reset(train_mode=train_mode)[default_brain]</span><br><span class=\"line\"></span><br><span class=\"line\">    num_inputs = brain.vector_observation_space_size</span><br><span class=\"line\">    num_actions = brain.vector_action_space_size</span><br><span class=\"line\">    num_agent = env._n_agents[default_brain]</span><br><span class=\"line\"></span><br><span class=\"line\">    print(<span class=\"string\">'the size of input dimension is '</span>, num_inputs)</span><br><span class=\"line\">    print(<span class=\"string\">'the size of action dimension is '</span>, num_actions)</span><br><span class=\"line\">    print(<span class=\"string\">'the number of agents is '</span>, num_agent)</span><br><span class=\"line\">   </span><br><span class=\"line\">    score = <span class=\"number\">0</span></span><br><span class=\"line\">    episode = <span class=\"number\">0</span></span><br><span class=\"line\">    actions = [<span class=\"number\">0</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(num_actions)] * num_agent</span><br><span class=\"line\">    <span class=\"keyword\">for</span> iter <span class=\"keyword\">in</span> range(<span class=\"number\">1000</span>):</span><br><span class=\"line\">        env_info = env.step(actions)[default_brain]</span><br><span class=\"line\">        rewards = env_info.rewards</span><br><span class=\"line\">        dones = env_info.local_done</span><br><span class=\"line\">        score += rewards[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> dones[<span class=\"number\">0</span>]:</span><br><span class=\"line\">            episode += <span class=\"number\">1</span></span><br><span class=\"line\">            score = <span class=\"number\">0</span></span><br><span class=\"line\">            print(<span class=\"string\">'&#123;&#125;th episode : mean score of 1st agent is &#123;:.2f&#125;'</span>.format(</span><br><span class=\"line\">                episode, score))</span><br></pre></td></tr></table></figure>\n<p>위 코드를 실행하면 다음과 같이 실행창에 뜹니다. External brain인 것을 알 수 있고 default_brain은 brain 중에 하나만 가져왔기 때문에 number of brain은 1이라고 출력합니다. input dimension은 212이고 action dimension은 39이고 agent 수는 11인 것으로봐서 제대로 환경이 불러와진 것을 확인할 수 있습니다.<br><img src=\"https://www.dropbox.com/s/cioa9h7qu25vonz/Screenshot%202018-08-24%2015.47.43.png?dl=1\"></p>\n<p>이 환경에서 행동하려면 agent 숫자만큼 행동을 줘야합니다. 모두 0로 행동을 주고 실행하면 다음과 같이 뒤로 넘어지는 행동을 반복합니다. env.step(actions)[default_brain]으로 env_info를 받아오면 거기서부터 reward와 done, next_state를 받아올 수 있습니다. 이제 학습하기만 하면 됩니다.<br><img src=\"https://www.dropbox.com/s/8qrmxoski6p4n07/Screenshot%202018-08-24%2016.00.21.png?dl=1\"></p>\n<p><br></p>\n<h3 id=\"2-3-Walker-학습하기\"><a href=\"#2-3-Walker-학습하기\" class=\"headerlink\" title=\"2.3 Walker 학습하기\"></a>2.3 Walker 학습하기</h3><p>기존에 Mujoco에 적용했던 PPO 코드를 그대로 Walker에 적용하니 잘 학습이 안되었습니다. 다음 사진이 저희가 중간 해커톤으로 모여서 이 상황을 공유할 때의 사진입니다.<br><img src=\"https://i.imgur.com/1aR2Z77.png\" width=\"500px\"></p>\n<p>Unity ml-agent에서는 PPO를 기본 agent로 제공합니다. 학습 코드도 제공하기 때문에 mujoco에 적용했던 코드와의 차이점을 분석할 수 있었습니다. mujoco 코드와 ml-agent baseline 코드의 차이점은 다음과 같습니다. </p>\n<ul>\n<li>agent 여러개를 이용, 별개의 memory에 저장한 후에 gradient를 합침</li>\n<li>GAE 및 time horizon 등 hyper parameter가 다름</li>\n<li>Actor와 Critic의 layer가 1층 더 두꺼우며 hidden layer 자체의 사이즈도 더 큼</li>\n<li>hidden layer의 activation function이 tanh가 아닌 swish</li>\n</ul>\n<p>ml-agent baseline 코드리뷰할 때 작성했던 마인드맵은 다음과 같습니다.<br><img src=\"https://i.imgur.com/YeaEntG.png\"></p>\n<p>크게는 두 가지를 개선해서 성능이 많이 향상했습니다.</p>\n<ol>\n<li>Network 수정</li>\n<li>multi-agent를 활용해서 학습</li>\n</ol>\n<p>Network 코드는 다음과 같습니다. Hidden Layer를 하나 더 늘렸으며 swish activation function을 사용할 수 있도록 변경했습니다. 사실 swish라는 activation function은 처음 들어보는 생소한 함수였습니다. 하지만 ml-agent baseline에서 사용했다는 사실과 구현이 상당히 간단하다는 점에서 저희 코드에 적용했습니다. 단순히 x * sigmoid(x) 를 하면 됩니다. swish는 별거 아닌 것 같지만 상당한 성능 개선을 가져다줬습니다. 사실 ReLU나 ELU 등 여러 다른 activation function을 적용해서 비교해보는게 best긴 하지만 시간 관계상 그렇게까지 테스트해보지는 못했습니다. 기존에 TRPO나 PPO는 왜 tanh를 사용했었는지도 의문인 점입니다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Actor</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, num_inputs, num_outputs, args)</span>:</span></span><br><span class=\"line\">        self.args = args</span><br><span class=\"line\">        self.num_inputs = num_inputs</span><br><span class=\"line\">        self.num_outputs = num_outputs</span><br><span class=\"line\">        super(Actor, self).__init__()</span><br><span class=\"line\">        self.fc1 = nn.Linear(num_inputs, args.hidden_size)</span><br><span class=\"line\">        self.fc2 = nn.Linear(args.hidden_size, args.hidden_size)</span><br><span class=\"line\">        self.fc3 = nn.Linear(args.hidden_size, args.hidden_size)</span><br><span class=\"line\">        self.fc4 = nn.Linear(args.hidden_size, num_outputs)</span><br><span class=\"line\"></span><br><span class=\"line\">        self.fc4.weight.data.mul_(<span class=\"number\">0.1</span>)</span><br><span class=\"line\">        self.fc4.bias.data.mul_(<span class=\"number\">0.0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.args.activation == <span class=\"string\">'tanh'</span>:</span><br><span class=\"line\">            x = F.tanh(self.fc1(x))</span><br><span class=\"line\">            x = F.tanh(self.fc2(x))</span><br><span class=\"line\">            x = F.tanh(self.fc3(x))</span><br><span class=\"line\">            mu = self.fc4(x)</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> self.args.activation == <span class=\"string\">'swish'</span>:</span><br><span class=\"line\">            x = self.fc1(x)</span><br><span class=\"line\">            x = x * F.sigmoid(x)</span><br><span class=\"line\">            x = self.fc2(x)</span><br><span class=\"line\">            x = x * F.sigmoid(x)</span><br><span class=\"line\">            x = self.fc3(x)</span><br><span class=\"line\">            x = x * F.sigmoid(x)</span><br><span class=\"line\">            mu = self.fc4(x)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">raise</span> ValueError</span><br><span class=\"line\"></span><br><span class=\"line\">        logstd = torch.zeros_like(mu)</span><br><span class=\"line\">        std = torch.exp(logstd)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> mu, std, logstd</span><br></pre></td></tr></table></figure>\n<p>swish와 tanh를 사용한 학습을 비교한 그래프입니다. 하늘색 그래프가 swish를 사용한 결과, 파란색이 tanh를 사용한 결과입니다. score는 episode 마다의 reward의 합입니다.</p>\n<center><img src=\"https://www.dropbox.com/s/3d07c1kql4h5oqk/Screenshot%202018-08-24%2016.33.45.png?dl=1\" width=\"350px\"></center>\n\n<p>이제 multi-agent로 학습하도록 변경하면 됩니다. PPO의 경우 memory에 time horizon 동안의 sample을 시간순서대로 저장하고 GAE를 구한 이후에 minibatch로 추출해서 학습합니다. 따라서 여러개의 agent로 학습하기 위해서는 memory를 따로 만들어서 각각의 GAE를 구해서 학습해야합니다. Unity에서는 Mujoco에서 했던 것처럼 deque로 memory를 만들지 않고 따로 named tuple로 구현한 memory class를 import 해서 사용했습니다. utils 폴더 밑에 memory.py 코드에 구현되어있으며 코드는 <a href=\"https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb\" target=\"_blank\" rel=\"noopener\">https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb</a><br>에서 가져왔습니다. </p>\n<p>state, action, reward, mask를 저장하는데 불러올 때 각각을 따로 불러올 수 있기 때문에 비효율적 시간을 많이 줄여줍니다.<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Transition = namedtuple(<span class=\"string\">'Transition'</span>, (<span class=\"string\">'state'</span>, <span class=\"string\">'action'</span>, <span class=\"string\">'reward'</span>, <span class=\"string\">'mask'</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Memory</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        self.memory = []</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">push</span><span class=\"params\">(self, state, action, reward, mask)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">\"\"\"Saves a transition.\"\"\"</span></span><br><span class=\"line\">        self.memory.append(Transition(state, action, reward, mask))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sample</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> Transition(*zip(*self.memory))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__len__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> len(self.memory)</span><br></pre></td></tr></table></figure></p>\n<p>main.py 에서는 이 memory를 agent의 개수만큼 생성합니다. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">memory = [Memory() <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> range(num_agent)]</span><br></pre></td></tr></table></figure>\n<p>sample을 저장할 때도 agent마다 따로 따로 저장합니다. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(num_agent):</span><br><span class=\"line\">    memory[i].push(states[i], actions[i], rewards[i], masks[i])</span><br></pre></td></tr></table></figure>\n<p>time horizon이 끝나면 모은 sample 들을 가지고 학습하기 위한 값으로 만드는 과정을 진행합니다. 각각의 memory를 가지고 GAE와 old_policy, old_value 등을 계산해서 하나의 batch로 합칩니다. 그렇게 train_model 메소드에 전달하면 기존과 동일하게 agent를 업데이트합니다.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sts, ats, returns, advants, old_policy, old_value = [], [], [], [], [], []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(num_agent):</span><br><span class=\"line\">    batch = memory[i].sample()</span><br><span class=\"line\">    st, at, rt, adv, old_p, old_v = process_memory(actor, critic, batch, args)</span><br><span class=\"line\">    sts.append(st)</span><br><span class=\"line\">    ats.append(at)</span><br><span class=\"line\">    returns.append(rt)</span><br><span class=\"line\">    advants.append(adv)</span><br><span class=\"line\">    old_policy.append(old_p)</span><br><span class=\"line\">    old_value.append(old_v)</span><br><span class=\"line\"></span><br><span class=\"line\">sts = torch.cat(sts)</span><br><span class=\"line\">ats = torch.cat(ats)</span><br><span class=\"line\">returns = torch.cat(returns)</span><br><span class=\"line\">advants = torch.cat(advants)</span><br><span class=\"line\">old_policy = torch.cat(old_policy)</span><br><span class=\"line\">old_value = torch.cat(old_value)</span><br><span class=\"line\"></span><br><span class=\"line\">train_model(actor, critic, actor_optim, critic_optim, sts, ats, returns, advants,</span><br><span class=\"line\">            old_policy, old_value, args)</span><br></pre></td></tr></table></figure>\n<p>이렇게 학습한 에이전트는 다음과 같이 걷습니다. 이렇게 walker를 학습시키고 나니 어떻게 하면 사람처럼 자연스럽게 걷는 것을 agent 스스로 학습할 수 있을까라는 고민을 하게 되었습니다.</p>\n<center><img src=\"https://www.dropbox.com/s/fyz1kn5v92l3rrk/plane-595.gif?dl=1\"></center>\n\n<p>Unity ml-agent에서 제공하는 pretrained된 모델을 다음과 같이 걷습니다. 저희가 학습한 agent와 상당히 다르게 걷는데 왜 그런 차이가 나는지도 분석하고 싶습니다. </p>\n<center><img src=\"https://www.dropbox.com/s/xwz766g7c4eiaia/plane-unity.gif?dl=1\"></center>\n\n\n<p><br></p>\n<h2 id=\"3-Unity-Curved-Surface-제작-및-학습기\"><a href=\"#3-Unity-Curved-Surface-제작-및-학습기\" class=\"headerlink\" title=\"3. Unity Curved Surface 제작 및 학습기\"></a>3. Unity Curved Surface 제작 및 학습기</h2><p>Unity ml-agent에서 제공하는 기본 Walker 환경에서 학습하고 나니 바닥을 조금 울퉁불퉁하게 혹은 경사가 진 곳에서 걷는 것을 학습해보고 싶다라는 생각이 들었습니다. 따라서 간단하게 걷는 배경을 다르게 하는 시도를 해봤습니다. </p>\n<p><br></p>\n<h3 id=\"3-1-Curved-Surface-만들기\"><a href=\"#3-1-Curved-Surface-만들기\" class=\"headerlink\" title=\"3.1 Curved Surface 만들기\"></a>3.1 Curved Surface 만들기</h3><p>Agent가 걸어갈 배경을 처음부터 만드는 것보다 구할 수 있다면 만들어진 배경을 구하기로 했습니다. Unity를 무료라는 점에서 선택했듯이 배경을 무료로 구할 수 있는 방법을 선택했습니다. </p>\n<center><img src=\"https://www.dropbox.com/s/e0tsp3e3c9uq2zh/Screenshot%202018-08-23%2000.19.14.png?dl=1\"></center>\n\n<p>무료로 공개되어있는 Unity 배경 중에서 Curved Ground 라는 것을 가져와서 작업하기로 했습니다. 이 환경 같은 경우 spline을 그리듯이 중간의 점을 이동시키면서 사용자가 곡면을 수정할 수 있습니다.</p>\n<center><img src=\"https://www.dropbox.com/s/3ppmxotrf6qhzaf/Screenshot%202018-08-23%2000.20.25.png?dl=1\"></center>\n\n<p>간단하게 곡면을 만들어서 공을 굴려보면 다음과 같이 잘 굴러갑니다. </p>\n<center><img src=\"https://www.dropbox.com/s/2e8yqvqj1a4th27/slope_walker_ball.gif?dl=1\"></center>\n\n<p>여러 에이전트가 학습할 수 있도록 오목한 경사면을 제작했습니다. 초반의 모습은 다음과 같았습니다.<br><img src=\"https://www.dropbox.com/s/m492xsfp4bolmz5/Screenshot%202018-08-23%2000.36.06.png?dl=1\"></p>\n<p>하지만 최종으로는 다음과 같은 곡면으로 사용했습니다. 위 사진의 배경과 아래 사진의 배경이 다른 점은 slope 길이, 내리막 경사, 오르막 경사입니다. Slope 길이의 경우 길이를 기존 plane 과 동일하게 했더니, 오르막 올라가는 부분이 학습이 잘 안 되었습니다. 따라서 길이를 줄였습니다. 내리막 경사의 경우 너무 경사지면 학습이 잘 안 되고, 너무 완만하니 내리막 티가 잘 안 나기 때문에 적절한 경사를 설정했습니다. 오르막 경사의 경우 내리막보다는 오르막이 더 어려울 것이라고 판단해서 오르막 경사를 낮게 설정했습니다.<br><img src=\"https://www.dropbox.com/s/idbov4wtd6jeqb2/Screenshot%202018-08-23%2000.36.54.png?dl=1\"></p>\n<p><br></p>\n<h3 id=\"3-2-Curved-Surface에서-학습하기\"><a href=\"#3-2-Curved-Surface에서-학습하기\" class=\"headerlink\" title=\"3.2 Curved Surface에서 학습하기\"></a>3.2 Curved Surface에서 학습하기</h3><p>위 환경으로 학습을 할 때, agent가 너무 초반에 빨리 쓰러지는 현상이 발생했습니다. 혹시 발의 각도가 문제일까 싶어서 발 각도를 변경해보았습니다. </p>\n<center><img src=\"https://www.dropbox.com/s/znvikbeoj7gku0u/Screenshot%202018-08-23%2000.38.22.png?dl=1\" width=\"400px\"></center>\n\n<p>하지만 역시 평지에서 걷는 것처럼 걷도록 학습이 안되었습니다. 이 환경에서 더 잘 학습하려면 더 여러가지를 시도해봐야할 것 같습니다. (그래도 걷는 게 기특합니다..)</p>\n<center><img src=\"https://www.dropbox.com/s/4fqpsdmnzvnvia0/curved-736.gif?dl=1\"></center>\n\n<p><img src=\"https://www.dropbox.com/s/t5ngr0io4xeex6y/curved-736-overview.gif?dl=1\"></p>\n<p><br></p>\n<h2 id=\"4-구현-후기\"><a href=\"#4-구현-후기\" class=\"headerlink\" title=\"4. 구현 후기\"></a>4. 구현 후기</h2><p>피지여행 구현팀은 총 4명으로 진행했습니다. 각 팀원의 후기를 적어보겠습니다.</p>\n<ul>\n<li>팀원 장수영: 사랑합니다. 행복합니다.</li>\n<li>팀원 공민서: 제가 핵심적인 기능을 구현하지는 못했지만 무조코 설치와 모델 테스트를 맡으면서 딥마인드나 openai의 영상으로만 보던 에이전트의 성장과정을 눈으로 지켜볼 수 있었습니다. 제대로 서있지도 못하던 hopper가 어느정도 훈련이 되고서는 넘어지려하다가도 추진력을 얻기위해 웅크렸다 뛰는 것을 관찰하는 것도 재미있고 육아일기를 보는 아버지의 마음을 조금이나마 이해할 수 있었습니다. 텐서보드를 넣는 걸 깜빡해 일일히 에피소드 별 스코어를 시각화 하면서 텐서보드의 소중함을 알았습니다. 유니티 코드리뷰를 하면서도 시스템 아키텍쳐 설계에 대해서도 배울 점이 있었던 것 같고 swish라는 활성화함수의 존재도 알았었고 curiosity도 알게되었고 역시 다른 사람의 코드를 읽는 것도 많은 공부가 된다고 되새기던 시간이었습니다. 물론 너무 크기가 방대해서 가독성은 많이 떨어졌습니다만 무조코보다 유니티가 훨씬 흥할거라고 생각했습니다. 마지막으로 누구 하나 열정적이지 않은 사람이 없이 치열한 고민을 함께 한 PG여행팀 분들, 저의 부족함과 상생의 기쁨을 알게해주셔서 정말 감사드립니다.</li>\n<li>팀원 양혁렬: 여러 에이전트가 함께하면 더 잘하는 걸 보면서 새삼 좋은 분들과 함께 할 수 있어서 행복했습니다</li>\n<li>팀원 이웅원: 저희가 직접 바닥부터 다 구현했던 것은 아니지만 구현을 해보면서 논문의 내용을 더 잘 이해할 수 있었습니다. 논문에 나와있지 않은 여러 노하우가 필요한 점들도 많았습니다. 역시 코드로 보고 성능 재현이 되어야 제대로 알고리즘을 이해하게 된다는 것을 다시 느낀 시간이었습니다. 또한 강화학습은 역시 환경세팅이 어렵다는 생각을 했습니다. 하지만 unity ml-agent를 사용해보면서 앞으로 강화학습 환경으로서 가능성이 상당히 크다는 생각을 했습니다. 또한 구현팀과 슬랙, 깃헙으로 협업하면서 온라인 협업에 대해서 더 배워가는 것 같습니다. 아직은 익숙하지 않지만 앞으로는 마치 바로 옆에서 같이 코딩하는 것 같이 될 거라고 생각합니다.</li>\n</ul>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cjrujltyd00005wfe5q61p70e","category_id":"cjrujltym00025wfeykwmiki4","_id":"cjrujltyu00085wfejo0rxnbr"},{"post_id":"cjrujltyj00015wfe3i3o2d0k","category_id":"cjrujltym00025wfeykwmiki4","_id":"cjrujltz1000b5wfer0rigvhs"},{"post_id":"cjrujltyo00045wfeol83b6z5","category_id":"cjrujltym00025wfeykwmiki4","_id":"cjrujltz5000e5wfe5qsht8z0"},{"post_id":"cjrujltzn000l5wfeljlyo9o0","category_id":"cjrujltym00025wfeykwmiki4","_id":"cjrujltzs000p5wfes2a53wb0"},{"post_id":"cjrujltzp000m5wfery95psnv","category_id":"cjrujltzs000o5wfe2dkxtr3c","_id":"cjrujltzw000u5wfe5an0hkpr"},{"post_id":"cjrujlu05000z5wfew0licp76","category_id":"cjrujltym00025wfeykwmiki4","_id":"cjrujlu0d00145wfehlu9ml5e"},{"post_id":"cjrujlu0600105wfe1dn7jdsf","category_id":"cjrujltym00025wfeykwmiki4","_id":"cjrujlu0f00175wfezcpr89h9"},{"post_id":"cjrujlu0800125wfeylhwu3ba","category_id":"cjrujltzs000o5wfe2dkxtr3c","_id":"cjrujlu0g00195wfel9k62l6w"},{"post_id":"cjrujlu0u001d5wfehdudraw1","category_id":"cjrujltym00025wfeykwmiki4","_id":"cjrujlu10001j5wfeyn8uprxf"},{"post_id":"cjrujlu0v001e5wfe04dwv4em","category_id":"cjrujltym00025wfeykwmiki4","_id":"cjrujlu13001m5wfenwyv4dmr"},{"post_id":"cjrujlu0x001g5wfen01yjb8o","category_id":"cjrujltym00025wfeykwmiki4","_id":"cjrujlu15001o5wfe4pvoxffa"},{"post_id":"cjrujlu0z001i5wfeqx7nxq57","category_id":"cjrujltym00025wfeykwmiki4","_id":"cjrujlu16001r5wfen50k14pi"},{"post_id":"cjrujlu12001l5wfe9b8501z6","category_id":"cjrujltzs000o5wfe2dkxtr3c","_id":"cjrujlu17001t5wfe28u2seor"},{"post_id":"cjrujlu1f001z5wfeybf5vjkx","category_id":"cjrujltym00025wfeykwmiki4","_id":"cjrujlu1l00255wfew3neeq8s"},{"post_id":"cjrujlu1g00205wfezjgshuze","category_id":"cjrujltym00025wfeykwmiki4","_id":"cjrujlu1p00285wfei41wiorz"},{"post_id":"cjrujlu1i00225wfe77ibd3es","category_id":"cjrujltym00025wfeykwmiki4","_id":"cjrujlu1r002a5wfe5uo91i4j"},{"post_id":"cjrujlu1k00245wfe304hcq04","category_id":"cjrujltym00025wfeykwmiki4","_id":"cjrujlu1s002c5wfe0nrw37jw"},{"post_id":"cjrujlu1n00275wfeuhgo5dxo","category_id":"cjrujltym00025wfeykwmiki4","_id":"cjrujlu1t002e5wfez922ue9k"},{"post_id":"cjrujlu1y002j5wfef2g0snts","category_id":"cjrujltym00025wfeykwmiki4","_id":"cjrujlu27002o5wfetwtqgvbl"},{"post_id":"cjrujlu20002k5wfeutanpeid","category_id":"cjrujltym00025wfeykwmiki4","_id":"cjrujlu28002q5wfedo8t1gzw"},{"post_id":"cjrujlu23002m5wfe8cg7s2g6","category_id":"cjrujltym00025wfeykwmiki4","_id":"cjrujlu2a002s5wfexqsi7fl8"}],"PostTag":[{"post_id":"cjrujltyd00005wfe5q61p70e","tag_id":"cjrujltyo00035wfeltcz8k5e","_id":"cjrujltyy000a5wfen6e1eu28"},{"post_id":"cjrujltyd00005wfe5q61p70e","tag_id":"cjrujltyr00065wfewbrktoll","_id":"cjrujltz2000c5wfe2no26xqx"},{"post_id":"cjrujltyj00015wfe3i3o2d0k","tag_id":"cjrujltyo00035wfeltcz8k5e","_id":"cjrujltz7000g5wfe4wkozhbl"},{"post_id":"cjrujltyj00015wfe3i3o2d0k","tag_id":"cjrujltz3000d5wfeq12p8wum","_id":"cjrujltz7000h5wfelntquygs"},{"post_id":"cjrujltyo00045wfeol83b6z5","tag_id":"cjrujltyo00035wfeltcz8k5e","_id":"cjrujltz8000j5wfexs6n2qjm"},{"post_id":"cjrujltyo00045wfeol83b6z5","tag_id":"cjrujltz8000i5wfe2dyrsse8","_id":"cjrujltz9000k5wfeyt8ysw87"},{"post_id":"cjrujltzn000l5wfeljlyo9o0","tag_id":"cjrujltyo00035wfeltcz8k5e","_id":"cjrujltzu000r5wfeoypm4yah"},{"post_id":"cjrujltzn000l5wfeljlyo9o0","tag_id":"cjrujltzr000n5wfemwpzi7yu","_id":"cjrujltzv000s5wfecgxh1gy8"},{"post_id":"cjrujltzp000m5wfery95psnv","tag_id":"cjrujltzt000q5wfe0khqsxxn","_id":"cjrujltzz000w5wfeymssg3ci"},{"post_id":"cjrujltzp000m5wfery95psnv","tag_id":"cjrujltzv000t5wfe1nzwsbhl","_id":"cjrujltzz000x5wfezvjusfzp"},{"post_id":"cjrujltzp000m5wfery95psnv","tag_id":"cjrujltzy000v5wfembv289fb","_id":"cjrujlu00000y5wfebhrkz6f9"},{"post_id":"cjrujlu05000z5wfew0licp76","tag_id":"cjrujltyo00035wfeltcz8k5e","_id":"cjrujlu0800115wfeloy4jbty"},{"post_id":"cjrujlu05000z5wfew0licp76","tag_id":"cjrujltzr000n5wfemwpzi7yu","_id":"cjrujlu0c00135wfem8f0zv6p"},{"post_id":"cjrujlu0600105wfe1dn7jdsf","tag_id":"cjrujltyo00035wfeltcz8k5e","_id":"cjrujlu0e00165wfefvx17g7c"},{"post_id":"cjrujlu0600105wfe1dn7jdsf","tag_id":"cjrujltzr000n5wfemwpzi7yu","_id":"cjrujlu0f00185wfe6bkhi6wb"},{"post_id":"cjrujlu0800125wfeylhwu3ba","tag_id":"cjrujltzt000q5wfe0khqsxxn","_id":"cjrujlu0h001a5wfe4lkjw62e"},{"post_id":"cjrujlu0800125wfeylhwu3ba","tag_id":"cjrujltzv000t5wfe1nzwsbhl","_id":"cjrujlu0h001b5wfem3toqhlv"},{"post_id":"cjrujlu0800125wfeylhwu3ba","tag_id":"cjrujlu0e00155wfe6zherdqk","_id":"cjrujlu0i001c5wfezxf4st99"},{"post_id":"cjrujlu0u001d5wfehdudraw1","tag_id":"cjrujltyo00035wfeltcz8k5e","_id":"cjrujlu0x001f5wfe7dbd6hvh"},{"post_id":"cjrujlu0u001d5wfehdudraw1","tag_id":"cjrujltyr00065wfewbrktoll","_id":"cjrujlu0y001h5wfeuavebon3"},{"post_id":"cjrujlu0v001e5wfe04dwv4em","tag_id":"cjrujltyo00035wfeltcz8k5e","_id":"cjrujlu11001k5wfe4eubdq4d"},{"post_id":"cjrujlu0v001e5wfe04dwv4em","tag_id":"cjrujltzr000n5wfemwpzi7yu","_id":"cjrujlu14001n5wfeytgq3035"},{"post_id":"cjrujlu0x001g5wfen01yjb8o","tag_id":"cjrujltyo00035wfeltcz8k5e","_id":"cjrujlu16001q5wfe7cqn9pbp"},{"post_id":"cjrujlu0x001g5wfen01yjb8o","tag_id":"cjrujltzr000n5wfemwpzi7yu","_id":"cjrujlu16001s5wferw5tlkl0"},{"post_id":"cjrujlu0z001i5wfeqx7nxq57","tag_id":"cjrujltyo00035wfeltcz8k5e","_id":"cjrujlu17001u5wfe4cmuz1ie"},{"post_id":"cjrujlu0z001i5wfeqx7nxq57","tag_id":"cjrujltyr00065wfewbrktoll","_id":"cjrujlu18001v5wfegbrdk9nv"},{"post_id":"cjrujlu12001l5wfe9b8501z6","tag_id":"cjrujltzt000q5wfe0khqsxxn","_id":"cjrujlu18001w5wfeylxozbi8"},{"post_id":"cjrujlu12001l5wfe9b8501z6","tag_id":"cjrujltzv000t5wfe1nzwsbhl","_id":"cjrujlu18001x5wfelmg3gu84"},{"post_id":"cjrujlu12001l5wfe9b8501z6","tag_id":"cjrujlu15001p5wfe07bfkg4e","_id":"cjrujlu19001y5wfeypx4knqf"},{"post_id":"cjrujlu1f001z5wfeybf5vjkx","tag_id":"cjrujltyo00035wfeltcz8k5e","_id":"cjrujlu1h00215wfepxmm7pm1"},{"post_id":"cjrujlu1f001z5wfeybf5vjkx","tag_id":"cjrujltyr00065wfewbrktoll","_id":"cjrujlu1j00235wfekr17v2zy"},{"post_id":"cjrujlu1g00205wfezjgshuze","tag_id":"cjrujltyo00035wfeltcz8k5e","_id":"cjrujlu1m00265wfekn3p8rct"},{"post_id":"cjrujlu1g00205wfezjgshuze","tag_id":"cjrujltyr00065wfewbrktoll","_id":"cjrujlu1q00295wfexvhey9vs"},{"post_id":"cjrujlu1i00225wfe77ibd3es","tag_id":"cjrujltyo00035wfeltcz8k5e","_id":"cjrujlu1s002b5wfecizxgse6"},{"post_id":"cjrujlu1i00225wfe77ibd3es","tag_id":"cjrujltzr000n5wfemwpzi7yu","_id":"cjrujlu1s002d5wfena53mtml"},{"post_id":"cjrujlu1k00245wfe304hcq04","tag_id":"cjrujltyo00035wfeltcz8k5e","_id":"cjrujlu1t002f5wfec828tldy"},{"post_id":"cjrujlu1k00245wfe304hcq04","tag_id":"cjrujltyr00065wfewbrktoll","_id":"cjrujlu1u002g5wfeangwwogu"},{"post_id":"cjrujlu1n00275wfeuhgo5dxo","tag_id":"cjrujltyo00035wfeltcz8k5e","_id":"cjrujlu1u002h5wfeltznt3wi"},{"post_id":"cjrujlu1n00275wfeuhgo5dxo","tag_id":"cjrujltzr000n5wfemwpzi7yu","_id":"cjrujlu1u002i5wfeoidaayoa"},{"post_id":"cjrujlu1y002j5wfef2g0snts","tag_id":"cjrujltyo00035wfeltcz8k5e","_id":"cjrujlu22002l5wfeep2bel02"},{"post_id":"cjrujlu1y002j5wfef2g0snts","tag_id":"cjrujltzr000n5wfemwpzi7yu","_id":"cjrujlu26002n5wfeurrwbzq6"},{"post_id":"cjrujlu20002k5wfeutanpeid","tag_id":"cjrujltyo00035wfeltcz8k5e","_id":"cjrujlu28002p5wfexx90e5ij"},{"post_id":"cjrujlu20002k5wfeutanpeid","tag_id":"cjrujltyr00065wfewbrktoll","_id":"cjrujlu29002r5wfe32i580z6"},{"post_id":"cjrujlu23002m5wfe8cg7s2g6","tag_id":"cjrujltyo00035wfeltcz8k5e","_id":"cjrujlu2b002t5wfeqhzn4ovt"},{"post_id":"cjrujlu23002m5wfe8cg7s2g6","tag_id":"cjrujltzr000n5wfemwpzi7yu","_id":"cjrujlu2b002u5wfeeg8cq5qt"}],"Tag":[{"name":"프로젝트","_id":"cjrujltyo00035wfeltcz8k5e"},{"name":"GAIL하자!","_id":"cjrujltyr00065wfewbrktoll"},{"name":"각잡고로봇팔","_id":"cjrujltz3000d5wfeq12p8wum"},{"name":"DistRL","_id":"cjrujltz8000i5wfe2dyrsse8"},{"name":"피지여행","_id":"cjrujltzr000n5wfemwpzi7yu"},{"name":"논문","_id":"cjrujltzt000q5wfe0khqsxxn"},{"name":"Distributional RL","_id":"cjrujltzv000t5wfe1nzwsbhl"},{"name":"C51","_id":"cjrujltzy000v5wfembv289fb"},{"name":"IQN","_id":"cjrujlu0e00155wfe6zherdqk"},{"name":"QR-DQN","_id":"cjrujlu15001p5wfe07bfkg4e"}]}}